2025-01-11 02:12:23,244 [trainer.py] => Time Str >>> 0111-02-12-23-150
2025-01-11 02:12:23,434 [trainer.py] => prefix: fair
2025-01-11 02:12:23,434 [trainer.py] => dataset: cifar100
2025-01-11 02:12:23,435 [trainer.py] => memory_size: 7400
2025-01-11 02:12:23,435 [trainer.py] => memory_per_class: 20
2025-01-11 02:12:23,435 [trainer.py] => fixed_memory: False
2025-01-11 02:12:23,435 [trainer.py] => shuffle: True
2025-01-11 02:12:23,435 [trainer.py] => init_cls: 10
2025-01-11 02:12:23,435 [trainer.py] => increment: 10
2025-01-11 02:12:23,435 [trainer.py] => model_name: podnet
2025-01-11 02:12:23,435 [trainer.py] => convnet_type: cosine_resnet32
2025-01-11 02:12:23,435 [trainer.py] => device: [device(type='cuda', index=3)]
2025-01-11 02:12:23,435 [trainer.py] => seed: 1993
2025-01-11 02:12:23,435 [trainer.py] => debug: False
2025-01-11 02:12:23,435 [trainer.py] => skip: False
2025-01-11 02:12:23,435 [trainer.py] => config: ./exps/podnet.json
2025-01-11 02:12:23,436 [trainer.py] => time_str: 0111-02-12-23-150
2025-01-11 02:12:23,436 [trainer.py] => exp_name: 0111-02-12-23-150_cifar100_cosine_resnet32_1993_B0_Inc10
2025-01-11 02:12:23,436 [trainer.py] => logfilename: logs/fair/cifar100/podnet/0111-02-12-23-150_cifar100_cosine_resnet32_1993_B0_Inc10
2025-01-11 02:12:23,436 [trainer.py] => csv_name: cifar100_1993_cosine_resnet32_B0_Inc10
2025-01-11 02:12:26,083 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-01-11 02:12:26,267 [trainer.py] => Start time:1736557946.2673182
2025-01-11 02:12:26,275 [trainer.py] => All params: 466256
2025-01-11 02:12:26,275 [trainer.py] => Trainable params: 466256
2025-01-11 02:12:26,275 [podnet.py] => Learning on 0-10
2025-01-11 02:12:26,287 [podnet.py] => Adaptive factor: 0
2025-01-11 02:12:46,403 [podnet.py] => Task 0, Epoch 1/160 (LR 0.09999) => LSC_loss 2.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 14.26, Test_acc 18.00
2025-01-11 02:12:49,478 [podnet.py] => Task 0, Epoch 2/160 (LR 0.09996) => LSC_loss 2.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 26.92, Test_acc 18.40
2025-01-11 02:12:52,593 [podnet.py] => Task 0, Epoch 3/160 (LR 0.09991) => LSC_loss 1.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 33.52, Test_acc 22.10
2025-01-11 02:12:55,670 [podnet.py] => Task 0, Epoch 4/160 (LR 0.09985) => LSC_loss 1.74, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 40.40, Test_acc 29.80
2025-01-11 02:12:58,727 [podnet.py] => Task 0, Epoch 5/160 (LR 0.09976) => LSC_loss 1.62, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 45.00, Test_acc 30.50
2025-01-11 02:13:01,805 [podnet.py] => Task 0, Epoch 6/160 (LR 0.09965) => LSC_loss 1.65, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 45.60, Test_acc 40.30
2025-01-11 02:13:04,773 [podnet.py] => Task 0, Epoch 7/160 (LR 0.09953) => LSC_loss 1.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 51.40, Test_acc 54.40
2025-01-11 02:13:07,756 [podnet.py] => Task 0, Epoch 8/160 (LR 0.09938) => LSC_loss 1.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 56.74, Test_acc 45.30
2025-01-11 02:13:10,789 [podnet.py] => Task 0, Epoch 9/160 (LR 0.09922) => LSC_loss 1.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 56.92, Test_acc 46.90
2025-01-11 02:13:13,727 [podnet.py] => Task 0, Epoch 10/160 (LR 0.09904) => LSC_loss 1.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.86, Test_acc 47.30
2025-01-11 02:13:16,682 [podnet.py] => Task 0, Epoch 11/160 (LR 0.09884) => LSC_loss 1.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 61.54, Test_acc 59.80
2025-01-11 02:13:19,687 [podnet.py] => Task 0, Epoch 12/160 (LR 0.09862) => LSC_loss 1.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.22, Test_acc 56.60
2025-01-11 02:13:22,609 [podnet.py] => Task 0, Epoch 13/160 (LR 0.09838) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.72, Test_acc 66.20
2025-01-11 02:13:25,734 [podnet.py] => Task 0, Epoch 14/160 (LR 0.09812) => LSC_loss 1.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.32, Test_acc 41.00
2025-01-11 02:13:28,634 [podnet.py] => Task 0, Epoch 15/160 (LR 0.09785) => LSC_loss 1.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 63.68, Test_acc 56.80
2025-01-11 02:13:31,550 [podnet.py] => Task 0, Epoch 16/160 (LR 0.09755) => LSC_loss 0.94, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.20, Test_acc 66.50
2025-01-11 02:13:34,577 [podnet.py] => Task 0, Epoch 17/160 (LR 0.09724) => LSC_loss 0.92, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.76, Test_acc 64.80
2025-01-11 02:13:37,553 [podnet.py] => Task 0, Epoch 18/160 (LR 0.09691) => LSC_loss 0.85, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 73.68, Test_acc 54.20
2025-01-11 02:13:40,473 [podnet.py] => Task 0, Epoch 19/160 (LR 0.09656) => LSC_loss 0.89, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.20, Test_acc 71.70
2025-01-11 02:13:43,404 [podnet.py] => Task 0, Epoch 20/160 (LR 0.09619) => LSC_loss 0.76, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.30, Test_acc 68.40
2025-01-11 02:13:46,275 [podnet.py] => Task 0, Epoch 21/160 (LR 0.09581) => LSC_loss 0.83, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.96, Test_acc 72.80
2025-01-11 02:13:49,255 [podnet.py] => Task 0, Epoch 22/160 (LR 0.09541) => LSC_loss 0.69, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.90, Test_acc 71.60
2025-01-11 02:13:52,254 [podnet.py] => Task 0, Epoch 23/160 (LR 0.09499) => LSC_loss 0.67, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.52, Test_acc 75.40
2025-01-11 02:13:55,256 [podnet.py] => Task 0, Epoch 24/160 (LR 0.09455) => LSC_loss 0.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.52, Test_acc 68.10
2025-01-11 02:13:58,323 [podnet.py] => Task 0, Epoch 25/160 (LR 0.09410) => LSC_loss 0.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.18, Test_acc 68.90
2025-01-11 02:14:01,325 [podnet.py] => Task 0, Epoch 26/160 (LR 0.09362) => LSC_loss 0.72, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.86, Test_acc 61.60
2025-01-11 02:14:04,621 [podnet.py] => Task 0, Epoch 27/160 (LR 0.09314) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.40, Test_acc 73.40
2025-01-11 02:14:07,869 [podnet.py] => Task 0, Epoch 28/160 (LR 0.09263) => LSC_loss 0.63, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.12, Test_acc 72.50
2025-01-11 02:14:10,790 [podnet.py] => Task 0, Epoch 29/160 (LR 0.09211) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.60, Test_acc 72.50
2025-01-11 02:14:13,856 [podnet.py] => Task 0, Epoch 30/160 (LR 0.09157) => LSC_loss 0.62, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.32, Test_acc 77.80
2025-01-11 02:14:16,880 [podnet.py] => Task 0, Epoch 31/160 (LR 0.09102) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.42, Test_acc 79.60
2025-01-11 02:14:19,814 [podnet.py] => Task 0, Epoch 32/160 (LR 0.09045) => LSC_loss 0.59, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.00, Test_acc 73.70
2025-01-11 02:14:22,723 [podnet.py] => Task 0, Epoch 33/160 (LR 0.08987) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.02, Test_acc 76.80
2025-01-11 02:14:25,743 [podnet.py] => Task 0, Epoch 34/160 (LR 0.08927) => LSC_loss 0.57, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.04, Test_acc 69.90
2025-01-11 02:14:28,705 [podnet.py] => Task 0, Epoch 35/160 (LR 0.08865) => LSC_loss 0.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.56, Test_acc 79.60
2025-01-11 02:14:31,758 [podnet.py] => Task 0, Epoch 36/160 (LR 0.08802) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.98, Test_acc 75.50
2025-01-11 02:14:34,706 [podnet.py] => Task 0, Epoch 37/160 (LR 0.08738) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.64, Test_acc 73.40
2025-01-11 02:14:37,619 [podnet.py] => Task 0, Epoch 38/160 (LR 0.08672) => LSC_loss 0.55, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.74, Test_acc 78.40
2025-01-11 02:14:40,694 [podnet.py] => Task 0, Epoch 39/160 (LR 0.08604) => LSC_loss 0.59, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.12, Test_acc 81.30
2025-01-11 02:14:43,779 [podnet.py] => Task 0, Epoch 40/160 (LR 0.08536) => LSC_loss 0.55, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.72, Test_acc 81.80
2025-01-11 02:14:46,759 [podnet.py] => Task 0, Epoch 41/160 (LR 0.08465) => LSC_loss 0.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.08, Test_acc 78.30
2025-01-11 02:14:49,772 [podnet.py] => Task 0, Epoch 42/160 (LR 0.08394) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.52, Test_acc 82.20
2025-01-11 02:14:52,632 [podnet.py] => Task 0, Epoch 43/160 (LR 0.08321) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.08, Test_acc 79.30
2025-01-11 02:14:55,687 [podnet.py] => Task 0, Epoch 44/160 (LR 0.08247) => LSC_loss 0.44, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.92, Test_acc 80.40
2025-01-11 02:14:58,781 [podnet.py] => Task 0, Epoch 45/160 (LR 0.08172) => LSC_loss 0.38, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.46, Test_acc 79.50
2025-01-11 02:15:01,876 [podnet.py] => Task 0, Epoch 46/160 (LR 0.08095) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.80, Test_acc 72.40
2025-01-11 02:15:04,724 [podnet.py] => Task 0, Epoch 47/160 (LR 0.08018) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.56, Test_acc 74.00
2025-01-11 02:15:07,890 [podnet.py] => Task 0, Epoch 48/160 (LR 0.07939) => LSC_loss 0.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.60, Test_acc 70.70
2025-01-11 02:15:10,855 [podnet.py] => Task 0, Epoch 49/160 (LR 0.07859) => LSC_loss 0.64, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.68, Test_acc 72.50
2025-01-11 02:15:13,779 [podnet.py] => Task 0, Epoch 50/160 (LR 0.07778) => LSC_loss 0.45, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.56, Test_acc 82.70
2025-01-11 02:15:16,955 [podnet.py] => Task 0, Epoch 51/160 (LR 0.07696) => LSC_loss 0.40, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.38, Test_acc 80.30
2025-01-11 02:15:19,967 [podnet.py] => Task 0, Epoch 52/160 (LR 0.07612) => LSC_loss 0.69, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.26, Test_acc 71.70
2025-01-11 02:15:23,012 [podnet.py] => Task 0, Epoch 53/160 (LR 0.07528) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.30, Test_acc 82.90
2025-01-11 02:15:26,083 [podnet.py] => Task 0, Epoch 54/160 (LR 0.07443) => LSC_loss 0.45, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.02, Test_acc 79.30
2025-01-11 02:15:29,105 [podnet.py] => Task 0, Epoch 55/160 (LR 0.07357) => LSC_loss 0.43, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.38, Test_acc 83.50
2025-01-11 02:15:32,208 [podnet.py] => Task 0, Epoch 56/160 (LR 0.07270) => LSC_loss 0.38, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.86, Test_acc 83.70
2025-01-11 02:15:35,269 [podnet.py] => Task 0, Epoch 57/160 (LR 0.07182) => LSC_loss 0.31, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.66, Test_acc 84.20
2025-01-11 02:15:38,284 [podnet.py] => Task 0, Epoch 58/160 (LR 0.07093) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.36, Test_acc 81.50
2025-01-11 02:15:41,239 [podnet.py] => Task 0, Epoch 59/160 (LR 0.07004) => LSC_loss 0.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.04, Test_acc 80.90
2025-01-11 02:15:44,062 [podnet.py] => Task 0, Epoch 60/160 (LR 0.06913) => LSC_loss 0.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.90, Test_acc 82.60
2025-01-11 02:15:47,188 [podnet.py] => Task 0, Epoch 61/160 (LR 0.06822) => LSC_loss 0.39, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.50, Test_acc 82.40
2025-01-11 02:15:50,444 [podnet.py] => Task 0, Epoch 62/160 (LR 0.06731) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.72, Test_acc 80.70
2025-01-11 02:15:53,417 [podnet.py] => Task 0, Epoch 63/160 (LR 0.06638) => LSC_loss 0.31, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.22, Test_acc 83.70
2025-01-11 02:15:56,279 [podnet.py] => Task 0, Epoch 64/160 (LR 0.06545) => LSC_loss 0.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.06, Test_acc 84.00
2025-01-11 02:15:59,253 [podnet.py] => Task 0, Epoch 65/160 (LR 0.06451) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.22, Test_acc 83.10
2025-01-11 02:16:02,366 [podnet.py] => Task 0, Epoch 66/160 (LR 0.06357) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.30, Test_acc 84.40
2025-01-11 02:16:05,469 [podnet.py] => Task 0, Epoch 67/160 (LR 0.06262) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.36, Test_acc 85.00
2025-01-11 02:16:08,429 [podnet.py] => Task 0, Epoch 68/160 (LR 0.06167) => LSC_loss 0.28, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.16, Test_acc 83.70
2025-01-11 02:16:11,464 [podnet.py] => Task 0, Epoch 69/160 (LR 0.06072) => LSC_loss 0.29, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.46, Test_acc 83.50
2025-01-11 02:16:14,409 [podnet.py] => Task 0, Epoch 70/160 (LR 0.05975) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.12, Test_acc 86.50
2025-01-11 02:16:17,289 [podnet.py] => Task 0, Epoch 71/160 (LR 0.05879) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.10, Test_acc 87.40
2025-01-11 02:16:20,188 [podnet.py] => Task 0, Epoch 72/160 (LR 0.05782) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.24, Test_acc 83.70
2025-01-11 02:16:23,038 [podnet.py] => Task 0, Epoch 73/160 (LR 0.05685) => LSC_loss 0.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.12, Test_acc 79.40
2025-01-11 02:16:25,905 [podnet.py] => Task 0, Epoch 74/160 (LR 0.05588) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.74, Test_acc 86.00
2025-01-11 02:16:28,944 [podnet.py] => Task 0, Epoch 75/160 (LR 0.05490) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.94, Test_acc 86.00
2025-01-11 02:16:31,894 [podnet.py] => Task 0, Epoch 76/160 (LR 0.05392) => LSC_loss 0.28, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.42, Test_acc 86.60
2025-01-11 02:16:34,873 [podnet.py] => Task 0, Epoch 77/160 (LR 0.05294) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.86, Test_acc 83.70
2025-01-11 02:16:37,826 [podnet.py] => Task 0, Epoch 78/160 (LR 0.05196) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.68, Test_acc 85.50
2025-01-11 02:16:40,674 [podnet.py] => Task 0, Epoch 79/160 (LR 0.05098) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.88, Test_acc 86.90
2025-01-11 02:16:43,501 [podnet.py] => Task 0, Epoch 80/160 (LR 0.05000) => LSC_loss 0.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.58, Test_acc 83.90
2025-01-11 02:16:46,529 [podnet.py] => Task 0, Epoch 81/160 (LR 0.04902) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.06, Test_acc 85.40
2025-01-11 02:16:49,527 [podnet.py] => Task 0, Epoch 82/160 (LR 0.04804) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.76, Test_acc 86.80
2025-01-11 02:16:52,462 [podnet.py] => Task 0, Epoch 83/160 (LR 0.04706) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.56, Test_acc 87.30
2025-01-11 02:16:55,450 [podnet.py] => Task 0, Epoch 84/160 (LR 0.04608) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.38, Test_acc 86.10
2025-01-11 02:16:58,457 [podnet.py] => Task 0, Epoch 85/160 (LR 0.04510) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.26, Test_acc 87.70
2025-01-11 02:17:01,409 [podnet.py] => Task 0, Epoch 86/160 (LR 0.04412) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.70, Test_acc 84.50
2025-01-11 02:17:04,260 [podnet.py] => Task 0, Epoch 87/160 (LR 0.04315) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.96, Test_acc 86.10
2025-01-11 02:17:07,127 [podnet.py] => Task 0, Epoch 88/160 (LR 0.04218) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.84, Test_acc 83.10
2025-01-11 02:17:10,064 [podnet.py] => Task 0, Epoch 89/160 (LR 0.04121) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.40, Test_acc 82.20
2025-01-11 02:17:13,097 [podnet.py] => Task 0, Epoch 90/160 (LR 0.04025) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.38, Test_acc 87.40
2025-01-11 02:17:16,082 [podnet.py] => Task 0, Epoch 91/160 (LR 0.03928) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.14, Test_acc 84.50
2025-01-11 02:17:19,106 [podnet.py] => Task 0, Epoch 92/160 (LR 0.03833) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.82, Test_acc 89.30
2025-01-11 02:17:22,041 [podnet.py] => Task 0, Epoch 93/160 (LR 0.03738) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.64, Test_acc 86.30
2025-01-11 02:17:25,009 [podnet.py] => Task 0, Epoch 94/160 (LR 0.03643) => LSC_loss 0.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.28, Test_acc 86.20
2025-01-11 02:17:28,079 [podnet.py] => Task 0, Epoch 95/160 (LR 0.03549) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.72, Test_acc 85.90
2025-01-11 02:17:31,038 [podnet.py] => Task 0, Epoch 96/160 (LR 0.03455) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.12, Test_acc 86.40
2025-01-11 02:17:33,993 [podnet.py] => Task 0, Epoch 97/160 (LR 0.03362) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.10, Test_acc 89.40
2025-01-11 02:17:36,954 [podnet.py] => Task 0, Epoch 98/160 (LR 0.03269) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.94, Test_acc 87.80
2025-01-11 02:17:39,859 [podnet.py] => Task 0, Epoch 99/160 (LR 0.03178) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.72, Test_acc 87.00
2025-01-11 02:17:42,792 [podnet.py] => Task 0, Epoch 100/160 (LR 0.03087) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.72, Test_acc 85.00
2025-01-11 02:17:45,796 [podnet.py] => Task 0, Epoch 101/160 (LR 0.02996) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.58, Test_acc 89.50
2025-01-11 02:17:48,815 [podnet.py] => Task 0, Epoch 102/160 (LR 0.02907) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.42, Test_acc 88.80
2025-01-11 02:17:51,923 [podnet.py] => Task 0, Epoch 103/160 (LR 0.02818) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.28, Test_acc 87.40
2025-01-11 02:17:55,079 [podnet.py] => Task 0, Epoch 104/160 (LR 0.02730) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.30, Test_acc 88.10
2025-01-11 02:17:58,182 [podnet.py] => Task 0, Epoch 105/160 (LR 0.02643) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.14, Test_acc 90.30
2025-01-11 02:18:01,295 [podnet.py] => Task 0, Epoch 106/160 (LR 0.02557) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.46, Test_acc 89.00
2025-01-11 02:18:04,225 [podnet.py] => Task 0, Epoch 107/160 (LR 0.02472) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.86, Test_acc 84.30
2025-01-11 02:18:07,201 [podnet.py] => Task 0, Epoch 108/160 (LR 0.02388) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.42, Test_acc 87.40
2025-01-11 02:18:10,202 [podnet.py] => Task 0, Epoch 109/160 (LR 0.02304) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.92, Test_acc 88.00
2025-01-11 02:18:13,190 [podnet.py] => Task 0, Epoch 110/160 (LR 0.02222) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.46, Test_acc 89.80
2025-01-11 02:18:16,066 [podnet.py] => Task 0, Epoch 111/160 (LR 0.02141) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.50, Test_acc 89.20
2025-01-11 02:18:19,018 [podnet.py] => Task 0, Epoch 112/160 (LR 0.02061) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.56, Test_acc 88.70
2025-01-11 02:18:21,956 [podnet.py] => Task 0, Epoch 113/160 (LR 0.01982) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.88, Test_acc 89.20
2025-01-11 02:18:25,032 [podnet.py] => Task 0, Epoch 114/160 (LR 0.01905) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.30, Test_acc 90.60
2025-01-11 02:18:27,998 [podnet.py] => Task 0, Epoch 115/160 (LR 0.01828) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.76, Test_acc 89.10
2025-01-11 02:18:31,028 [podnet.py] => Task 0, Epoch 116/160 (LR 0.01753) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.42, Test_acc 87.50
2025-01-11 02:18:34,027 [podnet.py] => Task 0, Epoch 117/160 (LR 0.01679) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.18, Test_acc 88.30
2025-01-11 02:18:36,987 [podnet.py] => Task 0, Epoch 118/160 (LR 0.01606) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.34, Test_acc 89.30
2025-01-11 02:18:39,950 [podnet.py] => Task 0, Epoch 119/160 (LR 0.01535) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 89.40
2025-01-11 02:18:42,873 [podnet.py] => Task 0, Epoch 120/160 (LR 0.01464) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 89.60
2025-01-11 02:18:45,885 [podnet.py] => Task 0, Epoch 121/160 (LR 0.01396) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.06, Test_acc 88.80
2025-01-11 02:18:48,946 [podnet.py] => Task 0, Epoch 122/160 (LR 0.01328) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.90, Test_acc 88.70
2025-01-11 02:18:51,983 [podnet.py] => Task 0, Epoch 123/160 (LR 0.01262) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.80, Test_acc 89.10
2025-01-11 02:18:55,025 [podnet.py] => Task 0, Epoch 124/160 (LR 0.01198) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 89.90
2025-01-11 02:18:58,036 [podnet.py] => Task 0, Epoch 125/160 (LR 0.01135) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.14, Test_acc 90.20
2025-01-11 02:19:00,926 [podnet.py] => Task 0, Epoch 126/160 (LR 0.01073) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 90.20
2025-01-11 02:19:03,962 [podnet.py] => Task 0, Epoch 127/160 (LR 0.01013) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 89.80
2025-01-11 02:19:06,888 [podnet.py] => Task 0, Epoch 128/160 (LR 0.00955) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.28, Test_acc 89.90
2025-01-11 02:19:09,859 [podnet.py] => Task 0, Epoch 129/160 (LR 0.00898) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 90.20
2025-01-11 02:19:12,715 [podnet.py] => Task 0, Epoch 130/160 (LR 0.00843) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.38, Test_acc 89.70
2025-01-11 02:19:15,621 [podnet.py] => Task 0, Epoch 131/160 (LR 0.00789) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 89.70
2025-01-11 02:19:18,561 [podnet.py] => Task 0, Epoch 132/160 (LR 0.00737) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.30, Test_acc 90.00
2025-01-11 02:19:21,727 [podnet.py] => Task 0, Epoch 133/160 (LR 0.00686) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 89.80
2025-01-11 02:19:24,790 [podnet.py] => Task 0, Epoch 134/160 (LR 0.00638) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.02, Test_acc 89.10
2025-01-11 02:19:27,717 [podnet.py] => Task 0, Epoch 135/160 (LR 0.00590) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.56, Test_acc 90.20
2025-01-11 02:19:30,780 [podnet.py] => Task 0, Epoch 136/160 (LR 0.00545) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.76, Test_acc 89.90
2025-01-11 02:19:33,759 [podnet.py] => Task 0, Epoch 137/160 (LR 0.00501) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 89.70
2025-01-11 02:19:36,756 [podnet.py] => Task 0, Epoch 138/160 (LR 0.00459) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.18, Test_acc 89.40
2025-01-11 02:19:39,805 [podnet.py] => Task 0, Epoch 139/160 (LR 0.00419) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 90.40
2025-01-11 02:19:42,871 [podnet.py] => Task 0, Epoch 140/160 (LR 0.00381) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.22, Test_acc 90.20
2025-01-11 02:19:45,779 [podnet.py] => Task 0, Epoch 141/160 (LR 0.00344) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.52, Test_acc 90.60
2025-01-11 02:19:48,913 [podnet.py] => Task 0, Epoch 142/160 (LR 0.00309) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 89.60
2025-01-11 02:19:52,002 [podnet.py] => Task 0, Epoch 143/160 (LR 0.00276) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.34, Test_acc 89.00
2025-01-11 02:19:55,056 [podnet.py] => Task 0, Epoch 144/160 (LR 0.00245) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 89.70
2025-01-11 02:19:58,049 [podnet.py] => Task 0, Epoch 145/160 (LR 0.00215) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.46, Test_acc 89.90
2025-01-11 02:20:01,131 [podnet.py] => Task 0, Epoch 146/160 (LR 0.00188) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.40, Test_acc 90.10
2025-01-11 02:20:04,113 [podnet.py] => Task 0, Epoch 147/160 (LR 0.00162) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 89.80
2025-01-11 02:20:07,181 [podnet.py] => Task 0, Epoch 148/160 (LR 0.00138) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 91.00
2025-01-11 02:20:10,279 [podnet.py] => Task 0, Epoch 149/160 (LR 0.00116) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 90.70
2025-01-11 02:20:13,259 [podnet.py] => Task 0, Epoch 150/160 (LR 0.00096) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.78, Test_acc 89.90
2025-01-11 02:20:16,278 [podnet.py] => Task 0, Epoch 151/160 (LR 0.00078) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 90.60
2025-01-11 02:20:19,397 [podnet.py] => Task 0, Epoch 152/160 (LR 0.00062) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.66, Test_acc 90.60
2025-01-11 02:20:22,501 [podnet.py] => Task 0, Epoch 153/160 (LR 0.00047) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 90.80
2025-01-11 02:20:25,542 [podnet.py] => Task 0, Epoch 154/160 (LR 0.00035) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 90.50
2025-01-11 02:20:28,552 [podnet.py] => Task 0, Epoch 155/160 (LR 0.00024) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 90.50
2025-01-11 02:20:31,612 [podnet.py] => Task 0, Epoch 156/160 (LR 0.00015) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 90.70
2025-01-11 02:20:34,573 [podnet.py] => Task 0, Epoch 157/160 (LR 0.00009) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 90.10
2025-01-11 02:20:37,592 [podnet.py] => Task 0, Epoch 158/160 (LR 0.00004) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 91.10
2025-01-11 02:20:40,561 [podnet.py] => Task 0, Epoch 159/160 (LR 0.00001) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 89.80
2025-01-11 02:20:43,485 [podnet.py] => Task 0, Epoch 160/160 (LR 0.00000) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 90.10
2025-01-11 02:20:43,487 [base.py] => Reducing exemplars...(740 per classes)
2025-01-11 02:20:43,487 [base.py] => Constructing exemplars...(740 per classes)
2025-01-11 02:21:01,207 [podnet.py] => Exemplar size: 5000
2025-01-11 02:21:01,217 [trainer.py] => CNN: {'total': np.float64(90.1), '00-09': np.float64(90.1), 'old': 0, 'new': np.float64(90.1)}
2025-01-11 02:21:01,217 [trainer.py] => NME: {'total': np.float64(90.0), '00-09': np.float64(90.0), 'old': 0, 'new': np.float64(90.0)}
2025-01-11 02:21:01,217 [trainer.py] => CNN top1 curve: [np.float64(90.1)]
2025-01-11 02:21:01,218 [trainer.py] => CNN top5 curve: [np.float64(99.4)]
2025-01-11 02:21:01,218 [trainer.py] => NME top1 curve: [np.float64(90.0)]
2025-01-11 02:21:01,218 [trainer.py] => NME top5 curve: [np.float64(99.4)]

2025-01-11 02:21:01,218 [trainer.py] => All params: 472657
2025-01-11 02:21:01,219 [trainer.py] => Trainable params: 472657
2025-01-11 02:21:01,220 [podnet.py] => Learning on 10-20
2025-01-11 02:21:01,308 [podnet.py] => Adaptive factor: 1.4142135623730951
2025-01-11 02:21:07,768 [podnet.py] => Task 1, Epoch 1/160 (LR 0.09999) => LSC_loss 1.55, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 56.17, Test_acc 51.90
2025-01-11 02:21:13,528 [podnet.py] => Task 1, Epoch 2/160 (LR 0.09996) => LSC_loss 1.25, Spatial_loss 1.39, Flat_loss 0.13, Train_acc 61.30, Test_acc 45.60
2025-01-11 02:21:19,236 [podnet.py] => Task 1, Epoch 3/160 (LR 0.09991) => LSC_loss 1.18, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 64.10, Test_acc 51.90
2025-01-11 02:21:25,178 [podnet.py] => Task 1, Epoch 4/160 (LR 0.09985) => LSC_loss 1.12, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 65.20, Test_acc 48.70
2025-01-11 02:21:31,012 [podnet.py] => Task 1, Epoch 5/160 (LR 0.09976) => LSC_loss 1.09, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 66.95, Test_acc 54.75
2025-01-11 02:21:36,921 [podnet.py] => Task 1, Epoch 6/160 (LR 0.09965) => LSC_loss 1.04, Spatial_loss 1.30, Flat_loss 0.13, Train_acc 68.32, Test_acc 56.80
2025-01-11 02:21:42,621 [podnet.py] => Task 1, Epoch 7/160 (LR 0.09953) => LSC_loss 1.00, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 69.86, Test_acc 57.75
2025-01-11 02:21:48,384 [podnet.py] => Task 1, Epoch 8/160 (LR 0.09938) => LSC_loss 0.98, Spatial_loss 1.27, Flat_loss 0.14, Train_acc 70.40, Test_acc 59.90
2025-01-11 02:21:54,166 [podnet.py] => Task 1, Epoch 9/160 (LR 0.09922) => LSC_loss 0.96, Spatial_loss 1.28, Flat_loss 0.14, Train_acc 70.91, Test_acc 58.75
2025-01-11 02:21:59,832 [podnet.py] => Task 1, Epoch 10/160 (LR 0.09904) => LSC_loss 0.95, Spatial_loss 1.30, Flat_loss 0.15, Train_acc 71.02, Test_acc 56.70
2025-01-11 02:22:05,829 [podnet.py] => Task 1, Epoch 11/160 (LR 0.09884) => LSC_loss 0.90, Spatial_loss 1.28, Flat_loss 0.15, Train_acc 72.97, Test_acc 60.50
2025-01-11 02:22:11,710 [podnet.py] => Task 1, Epoch 12/160 (LR 0.09862) => LSC_loss 0.89, Spatial_loss 1.27, Flat_loss 0.15, Train_acc 73.45, Test_acc 60.70
2025-01-11 02:22:17,580 [podnet.py] => Task 1, Epoch 13/160 (LR 0.09838) => LSC_loss 0.89, Spatial_loss 1.29, Flat_loss 0.15, Train_acc 73.09, Test_acc 61.05
2025-01-11 02:22:23,461 [podnet.py] => Task 1, Epoch 14/160 (LR 0.09812) => LSC_loss 0.86, Spatial_loss 1.26, Flat_loss 0.15, Train_acc 74.50, Test_acc 59.40
2025-01-11 02:22:29,367 [podnet.py] => Task 1, Epoch 15/160 (LR 0.09785) => LSC_loss 0.84, Spatial_loss 1.28, Flat_loss 0.15, Train_acc 74.58, Test_acc 58.45
2025-01-11 02:22:35,218 [podnet.py] => Task 1, Epoch 16/160 (LR 0.09755) => LSC_loss 0.83, Spatial_loss 1.27, Flat_loss 0.16, Train_acc 74.94, Test_acc 60.25
2025-01-11 02:22:41,169 [podnet.py] => Task 1, Epoch 17/160 (LR 0.09724) => LSC_loss 0.82, Spatial_loss 1.30, Flat_loss 0.16, Train_acc 75.66, Test_acc 58.45
2025-01-11 02:22:46,995 [podnet.py] => Task 1, Epoch 18/160 (LR 0.09691) => LSC_loss 0.84, Spatial_loss 1.35, Flat_loss 0.17, Train_acc 74.95, Test_acc 60.75
2025-01-11 02:22:52,660 [podnet.py] => Task 1, Epoch 19/160 (LR 0.09656) => LSC_loss 0.79, Spatial_loss 1.28, Flat_loss 0.16, Train_acc 75.93, Test_acc 57.30
2025-01-11 02:22:58,512 [podnet.py] => Task 1, Epoch 20/160 (LR 0.09619) => LSC_loss 0.77, Spatial_loss 1.25, Flat_loss 0.16, Train_acc 77.04, Test_acc 57.50
2025-01-11 02:23:04,301 [podnet.py] => Task 1, Epoch 21/160 (LR 0.09581) => LSC_loss 0.78, Spatial_loss 1.31, Flat_loss 0.17, Train_acc 77.09, Test_acc 63.20
2025-01-11 02:23:10,238 [podnet.py] => Task 1, Epoch 22/160 (LR 0.09541) => LSC_loss 0.78, Spatial_loss 1.32, Flat_loss 0.17, Train_acc 76.48, Test_acc 64.40
2025-01-11 02:23:15,925 [podnet.py] => Task 1, Epoch 23/160 (LR 0.09499) => LSC_loss 0.74, Spatial_loss 1.25, Flat_loss 0.16, Train_acc 78.41, Test_acc 64.70
2025-01-11 02:23:21,659 [podnet.py] => Task 1, Epoch 24/160 (LR 0.09455) => LSC_loss 0.74, Spatial_loss 1.27, Flat_loss 0.17, Train_acc 77.72, Test_acc 63.00
2025-01-11 02:23:27,438 [podnet.py] => Task 1, Epoch 25/160 (LR 0.09410) => LSC_loss 0.73, Spatial_loss 1.26, Flat_loss 0.17, Train_acc 78.09, Test_acc 66.10
2025-01-11 02:23:33,085 [podnet.py] => Task 1, Epoch 26/160 (LR 0.09362) => LSC_loss 0.71, Spatial_loss 1.29, Flat_loss 0.17, Train_acc 78.76, Test_acc 64.90
2025-01-11 02:23:38,901 [podnet.py] => Task 1, Epoch 27/160 (LR 0.09314) => LSC_loss 0.71, Spatial_loss 1.27, Flat_loss 0.17, Train_acc 79.25, Test_acc 59.65
2025-01-11 02:23:44,628 [podnet.py] => Task 1, Epoch 28/160 (LR 0.09263) => LSC_loss 0.76, Spatial_loss 1.38, Flat_loss 0.18, Train_acc 77.29, Test_acc 65.80
2025-01-11 02:23:50,419 [podnet.py] => Task 1, Epoch 29/160 (LR 0.09211) => LSC_loss 0.69, Spatial_loss 1.26, Flat_loss 0.17, Train_acc 79.64, Test_acc 62.80
2025-01-11 02:23:56,348 [podnet.py] => Task 1, Epoch 30/160 (LR 0.09157) => LSC_loss 0.68, Spatial_loss 1.27, Flat_loss 0.17, Train_acc 79.71, Test_acc 62.25
2025-01-11 02:24:02,082 [podnet.py] => Task 1, Epoch 31/160 (LR 0.09102) => LSC_loss 0.70, Spatial_loss 1.31, Flat_loss 0.18, Train_acc 79.79, Test_acc 62.35
2025-01-11 02:24:07,799 [podnet.py] => Task 1, Epoch 32/160 (LR 0.09045) => LSC_loss 0.69, Spatial_loss 1.31, Flat_loss 0.18, Train_acc 79.48, Test_acc 65.70
2025-01-11 02:24:13,432 [podnet.py] => Task 1, Epoch 33/160 (LR 0.08987) => LSC_loss 0.67, Spatial_loss 1.36, Flat_loss 0.18, Train_acc 80.42, Test_acc 66.10
2025-01-11 02:24:19,173 [podnet.py] => Task 1, Epoch 34/160 (LR 0.08927) => LSC_loss 0.71, Spatial_loss 1.36, Flat_loss 0.18, Train_acc 79.36, Test_acc 61.75
2025-01-11 02:24:24,971 [podnet.py] => Task 1, Epoch 35/160 (LR 0.08865) => LSC_loss 0.70, Spatial_loss 1.33, Flat_loss 0.18, Train_acc 79.41, Test_acc 56.65
2025-01-11 02:24:30,748 [podnet.py] => Task 1, Epoch 36/160 (LR 0.08802) => LSC_loss 0.70, Spatial_loss 1.37, Flat_loss 0.19, Train_acc 79.20, Test_acc 65.15
2025-01-11 02:24:36,600 [podnet.py] => Task 1, Epoch 37/160 (LR 0.08738) => LSC_loss 0.64, Spatial_loss 1.28, Flat_loss 0.18, Train_acc 81.08, Test_acc 62.20
2025-01-11 02:24:42,459 [podnet.py] => Task 1, Epoch 38/160 (LR 0.08672) => LSC_loss 0.65, Spatial_loss 1.29, Flat_loss 0.18, Train_acc 80.64, Test_acc 67.10
2025-01-11 02:24:48,332 [podnet.py] => Task 1, Epoch 39/160 (LR 0.08604) => LSC_loss 0.65, Spatial_loss 1.30, Flat_loss 0.18, Train_acc 81.04, Test_acc 66.15
2025-01-11 02:24:54,088 [podnet.py] => Task 1, Epoch 40/160 (LR 0.08536) => LSC_loss 0.62, Spatial_loss 1.26, Flat_loss 0.18, Train_acc 81.66, Test_acc 66.30
2025-01-11 02:24:59,811 [podnet.py] => Task 1, Epoch 41/160 (LR 0.08465) => LSC_loss 0.61, Spatial_loss 1.23, Flat_loss 0.17, Train_acc 81.97, Test_acc 64.75
2025-01-11 02:25:05,597 [podnet.py] => Task 1, Epoch 42/160 (LR 0.08394) => LSC_loss 0.59, Spatial_loss 1.23, Flat_loss 0.17, Train_acc 82.85, Test_acc 67.80
2025-01-11 02:25:11,414 [podnet.py] => Task 1, Epoch 43/160 (LR 0.08321) => LSC_loss 0.62, Spatial_loss 1.28, Flat_loss 0.18, Train_acc 81.15, Test_acc 64.40
2025-01-11 02:25:17,109 [podnet.py] => Task 1, Epoch 44/160 (LR 0.08247) => LSC_loss 0.58, Spatial_loss 1.23, Flat_loss 0.18, Train_acc 82.71, Test_acc 57.85
2025-01-11 02:25:22,822 [podnet.py] => Task 1, Epoch 45/160 (LR 0.08172) => LSC_loss 0.58, Spatial_loss 1.27, Flat_loss 0.18, Train_acc 82.96, Test_acc 66.30
2025-01-11 02:25:28,641 [podnet.py] => Task 1, Epoch 46/160 (LR 0.08095) => LSC_loss 0.57, Spatial_loss 1.24, Flat_loss 0.18, Train_acc 83.20, Test_acc 58.45
2025-01-11 02:25:34,361 [podnet.py] => Task 1, Epoch 47/160 (LR 0.08018) => LSC_loss 0.62, Spatial_loss 1.34, Flat_loss 0.19, Train_acc 82.10, Test_acc 65.85
2025-01-11 02:25:40,105 [podnet.py] => Task 1, Epoch 48/160 (LR 0.07939) => LSC_loss 0.59, Spatial_loss 1.25, Flat_loss 0.18, Train_acc 82.47, Test_acc 65.35
2025-01-11 02:25:45,929 [podnet.py] => Task 1, Epoch 49/160 (LR 0.07859) => LSC_loss 0.58, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 83.21, Test_acc 66.65
2025-01-11 02:25:51,778 [podnet.py] => Task 1, Epoch 50/160 (LR 0.07778) => LSC_loss 0.56, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 83.94, Test_acc 65.30
2025-01-11 02:25:57,505 [podnet.py] => Task 1, Epoch 51/160 (LR 0.07696) => LSC_loss 0.58, Spatial_loss 1.26, Flat_loss 0.18, Train_acc 83.10, Test_acc 64.00
2025-01-11 02:26:03,276 [podnet.py] => Task 1, Epoch 52/160 (LR 0.07612) => LSC_loss 0.58, Spatial_loss 1.28, Flat_loss 0.18, Train_acc 83.24, Test_acc 62.15
2025-01-11 02:26:09,621 [podnet.py] => Task 1, Epoch 53/160 (LR 0.07528) => LSC_loss 0.57, Spatial_loss 1.28, Flat_loss 0.18, Train_acc 82.95, Test_acc 68.70
2025-01-11 02:26:15,756 [podnet.py] => Task 1, Epoch 54/160 (LR 0.07443) => LSC_loss 0.56, Spatial_loss 1.26, Flat_loss 0.18, Train_acc 83.19, Test_acc 69.55
2025-01-11 02:26:21,617 [podnet.py] => Task 1, Epoch 55/160 (LR 0.07357) => LSC_loss 0.52, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 84.38, Test_acc 68.20
2025-01-11 02:26:27,306 [podnet.py] => Task 1, Epoch 56/160 (LR 0.07270) => LSC_loss 0.51, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 85.32, Test_acc 65.90
2025-01-11 02:26:33,119 [podnet.py] => Task 1, Epoch 57/160 (LR 0.07182) => LSC_loss 0.50, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 85.34, Test_acc 64.70
2025-01-11 02:26:38,912 [podnet.py] => Task 1, Epoch 58/160 (LR 0.07093) => LSC_loss 0.53, Spatial_loss 1.24, Flat_loss 0.18, Train_acc 84.28, Test_acc 67.15
2025-01-11 02:26:44,637 [podnet.py] => Task 1, Epoch 59/160 (LR 0.07004) => LSC_loss 0.48, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 85.68, Test_acc 67.65
2025-01-11 02:26:50,328 [podnet.py] => Task 1, Epoch 60/160 (LR 0.06913) => LSC_loss 0.51, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 85.43, Test_acc 60.45
2025-01-11 02:26:56,084 [podnet.py] => Task 1, Epoch 61/160 (LR 0.06822) => LSC_loss 0.52, Spatial_loss 1.26, Flat_loss 0.19, Train_acc 85.20, Test_acc 59.35
2025-01-11 02:27:01,812 [podnet.py] => Task 1, Epoch 62/160 (LR 0.06731) => LSC_loss 0.50, Spatial_loss 1.24, Flat_loss 0.18, Train_acc 85.38, Test_acc 63.75
2025-01-11 02:27:07,809 [podnet.py] => Task 1, Epoch 63/160 (LR 0.06638) => LSC_loss 0.53, Spatial_loss 1.28, Flat_loss 0.19, Train_acc 84.83, Test_acc 68.25
2025-01-11 02:27:13,593 [podnet.py] => Task 1, Epoch 64/160 (LR 0.06545) => LSC_loss 0.51, Spatial_loss 1.26, Flat_loss 0.19, Train_acc 85.10, Test_acc 67.80
2025-01-11 02:27:19,129 [podnet.py] => Task 1, Epoch 65/160 (LR 0.06451) => LSC_loss 0.49, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 85.75, Test_acc 69.20
2025-01-11 02:27:24,923 [podnet.py] => Task 1, Epoch 66/160 (LR 0.06357) => LSC_loss 0.54, Spatial_loss 1.29, Flat_loss 0.19, Train_acc 84.07, Test_acc 68.20
2025-01-11 02:27:30,695 [podnet.py] => Task 1, Epoch 67/160 (LR 0.06262) => LSC_loss 0.50, Spatial_loss 1.23, Flat_loss 0.18, Train_acc 85.78, Test_acc 65.95
2025-01-11 02:27:36,424 [podnet.py] => Task 1, Epoch 68/160 (LR 0.06167) => LSC_loss 0.51, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 85.82, Test_acc 66.40
2025-01-11 02:27:42,313 [podnet.py] => Task 1, Epoch 69/160 (LR 0.06072) => LSC_loss 0.50, Spatial_loss 1.27, Flat_loss 0.19, Train_acc 85.60, Test_acc 67.95
2025-01-11 02:27:47,983 [podnet.py] => Task 1, Epoch 70/160 (LR 0.05975) => LSC_loss 0.48, Spatial_loss 1.24, Flat_loss 0.19, Train_acc 85.89, Test_acc 64.20
2025-01-11 02:27:53,687 [podnet.py] => Task 1, Epoch 71/160 (LR 0.05879) => LSC_loss 0.47, Spatial_loss 1.21, Flat_loss 0.18, Train_acc 86.20, Test_acc 64.75
2025-01-11 02:27:59,391 [podnet.py] => Task 1, Epoch 72/160 (LR 0.05782) => LSC_loss 0.46, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 87.12, Test_acc 67.05
2025-01-11 02:28:05,281 [podnet.py] => Task 1, Epoch 73/160 (LR 0.05685) => LSC_loss 0.44, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 87.59, Test_acc 71.40
2025-01-11 02:28:11,008 [podnet.py] => Task 1, Epoch 74/160 (LR 0.05588) => LSC_loss 0.43, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 87.55, Test_acc 69.60
2025-01-11 02:28:16,782 [podnet.py] => Task 1, Epoch 75/160 (LR 0.05490) => LSC_loss 0.42, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 87.89, Test_acc 68.90
2025-01-11 02:28:22,483 [podnet.py] => Task 1, Epoch 76/160 (LR 0.05392) => LSC_loss 0.41, Spatial_loss 1.16, Flat_loss 0.18, Train_acc 88.37, Test_acc 68.70
2025-01-11 02:28:28,079 [podnet.py] => Task 1, Epoch 77/160 (LR 0.05294) => LSC_loss 0.43, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 87.32, Test_acc 66.55
2025-01-11 02:28:33,863 [podnet.py] => Task 1, Epoch 78/160 (LR 0.05196) => LSC_loss 0.41, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 87.72, Test_acc 67.20
2025-01-11 02:28:39,763 [podnet.py] => Task 1, Epoch 79/160 (LR 0.05098) => LSC_loss 0.41, Spatial_loss 1.18, Flat_loss 0.18, Train_acc 88.36, Test_acc 69.85
2025-01-11 02:28:45,658 [podnet.py] => Task 1, Epoch 80/160 (LR 0.05000) => LSC_loss 0.39, Spatial_loss 1.14, Flat_loss 0.18, Train_acc 88.82, Test_acc 63.80
2025-01-11 02:28:51,377 [podnet.py] => Task 1, Epoch 81/160 (LR 0.04902) => LSC_loss 0.41, Spatial_loss 1.15, Flat_loss 0.18, Train_acc 88.39, Test_acc 67.15
2025-01-11 02:28:57,170 [podnet.py] => Task 1, Epoch 82/160 (LR 0.04804) => LSC_loss 0.40, Spatial_loss 1.14, Flat_loss 0.18, Train_acc 88.86, Test_acc 65.30
2025-01-11 02:29:02,869 [podnet.py] => Task 1, Epoch 83/160 (LR 0.04706) => LSC_loss 0.42, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 88.03, Test_acc 67.20
2025-01-11 02:29:08,736 [podnet.py] => Task 1, Epoch 84/160 (LR 0.04608) => LSC_loss 0.40, Spatial_loss 1.18, Flat_loss 0.18, Train_acc 88.11, Test_acc 68.45
2025-01-11 02:29:14,536 [podnet.py] => Task 1, Epoch 85/160 (LR 0.04510) => LSC_loss 0.36, Spatial_loss 1.11, Flat_loss 0.17, Train_acc 89.94, Test_acc 69.05
2025-01-11 02:29:20,301 [podnet.py] => Task 1, Epoch 86/160 (LR 0.04412) => LSC_loss 0.38, Spatial_loss 1.13, Flat_loss 0.18, Train_acc 89.22, Test_acc 70.80
2025-01-11 02:29:25,974 [podnet.py] => Task 1, Epoch 87/160 (LR 0.04315) => LSC_loss 0.36, Spatial_loss 1.12, Flat_loss 0.18, Train_acc 89.83, Test_acc 70.20
2025-01-11 02:29:31,564 [podnet.py] => Task 1, Epoch 88/160 (LR 0.04218) => LSC_loss 0.36, Spatial_loss 1.13, Flat_loss 0.18, Train_acc 90.03, Test_acc 70.40
2025-01-11 02:29:37,381 [podnet.py] => Task 1, Epoch 89/160 (LR 0.04121) => LSC_loss 0.38, Spatial_loss 1.14, Flat_loss 0.18, Train_acc 89.21, Test_acc 67.95
2025-01-11 02:29:43,196 [podnet.py] => Task 1, Epoch 90/160 (LR 0.04025) => LSC_loss 0.33, Spatial_loss 1.10, Flat_loss 0.18, Train_acc 90.31, Test_acc 67.35
2025-01-11 02:29:48,920 [podnet.py] => Task 1, Epoch 91/160 (LR 0.03928) => LSC_loss 0.36, Spatial_loss 1.11, Flat_loss 0.18, Train_acc 90.14, Test_acc 68.60
2025-01-11 02:29:54,607 [podnet.py] => Task 1, Epoch 92/160 (LR 0.03833) => LSC_loss 0.36, Spatial_loss 1.12, Flat_loss 0.18, Train_acc 89.78, Test_acc 67.80
2025-01-11 02:30:00,406 [podnet.py] => Task 1, Epoch 93/160 (LR 0.03738) => LSC_loss 0.35, Spatial_loss 1.12, Flat_loss 0.18, Train_acc 89.82, Test_acc 68.00
2025-01-11 02:30:06,089 [podnet.py] => Task 1, Epoch 94/160 (LR 0.03643) => LSC_loss 0.32, Spatial_loss 1.07, Flat_loss 0.18, Train_acc 91.08, Test_acc 67.20
2025-01-11 02:30:11,832 [podnet.py] => Task 1, Epoch 95/160 (LR 0.03549) => LSC_loss 0.32, Spatial_loss 1.09, Flat_loss 0.18, Train_acc 91.10, Test_acc 66.65
2025-01-11 02:30:17,410 [podnet.py] => Task 1, Epoch 96/160 (LR 0.03455) => LSC_loss 0.29, Spatial_loss 1.05, Flat_loss 0.17, Train_acc 91.78, Test_acc 70.85
2025-01-11 02:30:23,336 [podnet.py] => Task 1, Epoch 97/160 (LR 0.03362) => LSC_loss 0.31, Spatial_loss 1.06, Flat_loss 0.17, Train_acc 91.94, Test_acc 66.05
2025-01-11 02:30:29,205 [podnet.py] => Task 1, Epoch 98/160 (LR 0.03269) => LSC_loss 0.33, Spatial_loss 1.10, Flat_loss 0.18, Train_acc 90.42, Test_acc 68.05
2025-01-11 02:30:34,941 [podnet.py] => Task 1, Epoch 99/160 (LR 0.03178) => LSC_loss 0.29, Spatial_loss 1.03, Flat_loss 0.17, Train_acc 91.95, Test_acc 67.55
2025-01-11 02:30:40,924 [podnet.py] => Task 1, Epoch 100/160 (LR 0.03087) => LSC_loss 0.29, Spatial_loss 1.07, Flat_loss 0.18, Train_acc 92.11, Test_acc 69.85
2025-01-11 02:30:46,713 [podnet.py] => Task 1, Epoch 101/160 (LR 0.02996) => LSC_loss 0.31, Spatial_loss 1.11, Flat_loss 0.18, Train_acc 91.39, Test_acc 71.70
2025-01-11 02:30:52,490 [podnet.py] => Task 1, Epoch 102/160 (LR 0.02907) => LSC_loss 0.26, Spatial_loss 1.01, Flat_loss 0.17, Train_acc 92.79, Test_acc 67.65
2025-01-11 02:30:58,187 [podnet.py] => Task 1, Epoch 103/160 (LR 0.02818) => LSC_loss 0.28, Spatial_loss 1.05, Flat_loss 0.17, Train_acc 92.30, Test_acc 70.85
2025-01-11 02:31:03,955 [podnet.py] => Task 1, Epoch 104/160 (LR 0.02730) => LSC_loss 0.27, Spatial_loss 1.01, Flat_loss 0.17, Train_acc 92.66, Test_acc 70.25
2025-01-11 02:31:10,036 [podnet.py] => Task 1, Epoch 105/160 (LR 0.02643) => LSC_loss 0.28, Spatial_loss 1.03, Flat_loss 0.17, Train_acc 92.32, Test_acc 68.15
2025-01-11 02:31:15,870 [podnet.py] => Task 1, Epoch 106/160 (LR 0.02557) => LSC_loss 0.27, Spatial_loss 1.02, Flat_loss 0.17, Train_acc 92.57, Test_acc 72.30
2025-01-11 02:31:21,718 [podnet.py] => Task 1, Epoch 107/160 (LR 0.02472) => LSC_loss 0.25, Spatial_loss 1.00, Flat_loss 0.17, Train_acc 93.16, Test_acc 71.50
2025-01-11 02:31:28,021 [podnet.py] => Task 1, Epoch 108/160 (LR 0.02388) => LSC_loss 0.28, Spatial_loss 1.04, Flat_loss 0.18, Train_acc 92.25, Test_acc 70.50
2025-01-11 02:31:34,150 [podnet.py] => Task 1, Epoch 109/160 (LR 0.02304) => LSC_loss 0.23, Spatial_loss 0.96, Flat_loss 0.17, Train_acc 94.01, Test_acc 71.35
2025-01-11 02:31:39,813 [podnet.py] => Task 1, Epoch 110/160 (LR 0.02222) => LSC_loss 0.24, Spatial_loss 0.96, Flat_loss 0.16, Train_acc 93.35, Test_acc 70.35
2025-01-11 02:31:45,377 [podnet.py] => Task 1, Epoch 111/160 (LR 0.02141) => LSC_loss 0.23, Spatial_loss 0.97, Flat_loss 0.17, Train_acc 94.07, Test_acc 71.00
2025-01-11 02:31:51,119 [podnet.py] => Task 1, Epoch 112/160 (LR 0.02061) => LSC_loss 0.24, Spatial_loss 0.97, Flat_loss 0.17, Train_acc 93.34, Test_acc 69.30
2025-01-11 02:31:56,872 [podnet.py] => Task 1, Epoch 113/160 (LR 0.01982) => LSC_loss 0.24, Spatial_loss 0.95, Flat_loss 0.17, Train_acc 93.72, Test_acc 68.45
2025-01-11 02:32:02,806 [podnet.py] => Task 1, Epoch 114/160 (LR 0.01905) => LSC_loss 0.23, Spatial_loss 0.95, Flat_loss 0.17, Train_acc 94.01, Test_acc 69.40
2025-01-11 02:32:09,003 [podnet.py] => Task 1, Epoch 115/160 (LR 0.01828) => LSC_loss 0.20, Spatial_loss 0.93, Flat_loss 0.16, Train_acc 94.84, Test_acc 72.20
2025-01-11 02:32:14,861 [podnet.py] => Task 1, Epoch 116/160 (LR 0.01753) => LSC_loss 0.20, Spatial_loss 0.92, Flat_loss 0.16, Train_acc 94.52, Test_acc 70.70
2025-01-11 02:32:21,092 [podnet.py] => Task 1, Epoch 117/160 (LR 0.01679) => LSC_loss 0.21, Spatial_loss 0.93, Flat_loss 0.16, Train_acc 94.54, Test_acc 70.25
2025-01-11 02:32:27,109 [podnet.py] => Task 1, Epoch 118/160 (LR 0.01606) => LSC_loss 0.20, Spatial_loss 0.92, Flat_loss 0.16, Train_acc 95.04, Test_acc 70.70
2025-01-11 02:32:33,120 [podnet.py] => Task 1, Epoch 119/160 (LR 0.01535) => LSC_loss 0.20, Spatial_loss 0.92, Flat_loss 0.16, Train_acc 95.04, Test_acc 72.30
2025-01-11 02:32:39,021 [podnet.py] => Task 1, Epoch 120/160 (LR 0.01464) => LSC_loss 0.20, Spatial_loss 0.92, Flat_loss 0.17, Train_acc 94.88, Test_acc 71.50
2025-01-11 02:32:44,887 [podnet.py] => Task 1, Epoch 121/160 (LR 0.01396) => LSC_loss 0.19, Spatial_loss 0.89, Flat_loss 0.16, Train_acc 95.38, Test_acc 71.60
2025-01-11 02:32:50,707 [podnet.py] => Task 1, Epoch 122/160 (LR 0.01328) => LSC_loss 0.18, Spatial_loss 0.89, Flat_loss 0.16, Train_acc 95.23, Test_acc 70.35
2025-01-11 02:32:56,486 [podnet.py] => Task 1, Epoch 123/160 (LR 0.01262) => LSC_loss 0.18, Spatial_loss 0.88, Flat_loss 0.16, Train_acc 95.66, Test_acc 71.10
2025-01-11 02:33:02,219 [podnet.py] => Task 1, Epoch 124/160 (LR 0.01198) => LSC_loss 0.18, Spatial_loss 0.90, Flat_loss 0.16, Train_acc 95.39, Test_acc 71.90
2025-01-11 02:33:08,070 [podnet.py] => Task 1, Epoch 125/160 (LR 0.01135) => LSC_loss 0.18, Spatial_loss 0.90, Flat_loss 0.16, Train_acc 95.20, Test_acc 70.80
2025-01-11 02:33:13,683 [podnet.py] => Task 1, Epoch 126/160 (LR 0.01073) => LSC_loss 0.16, Spatial_loss 0.89, Flat_loss 0.16, Train_acc 96.07, Test_acc 72.75
2025-01-11 02:33:19,358 [podnet.py] => Task 1, Epoch 127/160 (LR 0.01013) => LSC_loss 0.16, Spatial_loss 0.85, Flat_loss 0.16, Train_acc 96.23, Test_acc 72.35
2025-01-11 02:33:25,101 [podnet.py] => Task 1, Epoch 128/160 (LR 0.00955) => LSC_loss 0.16, Spatial_loss 0.86, Flat_loss 0.16, Train_acc 96.16, Test_acc 72.10
2025-01-11 02:33:30,828 [podnet.py] => Task 1, Epoch 129/160 (LR 0.00898) => LSC_loss 0.15, Spatial_loss 0.84, Flat_loss 0.16, Train_acc 96.50, Test_acc 71.30
2025-01-11 02:33:36,517 [podnet.py] => Task 1, Epoch 130/160 (LR 0.00843) => LSC_loss 0.15, Spatial_loss 0.85, Flat_loss 0.16, Train_acc 96.56, Test_acc 72.05
2025-01-11 02:33:42,305 [podnet.py] => Task 1, Epoch 131/160 (LR 0.00789) => LSC_loss 0.15, Spatial_loss 0.83, Flat_loss 0.16, Train_acc 96.79, Test_acc 72.25
2025-01-11 02:33:48,084 [podnet.py] => Task 1, Epoch 132/160 (LR 0.00737) => LSC_loss 0.14, Spatial_loss 0.81, Flat_loss 0.15, Train_acc 96.61, Test_acc 71.55
2025-01-11 02:33:53,868 [podnet.py] => Task 1, Epoch 133/160 (LR 0.00686) => LSC_loss 0.15, Spatial_loss 0.83, Flat_loss 0.15, Train_acc 96.42, Test_acc 72.70
2025-01-11 02:33:59,762 [podnet.py] => Task 1, Epoch 134/160 (LR 0.00638) => LSC_loss 0.14, Spatial_loss 0.82, Flat_loss 0.15, Train_acc 96.55, Test_acc 71.30
2025-01-11 02:34:05,626 [podnet.py] => Task 1, Epoch 135/160 (LR 0.00590) => LSC_loss 0.15, Spatial_loss 0.81, Flat_loss 0.15, Train_acc 96.70, Test_acc 71.50
2025-01-11 02:34:11,384 [podnet.py] => Task 1, Epoch 136/160 (LR 0.00545) => LSC_loss 0.14, Spatial_loss 0.81, Flat_loss 0.15, Train_acc 97.17, Test_acc 72.80
2025-01-11 02:34:17,177 [podnet.py] => Task 1, Epoch 137/160 (LR 0.00501) => LSC_loss 0.14, Spatial_loss 0.81, Flat_loss 0.15, Train_acc 97.09, Test_acc 71.50
2025-01-11 02:34:22,888 [podnet.py] => Task 1, Epoch 138/160 (LR 0.00459) => LSC_loss 0.14, Spatial_loss 0.80, Flat_loss 0.15, Train_acc 96.86, Test_acc 72.10
2025-01-11 02:34:28,530 [podnet.py] => Task 1, Epoch 139/160 (LR 0.00419) => LSC_loss 0.13, Spatial_loss 0.79, Flat_loss 0.15, Train_acc 97.14, Test_acc 71.95
2025-01-11 02:34:34,198 [podnet.py] => Task 1, Epoch 140/160 (LR 0.00381) => LSC_loss 0.13, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 97.24, Test_acc 72.10
2025-01-11 02:34:39,945 [podnet.py] => Task 1, Epoch 141/160 (LR 0.00344) => LSC_loss 0.12, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 97.37, Test_acc 71.80
2025-01-11 02:34:45,756 [podnet.py] => Task 1, Epoch 142/160 (LR 0.00309) => LSC_loss 0.13, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.24, Test_acc 71.95
2025-01-11 02:34:51,475 [podnet.py] => Task 1, Epoch 143/160 (LR 0.00276) => LSC_loss 0.13, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.45, Test_acc 72.45
2025-01-11 02:34:57,189 [podnet.py] => Task 1, Epoch 144/160 (LR 0.00245) => LSC_loss 0.13, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 97.51, Test_acc 72.65
2025-01-11 02:35:02,909 [podnet.py] => Task 1, Epoch 145/160 (LR 0.00215) => LSC_loss 0.13, Spatial_loss 0.78, Flat_loss 0.15, Train_acc 97.44, Test_acc 72.80
2025-01-11 02:35:09,148 [podnet.py] => Task 1, Epoch 146/160 (LR 0.00188) => LSC_loss 0.12, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.57, Test_acc 72.75
2025-01-11 02:35:14,917 [podnet.py] => Task 1, Epoch 147/160 (LR 0.00162) => LSC_loss 0.12, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.57, Test_acc 72.40
2025-01-11 02:35:20,617 [podnet.py] => Task 1, Epoch 148/160 (LR 0.00138) => LSC_loss 0.12, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.70, Test_acc 72.60
2025-01-11 02:35:26,374 [podnet.py] => Task 1, Epoch 149/160 (LR 0.00116) => LSC_loss 0.11, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.88, Test_acc 72.15
2025-01-11 02:35:32,163 [podnet.py] => Task 1, Epoch 150/160 (LR 0.00096) => LSC_loss 0.12, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.58, Test_acc 72.00
2025-01-11 02:35:38,071 [podnet.py] => Task 1, Epoch 151/160 (LR 0.00078) => LSC_loss 0.12, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.85, Test_acc 72.05
2025-01-11 02:35:43,695 [podnet.py] => Task 1, Epoch 152/160 (LR 0.00062) => LSC_loss 0.12, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.72, Test_acc 72.60
2025-01-11 02:35:49,268 [podnet.py] => Task 1, Epoch 153/160 (LR 0.00047) => LSC_loss 0.11, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.86, Test_acc 72.45
2025-01-11 02:35:54,984 [podnet.py] => Task 1, Epoch 154/160 (LR 0.00035) => LSC_loss 0.12, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.63, Test_acc 72.05
2025-01-11 02:36:00,701 [podnet.py] => Task 1, Epoch 155/160 (LR 0.00024) => LSC_loss 0.11, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.97, Test_acc 72.60
2025-01-11 02:36:06,632 [podnet.py] => Task 1, Epoch 156/160 (LR 0.00015) => LSC_loss 0.12, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.80, Test_acc 72.40
2025-01-11 02:36:12,383 [podnet.py] => Task 1, Epoch 157/160 (LR 0.00009) => LSC_loss 0.12, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.60, Test_acc 72.45
2025-01-11 02:36:18,140 [podnet.py] => Task 1, Epoch 158/160 (LR 0.00004) => LSC_loss 0.11, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.86, Test_acc 72.70
2025-01-11 02:36:23,857 [podnet.py] => Task 1, Epoch 159/160 (LR 0.00001) => LSC_loss 0.11, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.80, Test_acc 72.35
2025-01-11 02:36:29,718 [podnet.py] => Task 1, Epoch 160/160 (LR 0.00000) => LSC_loss 0.11, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.86, Test_acc 72.75
2025-01-11 02:36:29,719 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 02:36:29,719 [base.py] => Reducing exemplars...(740 per classes)
2025-01-11 02:36:36,968 [base.py] => Constructing exemplars...(740 per classes)
2025-01-11 02:36:52,538 [podnet.py] => The size of finetune dataset: 10000
2025-01-11 02:36:58,075 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.12, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.50, Test_acc 72.10
2025-01-11 02:37:03,956 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.13, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.27, Test_acc 71.60
2025-01-11 02:37:09,584 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.13, Spatial_loss 0.79, Flat_loss 0.15, Train_acc 96.96, Test_acc 72.40
2025-01-11 02:37:15,475 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.13, Spatial_loss 0.80, Flat_loss 0.15, Train_acc 97.08, Test_acc 72.20
2025-01-11 02:37:21,258 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.12, Spatial_loss 0.79, Flat_loss 0.15, Train_acc 97.34, Test_acc 72.20
2025-01-11 02:37:26,982 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.12, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.51, Test_acc 72.30
2025-01-11 02:37:33,080 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.13, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.20, Test_acc 72.45
2025-01-11 02:37:38,964 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.41, Test_acc 72.10
2025-01-11 02:37:44,859 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.12, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.49, Test_acc 72.25
2025-01-11 02:37:50,729 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 0.77, Flat_loss 0.15, Train_acc 97.28, Test_acc 72.45
2025-01-11 02:37:56,696 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.12, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.81, Test_acc 72.35
2025-01-11 02:38:02,466 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.13, Spatial_loss 0.76, Flat_loss 0.15, Train_acc 97.25, Test_acc 72.70
2025-01-11 02:38:08,263 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.12, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.46, Test_acc 71.70
2025-01-11 02:38:14,031 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.11, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.90, Test_acc 72.15
2025-01-11 02:38:19,848 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.12, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.69, Test_acc 72.20
2025-01-11 02:38:25,729 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.11, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.94, Test_acc 72.25
2025-01-11 02:38:31,760 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.13, Spatial_loss 0.75, Flat_loss 0.15, Train_acc 97.59, Test_acc 72.35
2025-01-11 02:38:38,000 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.11, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.84, Test_acc 72.55
2025-01-11 02:38:44,118 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.12, Spatial_loss 0.73, Flat_loss 0.15, Train_acc 97.65, Test_acc 72.80
2025-01-11 02:38:49,963 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.12, Spatial_loss 0.74, Flat_loss 0.15, Train_acc 97.94, Test_acc 72.30
2025-01-11 02:38:49,965 [base.py] => Reducing exemplars...(370 per classes)
2025-01-11 02:38:56,757 [base.py] => Constructing exemplars...(370 per classes)
2025-01-11 02:39:13,460 [podnet.py] => Exemplar size: 7400
2025-01-11 02:39:13,461 [trainer.py] => CNN: {'total': np.float64(72.3), '00-09': np.float64(85.1), '10-19': np.float64(59.5), 'old': np.float64(85.1), 'new': np.float64(59.5)}
2025-01-11 02:39:13,461 [trainer.py] => NME: {'total': np.float64(72.3), '00-09': np.float64(84.0), '10-19': np.float64(60.6), 'old': np.float64(84.0), 'new': np.float64(60.6)}
2025-01-11 02:39:13,461 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3)]
2025-01-11 02:39:13,461 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05)]
2025-01-11 02:39:13,461 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3)]
2025-01-11 02:39:13,461 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7)]

2025-01-11 02:39:13,461 [trainer.py] => All params: 479057
2025-01-11 02:39:13,461 [trainer.py] => Trainable params: 479057
2025-01-11 02:39:13,462 [podnet.py] => Learning on 20-30
2025-01-11 02:39:13,557 [podnet.py] => Adaptive factor: 1.7320508075688772
2025-01-11 02:39:20,411 [podnet.py] => Task 2, Epoch 1/160 (LR 0.09999) => LSC_loss 1.76, Spatial_loss 1.98, Flat_loss 0.31, Train_acc 53.20, Test_acc 36.23
2025-01-11 02:39:27,281 [podnet.py] => Task 2, Epoch 2/160 (LR 0.09996) => LSC_loss 1.28, Spatial_loss 1.77, Flat_loss 0.20, Train_acc 62.44, Test_acc 51.20
2025-01-11 02:39:33,979 [podnet.py] => Task 2, Epoch 3/160 (LR 0.09991) => LSC_loss 1.15, Spatial_loss 1.63, Flat_loss 0.18, Train_acc 66.44, Test_acc 54.17
2025-01-11 02:39:40,816 [podnet.py] => Task 2, Epoch 4/160 (LR 0.09985) => LSC_loss 1.10, Spatial_loss 1.65, Flat_loss 0.18, Train_acc 67.73, Test_acc 59.60
2025-01-11 02:39:47,589 [podnet.py] => Task 2, Epoch 5/160 (LR 0.09976) => LSC_loss 1.04, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 70.11, Test_acc 59.40
2025-01-11 02:39:54,505 [podnet.py] => Task 2, Epoch 6/160 (LR 0.09965) => LSC_loss 1.02, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 70.24, Test_acc 59.10
2025-01-11 02:40:01,343 [podnet.py] => Task 2, Epoch 7/160 (LR 0.09953) => LSC_loss 0.97, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 71.31, Test_acc 58.57
2025-01-11 02:40:08,350 [podnet.py] => Task 2, Epoch 8/160 (LR 0.09938) => LSC_loss 0.97, Spatial_loss 1.55, Flat_loss 0.17, Train_acc 71.50, Test_acc 54.83
2025-01-11 02:40:15,224 [podnet.py] => Task 2, Epoch 9/160 (LR 0.09922) => LSC_loss 0.94, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 72.57, Test_acc 51.53
2025-01-11 02:40:22,174 [podnet.py] => Task 2, Epoch 10/160 (LR 0.09904) => LSC_loss 0.91, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 73.10, Test_acc 50.80
2025-01-11 02:40:29,005 [podnet.py] => Task 2, Epoch 11/160 (LR 0.09884) => LSC_loss 0.91, Spatial_loss 1.55, Flat_loss 0.17, Train_acc 73.48, Test_acc 59.60
2025-01-11 02:40:35,993 [podnet.py] => Task 2, Epoch 12/160 (LR 0.09862) => LSC_loss 0.88, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 74.77, Test_acc 56.67
2025-01-11 02:40:43,026 [podnet.py] => Task 2, Epoch 13/160 (LR 0.09838) => LSC_loss 0.89, Spatial_loss 1.54, Flat_loss 0.18, Train_acc 74.06, Test_acc 60.43
2025-01-11 02:40:49,992 [podnet.py] => Task 2, Epoch 14/160 (LR 0.09812) => LSC_loss 0.85, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 75.72, Test_acc 58.67
2025-01-11 02:40:57,059 [podnet.py] => Task 2, Epoch 15/160 (LR 0.09785) => LSC_loss 0.85, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 75.36, Test_acc 54.33
2025-01-11 02:41:03,894 [podnet.py] => Task 2, Epoch 16/160 (LR 0.09755) => LSC_loss 0.84, Spatial_loss 1.55, Flat_loss 0.18, Train_acc 75.75, Test_acc 60.23
2025-01-11 02:41:10,785 [podnet.py] => Task 2, Epoch 17/160 (LR 0.09724) => LSC_loss 0.82, Spatial_loss 1.50, Flat_loss 0.17, Train_acc 76.08, Test_acc 60.10
2025-01-11 02:41:17,775 [podnet.py] => Task 2, Epoch 18/160 (LR 0.09691) => LSC_loss 0.84, Spatial_loss 1.52, Flat_loss 0.18, Train_acc 75.36, Test_acc 59.60
2025-01-11 02:41:24,966 [podnet.py] => Task 2, Epoch 19/160 (LR 0.09656) => LSC_loss 0.81, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 76.50, Test_acc 60.07
2025-01-11 02:41:32,097 [podnet.py] => Task 2, Epoch 20/160 (LR 0.09619) => LSC_loss 0.80, Spatial_loss 1.50, Flat_loss 0.17, Train_acc 77.11, Test_acc 60.97
2025-01-11 02:41:39,068 [podnet.py] => Task 2, Epoch 21/160 (LR 0.09581) => LSC_loss 0.80, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 76.32, Test_acc 61.83
2025-01-11 02:41:45,868 [podnet.py] => Task 2, Epoch 22/160 (LR 0.09541) => LSC_loss 0.76, Spatial_loss 1.50, Flat_loss 0.17, Train_acc 77.93, Test_acc 59.93
2025-01-11 02:41:52,755 [podnet.py] => Task 2, Epoch 23/160 (LR 0.09499) => LSC_loss 0.80, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 76.59, Test_acc 59.57
2025-01-11 02:42:00,019 [podnet.py] => Task 2, Epoch 24/160 (LR 0.09455) => LSC_loss 0.80, Spatial_loss 1.54, Flat_loss 0.18, Train_acc 77.19, Test_acc 52.87
2025-01-11 02:42:06,779 [podnet.py] => Task 2, Epoch 25/160 (LR 0.09410) => LSC_loss 0.76, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 78.24, Test_acc 57.90
2025-01-11 02:42:13,502 [podnet.py] => Task 2, Epoch 26/160 (LR 0.09362) => LSC_loss 0.76, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 77.94, Test_acc 57.80
2025-01-11 02:42:20,556 [podnet.py] => Task 2, Epoch 27/160 (LR 0.09314) => LSC_loss 0.76, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 77.78, Test_acc 62.57
2025-01-11 02:42:27,454 [podnet.py] => Task 2, Epoch 28/160 (LR 0.09263) => LSC_loss 0.73, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 78.95, Test_acc 62.33
2025-01-11 02:42:34,332 [podnet.py] => Task 2, Epoch 29/160 (LR 0.09211) => LSC_loss 0.76, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 77.77, Test_acc 59.23
2025-01-11 02:42:41,232 [podnet.py] => Task 2, Epoch 30/160 (LR 0.09157) => LSC_loss 0.74, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 78.65, Test_acc 59.17
2025-01-11 02:42:48,133 [podnet.py] => Task 2, Epoch 31/160 (LR 0.09102) => LSC_loss 0.74, Spatial_loss 1.52, Flat_loss 0.18, Train_acc 78.62, Test_acc 57.20
2025-01-11 02:42:54,986 [podnet.py] => Task 2, Epoch 32/160 (LR 0.09045) => LSC_loss 0.73, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 79.19, Test_acc 62.67
2025-01-11 02:43:01,821 [podnet.py] => Task 2, Epoch 33/160 (LR 0.08987) => LSC_loss 0.73, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 78.61, Test_acc 57.97
2025-01-11 02:43:08,726 [podnet.py] => Task 2, Epoch 34/160 (LR 0.08927) => LSC_loss 0.73, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 79.15, Test_acc 64.20
2025-01-11 02:43:15,700 [podnet.py] => Task 2, Epoch 35/160 (LR 0.08865) => LSC_loss 0.71, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 79.15, Test_acc 59.70
2025-01-11 02:43:22,795 [podnet.py] => Task 2, Epoch 36/160 (LR 0.08802) => LSC_loss 0.70, Spatial_loss 1.46, Flat_loss 0.17, Train_acc 79.77, Test_acc 62.80
2025-01-11 02:43:29,469 [podnet.py] => Task 2, Epoch 37/160 (LR 0.08738) => LSC_loss 0.69, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 80.16, Test_acc 59.73
2025-01-11 02:43:36,313 [podnet.py] => Task 2, Epoch 38/160 (LR 0.08672) => LSC_loss 0.70, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 79.49, Test_acc 62.13
2025-01-11 02:43:43,066 [podnet.py] => Task 2, Epoch 39/160 (LR 0.08604) => LSC_loss 0.70, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 79.51, Test_acc 60.40
2025-01-11 02:43:49,939 [podnet.py] => Task 2, Epoch 40/160 (LR 0.08536) => LSC_loss 0.68, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 79.99, Test_acc 57.53
2025-01-11 02:43:56,673 [podnet.py] => Task 2, Epoch 41/160 (LR 0.08465) => LSC_loss 0.67, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 80.80, Test_acc 59.70
2025-01-11 02:44:03,507 [podnet.py] => Task 2, Epoch 42/160 (LR 0.08394) => LSC_loss 0.68, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 80.15, Test_acc 64.17
2025-01-11 02:44:10,613 [podnet.py] => Task 2, Epoch 43/160 (LR 0.08321) => LSC_loss 0.68, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 80.53, Test_acc 61.90
2025-01-11 02:44:17,775 [podnet.py] => Task 2, Epoch 44/160 (LR 0.08247) => LSC_loss 0.68, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 80.56, Test_acc 58.27
2025-01-11 02:44:24,758 [podnet.py] => Task 2, Epoch 45/160 (LR 0.08172) => LSC_loss 0.68, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 80.39, Test_acc 60.83
2025-01-11 02:44:31,692 [podnet.py] => Task 2, Epoch 46/160 (LR 0.08095) => LSC_loss 0.66, Spatial_loss 1.46, Flat_loss 0.18, Train_acc 80.61, Test_acc 63.57
2025-01-11 02:44:38,669 [podnet.py] => Task 2, Epoch 47/160 (LR 0.08018) => LSC_loss 0.66, Spatial_loss 1.47, Flat_loss 0.18, Train_acc 80.62, Test_acc 63.17
2025-01-11 02:44:45,477 [podnet.py] => Task 2, Epoch 48/160 (LR 0.07939) => LSC_loss 0.64, Spatial_loss 1.46, Flat_loss 0.18, Train_acc 81.37, Test_acc 62.40
2025-01-11 02:44:52,346 [podnet.py] => Task 2, Epoch 49/160 (LR 0.07859) => LSC_loss 0.64, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 81.72, Test_acc 63.73
2025-01-11 02:44:59,047 [podnet.py] => Task 2, Epoch 50/160 (LR 0.07778) => LSC_loss 0.62, Spatial_loss 1.44, Flat_loss 0.17, Train_acc 82.18, Test_acc 60.57
2025-01-11 02:45:05,876 [podnet.py] => Task 2, Epoch 51/160 (LR 0.07696) => LSC_loss 0.65, Spatial_loss 1.46, Flat_loss 0.18, Train_acc 81.52, Test_acc 56.13
2025-01-11 02:45:12,557 [podnet.py] => Task 2, Epoch 52/160 (LR 0.07612) => LSC_loss 0.63, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 81.96, Test_acc 63.30
2025-01-11 02:45:19,426 [podnet.py] => Task 2, Epoch 53/160 (LR 0.07528) => LSC_loss 0.62, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 81.77, Test_acc 56.30
2025-01-11 02:45:26,280 [podnet.py] => Task 2, Epoch 54/160 (LR 0.07443) => LSC_loss 0.62, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 82.41, Test_acc 62.03
2025-01-11 02:45:33,237 [podnet.py] => Task 2, Epoch 55/160 (LR 0.07357) => LSC_loss 0.61, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 82.47, Test_acc 64.40
2025-01-11 02:45:40,075 [podnet.py] => Task 2, Epoch 56/160 (LR 0.07270) => LSC_loss 0.63, Spatial_loss 1.45, Flat_loss 0.18, Train_acc 81.83, Test_acc 66.33
2025-01-11 02:45:46,986 [podnet.py] => Task 2, Epoch 57/160 (LR 0.07182) => LSC_loss 0.60, Spatial_loss 1.41, Flat_loss 0.17, Train_acc 82.63, Test_acc 62.83
2025-01-11 02:45:53,952 [podnet.py] => Task 2, Epoch 58/160 (LR 0.07093) => LSC_loss 0.61, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 82.64, Test_acc 61.23
2025-01-11 02:46:00,835 [podnet.py] => Task 2, Epoch 59/160 (LR 0.07004) => LSC_loss 0.57, Spatial_loss 1.41, Flat_loss 0.17, Train_acc 83.65, Test_acc 66.93
2025-01-11 02:46:07,926 [podnet.py] => Task 2, Epoch 60/160 (LR 0.06913) => LSC_loss 0.58, Spatial_loss 1.42, Flat_loss 0.17, Train_acc 83.23, Test_acc 58.47
2025-01-11 02:46:14,818 [podnet.py] => Task 2, Epoch 61/160 (LR 0.06822) => LSC_loss 0.61, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 82.69, Test_acc 60.73
2025-01-11 02:46:22,019 [podnet.py] => Task 2, Epoch 62/160 (LR 0.06731) => LSC_loss 0.60, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 82.65, Test_acc 67.10
2025-01-11 02:46:29,144 [podnet.py] => Task 2, Epoch 63/160 (LR 0.06638) => LSC_loss 0.60, Spatial_loss 1.42, Flat_loss 0.17, Train_acc 83.04, Test_acc 62.70
2025-01-11 02:46:36,042 [podnet.py] => Task 2, Epoch 64/160 (LR 0.06545) => LSC_loss 0.57, Spatial_loss 1.38, Flat_loss 0.17, Train_acc 83.37, Test_acc 63.33
2025-01-11 02:46:43,197 [podnet.py] => Task 2, Epoch 65/160 (LR 0.06451) => LSC_loss 0.56, Spatial_loss 1.39, Flat_loss 0.17, Train_acc 83.87, Test_acc 61.87
2025-01-11 02:46:50,134 [podnet.py] => Task 2, Epoch 66/160 (LR 0.06357) => LSC_loss 0.58, Spatial_loss 1.42, Flat_loss 0.17, Train_acc 83.10, Test_acc 63.00
2025-01-11 02:46:57,186 [podnet.py] => Task 2, Epoch 67/160 (LR 0.06262) => LSC_loss 0.55, Spatial_loss 1.40, Flat_loss 0.17, Train_acc 84.43, Test_acc 65.57
2025-01-11 02:47:04,501 [podnet.py] => Task 2, Epoch 68/160 (LR 0.06167) => LSC_loss 0.56, Spatial_loss 1.41, Flat_loss 0.17, Train_acc 83.73, Test_acc 63.60
2025-01-11 02:47:11,733 [podnet.py] => Task 2, Epoch 69/160 (LR 0.06072) => LSC_loss 0.56, Spatial_loss 1.39, Flat_loss 0.17, Train_acc 83.98, Test_acc 63.87
2025-01-11 02:47:18,597 [podnet.py] => Task 2, Epoch 70/160 (LR 0.05975) => LSC_loss 0.54, Spatial_loss 1.35, Flat_loss 0.17, Train_acc 84.36, Test_acc 62.73
2025-01-11 02:47:25,487 [podnet.py] => Task 2, Epoch 71/160 (LR 0.05879) => LSC_loss 0.52, Spatial_loss 1.36, Flat_loss 0.17, Train_acc 85.12, Test_acc 64.53
2025-01-11 02:47:32,277 [podnet.py] => Task 2, Epoch 72/160 (LR 0.05782) => LSC_loss 0.53, Spatial_loss 1.37, Flat_loss 0.17, Train_acc 84.77, Test_acc 63.03
2025-01-11 02:47:39,163 [podnet.py] => Task 2, Epoch 73/160 (LR 0.05685) => LSC_loss 0.53, Spatial_loss 1.37, Flat_loss 0.17, Train_acc 85.06, Test_acc 60.43
2025-01-11 02:47:45,953 [podnet.py] => Task 2, Epoch 74/160 (LR 0.05588) => LSC_loss 0.52, Spatial_loss 1.36, Flat_loss 0.17, Train_acc 85.18, Test_acc 66.90
2025-01-11 02:47:52,765 [podnet.py] => Task 2, Epoch 75/160 (LR 0.05490) => LSC_loss 0.51, Spatial_loss 1.34, Flat_loss 0.17, Train_acc 85.61, Test_acc 62.33
2025-01-11 02:47:59,643 [podnet.py] => Task 2, Epoch 76/160 (LR 0.05392) => LSC_loss 0.50, Spatial_loss 1.33, Flat_loss 0.17, Train_acc 86.12, Test_acc 63.20
2025-01-11 02:48:06,585 [podnet.py] => Task 2, Epoch 77/160 (LR 0.05294) => LSC_loss 0.50, Spatial_loss 1.33, Flat_loss 0.17, Train_acc 86.09, Test_acc 63.50
2025-01-11 02:48:13,539 [podnet.py] => Task 2, Epoch 78/160 (LR 0.05196) => LSC_loss 0.49, Spatial_loss 1.32, Flat_loss 0.16, Train_acc 86.03, Test_acc 65.50
2025-01-11 02:48:20,315 [podnet.py] => Task 2, Epoch 79/160 (LR 0.05098) => LSC_loss 0.49, Spatial_loss 1.33, Flat_loss 0.17, Train_acc 86.02, Test_acc 60.97
2025-01-11 02:48:27,148 [podnet.py] => Task 2, Epoch 80/160 (LR 0.05000) => LSC_loss 0.48, Spatial_loss 1.31, Flat_loss 0.16, Train_acc 86.38, Test_acc 58.00
2025-01-11 02:48:34,096 [podnet.py] => Task 2, Epoch 81/160 (LR 0.04902) => LSC_loss 0.48, Spatial_loss 1.30, Flat_loss 0.16, Train_acc 86.69, Test_acc 64.03
2025-01-11 02:48:41,381 [podnet.py] => Task 2, Epoch 82/160 (LR 0.04804) => LSC_loss 0.47, Spatial_loss 1.31, Flat_loss 0.16, Train_acc 86.63, Test_acc 64.40
2025-01-11 02:48:48,796 [podnet.py] => Task 2, Epoch 83/160 (LR 0.04706) => LSC_loss 0.48, Spatial_loss 1.30, Flat_loss 0.16, Train_acc 86.52, Test_acc 63.60
2025-01-11 02:48:55,786 [podnet.py] => Task 2, Epoch 84/160 (LR 0.04608) => LSC_loss 0.46, Spatial_loss 1.29, Flat_loss 0.16, Train_acc 87.19, Test_acc 64.90
2025-01-11 02:49:03,278 [podnet.py] => Task 2, Epoch 85/160 (LR 0.04510) => LSC_loss 0.45, Spatial_loss 1.27, Flat_loss 0.16, Train_acc 87.36, Test_acc 63.53
2025-01-11 02:49:10,722 [podnet.py] => Task 2, Epoch 86/160 (LR 0.04412) => LSC_loss 0.44, Spatial_loss 1.27, Flat_loss 0.16, Train_acc 87.68, Test_acc 64.40
2025-01-11 02:49:18,294 [podnet.py] => Task 2, Epoch 87/160 (LR 0.04315) => LSC_loss 0.45, Spatial_loss 1.26, Flat_loss 0.16, Train_acc 87.38, Test_acc 66.53
2025-01-11 02:49:25,251 [podnet.py] => Task 2, Epoch 88/160 (LR 0.04218) => LSC_loss 0.45, Spatial_loss 1.26, Flat_loss 0.16, Train_acc 87.40, Test_acc 66.17
2025-01-11 02:49:32,228 [podnet.py] => Task 2, Epoch 89/160 (LR 0.04121) => LSC_loss 0.43, Spatial_loss 1.25, Flat_loss 0.16, Train_acc 87.92, Test_acc 65.07
2025-01-11 02:49:39,242 [podnet.py] => Task 2, Epoch 90/160 (LR 0.04025) => LSC_loss 0.43, Spatial_loss 1.26, Flat_loss 0.16, Train_acc 87.81, Test_acc 65.67
2025-01-11 02:49:46,382 [podnet.py] => Task 2, Epoch 91/160 (LR 0.03928) => LSC_loss 0.41, Spatial_loss 1.24, Flat_loss 0.16, Train_acc 88.54, Test_acc 61.17
2025-01-11 02:49:53,402 [podnet.py] => Task 2, Epoch 92/160 (LR 0.03833) => LSC_loss 0.41, Spatial_loss 1.23, Flat_loss 0.15, Train_acc 88.91, Test_acc 63.10
2025-01-11 02:50:00,661 [podnet.py] => Task 2, Epoch 93/160 (LR 0.03738) => LSC_loss 0.42, Spatial_loss 1.24, Flat_loss 0.16, Train_acc 88.24, Test_acc 63.67
2025-01-11 02:50:07,869 [podnet.py] => Task 2, Epoch 94/160 (LR 0.03643) => LSC_loss 0.43, Spatial_loss 1.26, Flat_loss 0.16, Train_acc 87.91, Test_acc 66.10
2025-01-11 02:50:14,930 [podnet.py] => Task 2, Epoch 95/160 (LR 0.03549) => LSC_loss 0.39, Spatial_loss 1.23, Flat_loss 0.16, Train_acc 88.97, Test_acc 64.07
2025-01-11 02:50:22,026 [podnet.py] => Task 2, Epoch 96/160 (LR 0.03455) => LSC_loss 0.38, Spatial_loss 1.21, Flat_loss 0.15, Train_acc 89.27, Test_acc 64.40
2025-01-11 02:50:29,142 [podnet.py] => Task 2, Epoch 97/160 (LR 0.03362) => LSC_loss 0.38, Spatial_loss 1.20, Flat_loss 0.15, Train_acc 89.75, Test_acc 65.23
2025-01-11 02:50:36,299 [podnet.py] => Task 2, Epoch 98/160 (LR 0.03269) => LSC_loss 0.36, Spatial_loss 1.18, Flat_loss 0.15, Train_acc 90.09, Test_acc 64.90
2025-01-11 02:50:43,117 [podnet.py] => Task 2, Epoch 99/160 (LR 0.03178) => LSC_loss 0.36, Spatial_loss 1.18, Flat_loss 0.15, Train_acc 90.17, Test_acc 65.50
2025-01-11 02:50:49,926 [podnet.py] => Task 2, Epoch 100/160 (LR 0.03087) => LSC_loss 0.35, Spatial_loss 1.16, Flat_loss 0.15, Train_acc 90.18, Test_acc 65.97
2025-01-11 02:50:56,859 [podnet.py] => Task 2, Epoch 101/160 (LR 0.02996) => LSC_loss 0.34, Spatial_loss 1.15, Flat_loss 0.15, Train_acc 90.71, Test_acc 66.47
2025-01-11 02:51:03,657 [podnet.py] => Task 2, Epoch 102/160 (LR 0.02907) => LSC_loss 0.35, Spatial_loss 1.15, Flat_loss 0.15, Train_acc 90.67, Test_acc 66.17
2025-01-11 02:51:10,636 [podnet.py] => Task 2, Epoch 103/160 (LR 0.02818) => LSC_loss 0.35, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 90.58, Test_acc 66.50
2025-01-11 02:51:17,643 [podnet.py] => Task 2, Epoch 104/160 (LR 0.02730) => LSC_loss 0.32, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 91.56, Test_acc 64.07
2025-01-11 02:51:24,596 [podnet.py] => Task 2, Epoch 105/160 (LR 0.02643) => LSC_loss 0.32, Spatial_loss 1.12, Flat_loss 0.15, Train_acc 91.41, Test_acc 64.83
2025-01-11 02:51:31,620 [podnet.py] => Task 2, Epoch 106/160 (LR 0.02557) => LSC_loss 0.32, Spatial_loss 1.13, Flat_loss 0.15, Train_acc 91.56, Test_acc 65.93
2025-01-11 02:51:38,983 [podnet.py] => Task 2, Epoch 107/160 (LR 0.02472) => LSC_loss 0.31, Spatial_loss 1.10, Flat_loss 0.15, Train_acc 91.88, Test_acc 66.47
2025-01-11 02:51:46,058 [podnet.py] => Task 2, Epoch 108/160 (LR 0.02388) => LSC_loss 0.32, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 91.76, Test_acc 65.00
2025-01-11 02:51:53,018 [podnet.py] => Task 2, Epoch 109/160 (LR 0.02304) => LSC_loss 0.30, Spatial_loss 1.08, Flat_loss 0.14, Train_acc 91.91, Test_acc 67.27
2025-01-11 02:51:59,898 [podnet.py] => Task 2, Epoch 110/160 (LR 0.02222) => LSC_loss 0.30, Spatial_loss 1.08, Flat_loss 0.14, Train_acc 92.30, Test_acc 67.27
2025-01-11 02:52:07,229 [podnet.py] => Task 2, Epoch 111/160 (LR 0.02141) => LSC_loss 0.29, Spatial_loss 1.06, Flat_loss 0.14, Train_acc 92.68, Test_acc 67.30
2025-01-11 02:52:14,000 [podnet.py] => Task 2, Epoch 112/160 (LR 0.02061) => LSC_loss 0.29, Spatial_loss 1.06, Flat_loss 0.14, Train_acc 92.53, Test_acc 66.00
2025-01-11 02:52:20,760 [podnet.py] => Task 2, Epoch 113/160 (LR 0.01982) => LSC_loss 0.29, Spatial_loss 1.06, Flat_loss 0.14, Train_acc 92.57, Test_acc 66.47
2025-01-11 02:52:27,680 [podnet.py] => Task 2, Epoch 114/160 (LR 0.01905) => LSC_loss 0.28, Spatial_loss 1.05, Flat_loss 0.14, Train_acc 92.42, Test_acc 67.33
2025-01-11 02:52:34,587 [podnet.py] => Task 2, Epoch 115/160 (LR 0.01828) => LSC_loss 0.27, Spatial_loss 1.04, Flat_loss 0.14, Train_acc 93.18, Test_acc 67.30
2025-01-11 02:52:41,430 [podnet.py] => Task 2, Epoch 116/160 (LR 0.01753) => LSC_loss 0.26, Spatial_loss 1.04, Flat_loss 0.14, Train_acc 93.37, Test_acc 67.80
2025-01-11 02:52:48,335 [podnet.py] => Task 2, Epoch 117/160 (LR 0.01679) => LSC_loss 0.27, Spatial_loss 1.03, Flat_loss 0.14, Train_acc 92.81, Test_acc 65.63
2025-01-11 02:52:55,130 [podnet.py] => Task 2, Epoch 118/160 (LR 0.01606) => LSC_loss 0.25, Spatial_loss 1.02, Flat_loss 0.13, Train_acc 93.69, Test_acc 67.23
2025-01-11 02:53:02,211 [podnet.py] => Task 2, Epoch 119/160 (LR 0.01535) => LSC_loss 0.25, Spatial_loss 1.02, Flat_loss 0.13, Train_acc 94.02, Test_acc 67.00
2025-01-11 02:53:09,136 [podnet.py] => Task 2, Epoch 120/160 (LR 0.01464) => LSC_loss 0.24, Spatial_loss 0.98, Flat_loss 0.13, Train_acc 94.03, Test_acc 67.30
2025-01-11 02:53:16,079 [podnet.py] => Task 2, Epoch 121/160 (LR 0.01396) => LSC_loss 0.24, Spatial_loss 0.99, Flat_loss 0.13, Train_acc 94.28, Test_acc 67.57
2025-01-11 02:53:23,030 [podnet.py] => Task 2, Epoch 122/160 (LR 0.01328) => LSC_loss 0.23, Spatial_loss 0.97, Flat_loss 0.13, Train_acc 94.33, Test_acc 66.17
2025-01-11 02:53:30,092 [podnet.py] => Task 2, Epoch 123/160 (LR 0.01262) => LSC_loss 0.23, Spatial_loss 0.97, Flat_loss 0.13, Train_acc 94.47, Test_acc 67.37
2025-01-11 02:53:37,386 [podnet.py] => Task 2, Epoch 124/160 (LR 0.01198) => LSC_loss 0.23, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 94.55, Test_acc 68.33
2025-01-11 02:53:44,451 [podnet.py] => Task 2, Epoch 125/160 (LR 0.01135) => LSC_loss 0.22, Spatial_loss 0.95, Flat_loss 0.13, Train_acc 94.75, Test_acc 67.87
2025-01-11 02:53:51,479 [podnet.py] => Task 2, Epoch 126/160 (LR 0.01073) => LSC_loss 0.22, Spatial_loss 0.94, Flat_loss 0.13, Train_acc 94.54, Test_acc 68.00
2025-01-11 02:53:58,399 [podnet.py] => Task 2, Epoch 127/160 (LR 0.01013) => LSC_loss 0.21, Spatial_loss 0.94, Flat_loss 0.13, Train_acc 95.11, Test_acc 69.27
2025-01-11 02:54:05,608 [podnet.py] => Task 2, Epoch 128/160 (LR 0.00955) => LSC_loss 0.21, Spatial_loss 0.94, Flat_loss 0.13, Train_acc 94.95, Test_acc 67.00
2025-01-11 02:54:12,659 [podnet.py] => Task 2, Epoch 129/160 (LR 0.00898) => LSC_loss 0.21, Spatial_loss 0.93, Flat_loss 0.13, Train_acc 95.04, Test_acc 68.80
2025-01-11 02:54:19,637 [podnet.py] => Task 2, Epoch 130/160 (LR 0.00843) => LSC_loss 0.21, Spatial_loss 0.93, Flat_loss 0.13, Train_acc 95.15, Test_acc 69.03
2025-01-11 02:54:26,400 [podnet.py] => Task 2, Epoch 131/160 (LR 0.00789) => LSC_loss 0.19, Spatial_loss 0.91, Flat_loss 0.12, Train_acc 95.70, Test_acc 68.87
2025-01-11 02:54:33,231 [podnet.py] => Task 2, Epoch 132/160 (LR 0.00737) => LSC_loss 0.18, Spatial_loss 0.90, Flat_loss 0.12, Train_acc 96.21, Test_acc 68.23
2025-01-11 02:54:40,131 [podnet.py] => Task 2, Epoch 133/160 (LR 0.00686) => LSC_loss 0.19, Spatial_loss 0.89, Flat_loss 0.12, Train_acc 95.87, Test_acc 69.23
2025-01-11 02:54:46,903 [podnet.py] => Task 2, Epoch 134/160 (LR 0.00638) => LSC_loss 0.18, Spatial_loss 0.87, Flat_loss 0.12, Train_acc 96.16, Test_acc 68.77
2025-01-11 02:54:53,726 [podnet.py] => Task 2, Epoch 135/160 (LR 0.00590) => LSC_loss 0.19, Spatial_loss 0.88, Flat_loss 0.12, Train_acc 95.62, Test_acc 68.97
2025-01-11 02:55:00,666 [podnet.py] => Task 2, Epoch 136/160 (LR 0.00545) => LSC_loss 0.18, Spatial_loss 0.87, Flat_loss 0.12, Train_acc 96.21, Test_acc 69.33
2025-01-11 02:55:07,674 [podnet.py] => Task 2, Epoch 137/160 (LR 0.00501) => LSC_loss 0.18, Spatial_loss 0.87, Flat_loss 0.12, Train_acc 96.15, Test_acc 68.57
2025-01-11 02:55:14,628 [podnet.py] => Task 2, Epoch 138/160 (LR 0.00459) => LSC_loss 0.17, Spatial_loss 0.85, Flat_loss 0.12, Train_acc 96.48, Test_acc 68.70
2025-01-11 02:55:21,511 [podnet.py] => Task 2, Epoch 139/160 (LR 0.00419) => LSC_loss 0.18, Spatial_loss 0.85, Flat_loss 0.12, Train_acc 96.29, Test_acc 69.07
2025-01-11 02:55:28,456 [podnet.py] => Task 2, Epoch 140/160 (LR 0.00381) => LSC_loss 0.17, Spatial_loss 0.85, Flat_loss 0.12, Train_acc 96.35, Test_acc 68.47
2025-01-11 02:55:35,299 [podnet.py] => Task 2, Epoch 141/160 (LR 0.00344) => LSC_loss 0.16, Spatial_loss 0.84, Flat_loss 0.12, Train_acc 96.98, Test_acc 68.73
2025-01-11 02:55:42,139 [podnet.py] => Task 2, Epoch 142/160 (LR 0.00309) => LSC_loss 0.16, Spatial_loss 0.84, Flat_loss 0.12, Train_acc 96.74, Test_acc 69.43
2025-01-11 02:55:48,892 [podnet.py] => Task 2, Epoch 143/160 (LR 0.00276) => LSC_loss 0.16, Spatial_loss 0.82, Flat_loss 0.12, Train_acc 97.01, Test_acc 68.83
2025-01-11 02:55:55,700 [podnet.py] => Task 2, Epoch 144/160 (LR 0.00245) => LSC_loss 0.16, Spatial_loss 0.82, Flat_loss 0.12, Train_acc 96.96, Test_acc 68.87
2025-01-11 02:56:02,702 [podnet.py] => Task 2, Epoch 145/160 (LR 0.00215) => LSC_loss 0.16, Spatial_loss 0.83, Flat_loss 0.12, Train_acc 96.86, Test_acc 68.97
2025-01-11 02:56:09,756 [podnet.py] => Task 2, Epoch 146/160 (LR 0.00188) => LSC_loss 0.16, Spatial_loss 0.83, Flat_loss 0.12, Train_acc 96.90, Test_acc 69.07
2025-01-11 02:56:16,540 [podnet.py] => Task 2, Epoch 147/160 (LR 0.00162) => LSC_loss 0.16, Spatial_loss 0.82, Flat_loss 0.12, Train_acc 97.17, Test_acc 69.07
2025-01-11 02:56:23,387 [podnet.py] => Task 2, Epoch 148/160 (LR 0.00138) => LSC_loss 0.16, Spatial_loss 0.82, Flat_loss 0.12, Train_acc 96.78, Test_acc 69.37
2025-01-11 02:56:30,281 [podnet.py] => Task 2, Epoch 149/160 (LR 0.00116) => LSC_loss 0.16, Spatial_loss 0.80, Flat_loss 0.12, Train_acc 96.98, Test_acc 69.53
2025-01-11 02:56:37,075 [podnet.py] => Task 2, Epoch 150/160 (LR 0.00096) => LSC_loss 0.16, Spatial_loss 0.81, Flat_loss 0.11, Train_acc 96.98, Test_acc 69.03
2025-01-11 02:56:43,903 [podnet.py] => Task 2, Epoch 151/160 (LR 0.00078) => LSC_loss 0.15, Spatial_loss 0.81, Flat_loss 0.12, Train_acc 97.26, Test_acc 69.20
2025-01-11 02:56:50,716 [podnet.py] => Task 2, Epoch 152/160 (LR 0.00062) => LSC_loss 0.15, Spatial_loss 0.80, Flat_loss 0.12, Train_acc 97.27, Test_acc 69.20
2025-01-11 02:56:57,684 [podnet.py] => Task 2, Epoch 153/160 (LR 0.00047) => LSC_loss 0.15, Spatial_loss 0.80, Flat_loss 0.11, Train_acc 97.21, Test_acc 69.53
2025-01-11 02:57:04,578 [podnet.py] => Task 2, Epoch 154/160 (LR 0.00035) => LSC_loss 0.15, Spatial_loss 0.80, Flat_loss 0.11, Train_acc 97.08, Test_acc 69.53
2025-01-11 02:57:11,515 [podnet.py] => Task 2, Epoch 155/160 (LR 0.00024) => LSC_loss 0.16, Spatial_loss 0.80, Flat_loss 0.11, Train_acc 96.99, Test_acc 69.50
2025-01-11 02:57:18,286 [podnet.py] => Task 2, Epoch 156/160 (LR 0.00015) => LSC_loss 0.15, Spatial_loss 0.79, Flat_loss 0.11, Train_acc 97.33, Test_acc 69.23
2025-01-11 02:57:25,234 [podnet.py] => Task 2, Epoch 157/160 (LR 0.00009) => LSC_loss 0.15, Spatial_loss 0.80, Flat_loss 0.11, Train_acc 97.27, Test_acc 69.43
2025-01-11 02:57:32,088 [podnet.py] => Task 2, Epoch 158/160 (LR 0.00004) => LSC_loss 0.15, Spatial_loss 0.79, Flat_loss 0.11, Train_acc 97.37, Test_acc 69.07
2025-01-11 02:57:39,145 [podnet.py] => Task 2, Epoch 159/160 (LR 0.00001) => LSC_loss 0.15, Spatial_loss 0.79, Flat_loss 0.11, Train_acc 97.07, Test_acc 69.43
2025-01-11 02:57:46,225 [podnet.py] => Task 2, Epoch 160/160 (LR 0.00000) => LSC_loss 0.15, Spatial_loss 0.79, Flat_loss 0.11, Train_acc 97.38, Test_acc 69.23
2025-01-11 02:57:46,226 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 02:57:46,227 [base.py] => Reducing exemplars...(370 per classes)
2025-01-11 02:57:59,477 [base.py] => Constructing exemplars...(370 per classes)
2025-01-11 02:58:18,091 [podnet.py] => The size of finetune dataset: 11100
2025-01-11 02:58:24,198 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.16, Spatial_loss 0.81, Flat_loss 0.10, Train_acc 96.98, Test_acc 68.90
2025-01-11 02:58:30,418 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.16, Spatial_loss 0.83, Flat_loss 0.10, Train_acc 96.79, Test_acc 68.63
2025-01-11 02:58:36,561 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.16, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 96.70, Test_acc 68.87
2025-01-11 02:58:42,838 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.16, Spatial_loss 0.83, Flat_loss 0.11, Train_acc 96.59, Test_acc 67.70
2025-01-11 02:58:49,130 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.16, Spatial_loss 0.82, Flat_loss 0.11, Train_acc 96.86, Test_acc 69.00
2025-01-11 02:58:55,318 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.16, Spatial_loss 0.84, Flat_loss 0.11, Train_acc 96.75, Test_acc 67.73
2025-01-11 02:59:01,747 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.15, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 97.12, Test_acc 68.63
2025-01-11 02:59:08,163 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.15, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 97.01, Test_acc 68.47
2025-01-11 02:59:14,657 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.15, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 97.08, Test_acc 69.60
2025-01-11 02:59:21,002 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.15, Spatial_loss 0.81, Flat_loss 0.10, Train_acc 96.98, Test_acc 69.43
2025-01-11 02:59:27,375 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.15, Spatial_loss 0.80, Flat_loss 0.10, Train_acc 96.94, Test_acc 68.47
2025-01-11 02:59:33,801 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.15, Spatial_loss 0.81, Flat_loss 0.11, Train_acc 97.20, Test_acc 68.53
2025-01-11 02:59:40,253 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.14, Spatial_loss 0.79, Flat_loss 0.10, Train_acc 97.34, Test_acc 68.87
2025-01-11 02:59:46,519 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.14, Spatial_loss 0.79, Flat_loss 0.10, Train_acc 97.32, Test_acc 68.63
2025-01-11 02:59:52,852 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.14, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 97.51, Test_acc 69.20
2025-01-11 02:59:59,114 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.14, Spatial_loss 0.79, Flat_loss 0.10, Train_acc 97.16, Test_acc 69.20
2025-01-11 03:00:05,833 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.14, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 97.26, Test_acc 69.17
2025-01-11 03:00:12,286 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.14, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 97.55, Test_acc 69.03
2025-01-11 03:00:18,671 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.13, Spatial_loss 0.77, Flat_loss 0.10, Train_acc 97.78, Test_acc 68.90
2025-01-11 03:00:25,018 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.14, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 97.48, Test_acc 68.93
2025-01-11 03:00:25,023 [base.py] => Reducing exemplars...(246 per classes)
2025-01-11 03:00:38,117 [base.py] => Constructing exemplars...(246 per classes)
2025-01-11 03:00:54,078 [podnet.py] => Exemplar size: 7380
2025-01-11 03:00:54,079 [trainer.py] => CNN: {'total': np.float64(68.93), '00-09': np.float64(81.5), '10-19': np.float64(57.9), '20-29': np.float64(67.4), 'old': np.float64(69.7), 'new': np.float64(67.4)}
2025-01-11 03:00:54,079 [trainer.py] => NME: {'total': np.float64(68.67), '00-09': np.float64(81.0), '10-19': np.float64(56.2), '20-29': np.float64(68.8), 'old': np.float64(68.6), 'new': np.float64(68.8)}
2025-01-11 03:00:54,079 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93)]
2025-01-11 03:00:54,079 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37)]
2025-01-11 03:00:54,079 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67)]
2025-01-11 03:00:54,079 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37)]

2025-01-11 03:00:54,079 [trainer.py] => All params: 485457
2025-01-11 03:00:54,080 [trainer.py] => Trainable params: 485457
2025-01-11 03:00:54,080 [podnet.py] => Learning on 30-40
2025-01-11 03:00:54,200 [podnet.py] => Adaptive factor: 2.0
2025-01-11 03:01:01,179 [podnet.py] => Task 3, Epoch 1/160 (LR 0.09999) => LSC_loss 2.10, Spatial_loss 2.31, Flat_loss 0.40, Train_acc 45.59, Test_acc 42.58
2025-01-11 03:01:08,048 [podnet.py] => Task 3, Epoch 2/160 (LR 0.09996) => LSC_loss 1.50, Spatial_loss 1.99, Flat_loss 0.24, Train_acc 56.65, Test_acc 52.12
2025-01-11 03:01:15,138 [podnet.py] => Task 3, Epoch 3/160 (LR 0.09991) => LSC_loss 1.35, Spatial_loss 1.90, Flat_loss 0.22, Train_acc 60.90, Test_acc 42.50
2025-01-11 03:01:22,131 [podnet.py] => Task 3, Epoch 4/160 (LR 0.09985) => LSC_loss 1.30, Spatial_loss 1.84, Flat_loss 0.21, Train_acc 62.42, Test_acc 50.22
2025-01-11 03:01:29,357 [podnet.py] => Task 3, Epoch 5/160 (LR 0.09976) => LSC_loss 1.25, Spatial_loss 1.79, Flat_loss 0.20, Train_acc 64.38, Test_acc 51.05
2025-01-11 03:01:36,630 [podnet.py] => Task 3, Epoch 6/160 (LR 0.09965) => LSC_loss 1.21, Spatial_loss 1.78, Flat_loss 0.20, Train_acc 65.23, Test_acc 51.55
2025-01-11 03:01:43,688 [podnet.py] => Task 3, Epoch 7/160 (LR 0.09953) => LSC_loss 1.18, Spatial_loss 1.77, Flat_loss 0.20, Train_acc 65.56, Test_acc 45.60
2025-01-11 03:01:50,573 [podnet.py] => Task 3, Epoch 8/160 (LR 0.09938) => LSC_loss 1.16, Spatial_loss 1.75, Flat_loss 0.20, Train_acc 66.22, Test_acc 52.45
2025-01-11 03:01:57,619 [podnet.py] => Task 3, Epoch 9/160 (LR 0.09922) => LSC_loss 1.15, Spatial_loss 1.74, Flat_loss 0.20, Train_acc 66.95, Test_acc 54.82
2025-01-11 03:02:04,713 [podnet.py] => Task 3, Epoch 10/160 (LR 0.09904) => LSC_loss 1.10, Spatial_loss 1.71, Flat_loss 0.19, Train_acc 67.91, Test_acc 54.10
2025-01-11 03:02:11,815 [podnet.py] => Task 3, Epoch 11/160 (LR 0.09884) => LSC_loss 1.09, Spatial_loss 1.74, Flat_loss 0.20, Train_acc 68.32, Test_acc 50.20
2025-01-11 03:02:18,989 [podnet.py] => Task 3, Epoch 12/160 (LR 0.09862) => LSC_loss 1.10, Spatial_loss 1.77, Flat_loss 0.20, Train_acc 67.94, Test_acc 55.02
2025-01-11 03:02:26,532 [podnet.py] => Task 3, Epoch 13/160 (LR 0.09838) => LSC_loss 1.08, Spatial_loss 1.73, Flat_loss 0.20, Train_acc 69.07, Test_acc 47.80
2025-01-11 03:02:33,862 [podnet.py] => Task 3, Epoch 14/160 (LR 0.09812) => LSC_loss 1.06, Spatial_loss 1.71, Flat_loss 0.20, Train_acc 69.53, Test_acc 53.80
2025-01-11 03:02:41,034 [podnet.py] => Task 3, Epoch 15/160 (LR 0.09785) => LSC_loss 1.05, Spatial_loss 1.70, Flat_loss 0.20, Train_acc 69.81, Test_acc 52.10
2025-01-11 03:02:48,223 [podnet.py] => Task 3, Epoch 16/160 (LR 0.09755) => LSC_loss 1.02, Spatial_loss 1.67, Flat_loss 0.19, Train_acc 70.68, Test_acc 48.85
2025-01-11 03:02:55,376 [podnet.py] => Task 3, Epoch 17/160 (LR 0.09724) => LSC_loss 1.05, Spatial_loss 1.74, Flat_loss 0.20, Train_acc 70.11, Test_acc 44.80
2025-01-11 03:03:02,339 [podnet.py] => Task 3, Epoch 18/160 (LR 0.09691) => LSC_loss 1.03, Spatial_loss 1.71, Flat_loss 0.20, Train_acc 70.53, Test_acc 53.18
2025-01-11 03:03:09,625 [podnet.py] => Task 3, Epoch 19/160 (LR 0.09656) => LSC_loss 0.99, Spatial_loss 1.73, Flat_loss 0.20, Train_acc 71.63, Test_acc 51.55
2025-01-11 03:03:16,725 [podnet.py] => Task 3, Epoch 20/160 (LR 0.09619) => LSC_loss 1.01, Spatial_loss 1.70, Flat_loss 0.20, Train_acc 70.70, Test_acc 53.32
2025-01-11 03:03:23,989 [podnet.py] => Task 3, Epoch 21/160 (LR 0.09581) => LSC_loss 1.00, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 70.86, Test_acc 51.15
2025-01-11 03:03:31,229 [podnet.py] => Task 3, Epoch 22/160 (LR 0.09541) => LSC_loss 0.98, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 72.18, Test_acc 50.58
2025-01-11 03:03:38,603 [podnet.py] => Task 3, Epoch 23/160 (LR 0.09499) => LSC_loss 0.98, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 71.76, Test_acc 55.78
2025-01-11 03:03:45,876 [podnet.py] => Task 3, Epoch 24/160 (LR 0.09455) => LSC_loss 0.97, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 72.19, Test_acc 47.58
2025-01-11 03:03:52,914 [podnet.py] => Task 3, Epoch 25/160 (LR 0.09410) => LSC_loss 0.96, Spatial_loss 1.71, Flat_loss 0.20, Train_acc 72.61, Test_acc 53.98
2025-01-11 03:03:59,957 [podnet.py] => Task 3, Epoch 26/160 (LR 0.09362) => LSC_loss 0.92, Spatial_loss 1.67, Flat_loss 0.19, Train_acc 73.80, Test_acc 53.18
2025-01-11 03:04:07,682 [podnet.py] => Task 3, Epoch 27/160 (LR 0.09314) => LSC_loss 0.95, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 72.54, Test_acc 50.62
2025-01-11 03:04:15,017 [podnet.py] => Task 3, Epoch 28/160 (LR 0.09263) => LSC_loss 0.95, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 72.39, Test_acc 53.30
2025-01-11 03:04:22,096 [podnet.py] => Task 3, Epoch 29/160 (LR 0.09211) => LSC_loss 0.93, Spatial_loss 1.66, Flat_loss 0.20, Train_acc 73.31, Test_acc 54.92
2025-01-11 03:04:29,201 [podnet.py] => Task 3, Epoch 30/160 (LR 0.09157) => LSC_loss 0.95, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 72.92, Test_acc 48.48
2025-01-11 03:04:36,185 [podnet.py] => Task 3, Epoch 31/160 (LR 0.09102) => LSC_loss 0.93, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 73.42, Test_acc 50.98
2025-01-11 03:04:43,063 [podnet.py] => Task 3, Epoch 32/160 (LR 0.09045) => LSC_loss 0.92, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 73.51, Test_acc 55.58
2025-01-11 03:04:50,085 [podnet.py] => Task 3, Epoch 33/160 (LR 0.08987) => LSC_loss 0.91, Spatial_loss 1.65, Flat_loss 0.20, Train_acc 74.03, Test_acc 51.22
2025-01-11 03:04:57,026 [podnet.py] => Task 3, Epoch 34/160 (LR 0.08927) => LSC_loss 0.91, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 73.59, Test_acc 56.88
2025-01-11 03:05:03,876 [podnet.py] => Task 3, Epoch 35/160 (LR 0.08865) => LSC_loss 0.91, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 73.70, Test_acc 50.82
2025-01-11 03:05:10,825 [podnet.py] => Task 3, Epoch 36/160 (LR 0.08802) => LSC_loss 0.90, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 74.05, Test_acc 52.22
2025-01-11 03:05:17,852 [podnet.py] => Task 3, Epoch 37/160 (LR 0.08738) => LSC_loss 0.89, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 74.47, Test_acc 56.50
2025-01-11 03:05:24,838 [podnet.py] => Task 3, Epoch 38/160 (LR 0.08672) => LSC_loss 0.90, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 74.14, Test_acc 55.60
2025-01-11 03:05:31,750 [podnet.py] => Task 3, Epoch 39/160 (LR 0.08604) => LSC_loss 0.89, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 74.54, Test_acc 54.30
2025-01-11 03:05:38,778 [podnet.py] => Task 3, Epoch 40/160 (LR 0.08536) => LSC_loss 0.86, Spatial_loss 1.61, Flat_loss 0.19, Train_acc 75.54, Test_acc 54.28
2025-01-11 03:05:45,870 [podnet.py] => Task 3, Epoch 41/160 (LR 0.08465) => LSC_loss 0.89, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 74.38, Test_acc 51.15
2025-01-11 03:05:52,882 [podnet.py] => Task 3, Epoch 42/160 (LR 0.08394) => LSC_loss 0.86, Spatial_loss 1.63, Flat_loss 0.20, Train_acc 75.35, Test_acc 52.12
2025-01-11 03:05:59,923 [podnet.py] => Task 3, Epoch 43/160 (LR 0.08321) => LSC_loss 0.86, Spatial_loss 1.66, Flat_loss 0.20, Train_acc 75.58, Test_acc 55.32
2025-01-11 03:06:07,051 [podnet.py] => Task 3, Epoch 44/160 (LR 0.08247) => LSC_loss 0.85, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 75.76, Test_acc 54.70
2025-01-11 03:06:14,109 [podnet.py] => Task 3, Epoch 45/160 (LR 0.08172) => LSC_loss 0.84, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 76.11, Test_acc 53.78
2025-01-11 03:06:21,075 [podnet.py] => Task 3, Epoch 46/160 (LR 0.08095) => LSC_loss 0.83, Spatial_loss 1.62, Flat_loss 0.19, Train_acc 76.14, Test_acc 54.45
2025-01-11 03:06:27,980 [podnet.py] => Task 3, Epoch 47/160 (LR 0.08018) => LSC_loss 0.84, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 76.21, Test_acc 52.08
2025-01-11 03:06:34,895 [podnet.py] => Task 3, Epoch 48/160 (LR 0.07939) => LSC_loss 0.82, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 76.24, Test_acc 54.50
2025-01-11 03:06:41,881 [podnet.py] => Task 3, Epoch 49/160 (LR 0.07859) => LSC_loss 0.85, Spatial_loss 1.65, Flat_loss 0.20, Train_acc 75.78, Test_acc 53.32
2025-01-11 03:06:49,041 [podnet.py] => Task 3, Epoch 50/160 (LR 0.07778) => LSC_loss 0.83, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 76.17, Test_acc 53.75
2025-01-11 03:06:55,957 [podnet.py] => Task 3, Epoch 51/160 (LR 0.07696) => LSC_loss 0.81, Spatial_loss 1.63, Flat_loss 0.19, Train_acc 76.88, Test_acc 58.08
2025-01-11 03:07:02,993 [podnet.py] => Task 3, Epoch 52/160 (LR 0.07612) => LSC_loss 0.80, Spatial_loss 1.60, Flat_loss 0.19, Train_acc 76.90, Test_acc 56.55
2025-01-11 03:07:09,953 [podnet.py] => Task 3, Epoch 53/160 (LR 0.07528) => LSC_loss 0.81, Spatial_loss 1.63, Flat_loss 0.20, Train_acc 76.49, Test_acc 56.48
2025-01-11 03:07:16,985 [podnet.py] => Task 3, Epoch 54/160 (LR 0.07443) => LSC_loss 0.81, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 77.00, Test_acc 54.85
2025-01-11 03:07:23,853 [podnet.py] => Task 3, Epoch 55/160 (LR 0.07357) => LSC_loss 0.79, Spatial_loss 1.61, Flat_loss 0.20, Train_acc 76.95, Test_acc 56.12
2025-01-11 03:07:31,021 [podnet.py] => Task 3, Epoch 56/160 (LR 0.07270) => LSC_loss 0.78, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 77.69, Test_acc 50.52
2025-01-11 03:07:37,935 [podnet.py] => Task 3, Epoch 57/160 (LR 0.07182) => LSC_loss 0.80, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 76.99, Test_acc 48.95
2025-01-11 03:07:44,963 [podnet.py] => Task 3, Epoch 58/160 (LR 0.07093) => LSC_loss 0.78, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 77.18, Test_acc 57.25
2025-01-11 03:07:52,073 [podnet.py] => Task 3, Epoch 59/160 (LR 0.07004) => LSC_loss 0.78, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 77.41, Test_acc 56.65
2025-01-11 03:07:59,074 [podnet.py] => Task 3, Epoch 60/160 (LR 0.06913) => LSC_loss 0.77, Spatial_loss 1.61, Flat_loss 0.20, Train_acc 77.95, Test_acc 54.20
2025-01-11 03:08:07,386 [podnet.py] => Task 3, Epoch 61/160 (LR 0.06822) => LSC_loss 0.77, Spatial_loss 1.59, Flat_loss 0.19, Train_acc 77.84, Test_acc 54.25
2025-01-11 03:08:14,429 [podnet.py] => Task 3, Epoch 62/160 (LR 0.06731) => LSC_loss 0.77, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 78.09, Test_acc 54.42
2025-01-11 03:08:21,420 [podnet.py] => Task 3, Epoch 63/160 (LR 0.06638) => LSC_loss 0.75, Spatial_loss 1.58, Flat_loss 0.19, Train_acc 78.58, Test_acc 54.08
2025-01-11 03:08:28,465 [podnet.py] => Task 3, Epoch 64/160 (LR 0.06545) => LSC_loss 0.74, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 79.26, Test_acc 56.78
2025-01-11 03:08:35,507 [podnet.py] => Task 3, Epoch 65/160 (LR 0.06451) => LSC_loss 0.75, Spatial_loss 1.60, Flat_loss 0.19, Train_acc 78.37, Test_acc 58.22
2025-01-11 03:08:42,682 [podnet.py] => Task 3, Epoch 66/160 (LR 0.06357) => LSC_loss 0.74, Spatial_loss 1.57, Flat_loss 0.19, Train_acc 78.74, Test_acc 56.38
2025-01-11 03:08:50,077 [podnet.py] => Task 3, Epoch 67/160 (LR 0.06262) => LSC_loss 0.71, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 79.74, Test_acc 57.72
2025-01-11 03:08:57,042 [podnet.py] => Task 3, Epoch 68/160 (LR 0.06167) => LSC_loss 0.72, Spatial_loss 1.55, Flat_loss 0.19, Train_acc 78.96, Test_acc 56.35
2025-01-11 03:09:04,262 [podnet.py] => Task 3, Epoch 69/160 (LR 0.06072) => LSC_loss 0.72, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 79.85, Test_acc 52.70
2025-01-11 03:09:11,514 [podnet.py] => Task 3, Epoch 70/160 (LR 0.05975) => LSC_loss 0.72, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 79.06, Test_acc 59.15
2025-01-11 03:09:18,697 [podnet.py] => Task 3, Epoch 71/160 (LR 0.05879) => LSC_loss 0.70, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 79.67, Test_acc 56.70
2025-01-11 03:09:25,909 [podnet.py] => Task 3, Epoch 72/160 (LR 0.05782) => LSC_loss 0.71, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 79.82, Test_acc 50.20
2025-01-11 03:09:33,047 [podnet.py] => Task 3, Epoch 73/160 (LR 0.05685) => LSC_loss 0.69, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 80.22, Test_acc 59.32
2025-01-11 03:09:40,260 [podnet.py] => Task 3, Epoch 74/160 (LR 0.05588) => LSC_loss 0.68, Spatial_loss 1.53, Flat_loss 0.19, Train_acc 80.95, Test_acc 56.68
2025-01-11 03:09:47,293 [podnet.py] => Task 3, Epoch 75/160 (LR 0.05490) => LSC_loss 0.67, Spatial_loss 1.53, Flat_loss 0.19, Train_acc 80.82, Test_acc 54.20
2025-01-11 03:09:54,412 [podnet.py] => Task 3, Epoch 76/160 (LR 0.05392) => LSC_loss 0.66, Spatial_loss 1.51, Flat_loss 0.19, Train_acc 81.48, Test_acc 57.10
2025-01-11 03:10:01,708 [podnet.py] => Task 3, Epoch 77/160 (LR 0.05294) => LSC_loss 0.68, Spatial_loss 1.52, Flat_loss 0.19, Train_acc 80.58, Test_acc 57.65
2025-01-11 03:10:08,732 [podnet.py] => Task 3, Epoch 78/160 (LR 0.05196) => LSC_loss 0.65, Spatial_loss 1.49, Flat_loss 0.19, Train_acc 81.48, Test_acc 56.85
2025-01-11 03:10:16,160 [podnet.py] => Task 3, Epoch 79/160 (LR 0.05098) => LSC_loss 0.64, Spatial_loss 1.46, Flat_loss 0.18, Train_acc 82.28, Test_acc 56.42
2025-01-11 03:10:23,237 [podnet.py] => Task 3, Epoch 80/160 (LR 0.05000) => LSC_loss 0.63, Spatial_loss 1.47, Flat_loss 0.18, Train_acc 82.25, Test_acc 58.58
2025-01-11 03:10:30,575 [podnet.py] => Task 3, Epoch 81/160 (LR 0.04902) => LSC_loss 0.62, Spatial_loss 1.45, Flat_loss 0.18, Train_acc 82.51, Test_acc 52.25
2025-01-11 03:10:37,951 [podnet.py] => Task 3, Epoch 82/160 (LR 0.04804) => LSC_loss 0.65, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 81.28, Test_acc 56.80
2025-01-11 03:10:45,113 [podnet.py] => Task 3, Epoch 83/160 (LR 0.04706) => LSC_loss 0.62, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 82.68, Test_acc 57.32
2025-01-11 03:10:52,151 [podnet.py] => Task 3, Epoch 84/160 (LR 0.04608) => LSC_loss 0.61, Spatial_loss 1.47, Flat_loss 0.18, Train_acc 82.55, Test_acc 56.22
2025-01-11 03:10:59,561 [podnet.py] => Task 3, Epoch 85/160 (LR 0.04510) => LSC_loss 0.60, Spatial_loss 1.46, Flat_loss 0.18, Train_acc 82.94, Test_acc 57.78
2025-01-11 03:11:06,780 [podnet.py] => Task 3, Epoch 86/160 (LR 0.04412) => LSC_loss 0.58, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 83.64, Test_acc 55.90
2025-01-11 03:11:14,031 [podnet.py] => Task 3, Epoch 87/160 (LR 0.04315) => LSC_loss 0.60, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 82.86, Test_acc 52.85
2025-01-11 03:11:21,318 [podnet.py] => Task 3, Epoch 88/160 (LR 0.04218) => LSC_loss 0.57, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 84.26, Test_acc 57.30
2025-01-11 03:11:28,986 [podnet.py] => Task 3, Epoch 89/160 (LR 0.04121) => LSC_loss 0.58, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 83.63, Test_acc 56.42
2025-01-11 03:11:35,991 [podnet.py] => Task 3, Epoch 90/160 (LR 0.04025) => LSC_loss 0.57, Spatial_loss 1.40, Flat_loss 0.18, Train_acc 83.99, Test_acc 55.12
2025-01-11 03:11:43,157 [podnet.py] => Task 3, Epoch 91/160 (LR 0.03928) => LSC_loss 0.57, Spatial_loss 1.38, Flat_loss 0.18, Train_acc 83.80, Test_acc 57.40
2025-01-11 03:11:50,239 [podnet.py] => Task 3, Epoch 92/160 (LR 0.03833) => LSC_loss 0.55, Spatial_loss 1.39, Flat_loss 0.17, Train_acc 84.47, Test_acc 57.65
2025-01-11 03:11:57,344 [podnet.py] => Task 3, Epoch 93/160 (LR 0.03738) => LSC_loss 0.54, Spatial_loss 1.38, Flat_loss 0.17, Train_acc 85.04, Test_acc 57.20
2025-01-11 03:12:04,719 [podnet.py] => Task 3, Epoch 94/160 (LR 0.03643) => LSC_loss 0.54, Spatial_loss 1.39, Flat_loss 0.18, Train_acc 85.06, Test_acc 55.50
2025-01-11 03:12:11,882 [podnet.py] => Task 3, Epoch 95/160 (LR 0.03549) => LSC_loss 0.53, Spatial_loss 1.39, Flat_loss 0.17, Train_acc 85.21, Test_acc 58.38
2025-01-11 03:12:18,923 [podnet.py] => Task 3, Epoch 96/160 (LR 0.03455) => LSC_loss 0.51, Spatial_loss 1.34, Flat_loss 0.17, Train_acc 86.06, Test_acc 58.38
2025-01-11 03:12:26,236 [podnet.py] => Task 3, Epoch 97/160 (LR 0.03362) => LSC_loss 0.52, Spatial_loss 1.36, Flat_loss 0.17, Train_acc 85.53, Test_acc 56.45
2025-01-11 03:12:33,516 [podnet.py] => Task 3, Epoch 98/160 (LR 0.03269) => LSC_loss 0.49, Spatial_loss 1.32, Flat_loss 0.17, Train_acc 86.58, Test_acc 59.05
2025-01-11 03:12:40,606 [podnet.py] => Task 3, Epoch 99/160 (LR 0.03178) => LSC_loss 0.49, Spatial_loss 1.33, Flat_loss 0.17, Train_acc 86.35, Test_acc 56.38
2025-01-11 03:12:47,598 [podnet.py] => Task 3, Epoch 100/160 (LR 0.03087) => LSC_loss 0.48, Spatial_loss 1.30, Flat_loss 0.16, Train_acc 87.16, Test_acc 60.62
2025-01-11 03:12:54,544 [podnet.py] => Task 3, Epoch 101/160 (LR 0.02996) => LSC_loss 0.47, Spatial_loss 1.31, Flat_loss 0.17, Train_acc 87.30, Test_acc 61.40
2025-01-11 03:13:01,640 [podnet.py] => Task 3, Epoch 102/160 (LR 0.02907) => LSC_loss 0.49, Spatial_loss 1.30, Flat_loss 0.17, Train_acc 86.29, Test_acc 58.05
2025-01-11 03:13:08,952 [podnet.py] => Task 3, Epoch 103/160 (LR 0.02818) => LSC_loss 0.47, Spatial_loss 1.29, Flat_loss 0.17, Train_acc 87.33, Test_acc 60.45
2025-01-11 03:13:15,937 [podnet.py] => Task 3, Epoch 104/160 (LR 0.02730) => LSC_loss 0.47, Spatial_loss 1.28, Flat_loss 0.17, Train_acc 87.23, Test_acc 58.85
2025-01-11 03:13:22,844 [podnet.py] => Task 3, Epoch 105/160 (LR 0.02643) => LSC_loss 0.46, Spatial_loss 1.28, Flat_loss 0.16, Train_acc 87.76, Test_acc 60.25
2025-01-11 03:13:29,800 [podnet.py] => Task 3, Epoch 106/160 (LR 0.02557) => LSC_loss 0.44, Spatial_loss 1.26, Flat_loss 0.16, Train_acc 88.53, Test_acc 59.00
2025-01-11 03:13:36,806 [podnet.py] => Task 3, Epoch 107/160 (LR 0.02472) => LSC_loss 0.44, Spatial_loss 1.27, Flat_loss 0.16, Train_acc 87.67, Test_acc 59.25
2025-01-11 03:13:43,959 [podnet.py] => Task 3, Epoch 108/160 (LR 0.02388) => LSC_loss 0.42, Spatial_loss 1.22, Flat_loss 0.16, Train_acc 89.16, Test_acc 59.18
2025-01-11 03:13:51,131 [podnet.py] => Task 3, Epoch 109/160 (LR 0.02304) => LSC_loss 0.41, Spatial_loss 1.22, Flat_loss 0.16, Train_acc 89.04, Test_acc 58.70
2025-01-11 03:13:58,094 [podnet.py] => Task 3, Epoch 110/160 (LR 0.02222) => LSC_loss 0.40, Spatial_loss 1.22, Flat_loss 0.16, Train_acc 89.47, Test_acc 61.35
2025-01-11 03:14:05,091 [podnet.py] => Task 3, Epoch 111/160 (LR 0.02141) => LSC_loss 0.40, Spatial_loss 1.20, Flat_loss 0.16, Train_acc 89.29, Test_acc 60.58
2025-01-11 03:14:12,320 [podnet.py] => Task 3, Epoch 112/160 (LR 0.02061) => LSC_loss 0.40, Spatial_loss 1.20, Flat_loss 0.15, Train_acc 89.52, Test_acc 58.40
2025-01-11 03:14:19,227 [podnet.py] => Task 3, Epoch 113/160 (LR 0.01982) => LSC_loss 0.39, Spatial_loss 1.20, Flat_loss 0.16, Train_acc 89.89, Test_acc 58.80
2025-01-11 03:14:26,226 [podnet.py] => Task 3, Epoch 114/160 (LR 0.01905) => LSC_loss 0.39, Spatial_loss 1.19, Flat_loss 0.16, Train_acc 89.83, Test_acc 59.55
2025-01-11 03:14:33,223 [podnet.py] => Task 3, Epoch 115/160 (LR 0.01828) => LSC_loss 0.37, Spatial_loss 1.17, Flat_loss 0.15, Train_acc 90.39, Test_acc 61.45
2025-01-11 03:14:40,129 [podnet.py] => Task 3, Epoch 116/160 (LR 0.01753) => LSC_loss 0.36, Spatial_loss 1.16, Flat_loss 0.15, Train_acc 91.22, Test_acc 59.75
2025-01-11 03:14:47,205 [podnet.py] => Task 3, Epoch 117/160 (LR 0.01679) => LSC_loss 0.36, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 90.76, Test_acc 60.42
2025-01-11 03:14:54,164 [podnet.py] => Task 3, Epoch 118/160 (LR 0.01606) => LSC_loss 0.35, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 91.11, Test_acc 60.50
2025-01-11 03:15:01,039 [podnet.py] => Task 3, Epoch 119/160 (LR 0.01535) => LSC_loss 0.35, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 91.54, Test_acc 62.18
2025-01-11 03:15:07,914 [podnet.py] => Task 3, Epoch 120/160 (LR 0.01464) => LSC_loss 0.33, Spatial_loss 1.12, Flat_loss 0.15, Train_acc 92.12, Test_acc 62.10
2025-01-11 03:15:14,917 [podnet.py] => Task 3, Epoch 121/160 (LR 0.01396) => LSC_loss 0.33, Spatial_loss 1.13, Flat_loss 0.15, Train_acc 91.80, Test_acc 60.78
2025-01-11 03:15:21,885 [podnet.py] => Task 3, Epoch 122/160 (LR 0.01328) => LSC_loss 0.33, Spatial_loss 1.11, Flat_loss 0.15, Train_acc 91.88, Test_acc 60.75
2025-01-11 03:15:28,922 [podnet.py] => Task 3, Epoch 123/160 (LR 0.01262) => LSC_loss 0.33, Spatial_loss 1.09, Flat_loss 0.14, Train_acc 91.95, Test_acc 59.68
2025-01-11 03:15:35,841 [podnet.py] => Task 3, Epoch 124/160 (LR 0.01198) => LSC_loss 0.31, Spatial_loss 1.08, Flat_loss 0.14, Train_acc 92.29, Test_acc 62.30
2025-01-11 03:15:42,764 [podnet.py] => Task 3, Epoch 125/160 (LR 0.01135) => LSC_loss 0.31, Spatial_loss 1.07, Flat_loss 0.14, Train_acc 92.51, Test_acc 61.40
2025-01-11 03:15:49,732 [podnet.py] => Task 3, Epoch 126/160 (LR 0.01073) => LSC_loss 0.30, Spatial_loss 1.06, Flat_loss 0.14, Train_acc 92.92, Test_acc 60.75
2025-01-11 03:15:56,819 [podnet.py] => Task 3, Epoch 127/160 (LR 0.01013) => LSC_loss 0.30, Spatial_loss 1.06, Flat_loss 0.14, Train_acc 92.79, Test_acc 61.98
2025-01-11 03:16:04,114 [podnet.py] => Task 3, Epoch 128/160 (LR 0.00955) => LSC_loss 0.29, Spatial_loss 1.05, Flat_loss 0.14, Train_acc 93.27, Test_acc 62.12
2025-01-11 03:16:11,348 [podnet.py] => Task 3, Epoch 129/160 (LR 0.00898) => LSC_loss 0.29, Spatial_loss 1.04, Flat_loss 0.14, Train_acc 93.02, Test_acc 60.80
2025-01-11 03:16:18,223 [podnet.py] => Task 3, Epoch 130/160 (LR 0.00843) => LSC_loss 0.28, Spatial_loss 1.02, Flat_loss 0.14, Train_acc 93.78, Test_acc 62.28
2025-01-11 03:16:25,173 [podnet.py] => Task 3, Epoch 131/160 (LR 0.00789) => LSC_loss 0.28, Spatial_loss 1.02, Flat_loss 0.14, Train_acc 93.48, Test_acc 62.80
2025-01-11 03:16:32,073 [podnet.py] => Task 3, Epoch 132/160 (LR 0.00737) => LSC_loss 0.27, Spatial_loss 1.01, Flat_loss 0.14, Train_acc 94.08, Test_acc 62.58
2025-01-11 03:16:38,948 [podnet.py] => Task 3, Epoch 133/160 (LR 0.00686) => LSC_loss 0.27, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 94.09, Test_acc 62.25
2025-01-11 03:16:45,836 [podnet.py] => Task 3, Epoch 134/160 (LR 0.00638) => LSC_loss 0.27, Spatial_loss 0.99, Flat_loss 0.14, Train_acc 94.14, Test_acc 62.55
2025-01-11 03:16:52,951 [podnet.py] => Task 3, Epoch 135/160 (LR 0.00590) => LSC_loss 0.26, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 94.38, Test_acc 62.70
2025-01-11 03:16:59,973 [podnet.py] => Task 3, Epoch 136/160 (LR 0.00545) => LSC_loss 0.26, Spatial_loss 0.99, Flat_loss 0.13, Train_acc 94.30, Test_acc 63.22
2025-01-11 03:17:06,985 [podnet.py] => Task 3, Epoch 137/160 (LR 0.00501) => LSC_loss 0.26, Spatial_loss 0.98, Flat_loss 0.13, Train_acc 94.49, Test_acc 63.32
2025-01-11 03:17:13,979 [podnet.py] => Task 3, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 0.97, Flat_loss 0.13, Train_acc 94.61, Test_acc 63.35
2025-01-11 03:17:20,968 [podnet.py] => Task 3, Epoch 139/160 (LR 0.00419) => LSC_loss 0.25, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 94.55, Test_acc 63.00
2025-01-11 03:17:27,962 [podnet.py] => Task 3, Epoch 140/160 (LR 0.00381) => LSC_loss 0.24, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 95.08, Test_acc 62.90
2025-01-11 03:17:34,847 [podnet.py] => Task 3, Epoch 141/160 (LR 0.00344) => LSC_loss 0.24, Spatial_loss 0.95, Flat_loss 0.13, Train_acc 95.08, Test_acc 63.15
2025-01-11 03:17:41,904 [podnet.py] => Task 3, Epoch 142/160 (LR 0.00309) => LSC_loss 0.24, Spatial_loss 0.95, Flat_loss 0.13, Train_acc 95.04, Test_acc 63.02
2025-01-11 03:17:48,786 [podnet.py] => Task 3, Epoch 143/160 (LR 0.00276) => LSC_loss 0.23, Spatial_loss 0.93, Flat_loss 0.13, Train_acc 95.40, Test_acc 62.98
2025-01-11 03:17:55,776 [podnet.py] => Task 3, Epoch 144/160 (LR 0.00245) => LSC_loss 0.24, Spatial_loss 0.94, Flat_loss 0.13, Train_acc 95.06, Test_acc 63.30
2025-01-11 03:18:02,689 [podnet.py] => Task 3, Epoch 145/160 (LR 0.00215) => LSC_loss 0.23, Spatial_loss 0.93, Flat_loss 0.13, Train_acc 95.10, Test_acc 63.10
2025-01-11 03:18:09,834 [podnet.py] => Task 3, Epoch 146/160 (LR 0.00188) => LSC_loss 0.24, Spatial_loss 0.93, Flat_loss 0.13, Train_acc 95.16, Test_acc 63.12
2025-01-11 03:18:16,979 [podnet.py] => Task 3, Epoch 147/160 (LR 0.00162) => LSC_loss 0.23, Spatial_loss 0.91, Flat_loss 0.13, Train_acc 95.63, Test_acc 63.35
2025-01-11 03:18:24,036 [podnet.py] => Task 3, Epoch 148/160 (LR 0.00138) => LSC_loss 0.23, Spatial_loss 0.92, Flat_loss 0.13, Train_acc 95.19, Test_acc 63.30
2025-01-11 03:18:31,109 [podnet.py] => Task 3, Epoch 149/160 (LR 0.00116) => LSC_loss 0.23, Spatial_loss 0.91, Flat_loss 0.13, Train_acc 95.63, Test_acc 63.18
2025-01-11 03:18:38,066 [podnet.py] => Task 3, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 0.92, Flat_loss 0.13, Train_acc 95.68, Test_acc 63.72
2025-01-11 03:18:45,085 [podnet.py] => Task 3, Epoch 151/160 (LR 0.00078) => LSC_loss 0.23, Spatial_loss 0.90, Flat_loss 0.13, Train_acc 95.49, Test_acc 63.35
2025-01-11 03:18:51,964 [podnet.py] => Task 3, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 0.90, Flat_loss 0.13, Train_acc 96.00, Test_acc 63.65
2025-01-11 03:18:58,900 [podnet.py] => Task 3, Epoch 153/160 (LR 0.00047) => LSC_loss 0.22, Spatial_loss 0.92, Flat_loss 0.13, Train_acc 95.98, Test_acc 63.42
2025-01-11 03:19:05,842 [podnet.py] => Task 3, Epoch 154/160 (LR 0.00035) => LSC_loss 0.23, Spatial_loss 0.90, Flat_loss 0.13, Train_acc 95.50, Test_acc 63.60
2025-01-11 03:19:12,794 [podnet.py] => Task 3, Epoch 155/160 (LR 0.00024) => LSC_loss 0.22, Spatial_loss 0.90, Flat_loss 0.13, Train_acc 95.82, Test_acc 63.48
2025-01-11 03:19:19,729 [podnet.py] => Task 3, Epoch 156/160 (LR 0.00015) => LSC_loss 0.22, Spatial_loss 0.89, Flat_loss 0.13, Train_acc 96.08, Test_acc 63.25
2025-01-11 03:19:26,780 [podnet.py] => Task 3, Epoch 157/160 (LR 0.00009) => LSC_loss 0.22, Spatial_loss 0.88, Flat_loss 0.12, Train_acc 95.89, Test_acc 63.48
2025-01-11 03:19:33,737 [podnet.py] => Task 3, Epoch 158/160 (LR 0.00004) => LSC_loss 0.22, Spatial_loss 0.90, Flat_loss 0.13, Train_acc 95.95, Test_acc 63.45
2025-01-11 03:19:40,671 [podnet.py] => Task 3, Epoch 159/160 (LR 0.00001) => LSC_loss 0.22, Spatial_loss 0.88, Flat_loss 0.13, Train_acc 95.90, Test_acc 63.55
2025-01-11 03:19:47,616 [podnet.py] => Task 3, Epoch 160/160 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 0.89, Flat_loss 0.13, Train_acc 95.99, Test_acc 63.30
2025-01-11 03:19:47,617 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 03:19:47,617 [base.py] => Reducing exemplars...(246 per classes)
2025-01-11 03:20:07,380 [base.py] => Constructing exemplars...(246 per classes)
2025-01-11 03:20:21,825 [podnet.py] => The size of finetune dataset: 9840
2025-01-11 03:20:27,698 [podnet.py] => Task 3, Epoch 1/20 (LR 0.00497) => LSC_loss 0.19, Spatial_loss 0.93, Flat_loss 0.10, Train_acc 96.24, Test_acc 62.55
2025-01-11 03:20:33,621 [podnet.py] => Task 3, Epoch 2/20 (LR 0.00488) => LSC_loss 0.20, Spatial_loss 0.94, Flat_loss 0.10, Train_acc 96.02, Test_acc 62.40
2025-01-11 03:20:39,564 [podnet.py] => Task 3, Epoch 3/20 (LR 0.00473) => LSC_loss 0.21, Spatial_loss 0.93, Flat_loss 0.10, Train_acc 95.59, Test_acc 62.98
2025-01-11 03:20:45,468 [podnet.py] => Task 3, Epoch 4/20 (LR 0.00452) => LSC_loss 0.20, Spatial_loss 0.94, Flat_loss 0.10, Train_acc 95.65, Test_acc 62.22
2025-01-11 03:20:51,128 [podnet.py] => Task 3, Epoch 5/20 (LR 0.00427) => LSC_loss 0.21, Spatial_loss 0.94, Flat_loss 0.10, Train_acc 95.56, Test_acc 62.90
2025-01-11 03:20:56,870 [podnet.py] => Task 3, Epoch 6/20 (LR 0.00397) => LSC_loss 0.20, Spatial_loss 0.93, Flat_loss 0.10, Train_acc 95.70, Test_acc 63.18
2025-01-11 03:21:02,730 [podnet.py] => Task 3, Epoch 7/20 (LR 0.00363) => LSC_loss 0.20, Spatial_loss 0.91, Flat_loss 0.10, Train_acc 95.69, Test_acc 61.80
2025-01-11 03:21:08,748 [podnet.py] => Task 3, Epoch 8/20 (LR 0.00327) => LSC_loss 0.20, Spatial_loss 0.92, Flat_loss 0.10, Train_acc 96.14, Test_acc 63.30
2025-01-11 03:21:14,720 [podnet.py] => Task 3, Epoch 9/20 (LR 0.00289) => LSC_loss 0.19, Spatial_loss 0.90, Flat_loss 0.10, Train_acc 96.39, Test_acc 62.98
2025-01-11 03:21:20,541 [podnet.py] => Task 3, Epoch 10/20 (LR 0.00250) => LSC_loss 0.19, Spatial_loss 0.89, Flat_loss 0.10, Train_acc 96.18, Test_acc 63.10
2025-01-11 03:21:26,516 [podnet.py] => Task 3, Epoch 11/20 (LR 0.00211) => LSC_loss 0.19, Spatial_loss 0.91, Flat_loss 0.10, Train_acc 96.15, Test_acc 62.92
2025-01-11 03:21:32,491 [podnet.py] => Task 3, Epoch 12/20 (LR 0.00173) => LSC_loss 0.18, Spatial_loss 0.89, Flat_loss 0.10, Train_acc 96.67, Test_acc 63.32
2025-01-11 03:21:38,442 [podnet.py] => Task 3, Epoch 13/20 (LR 0.00137) => LSC_loss 0.18, Spatial_loss 0.87, Flat_loss 0.10, Train_acc 96.49, Test_acc 63.32
2025-01-11 03:21:44,439 [podnet.py] => Task 3, Epoch 14/20 (LR 0.00103) => LSC_loss 0.18, Spatial_loss 0.88, Flat_loss 0.10, Train_acc 96.74, Test_acc 63.42
2025-01-11 03:21:50,250 [podnet.py] => Task 3, Epoch 15/20 (LR 0.00073) => LSC_loss 0.18, Spatial_loss 0.88, Flat_loss 0.10, Train_acc 96.80, Test_acc 63.55
2025-01-11 03:21:56,208 [podnet.py] => Task 3, Epoch 16/20 (LR 0.00048) => LSC_loss 0.18, Spatial_loss 0.86, Flat_loss 0.10, Train_acc 96.78, Test_acc 63.50
2025-01-11 03:22:02,229 [podnet.py] => Task 3, Epoch 17/20 (LR 0.00027) => LSC_loss 0.18, Spatial_loss 0.87, Flat_loss 0.10, Train_acc 96.90, Test_acc 63.45
2025-01-11 03:22:08,164 [podnet.py] => Task 3, Epoch 18/20 (LR 0.00012) => LSC_loss 0.17, Spatial_loss 0.87, Flat_loss 0.10, Train_acc 96.80, Test_acc 63.52
2025-01-11 03:22:14,055 [podnet.py] => Task 3, Epoch 19/20 (LR 0.00003) => LSC_loss 0.18, Spatial_loss 0.87, Flat_loss 0.10, Train_acc 96.67, Test_acc 63.70
2025-01-11 03:22:20,041 [podnet.py] => Task 3, Epoch 20/20 (LR 0.00000) => LSC_loss 0.17, Spatial_loss 0.86, Flat_loss 0.10, Train_acc 97.08, Test_acc 63.42
2025-01-11 03:22:20,043 [base.py] => Reducing exemplars...(185 per classes)
2025-01-11 03:22:39,241 [base.py] => Constructing exemplars...(185 per classes)
2025-01-11 03:22:55,352 [podnet.py] => Exemplar size: 7400
2025-01-11 03:22:55,352 [trainer.py] => CNN: {'total': np.float64(63.42), '00-09': np.float64(77.9), '10-19': np.float64(51.8), '20-29': np.float64(65.6), '30-39': np.float64(58.4), 'old': np.float64(65.1), 'new': np.float64(58.4)}
2025-01-11 03:22:55,352 [trainer.py] => NME: {'total': np.float64(61.38), '00-09': np.float64(77.4), '10-19': np.float64(49.0), '20-29': np.float64(62.5), '30-39': np.float64(56.6), 'old': np.float64(62.97), 'new': np.float64(56.6)}
2025-01-11 03:22:55,352 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42)]
2025-01-11 03:22:55,352 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82)]
2025-01-11 03:22:55,352 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38)]
2025-01-11 03:22:55,353 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02)]

2025-01-11 03:22:55,353 [trainer.py] => All params: 491857
2025-01-11 03:22:55,353 [trainer.py] => Trainable params: 491857
2025-01-11 03:22:55,354 [podnet.py] => Learning on 40-50
2025-01-11 03:22:55,459 [podnet.py] => Adaptive factor: 2.23606797749979
2025-01-11 03:23:02,715 [podnet.py] => Task 4, Epoch 1/160 (LR 0.09999) => LSC_loss 2.16, Spatial_loss 2.62, Flat_loss 0.53, Train_acc 47.52, Test_acc 37.58
2025-01-11 03:23:10,318 [podnet.py] => Task 4, Epoch 2/160 (LR 0.09996) => LSC_loss 1.48, Spatial_loss 2.24, Flat_loss 0.30, Train_acc 59.26, Test_acc 42.02
2025-01-11 03:23:17,727 [podnet.py] => Task 4, Epoch 3/160 (LR 0.09991) => LSC_loss 1.33, Spatial_loss 2.07, Flat_loss 0.26, Train_acc 62.89, Test_acc 48.70
2025-01-11 03:23:24,858 [podnet.py] => Task 4, Epoch 4/160 (LR 0.09985) => LSC_loss 1.26, Spatial_loss 2.00, Flat_loss 0.24, Train_acc 64.85, Test_acc 50.46
2025-01-11 03:23:32,278 [podnet.py] => Task 4, Epoch 5/160 (LR 0.09976) => LSC_loss 1.21, Spatial_loss 1.94, Flat_loss 0.23, Train_acc 66.63, Test_acc 47.88
2025-01-11 03:23:39,552 [podnet.py] => Task 4, Epoch 6/160 (LR 0.09965) => LSC_loss 1.19, Spatial_loss 1.92, Flat_loss 0.23, Train_acc 66.94, Test_acc 47.56
2025-01-11 03:23:46,931 [podnet.py] => Task 4, Epoch 7/160 (LR 0.09953) => LSC_loss 1.15, Spatial_loss 1.90, Flat_loss 0.23, Train_acc 68.20, Test_acc 50.16
2025-01-11 03:23:54,863 [podnet.py] => Task 4, Epoch 8/160 (LR 0.09938) => LSC_loss 1.15, Spatial_loss 1.89, Flat_loss 0.23, Train_acc 67.99, Test_acc 48.86
2025-01-11 03:24:02,401 [podnet.py] => Task 4, Epoch 9/160 (LR 0.09922) => LSC_loss 1.09, Spatial_loss 1.85, Flat_loss 0.22, Train_acc 69.36, Test_acc 46.34
2025-01-11 03:24:10,233 [podnet.py] => Task 4, Epoch 10/160 (LR 0.09904) => LSC_loss 1.11, Spatial_loss 1.89, Flat_loss 0.22, Train_acc 69.31, Test_acc 52.46
2025-01-11 03:24:17,847 [podnet.py] => Task 4, Epoch 11/160 (LR 0.09884) => LSC_loss 1.08, Spatial_loss 1.86, Flat_loss 0.22, Train_acc 69.94, Test_acc 50.80
2025-01-11 03:24:25,296 [podnet.py] => Task 4, Epoch 12/160 (LR 0.09862) => LSC_loss 1.05, Spatial_loss 1.84, Flat_loss 0.22, Train_acc 70.90, Test_acc 47.64
2025-01-11 03:24:33,420 [podnet.py] => Task 4, Epoch 13/160 (LR 0.09838) => LSC_loss 1.04, Spatial_loss 1.81, Flat_loss 0.21, Train_acc 70.91, Test_acc 52.24
2025-01-11 03:24:41,464 [podnet.py] => Task 4, Epoch 14/160 (LR 0.09812) => LSC_loss 1.04, Spatial_loss 1.84, Flat_loss 0.22, Train_acc 70.86, Test_acc 50.58
2025-01-11 03:24:49,743 [podnet.py] => Task 4, Epoch 15/160 (LR 0.09785) => LSC_loss 1.02, Spatial_loss 1.85, Flat_loss 0.22, Train_acc 71.03, Test_acc 52.94
2025-01-11 03:24:57,548 [podnet.py] => Task 4, Epoch 16/160 (LR 0.09755) => LSC_loss 1.02, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 71.71, Test_acc 48.94
2025-01-11 03:25:05,646 [podnet.py] => Task 4, Epoch 17/160 (LR 0.09724) => LSC_loss 1.00, Spatial_loss 1.81, Flat_loss 0.22, Train_acc 72.31, Test_acc 50.16
2025-01-11 03:25:13,334 [podnet.py] => Task 4, Epoch 18/160 (LR 0.09691) => LSC_loss 0.99, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 71.84, Test_acc 47.98
2025-01-11 03:25:21,006 [podnet.py] => Task 4, Epoch 19/160 (LR 0.09656) => LSC_loss 1.00, Spatial_loss 1.84, Flat_loss 0.22, Train_acc 71.84, Test_acc 52.92
2025-01-11 03:25:28,440 [podnet.py] => Task 4, Epoch 20/160 (LR 0.09619) => LSC_loss 0.97, Spatial_loss 1.83, Flat_loss 0.22, Train_acc 72.73, Test_acc 50.86
2025-01-11 03:25:36,111 [podnet.py] => Task 4, Epoch 21/160 (LR 0.09581) => LSC_loss 0.98, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 72.57, Test_acc 51.16
2025-01-11 03:25:43,564 [podnet.py] => Task 4, Epoch 22/160 (LR 0.09541) => LSC_loss 0.97, Spatial_loss 1.81, Flat_loss 0.21, Train_acc 72.85, Test_acc 49.40
2025-01-11 03:25:50,627 [podnet.py] => Task 4, Epoch 23/160 (LR 0.09499) => LSC_loss 0.97, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 73.00, Test_acc 48.04
2025-01-11 03:25:57,727 [podnet.py] => Task 4, Epoch 24/160 (LR 0.09455) => LSC_loss 0.94, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 73.28, Test_acc 47.00
2025-01-11 03:26:04,762 [podnet.py] => Task 4, Epoch 25/160 (LR 0.09410) => LSC_loss 0.96, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 73.44, Test_acc 49.10
2025-01-11 03:26:12,036 [podnet.py] => Task 4, Epoch 26/160 (LR 0.09362) => LSC_loss 0.95, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 73.46, Test_acc 47.18
2025-01-11 03:26:19,149 [podnet.py] => Task 4, Epoch 27/160 (LR 0.09314) => LSC_loss 0.94, Spatial_loss 1.79, Flat_loss 0.21, Train_acc 74.12, Test_acc 52.58
2025-01-11 03:26:26,305 [podnet.py] => Task 4, Epoch 28/160 (LR 0.09263) => LSC_loss 0.93, Spatial_loss 1.80, Flat_loss 0.21, Train_acc 74.10, Test_acc 50.32
2025-01-11 03:26:33,384 [podnet.py] => Task 4, Epoch 29/160 (LR 0.09211) => LSC_loss 0.96, Spatial_loss 1.83, Flat_loss 0.22, Train_acc 73.03, Test_acc 43.94
2025-01-11 03:26:40,486 [podnet.py] => Task 4, Epoch 30/160 (LR 0.09157) => LSC_loss 0.94, Spatial_loss 1.81, Flat_loss 0.22, Train_acc 73.60, Test_acc 54.12
2025-01-11 03:26:47,619 [podnet.py] => Task 4, Epoch 31/160 (LR 0.09102) => LSC_loss 0.93, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 73.85, Test_acc 51.80
2025-01-11 03:26:54,943 [podnet.py] => Task 4, Epoch 32/160 (LR 0.09045) => LSC_loss 0.92, Spatial_loss 1.78, Flat_loss 0.21, Train_acc 74.48, Test_acc 52.30
2025-01-11 03:27:02,132 [podnet.py] => Task 4, Epoch 33/160 (LR 0.08987) => LSC_loss 0.90, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 75.04, Test_acc 54.92
2025-01-11 03:27:09,402 [podnet.py] => Task 4, Epoch 34/160 (LR 0.08927) => LSC_loss 0.91, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 74.28, Test_acc 49.08
2025-01-11 03:27:16,533 [podnet.py] => Task 4, Epoch 35/160 (LR 0.08865) => LSC_loss 0.93, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 73.78, Test_acc 51.06
2025-01-11 03:27:23,590 [podnet.py] => Task 4, Epoch 36/160 (LR 0.08802) => LSC_loss 0.91, Spatial_loss 1.75, Flat_loss 0.21, Train_acc 74.47, Test_acc 48.54
2025-01-11 03:27:30,830 [podnet.py] => Task 4, Epoch 37/160 (LR 0.08738) => LSC_loss 0.88, Spatial_loss 1.75, Flat_loss 0.21, Train_acc 75.26, Test_acc 50.64
2025-01-11 03:27:37,949 [podnet.py] => Task 4, Epoch 38/160 (LR 0.08672) => LSC_loss 0.90, Spatial_loss 1.78, Flat_loss 0.21, Train_acc 74.66, Test_acc 53.84
2025-01-11 03:27:45,266 [podnet.py] => Task 4, Epoch 39/160 (LR 0.08604) => LSC_loss 0.88, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 75.50, Test_acc 54.10
2025-01-11 03:27:52,274 [podnet.py] => Task 4, Epoch 40/160 (LR 0.08536) => LSC_loss 0.88, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 75.38, Test_acc 49.20
2025-01-11 03:27:59,571 [podnet.py] => Task 4, Epoch 41/160 (LR 0.08465) => LSC_loss 0.88, Spatial_loss 1.75, Flat_loss 0.21, Train_acc 75.52, Test_acc 46.68
2025-01-11 03:28:06,644 [podnet.py] => Task 4, Epoch 42/160 (LR 0.08394) => LSC_loss 0.90, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 74.78, Test_acc 54.66
2025-01-11 03:28:13,771 [podnet.py] => Task 4, Epoch 43/160 (LR 0.08321) => LSC_loss 0.85, Spatial_loss 1.74, Flat_loss 0.21, Train_acc 76.15, Test_acc 52.78
2025-01-11 03:28:21,019 [podnet.py] => Task 4, Epoch 44/160 (LR 0.08247) => LSC_loss 0.87, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 75.37, Test_acc 49.42
2025-01-11 03:28:28,035 [podnet.py] => Task 4, Epoch 45/160 (LR 0.08172) => LSC_loss 0.87, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 75.87, Test_acc 50.64
2025-01-11 03:28:35,337 [podnet.py] => Task 4, Epoch 46/160 (LR 0.08095) => LSC_loss 0.87, Spatial_loss 1.74, Flat_loss 0.21, Train_acc 75.54, Test_acc 53.92
2025-01-11 03:28:42,350 [podnet.py] => Task 4, Epoch 47/160 (LR 0.08018) => LSC_loss 0.86, Spatial_loss 1.75, Flat_loss 0.21, Train_acc 76.17, Test_acc 48.56
2025-01-11 03:28:49,451 [podnet.py] => Task 4, Epoch 48/160 (LR 0.07939) => LSC_loss 0.85, Spatial_loss 1.73, Flat_loss 0.21, Train_acc 76.24, Test_acc 51.06
2025-01-11 03:28:56,469 [podnet.py] => Task 4, Epoch 49/160 (LR 0.07859) => LSC_loss 0.83, Spatial_loss 1.74, Flat_loss 0.21, Train_acc 76.11, Test_acc 53.76
2025-01-11 03:29:03,883 [podnet.py] => Task 4, Epoch 50/160 (LR 0.07778) => LSC_loss 0.86, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 76.23, Test_acc 52.96
2025-01-11 03:29:11,080 [podnet.py] => Task 4, Epoch 51/160 (LR 0.07696) => LSC_loss 0.83, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 76.57, Test_acc 53.28
2025-01-11 03:29:18,235 [podnet.py] => Task 4, Epoch 52/160 (LR 0.07612) => LSC_loss 0.83, Spatial_loss 1.74, Flat_loss 0.21, Train_acc 76.49, Test_acc 49.98
2025-01-11 03:29:25,413 [podnet.py] => Task 4, Epoch 53/160 (LR 0.07528) => LSC_loss 0.82, Spatial_loss 1.71, Flat_loss 0.21, Train_acc 77.33, Test_acc 53.44
2025-01-11 03:29:32,678 [podnet.py] => Task 4, Epoch 54/160 (LR 0.07443) => LSC_loss 0.81, Spatial_loss 1.70, Flat_loss 0.21, Train_acc 77.35, Test_acc 53.64
2025-01-11 03:29:39,792 [podnet.py] => Task 4, Epoch 55/160 (LR 0.07357) => LSC_loss 0.82, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 76.73, Test_acc 55.14
2025-01-11 03:29:46,825 [podnet.py] => Task 4, Epoch 56/160 (LR 0.07270) => LSC_loss 0.80, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 77.72, Test_acc 51.02
2025-01-11 03:29:54,395 [podnet.py] => Task 4, Epoch 57/160 (LR 0.07182) => LSC_loss 0.80, Spatial_loss 1.70, Flat_loss 0.21, Train_acc 77.57, Test_acc 53.62
2025-01-11 03:30:01,704 [podnet.py] => Task 4, Epoch 58/160 (LR 0.07093) => LSC_loss 0.81, Spatial_loss 1.70, Flat_loss 0.21, Train_acc 77.03, Test_acc 52.66
2025-01-11 03:30:09,107 [podnet.py] => Task 4, Epoch 59/160 (LR 0.07004) => LSC_loss 0.78, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 78.48, Test_acc 55.32
2025-01-11 03:30:16,277 [podnet.py] => Task 4, Epoch 60/160 (LR 0.06913) => LSC_loss 0.78, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 78.19, Test_acc 48.54
2025-01-11 03:30:23,539 [podnet.py] => Task 4, Epoch 61/160 (LR 0.06822) => LSC_loss 0.77, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 78.35, Test_acc 52.90
2025-01-11 03:30:30,962 [podnet.py] => Task 4, Epoch 62/160 (LR 0.06731) => LSC_loss 0.77, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 78.55, Test_acc 54.64
2025-01-11 03:30:38,562 [podnet.py] => Task 4, Epoch 63/160 (LR 0.06638) => LSC_loss 0.77, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 78.52, Test_acc 51.72
2025-01-11 03:30:45,943 [podnet.py] => Task 4, Epoch 64/160 (LR 0.06545) => LSC_loss 0.76, Spatial_loss 1.66, Flat_loss 0.20, Train_acc 78.73, Test_acc 51.52
2025-01-11 03:30:53,356 [podnet.py] => Task 4, Epoch 65/160 (LR 0.06451) => LSC_loss 0.76, Spatial_loss 1.65, Flat_loss 0.20, Train_acc 79.05, Test_acc 52.32
2025-01-11 03:31:00,771 [podnet.py] => Task 4, Epoch 66/160 (LR 0.06357) => LSC_loss 0.74, Spatial_loss 1.63, Flat_loss 0.20, Train_acc 79.19, Test_acc 52.60
2025-01-11 03:31:08,140 [podnet.py] => Task 4, Epoch 67/160 (LR 0.06262) => LSC_loss 0.74, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 79.00, Test_acc 53.12
2025-01-11 03:31:15,496 [podnet.py] => Task 4, Epoch 68/160 (LR 0.06167) => LSC_loss 0.75, Spatial_loss 1.66, Flat_loss 0.20, Train_acc 78.90, Test_acc 51.30
2025-01-11 03:31:22,633 [podnet.py] => Task 4, Epoch 69/160 (LR 0.06072) => LSC_loss 0.73, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 79.86, Test_acc 53.52
2025-01-11 03:31:29,738 [podnet.py] => Task 4, Epoch 70/160 (LR 0.05975) => LSC_loss 0.72, Spatial_loss 1.61, Flat_loss 0.20, Train_acc 79.94, Test_acc 54.76
2025-01-11 03:31:37,141 [podnet.py] => Task 4, Epoch 71/160 (LR 0.05879) => LSC_loss 0.72, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 79.76, Test_acc 51.26
2025-01-11 03:31:44,577 [podnet.py] => Task 4, Epoch 72/160 (LR 0.05782) => LSC_loss 0.73, Spatial_loss 1.64, Flat_loss 0.20, Train_acc 79.79, Test_acc 51.92
2025-01-11 03:31:51,991 [podnet.py] => Task 4, Epoch 73/160 (LR 0.05685) => LSC_loss 0.71, Spatial_loss 1.62, Flat_loss 0.19, Train_acc 79.81, Test_acc 54.64
2025-01-11 03:31:59,027 [podnet.py] => Task 4, Epoch 74/160 (LR 0.05588) => LSC_loss 0.70, Spatial_loss 1.59, Flat_loss 0.19, Train_acc 80.60, Test_acc 53.02
2025-01-11 03:32:06,586 [podnet.py] => Task 4, Epoch 75/160 (LR 0.05490) => LSC_loss 0.70, Spatial_loss 1.60, Flat_loss 0.19, Train_acc 80.40, Test_acc 53.40
2025-01-11 03:32:13,632 [podnet.py] => Task 4, Epoch 76/160 (LR 0.05392) => LSC_loss 0.68, Spatial_loss 1.58, Flat_loss 0.19, Train_acc 81.04, Test_acc 52.10
2025-01-11 03:32:20,924 [podnet.py] => Task 4, Epoch 77/160 (LR 0.05294) => LSC_loss 0.66, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 81.94, Test_acc 54.48
2025-01-11 03:32:28,087 [podnet.py] => Task 4, Epoch 78/160 (LR 0.05196) => LSC_loss 0.68, Spatial_loss 1.55, Flat_loss 0.18, Train_acc 81.00, Test_acc 54.96
2025-01-11 03:32:35,491 [podnet.py] => Task 4, Epoch 79/160 (LR 0.05098) => LSC_loss 0.66, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 81.59, Test_acc 51.82
2025-01-11 03:32:42,672 [podnet.py] => Task 4, Epoch 80/160 (LR 0.05000) => LSC_loss 0.65, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 81.47, Test_acc 53.88
2025-01-11 03:32:49,664 [podnet.py] => Task 4, Epoch 81/160 (LR 0.04902) => LSC_loss 0.64, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 82.14, Test_acc 52.48
2025-01-11 03:32:57,004 [podnet.py] => Task 4, Epoch 82/160 (LR 0.04804) => LSC_loss 0.65, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 81.61, Test_acc 54.40
2025-01-11 03:33:05,612 [podnet.py] => Task 4, Epoch 83/160 (LR 0.04706) => LSC_loss 0.65, Spatial_loss 1.55, Flat_loss 0.19, Train_acc 82.12, Test_acc 52.02
2025-01-11 03:33:12,655 [podnet.py] => Task 4, Epoch 84/160 (LR 0.04608) => LSC_loss 0.63, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 82.84, Test_acc 52.76
2025-01-11 03:33:19,859 [podnet.py] => Task 4, Epoch 85/160 (LR 0.04510) => LSC_loss 0.62, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 82.83, Test_acc 54.64
2025-01-11 03:33:27,154 [podnet.py] => Task 4, Epoch 86/160 (LR 0.04412) => LSC_loss 0.60, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 82.94, Test_acc 55.36
2025-01-11 03:33:34,443 [podnet.py] => Task 4, Epoch 87/160 (LR 0.04315) => LSC_loss 0.59, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 83.44, Test_acc 55.46
2025-01-11 03:33:41,867 [podnet.py] => Task 4, Epoch 88/160 (LR 0.04218) => LSC_loss 0.58, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 83.95, Test_acc 53.30
2025-01-11 03:33:49,179 [podnet.py] => Task 4, Epoch 89/160 (LR 0.04121) => LSC_loss 0.60, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 83.02, Test_acc 56.34
2025-01-11 03:33:56,261 [podnet.py] => Task 4, Epoch 90/160 (LR 0.04025) => LSC_loss 0.57, Spatial_loss 1.46, Flat_loss 0.17, Train_acc 84.82, Test_acc 50.18
2025-01-11 03:34:03,636 [podnet.py] => Task 4, Epoch 91/160 (LR 0.03928) => LSC_loss 0.59, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 83.35, Test_acc 54.78
2025-01-11 03:34:10,772 [podnet.py] => Task 4, Epoch 92/160 (LR 0.03833) => LSC_loss 0.57, Spatial_loss 1.47, Flat_loss 0.17, Train_acc 84.14, Test_acc 54.56
2025-01-11 03:34:17,768 [podnet.py] => Task 4, Epoch 93/160 (LR 0.03738) => LSC_loss 0.56, Spatial_loss 1.46, Flat_loss 0.17, Train_acc 84.45, Test_acc 55.74
2025-01-11 03:34:24,873 [podnet.py] => Task 4, Epoch 94/160 (LR 0.03643) => LSC_loss 0.55, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 84.95, Test_acc 55.08
2025-01-11 03:34:32,050 [podnet.py] => Task 4, Epoch 95/160 (LR 0.03549) => LSC_loss 0.55, Spatial_loss 1.43, Flat_loss 0.17, Train_acc 85.20, Test_acc 53.54
2025-01-11 03:34:39,065 [podnet.py] => Task 4, Epoch 96/160 (LR 0.03455) => LSC_loss 0.54, Spatial_loss 1.41, Flat_loss 0.17, Train_acc 85.41, Test_acc 54.58
2025-01-11 03:34:46,075 [podnet.py] => Task 4, Epoch 97/160 (LR 0.03362) => LSC_loss 0.54, Spatial_loss 1.42, Flat_loss 0.17, Train_acc 85.48, Test_acc 55.76
2025-01-11 03:34:53,329 [podnet.py] => Task 4, Epoch 98/160 (LR 0.03269) => LSC_loss 0.52, Spatial_loss 1.39, Flat_loss 0.17, Train_acc 86.08, Test_acc 58.20
2025-01-11 03:35:00,488 [podnet.py] => Task 4, Epoch 99/160 (LR 0.03178) => LSC_loss 0.52, Spatial_loss 1.38, Flat_loss 0.17, Train_acc 86.05, Test_acc 56.72
2025-01-11 03:35:07,750 [podnet.py] => Task 4, Epoch 100/160 (LR 0.03087) => LSC_loss 0.51, Spatial_loss 1.37, Flat_loss 0.17, Train_acc 86.44, Test_acc 55.86
2025-01-11 03:35:14,788 [podnet.py] => Task 4, Epoch 101/160 (LR 0.02996) => LSC_loss 0.50, Spatial_loss 1.36, Flat_loss 0.16, Train_acc 86.73, Test_acc 54.12
2025-01-11 03:35:21,890 [podnet.py] => Task 4, Epoch 102/160 (LR 0.02907) => LSC_loss 0.50, Spatial_loss 1.35, Flat_loss 0.16, Train_acc 86.37, Test_acc 55.12
2025-01-11 03:35:29,085 [podnet.py] => Task 4, Epoch 103/160 (LR 0.02818) => LSC_loss 0.49, Spatial_loss 1.35, Flat_loss 0.16, Train_acc 86.99, Test_acc 56.46
2025-01-11 03:35:36,050 [podnet.py] => Task 4, Epoch 104/160 (LR 0.02730) => LSC_loss 0.48, Spatial_loss 1.34, Flat_loss 0.16, Train_acc 87.22, Test_acc 56.28
2025-01-11 03:35:43,149 [podnet.py] => Task 4, Epoch 105/160 (LR 0.02643) => LSC_loss 0.48, Spatial_loss 1.33, Flat_loss 0.16, Train_acc 87.45, Test_acc 56.26
2025-01-11 03:35:50,220 [podnet.py] => Task 4, Epoch 106/160 (LR 0.02557) => LSC_loss 0.46, Spatial_loss 1.33, Flat_loss 0.16, Train_acc 87.66, Test_acc 57.94
2025-01-11 03:35:57,335 [podnet.py] => Task 4, Epoch 107/160 (LR 0.02472) => LSC_loss 0.46, Spatial_loss 1.31, Flat_loss 0.16, Train_acc 87.80, Test_acc 55.96
2025-01-11 03:36:04,519 [podnet.py] => Task 4, Epoch 108/160 (LR 0.02388) => LSC_loss 0.44, Spatial_loss 1.27, Flat_loss 0.15, Train_acc 88.27, Test_acc 57.10
2025-01-11 03:36:11,618 [podnet.py] => Task 4, Epoch 109/160 (LR 0.02304) => LSC_loss 0.44, Spatial_loss 1.29, Flat_loss 0.15, Train_acc 88.61, Test_acc 56.70
2025-01-11 03:36:18,684 [podnet.py] => Task 4, Epoch 110/160 (LR 0.02222) => LSC_loss 0.43, Spatial_loss 1.26, Flat_loss 0.15, Train_acc 88.93, Test_acc 56.94
2025-01-11 03:36:25,786 [podnet.py] => Task 4, Epoch 111/160 (LR 0.02141) => LSC_loss 0.43, Spatial_loss 1.24, Flat_loss 0.15, Train_acc 89.15, Test_acc 56.64
2025-01-11 03:36:32,992 [podnet.py] => Task 4, Epoch 112/160 (LR 0.02061) => LSC_loss 0.40, Spatial_loss 1.22, Flat_loss 0.15, Train_acc 89.77, Test_acc 56.16
2025-01-11 03:36:40,076 [podnet.py] => Task 4, Epoch 113/160 (LR 0.01982) => LSC_loss 0.41, Spatial_loss 1.22, Flat_loss 0.15, Train_acc 89.52, Test_acc 58.26
2025-01-11 03:36:47,097 [podnet.py] => Task 4, Epoch 114/160 (LR 0.01905) => LSC_loss 0.40, Spatial_loss 1.21, Flat_loss 0.15, Train_acc 90.05, Test_acc 56.94
2025-01-11 03:36:54,240 [podnet.py] => Task 4, Epoch 115/160 (LR 0.01828) => LSC_loss 0.40, Spatial_loss 1.19, Flat_loss 0.14, Train_acc 90.36, Test_acc 57.74
2025-01-11 03:37:01,174 [podnet.py] => Task 4, Epoch 116/160 (LR 0.01753) => LSC_loss 0.40, Spatial_loss 1.20, Flat_loss 0.14, Train_acc 90.28, Test_acc 57.20
2025-01-11 03:37:08,528 [podnet.py] => Task 4, Epoch 117/160 (LR 0.01679) => LSC_loss 0.38, Spatial_loss 1.18, Flat_loss 0.14, Train_acc 90.53, Test_acc 57.60
2025-01-11 03:37:15,714 [podnet.py] => Task 4, Epoch 118/160 (LR 0.01606) => LSC_loss 0.37, Spatial_loss 1.17, Flat_loss 0.14, Train_acc 91.01, Test_acc 58.58
2025-01-11 03:37:22,688 [podnet.py] => Task 4, Epoch 119/160 (LR 0.01535) => LSC_loss 0.37, Spatial_loss 1.14, Flat_loss 0.14, Train_acc 91.06, Test_acc 56.16
2025-01-11 03:37:29,724 [podnet.py] => Task 4, Epoch 120/160 (LR 0.01464) => LSC_loss 0.36, Spatial_loss 1.15, Flat_loss 0.14, Train_acc 91.35, Test_acc 58.38
2025-01-11 03:37:36,861 [podnet.py] => Task 4, Epoch 121/160 (LR 0.01396) => LSC_loss 0.35, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 91.83, Test_acc 58.98
2025-01-11 03:37:44,030 [podnet.py] => Task 4, Epoch 122/160 (LR 0.01328) => LSC_loss 0.35, Spatial_loss 1.13, Flat_loss 0.14, Train_acc 91.85, Test_acc 59.46
2025-01-11 03:37:51,075 [podnet.py] => Task 4, Epoch 123/160 (LR 0.01262) => LSC_loss 0.35, Spatial_loss 1.11, Flat_loss 0.13, Train_acc 91.70, Test_acc 58.06
2025-01-11 03:37:58,274 [podnet.py] => Task 4, Epoch 124/160 (LR 0.01198) => LSC_loss 0.35, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 91.48, Test_acc 57.84
2025-01-11 03:38:05,280 [podnet.py] => Task 4, Epoch 125/160 (LR 0.01135) => LSC_loss 0.34, Spatial_loss 1.10, Flat_loss 0.13, Train_acc 91.98, Test_acc 58.38
2025-01-11 03:38:12,357 [podnet.py] => Task 4, Epoch 126/160 (LR 0.01073) => LSC_loss 0.33, Spatial_loss 1.09, Flat_loss 0.13, Train_acc 92.23, Test_acc 57.96
2025-01-11 03:38:19,483 [podnet.py] => Task 4, Epoch 127/160 (LR 0.01013) => LSC_loss 0.32, Spatial_loss 1.09, Flat_loss 0.13, Train_acc 92.41, Test_acc 55.72
2025-01-11 03:38:26,739 [podnet.py] => Task 4, Epoch 128/160 (LR 0.00955) => LSC_loss 0.31, Spatial_loss 1.07, Flat_loss 0.13, Train_acc 92.94, Test_acc 59.14
2025-01-11 03:38:33,898 [podnet.py] => Task 4, Epoch 129/160 (LR 0.00898) => LSC_loss 0.31, Spatial_loss 1.05, Flat_loss 0.13, Train_acc 92.73, Test_acc 58.76
2025-01-11 03:38:41,013 [podnet.py] => Task 4, Epoch 130/160 (LR 0.00843) => LSC_loss 0.31, Spatial_loss 1.03, Flat_loss 0.13, Train_acc 93.00, Test_acc 59.86
2025-01-11 03:38:48,229 [podnet.py] => Task 4, Epoch 131/160 (LR 0.00789) => LSC_loss 0.31, Spatial_loss 1.04, Flat_loss 0.13, Train_acc 93.23, Test_acc 59.02
2025-01-11 03:38:55,165 [podnet.py] => Task 4, Epoch 132/160 (LR 0.00737) => LSC_loss 0.31, Spatial_loss 1.04, Flat_loss 0.13, Train_acc 93.02, Test_acc 59.16
2025-01-11 03:39:02,276 [podnet.py] => Task 4, Epoch 133/160 (LR 0.00686) => LSC_loss 0.30, Spatial_loss 1.02, Flat_loss 0.13, Train_acc 93.34, Test_acc 59.42
2025-01-11 03:39:09,696 [podnet.py] => Task 4, Epoch 134/160 (LR 0.00638) => LSC_loss 0.30, Spatial_loss 1.02, Flat_loss 0.12, Train_acc 93.54, Test_acc 59.38
2025-01-11 03:39:16,804 [podnet.py] => Task 4, Epoch 135/160 (LR 0.00590) => LSC_loss 0.29, Spatial_loss 0.98, Flat_loss 0.12, Train_acc 93.80, Test_acc 59.78
2025-01-11 03:39:24,122 [podnet.py] => Task 4, Epoch 136/160 (LR 0.00545) => LSC_loss 0.29, Spatial_loss 0.99, Flat_loss 0.12, Train_acc 93.75, Test_acc 59.70
2025-01-11 03:39:31,119 [podnet.py] => Task 4, Epoch 137/160 (LR 0.00501) => LSC_loss 0.29, Spatial_loss 0.98, Flat_loss 0.12, Train_acc 93.82, Test_acc 59.08
2025-01-11 03:39:38,242 [podnet.py] => Task 4, Epoch 138/160 (LR 0.00459) => LSC_loss 0.28, Spatial_loss 0.99, Flat_loss 0.12, Train_acc 94.39, Test_acc 60.54
2025-01-11 03:39:45,406 [podnet.py] => Task 4, Epoch 139/160 (LR 0.00419) => LSC_loss 0.28, Spatial_loss 0.98, Flat_loss 0.12, Train_acc 94.31, Test_acc 59.82
2025-01-11 03:39:52,352 [podnet.py] => Task 4, Epoch 140/160 (LR 0.00381) => LSC_loss 0.28, Spatial_loss 0.97, Flat_loss 0.12, Train_acc 94.14, Test_acc 60.10
2025-01-11 03:39:59,479 [podnet.py] => Task 4, Epoch 141/160 (LR 0.00344) => LSC_loss 0.28, Spatial_loss 0.96, Flat_loss 0.12, Train_acc 94.21, Test_acc 59.54
2025-01-11 03:40:06,663 [podnet.py] => Task 4, Epoch 142/160 (LR 0.00309) => LSC_loss 0.27, Spatial_loss 0.95, Flat_loss 0.12, Train_acc 94.62, Test_acc 60.32
2025-01-11 03:40:13,788 [podnet.py] => Task 4, Epoch 143/160 (LR 0.00276) => LSC_loss 0.26, Spatial_loss 0.95, Flat_loss 0.12, Train_acc 94.78, Test_acc 60.04
2025-01-11 03:40:20,861 [podnet.py] => Task 4, Epoch 144/160 (LR 0.00245) => LSC_loss 0.26, Spatial_loss 0.95, Flat_loss 0.12, Train_acc 94.97, Test_acc 60.28
2025-01-11 03:40:27,973 [podnet.py] => Task 4, Epoch 145/160 (LR 0.00215) => LSC_loss 0.26, Spatial_loss 0.94, Flat_loss 0.11, Train_acc 94.78, Test_acc 60.50
2025-01-11 03:40:34,989 [podnet.py] => Task 4, Epoch 146/160 (LR 0.00188) => LSC_loss 0.26, Spatial_loss 0.94, Flat_loss 0.12, Train_acc 94.84, Test_acc 60.36
2025-01-11 03:40:42,147 [podnet.py] => Task 4, Epoch 147/160 (LR 0.00162) => LSC_loss 0.27, Spatial_loss 0.93, Flat_loss 0.11, Train_acc 94.53, Test_acc 59.92
2025-01-11 03:40:49,238 [podnet.py] => Task 4, Epoch 148/160 (LR 0.00138) => LSC_loss 0.26, Spatial_loss 0.93, Flat_loss 0.11, Train_acc 94.88, Test_acc 60.42
2025-01-11 03:40:56,403 [podnet.py] => Task 4, Epoch 149/160 (LR 0.00116) => LSC_loss 0.26, Spatial_loss 0.92, Flat_loss 0.11, Train_acc 95.23, Test_acc 60.12
2025-01-11 03:41:03,545 [podnet.py] => Task 4, Epoch 150/160 (LR 0.00096) => LSC_loss 0.26, Spatial_loss 0.92, Flat_loss 0.11, Train_acc 94.95, Test_acc 60.16
2025-01-11 03:41:10,653 [podnet.py] => Task 4, Epoch 151/160 (LR 0.00078) => LSC_loss 0.26, Spatial_loss 0.92, Flat_loss 0.11, Train_acc 94.97, Test_acc 60.48
2025-01-11 03:41:17,871 [podnet.py] => Task 4, Epoch 152/160 (LR 0.00062) => LSC_loss 0.25, Spatial_loss 0.90, Flat_loss 0.11, Train_acc 95.08, Test_acc 60.30
2025-01-11 03:41:24,869 [podnet.py] => Task 4, Epoch 153/160 (LR 0.00047) => LSC_loss 0.25, Spatial_loss 0.91, Flat_loss 0.11, Train_acc 94.90, Test_acc 60.12
2025-01-11 03:41:32,369 [podnet.py] => Task 4, Epoch 154/160 (LR 0.00035) => LSC_loss 0.25, Spatial_loss 0.89, Flat_loss 0.11, Train_acc 95.28, Test_acc 60.30
2025-01-11 03:41:39,641 [podnet.py] => Task 4, Epoch 155/160 (LR 0.00024) => LSC_loss 0.25, Spatial_loss 0.91, Flat_loss 0.11, Train_acc 95.10, Test_acc 60.46
2025-01-11 03:41:46,629 [podnet.py] => Task 4, Epoch 156/160 (LR 0.00015) => LSC_loss 0.25, Spatial_loss 0.89, Flat_loss 0.11, Train_acc 95.48, Test_acc 60.32
2025-01-11 03:41:53,782 [podnet.py] => Task 4, Epoch 157/160 (LR 0.00009) => LSC_loss 0.25, Spatial_loss 0.90, Flat_loss 0.11, Train_acc 95.39, Test_acc 60.24
2025-01-11 03:42:00,778 [podnet.py] => Task 4, Epoch 158/160 (LR 0.00004) => LSC_loss 0.25, Spatial_loss 0.90, Flat_loss 0.11, Train_acc 95.58, Test_acc 60.50
2025-01-11 03:42:08,154 [podnet.py] => Task 4, Epoch 159/160 (LR 0.00001) => LSC_loss 0.24, Spatial_loss 0.90, Flat_loss 0.11, Train_acc 95.83, Test_acc 60.48
2025-01-11 03:42:15,110 [podnet.py] => Task 4, Epoch 160/160 (LR 0.00000) => LSC_loss 0.25, Spatial_loss 0.89, Flat_loss 0.11, Train_acc 95.55, Test_acc 60.40
2025-01-11 03:42:15,111 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 03:42:15,111 [base.py] => Reducing exemplars...(185 per classes)
2025-01-11 03:42:40,731 [base.py] => Constructing exemplars...(185 per classes)
2025-01-11 03:42:54,584 [podnet.py] => The size of finetune dataset: 9250
2025-01-11 03:43:00,162 [podnet.py] => Task 4, Epoch 1/20 (LR 0.00497) => LSC_loss 0.22, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 95.86, Test_acc 60.52
2025-01-11 03:43:05,922 [podnet.py] => Task 4, Epoch 2/20 (LR 0.00488) => LSC_loss 0.23, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 95.17, Test_acc 59.42
2025-01-11 03:43:11,652 [podnet.py] => Task 4, Epoch 3/20 (LR 0.00473) => LSC_loss 0.23, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 95.26, Test_acc 60.10
2025-01-11 03:43:17,195 [podnet.py] => Task 4, Epoch 4/20 (LR 0.00452) => LSC_loss 0.23, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 95.45, Test_acc 59.94
2025-01-11 03:43:22,935 [podnet.py] => Task 4, Epoch 5/20 (LR 0.00427) => LSC_loss 0.22, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 95.54, Test_acc 60.40
2025-01-11 03:43:28,498 [podnet.py] => Task 4, Epoch 6/20 (LR 0.00397) => LSC_loss 0.22, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 95.54, Test_acc 59.96
2025-01-11 03:43:34,078 [podnet.py] => Task 4, Epoch 7/20 (LR 0.00363) => LSC_loss 0.22, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 95.51, Test_acc 60.02
2025-01-11 03:43:39,892 [podnet.py] => Task 4, Epoch 8/20 (LR 0.00327) => LSC_loss 0.21, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 95.85, Test_acc 60.54
2025-01-11 03:43:45,716 [podnet.py] => Task 4, Epoch 9/20 (LR 0.00289) => LSC_loss 0.22, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 95.96, Test_acc 61.06
2025-01-11 03:43:51,405 [podnet.py] => Task 4, Epoch 10/20 (LR 0.00250) => LSC_loss 0.21, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 95.99, Test_acc 60.36
2025-01-11 03:43:57,296 [podnet.py] => Task 4, Epoch 11/20 (LR 0.00211) => LSC_loss 0.22, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 95.78, Test_acc 60.14
2025-01-11 03:44:03,048 [podnet.py] => Task 4, Epoch 12/20 (LR 0.00173) => LSC_loss 0.21, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 96.02, Test_acc 60.38
2025-01-11 03:44:08,837 [podnet.py] => Task 4, Epoch 13/20 (LR 0.00137) => LSC_loss 0.21, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 96.03, Test_acc 60.48
2025-01-11 03:44:14,574 [podnet.py] => Task 4, Epoch 14/20 (LR 0.00103) => LSC_loss 0.20, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 96.34, Test_acc 60.02
2025-01-11 03:44:20,373 [podnet.py] => Task 4, Epoch 15/20 (LR 0.00073) => LSC_loss 0.20, Spatial_loss 0.87, Flat_loss 0.08, Train_acc 96.42, Test_acc 60.62
2025-01-11 03:44:26,107 [podnet.py] => Task 4, Epoch 16/20 (LR 0.00048) => LSC_loss 0.20, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 96.02, Test_acc 60.40
2025-01-11 03:44:31,849 [podnet.py] => Task 4, Epoch 17/20 (LR 0.00027) => LSC_loss 0.21, Spatial_loss 0.86, Flat_loss 0.08, Train_acc 96.13, Test_acc 60.40
2025-01-11 03:44:37,901 [podnet.py] => Task 4, Epoch 18/20 (LR 0.00012) => LSC_loss 0.20, Spatial_loss 0.86, Flat_loss 0.08, Train_acc 96.54, Test_acc 60.54
2025-01-11 03:44:43,919 [podnet.py] => Task 4, Epoch 19/20 (LR 0.00003) => LSC_loss 0.21, Spatial_loss 0.86, Flat_loss 0.08, Train_acc 95.89, Test_acc 60.60
2025-01-11 03:44:49,579 [podnet.py] => Task 4, Epoch 20/20 (LR 0.00000) => LSC_loss 0.20, Spatial_loss 0.85, Flat_loss 0.08, Train_acc 96.37, Test_acc 60.28
2025-01-11 03:44:49,580 [base.py] => Reducing exemplars...(148 per classes)
2025-01-11 03:45:15,275 [base.py] => Constructing exemplars...(148 per classes)
2025-01-11 03:45:30,902 [podnet.py] => Exemplar size: 7400
2025-01-11 03:45:30,902 [trainer.py] => CNN: {'total': np.float64(60.28), '00-09': np.float64(73.7), '10-19': np.float64(47.2), '20-29': np.float64(62.1), '30-39': np.float64(52.4), '40-49': np.float64(66.0), 'old': np.float64(58.85), 'new': np.float64(66.0)}
2025-01-11 03:45:30,902 [trainer.py] => NME: {'total': np.float64(59.32), '00-09': np.float64(75.5), '10-19': np.float64(45.8), '20-29': np.float64(60.6), '30-39': np.float64(51.0), '40-49': np.float64(63.7), 'old': np.float64(58.22), 'new': np.float64(63.7)}
2025-01-11 03:45:30,902 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42), np.float64(60.28)]
2025-01-11 03:45:30,902 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82), np.float64(85.98)]
2025-01-11 03:45:30,902 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38), np.float64(59.32)]
2025-01-11 03:45:30,902 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02), np.float64(85.44)]

2025-01-11 03:45:30,903 [trainer.py] => All params: 498257
2025-01-11 03:45:30,903 [trainer.py] => Trainable params: 498257
2025-01-11 03:45:30,904 [podnet.py] => Learning on 50-60
2025-01-11 03:45:31,017 [podnet.py] => Adaptive factor: 2.449489742783178
2025-01-11 03:45:38,087 [podnet.py] => Task 5, Epoch 1/160 (LR 0.09999) => LSC_loss 2.37, Spatial_loss 2.77, Flat_loss 0.62, Train_acc 41.76, Test_acc 36.67
2025-01-11 03:45:45,247 [podnet.py] => Task 5, Epoch 2/160 (LR 0.09996) => LSC_loss 1.70, Spatial_loss 2.39, Flat_loss 0.35, Train_acc 52.29, Test_acc 36.00
2025-01-11 03:45:52,327 [podnet.py] => Task 5, Epoch 3/160 (LR 0.09991) => LSC_loss 1.54, Spatial_loss 2.22, Flat_loss 0.30, Train_acc 56.86, Test_acc 42.23
2025-01-11 03:45:59,452 [podnet.py] => Task 5, Epoch 4/160 (LR 0.09985) => LSC_loss 1.46, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 58.90, Test_acc 41.27
2025-01-11 03:46:06,639 [podnet.py] => Task 5, Epoch 5/160 (LR 0.09976) => LSC_loss 1.41, Spatial_loss 2.09, Flat_loss 0.27, Train_acc 60.38, Test_acc 44.97
2025-01-11 03:46:13,856 [podnet.py] => Task 5, Epoch 6/160 (LR 0.09965) => LSC_loss 1.37, Spatial_loss 2.06, Flat_loss 0.26, Train_acc 61.47, Test_acc 44.08
2025-01-11 03:46:21,078 [podnet.py] => Task 5, Epoch 7/160 (LR 0.09953) => LSC_loss 1.35, Spatial_loss 2.04, Flat_loss 0.26, Train_acc 62.17, Test_acc 45.57
2025-01-11 03:46:28,174 [podnet.py] => Task 5, Epoch 8/160 (LR 0.09938) => LSC_loss 1.31, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 62.87, Test_acc 44.48
2025-01-11 03:46:35,447 [podnet.py] => Task 5, Epoch 9/160 (LR 0.09922) => LSC_loss 1.29, Spatial_loss 2.03, Flat_loss 0.26, Train_acc 63.44, Test_acc 43.23
2025-01-11 03:46:42,667 [podnet.py] => Task 5, Epoch 10/160 (LR 0.09904) => LSC_loss 1.27, Spatial_loss 1.99, Flat_loss 0.26, Train_acc 64.38, Test_acc 45.92
2025-01-11 03:46:50,158 [podnet.py] => Task 5, Epoch 11/160 (LR 0.09884) => LSC_loss 1.25, Spatial_loss 1.98, Flat_loss 0.26, Train_acc 65.23, Test_acc 41.07
2025-01-11 03:46:57,179 [podnet.py] => Task 5, Epoch 12/160 (LR 0.09862) => LSC_loss 1.22, Spatial_loss 2.00, Flat_loss 0.25, Train_acc 66.04, Test_acc 45.95
2025-01-11 03:47:04,810 [podnet.py] => Task 5, Epoch 13/160 (LR 0.09838) => LSC_loss 1.21, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 66.07, Test_acc 40.65
2025-01-11 03:47:12,028 [podnet.py] => Task 5, Epoch 14/160 (LR 0.09812) => LSC_loss 1.21, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 66.10, Test_acc 47.77
2025-01-11 03:47:19,429 [podnet.py] => Task 5, Epoch 15/160 (LR 0.09785) => LSC_loss 1.20, Spatial_loss 1.98, Flat_loss 0.25, Train_acc 66.32, Test_acc 38.38
2025-01-11 03:47:26,664 [podnet.py] => Task 5, Epoch 16/160 (LR 0.09755) => LSC_loss 1.17, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 66.79, Test_acc 42.62
2025-01-11 03:47:33,956 [podnet.py] => Task 5, Epoch 17/160 (LR 0.09724) => LSC_loss 1.20, Spatial_loss 1.98, Flat_loss 0.25, Train_acc 66.38, Test_acc 40.22
2025-01-11 03:47:41,069 [podnet.py] => Task 5, Epoch 18/160 (LR 0.09691) => LSC_loss 1.17, Spatial_loss 1.97, Flat_loss 0.25, Train_acc 67.49, Test_acc 46.60
2025-01-11 03:47:48,336 [podnet.py] => Task 5, Epoch 19/160 (LR 0.09656) => LSC_loss 1.14, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 68.48, Test_acc 42.75
2025-01-11 03:47:55,548 [podnet.py] => Task 5, Epoch 20/160 (LR 0.09619) => LSC_loss 1.15, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 67.78, Test_acc 45.72
2025-01-11 03:48:02,789 [podnet.py] => Task 5, Epoch 21/160 (LR 0.09581) => LSC_loss 1.15, Spatial_loss 1.95, Flat_loss 0.26, Train_acc 68.06, Test_acc 44.38
2025-01-11 03:48:10,404 [podnet.py] => Task 5, Epoch 22/160 (LR 0.09541) => LSC_loss 1.12, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 68.65, Test_acc 47.13
2025-01-11 03:48:17,590 [podnet.py] => Task 5, Epoch 23/160 (LR 0.09499) => LSC_loss 1.13, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 68.39, Test_acc 47.17
2025-01-11 03:48:24,830 [podnet.py] => Task 5, Epoch 24/160 (LR 0.09455) => LSC_loss 1.11, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 68.99, Test_acc 41.10
2025-01-11 03:48:31,928 [podnet.py] => Task 5, Epoch 25/160 (LR 0.09410) => LSC_loss 1.12, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 68.26, Test_acc 46.35
2025-01-11 03:48:39,041 [podnet.py] => Task 5, Epoch 26/160 (LR 0.09362) => LSC_loss 1.11, Spatial_loss 1.94, Flat_loss 0.25, Train_acc 68.99, Test_acc 46.52
2025-01-11 03:48:46,312 [podnet.py] => Task 5, Epoch 27/160 (LR 0.09314) => LSC_loss 1.11, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 68.98, Test_acc 46.25
2025-01-11 03:48:53,689 [podnet.py] => Task 5, Epoch 28/160 (LR 0.09263) => LSC_loss 1.11, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 69.27, Test_acc 43.65
2025-01-11 03:49:00,917 [podnet.py] => Task 5, Epoch 29/160 (LR 0.09211) => LSC_loss 1.08, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 69.73, Test_acc 45.02
2025-01-11 03:49:08,642 [podnet.py] => Task 5, Epoch 30/160 (LR 0.09157) => LSC_loss 1.10, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 69.28, Test_acc 46.97
2025-01-11 03:49:15,855 [podnet.py] => Task 5, Epoch 31/160 (LR 0.09102) => LSC_loss 1.06, Spatial_loss 1.92, Flat_loss 0.25, Train_acc 70.08, Test_acc 44.30
2025-01-11 03:49:23,112 [podnet.py] => Task 5, Epoch 32/160 (LR 0.09045) => LSC_loss 1.10, Spatial_loss 1.94, Flat_loss 0.26, Train_acc 69.09, Test_acc 41.48
2025-01-11 03:49:30,247 [podnet.py] => Task 5, Epoch 33/160 (LR 0.08987) => LSC_loss 1.06, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 69.52, Test_acc 46.78
2025-01-11 03:49:37,650 [podnet.py] => Task 5, Epoch 34/160 (LR 0.08927) => LSC_loss 1.08, Spatial_loss 1.92, Flat_loss 0.26, Train_acc 69.77, Test_acc 46.52
2025-01-11 03:49:44,954 [podnet.py] => Task 5, Epoch 35/160 (LR 0.08865) => LSC_loss 1.04, Spatial_loss 1.88, Flat_loss 0.25, Train_acc 70.90, Test_acc 45.45
2025-01-11 03:49:52,419 [podnet.py] => Task 5, Epoch 36/160 (LR 0.08802) => LSC_loss 1.06, Spatial_loss 1.90, Flat_loss 0.25, Train_acc 69.85, Test_acc 47.37
2025-01-11 03:49:59,752 [podnet.py] => Task 5, Epoch 37/160 (LR 0.08738) => LSC_loss 1.03, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 70.82, Test_acc 49.80
2025-01-11 03:50:07,306 [podnet.py] => Task 5, Epoch 38/160 (LR 0.08672) => LSC_loss 1.02, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 71.69, Test_acc 46.98
2025-01-11 03:50:14,364 [podnet.py] => Task 5, Epoch 39/160 (LR 0.08604) => LSC_loss 1.03, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 71.03, Test_acc 43.67
2025-01-11 03:50:21,551 [podnet.py] => Task 5, Epoch 40/160 (LR 0.08536) => LSC_loss 1.02, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 71.60, Test_acc 45.27
2025-01-11 03:50:28,645 [podnet.py] => Task 5, Epoch 41/160 (LR 0.08465) => LSC_loss 1.01, Spatial_loss 1.88, Flat_loss 0.25, Train_acc 71.56, Test_acc 44.15
2025-01-11 03:50:35,939 [podnet.py] => Task 5, Epoch 42/160 (LR 0.08394) => LSC_loss 1.03, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 70.85, Test_acc 48.62
2025-01-11 03:50:43,319 [podnet.py] => Task 5, Epoch 43/160 (LR 0.08321) => LSC_loss 1.01, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 71.41, Test_acc 46.60
2025-01-11 03:50:50,500 [podnet.py] => Task 5, Epoch 44/160 (LR 0.08247) => LSC_loss 1.03, Spatial_loss 1.90, Flat_loss 0.25, Train_acc 71.17, Test_acc 47.03
2025-01-11 03:50:57,554 [podnet.py] => Task 5, Epoch 45/160 (LR 0.08172) => LSC_loss 0.99, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 72.24, Test_acc 45.18
2025-01-11 03:51:04,688 [podnet.py] => Task 5, Epoch 46/160 (LR 0.08095) => LSC_loss 1.00, Spatial_loss 1.87, Flat_loss 0.25, Train_acc 71.68, Test_acc 45.88
2025-01-11 03:51:11,935 [podnet.py] => Task 5, Epoch 47/160 (LR 0.08018) => LSC_loss 1.01, Spatial_loss 1.90, Flat_loss 0.25, Train_acc 71.37, Test_acc 47.33
2025-01-11 03:51:19,093 [podnet.py] => Task 5, Epoch 48/160 (LR 0.07939) => LSC_loss 0.97, Spatial_loss 1.85, Flat_loss 0.24, Train_acc 72.72, Test_acc 49.27
2025-01-11 03:51:26,343 [podnet.py] => Task 5, Epoch 49/160 (LR 0.07859) => LSC_loss 0.98, Spatial_loss 1.88, Flat_loss 0.25, Train_acc 72.73, Test_acc 50.07
2025-01-11 03:51:33,451 [podnet.py] => Task 5, Epoch 50/160 (LR 0.07778) => LSC_loss 0.98, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 72.51, Test_acc 47.02
2025-01-11 03:51:40,606 [podnet.py] => Task 5, Epoch 51/160 (LR 0.07696) => LSC_loss 0.96, Spatial_loss 1.83, Flat_loss 0.24, Train_acc 73.01, Test_acc 48.23
2025-01-11 03:51:47,749 [podnet.py] => Task 5, Epoch 52/160 (LR 0.07612) => LSC_loss 0.99, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 72.07, Test_acc 48.30
2025-01-11 03:51:55,054 [podnet.py] => Task 5, Epoch 53/160 (LR 0.07528) => LSC_loss 0.96, Spatial_loss 1.83, Flat_loss 0.24, Train_acc 73.08, Test_acc 47.33
2025-01-11 03:52:02,290 [podnet.py] => Task 5, Epoch 54/160 (LR 0.07443) => LSC_loss 0.97, Spatial_loss 1.87, Flat_loss 0.25, Train_acc 72.92, Test_acc 45.67
2025-01-11 03:52:09,455 [podnet.py] => Task 5, Epoch 55/160 (LR 0.07357) => LSC_loss 0.93, Spatial_loss 1.80, Flat_loss 0.24, Train_acc 74.22, Test_acc 45.40
2025-01-11 03:52:16,639 [podnet.py] => Task 5, Epoch 56/160 (LR 0.07270) => LSC_loss 0.95, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 73.03, Test_acc 49.73
2025-01-11 03:52:23,730 [podnet.py] => Task 5, Epoch 57/160 (LR 0.07182) => LSC_loss 0.95, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 73.37, Test_acc 45.28
2025-01-11 03:52:31,021 [podnet.py] => Task 5, Epoch 58/160 (LR 0.07093) => LSC_loss 0.94, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 74.24, Test_acc 46.23
2025-01-11 03:52:38,144 [podnet.py] => Task 5, Epoch 59/160 (LR 0.07004) => LSC_loss 0.95, Spatial_loss 1.82, Flat_loss 0.24, Train_acc 73.28, Test_acc 43.70
2025-01-11 03:52:45,399 [podnet.py] => Task 5, Epoch 60/160 (LR 0.06913) => LSC_loss 0.93, Spatial_loss 1.80, Flat_loss 0.24, Train_acc 74.06, Test_acc 51.28
2025-01-11 03:52:52,584 [podnet.py] => Task 5, Epoch 61/160 (LR 0.06822) => LSC_loss 0.91, Spatial_loss 1.79, Flat_loss 0.23, Train_acc 74.65, Test_acc 47.15
2025-01-11 03:52:59,701 [podnet.py] => Task 5, Epoch 62/160 (LR 0.06731) => LSC_loss 0.90, Spatial_loss 1.83, Flat_loss 0.24, Train_acc 74.78, Test_acc 47.90
2025-01-11 03:53:07,349 [podnet.py] => Task 5, Epoch 63/160 (LR 0.06638) => LSC_loss 0.89, Spatial_loss 1.77, Flat_loss 0.23, Train_acc 75.06, Test_acc 44.88
2025-01-11 03:53:14,475 [podnet.py] => Task 5, Epoch 64/160 (LR 0.06545) => LSC_loss 0.89, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 75.10, Test_acc 51.08
2025-01-11 03:53:21,732 [podnet.py] => Task 5, Epoch 65/160 (LR 0.06451) => LSC_loss 0.88, Spatial_loss 1.76, Flat_loss 0.23, Train_acc 75.54, Test_acc 46.62
2025-01-11 03:53:28,869 [podnet.py] => Task 5, Epoch 66/160 (LR 0.06357) => LSC_loss 0.85, Spatial_loss 1.77, Flat_loss 0.23, Train_acc 76.01, Test_acc 48.95
2025-01-11 03:53:35,974 [podnet.py] => Task 5, Epoch 67/160 (LR 0.06262) => LSC_loss 0.86, Spatial_loss 1.78, Flat_loss 0.23, Train_acc 75.77, Test_acc 42.37
2025-01-11 03:53:43,141 [podnet.py] => Task 5, Epoch 68/160 (LR 0.06167) => LSC_loss 0.89, Spatial_loss 1.78, Flat_loss 0.24, Train_acc 74.91, Test_acc 50.75
2025-01-11 03:53:50,385 [podnet.py] => Task 5, Epoch 69/160 (LR 0.06072) => LSC_loss 0.85, Spatial_loss 1.76, Flat_loss 0.23, Train_acc 76.51, Test_acc 48.00
2025-01-11 03:53:57,621 [podnet.py] => Task 5, Epoch 70/160 (LR 0.05975) => LSC_loss 0.86, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 75.81, Test_acc 46.43
2025-01-11 03:54:04,988 [podnet.py] => Task 5, Epoch 71/160 (LR 0.05879) => LSC_loss 0.85, Spatial_loss 1.77, Flat_loss 0.23, Train_acc 76.05, Test_acc 47.98
2025-01-11 03:54:12,349 [podnet.py] => Task 5, Epoch 72/160 (LR 0.05782) => LSC_loss 0.83, Spatial_loss 1.71, Flat_loss 0.23, Train_acc 77.04, Test_acc 48.68
2025-01-11 03:54:19,634 [podnet.py] => Task 5, Epoch 73/160 (LR 0.05685) => LSC_loss 0.85, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 76.33, Test_acc 47.17
2025-01-11 03:54:26,859 [podnet.py] => Task 5, Epoch 74/160 (LR 0.05588) => LSC_loss 0.84, Spatial_loss 1.72, Flat_loss 0.23, Train_acc 76.69, Test_acc 49.68
2025-01-11 03:54:34,153 [podnet.py] => Task 5, Epoch 75/160 (LR 0.05490) => LSC_loss 0.82, Spatial_loss 1.72, Flat_loss 0.23, Train_acc 76.80, Test_acc 47.93
2025-01-11 03:54:41,459 [podnet.py] => Task 5, Epoch 76/160 (LR 0.05392) => LSC_loss 0.81, Spatial_loss 1.70, Flat_loss 0.22, Train_acc 77.35, Test_acc 50.88
2025-01-11 03:54:48,689 [podnet.py] => Task 5, Epoch 77/160 (LR 0.05294) => LSC_loss 0.81, Spatial_loss 1.71, Flat_loss 0.23, Train_acc 77.15, Test_acc 50.20
2025-01-11 03:54:55,852 [podnet.py] => Task 5, Epoch 78/160 (LR 0.05196) => LSC_loss 0.80, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 78.17, Test_acc 49.72
2025-01-11 03:55:03,020 [podnet.py] => Task 5, Epoch 79/160 (LR 0.05098) => LSC_loss 0.80, Spatial_loss 1.70, Flat_loss 0.23, Train_acc 77.41, Test_acc 51.17
2025-01-11 03:55:10,247 [podnet.py] => Task 5, Epoch 80/160 (LR 0.05000) => LSC_loss 0.81, Spatial_loss 1.73, Flat_loss 0.23, Train_acc 76.80, Test_acc 51.18
2025-01-11 03:55:17,654 [podnet.py] => Task 5, Epoch 81/160 (LR 0.04902) => LSC_loss 0.78, Spatial_loss 1.69, Flat_loss 0.22, Train_acc 78.31, Test_acc 48.95
2025-01-11 03:55:24,857 [podnet.py] => Task 5, Epoch 82/160 (LR 0.04804) => LSC_loss 0.75, Spatial_loss 1.63, Flat_loss 0.22, Train_acc 79.17, Test_acc 49.70
2025-01-11 03:55:32,051 [podnet.py] => Task 5, Epoch 83/160 (LR 0.04706) => LSC_loss 0.76, Spatial_loss 1.66, Flat_loss 0.22, Train_acc 78.90, Test_acc 49.20
2025-01-11 03:55:39,252 [podnet.py] => Task 5, Epoch 84/160 (LR 0.04608) => LSC_loss 0.72, Spatial_loss 1.61, Flat_loss 0.21, Train_acc 80.38, Test_acc 51.55
2025-01-11 03:55:46,562 [podnet.py] => Task 5, Epoch 85/160 (LR 0.04510) => LSC_loss 0.73, Spatial_loss 1.62, Flat_loss 0.22, Train_acc 79.94, Test_acc 50.32
2025-01-11 03:55:53,757 [podnet.py] => Task 5, Epoch 86/160 (LR 0.04412) => LSC_loss 0.73, Spatial_loss 1.60, Flat_loss 0.21, Train_acc 79.31, Test_acc 48.87
2025-01-11 03:56:01,053 [podnet.py] => Task 5, Epoch 87/160 (LR 0.04315) => LSC_loss 0.74, Spatial_loss 1.64, Flat_loss 0.22, Train_acc 79.10, Test_acc 51.40
2025-01-11 03:56:08,315 [podnet.py] => Task 5, Epoch 88/160 (LR 0.04218) => LSC_loss 0.73, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 79.64, Test_acc 49.43
2025-01-11 03:56:15,430 [podnet.py] => Task 5, Epoch 89/160 (LR 0.04121) => LSC_loss 0.71, Spatial_loss 1.62, Flat_loss 0.21, Train_acc 80.31, Test_acc 51.48
2025-01-11 03:56:22,555 [podnet.py] => Task 5, Epoch 90/160 (LR 0.04025) => LSC_loss 0.68, Spatial_loss 1.56, Flat_loss 0.21, Train_acc 81.74, Test_acc 50.43
2025-01-11 03:56:29,846 [podnet.py] => Task 5, Epoch 91/160 (LR 0.03928) => LSC_loss 0.69, Spatial_loss 1.55, Flat_loss 0.21, Train_acc 80.52, Test_acc 50.72
2025-01-11 03:56:37,044 [podnet.py] => Task 5, Epoch 92/160 (LR 0.03833) => LSC_loss 0.70, Spatial_loss 1.61, Flat_loss 0.21, Train_acc 80.44, Test_acc 50.35
2025-01-11 03:56:44,176 [podnet.py] => Task 5, Epoch 93/160 (LR 0.03738) => LSC_loss 0.67, Spatial_loss 1.54, Flat_loss 0.21, Train_acc 81.78, Test_acc 51.85
2025-01-11 03:56:51,388 [podnet.py] => Task 5, Epoch 94/160 (LR 0.03643) => LSC_loss 0.66, Spatial_loss 1.53, Flat_loss 0.20, Train_acc 81.89, Test_acc 51.95
2025-01-11 03:56:58,505 [podnet.py] => Task 5, Epoch 95/160 (LR 0.03549) => LSC_loss 0.65, Spatial_loss 1.51, Flat_loss 0.20, Train_acc 82.40, Test_acc 48.60
2025-01-11 03:57:05,677 [podnet.py] => Task 5, Epoch 96/160 (LR 0.03455) => LSC_loss 0.66, Spatial_loss 1.54, Flat_loss 0.20, Train_acc 81.79, Test_acc 50.53
2025-01-11 03:57:13,216 [podnet.py] => Task 5, Epoch 97/160 (LR 0.03362) => LSC_loss 0.62, Spatial_loss 1.51, Flat_loss 0.20, Train_acc 82.86, Test_acc 48.27
2025-01-11 03:57:20,351 [podnet.py] => Task 5, Epoch 98/160 (LR 0.03269) => LSC_loss 0.62, Spatial_loss 1.50, Flat_loss 0.20, Train_acc 83.01, Test_acc 51.85
2025-01-11 03:57:27,417 [podnet.py] => Task 5, Epoch 99/160 (LR 0.03178) => LSC_loss 0.62, Spatial_loss 1.48, Flat_loss 0.20, Train_acc 83.06, Test_acc 52.72
2025-01-11 03:57:34,588 [podnet.py] => Task 5, Epoch 100/160 (LR 0.03087) => LSC_loss 0.63, Spatial_loss 1.48, Flat_loss 0.20, Train_acc 82.85, Test_acc 52.47
2025-01-11 03:57:41,778 [podnet.py] => Task 5, Epoch 101/160 (LR 0.02996) => LSC_loss 0.60, Spatial_loss 1.47, Flat_loss 0.19, Train_acc 83.53, Test_acc 52.72
2025-01-11 03:57:48,977 [podnet.py] => Task 5, Epoch 102/160 (LR 0.02907) => LSC_loss 0.60, Spatial_loss 1.46, Flat_loss 0.19, Train_acc 83.56, Test_acc 51.20
2025-01-11 03:57:56,054 [podnet.py] => Task 5, Epoch 103/160 (LR 0.02818) => LSC_loss 0.59, Spatial_loss 1.46, Flat_loss 0.19, Train_acc 84.20, Test_acc 51.23
2025-01-11 03:58:03,583 [podnet.py] => Task 5, Epoch 104/160 (LR 0.02730) => LSC_loss 0.59, Spatial_loss 1.47, Flat_loss 0.19, Train_acc 84.06, Test_acc 50.85
2025-01-11 03:58:10,786 [podnet.py] => Task 5, Epoch 105/160 (LR 0.02643) => LSC_loss 0.56, Spatial_loss 1.42, Flat_loss 0.19, Train_acc 84.80, Test_acc 50.20
2025-01-11 03:58:18,057 [podnet.py] => Task 5, Epoch 106/160 (LR 0.02557) => LSC_loss 0.56, Spatial_loss 1.41, Flat_loss 0.19, Train_acc 85.30, Test_acc 53.30
2025-01-11 03:58:25,289 [podnet.py] => Task 5, Epoch 107/160 (LR 0.02472) => LSC_loss 0.54, Spatial_loss 1.38, Flat_loss 0.18, Train_acc 85.73, Test_acc 49.75
2025-01-11 03:58:32,604 [podnet.py] => Task 5, Epoch 108/160 (LR 0.02388) => LSC_loss 0.55, Spatial_loss 1.40, Flat_loss 0.19, Train_acc 85.23, Test_acc 53.50
2025-01-11 03:58:39,801 [podnet.py] => Task 5, Epoch 109/160 (LR 0.02304) => LSC_loss 0.54, Spatial_loss 1.37, Flat_loss 0.18, Train_acc 86.02, Test_acc 53.83
2025-01-11 03:58:46,950 [podnet.py] => Task 5, Epoch 110/160 (LR 0.02222) => LSC_loss 0.51, Spatial_loss 1.34, Flat_loss 0.18, Train_acc 86.60, Test_acc 52.33
2025-01-11 03:58:54,217 [podnet.py] => Task 5, Epoch 111/160 (LR 0.02141) => LSC_loss 0.53, Spatial_loss 1.37, Flat_loss 0.18, Train_acc 86.21, Test_acc 52.25
2025-01-11 03:59:01,420 [podnet.py] => Task 5, Epoch 112/160 (LR 0.02061) => LSC_loss 0.50, Spatial_loss 1.35, Flat_loss 0.18, Train_acc 86.96, Test_acc 53.12
2025-01-11 03:59:08,785 [podnet.py] => Task 5, Epoch 113/160 (LR 0.01982) => LSC_loss 0.51, Spatial_loss 1.33, Flat_loss 0.18, Train_acc 87.02, Test_acc 51.87
2025-01-11 03:59:15,958 [podnet.py] => Task 5, Epoch 114/160 (LR 0.01905) => LSC_loss 0.49, Spatial_loss 1.32, Flat_loss 0.18, Train_acc 87.54, Test_acc 52.93
2025-01-11 03:59:23,103 [podnet.py] => Task 5, Epoch 115/160 (LR 0.01828) => LSC_loss 0.48, Spatial_loss 1.30, Flat_loss 0.18, Train_acc 88.06, Test_acc 53.65
2025-01-11 03:59:30,412 [podnet.py] => Task 5, Epoch 116/160 (LR 0.01753) => LSC_loss 0.48, Spatial_loss 1.29, Flat_loss 0.17, Train_acc 87.82, Test_acc 52.60
2025-01-11 03:59:37,636 [podnet.py] => Task 5, Epoch 117/160 (LR 0.01679) => LSC_loss 0.46, Spatial_loss 1.30, Flat_loss 0.17, Train_acc 88.19, Test_acc 53.07
2025-01-11 03:59:44,819 [podnet.py] => Task 5, Epoch 118/160 (LR 0.01606) => LSC_loss 0.45, Spatial_loss 1.29, Flat_loss 0.17, Train_acc 88.75, Test_acc 53.77
2025-01-11 03:59:52,061 [podnet.py] => Task 5, Epoch 119/160 (LR 0.01535) => LSC_loss 0.46, Spatial_loss 1.28, Flat_loss 0.17, Train_acc 88.31, Test_acc 53.07
2025-01-11 03:59:59,331 [podnet.py] => Task 5, Epoch 120/160 (LR 0.01464) => LSC_loss 0.45, Spatial_loss 1.27, Flat_loss 0.17, Train_acc 88.84, Test_acc 54.90
2025-01-11 04:00:06,661 [podnet.py] => Task 5, Epoch 121/160 (LR 0.01396) => LSC_loss 0.44, Spatial_loss 1.24, Flat_loss 0.17, Train_acc 89.03, Test_acc 54.53
2025-01-11 04:00:13,930 [podnet.py] => Task 5, Epoch 122/160 (LR 0.01328) => LSC_loss 0.44, Spatial_loss 1.23, Flat_loss 0.17, Train_acc 89.44, Test_acc 55.03
2025-01-11 04:00:21,009 [podnet.py] => Task 5, Epoch 123/160 (LR 0.01262) => LSC_loss 0.42, Spatial_loss 1.20, Flat_loss 0.16, Train_acc 89.79, Test_acc 54.67
2025-01-11 04:00:28,164 [podnet.py] => Task 5, Epoch 124/160 (LR 0.01198) => LSC_loss 0.42, Spatial_loss 1.18, Flat_loss 0.16, Train_acc 89.79, Test_acc 55.30
2025-01-11 04:00:35,312 [podnet.py] => Task 5, Epoch 125/160 (LR 0.01135) => LSC_loss 0.41, Spatial_loss 1.20, Flat_loss 0.16, Train_acc 90.02, Test_acc 54.37
2025-01-11 04:00:42,418 [podnet.py] => Task 5, Epoch 126/160 (LR 0.01073) => LSC_loss 0.40, Spatial_loss 1.18, Flat_loss 0.16, Train_acc 90.34, Test_acc 54.85
2025-01-11 04:00:49,669 [podnet.py] => Task 5, Epoch 127/160 (LR 0.01013) => LSC_loss 0.39, Spatial_loss 1.18, Flat_loss 0.16, Train_acc 90.69, Test_acc 54.70
2025-01-11 04:00:56,801 [podnet.py] => Task 5, Epoch 128/160 (LR 0.00955) => LSC_loss 0.39, Spatial_loss 1.15, Flat_loss 0.16, Train_acc 90.89, Test_acc 54.77
2025-01-11 04:01:04,125 [podnet.py] => Task 5, Epoch 129/160 (LR 0.00898) => LSC_loss 0.39, Spatial_loss 1.15, Flat_loss 0.16, Train_acc 91.19, Test_acc 55.32
2025-01-11 04:01:11,385 [podnet.py] => Task 5, Epoch 130/160 (LR 0.00843) => LSC_loss 0.37, Spatial_loss 1.14, Flat_loss 0.16, Train_acc 91.66, Test_acc 54.22
2025-01-11 04:01:18,521 [podnet.py] => Task 5, Epoch 131/160 (LR 0.00789) => LSC_loss 0.37, Spatial_loss 1.12, Flat_loss 0.15, Train_acc 91.07, Test_acc 55.72
2025-01-11 04:01:25,897 [podnet.py] => Task 5, Epoch 132/160 (LR 0.00737) => LSC_loss 0.37, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 91.58, Test_acc 55.12
2025-01-11 04:01:33,335 [podnet.py] => Task 5, Epoch 133/160 (LR 0.00686) => LSC_loss 0.36, Spatial_loss 1.11, Flat_loss 0.15, Train_acc 91.88, Test_acc 55.28
2025-01-11 04:01:40,427 [podnet.py] => Task 5, Epoch 134/160 (LR 0.00638) => LSC_loss 0.36, Spatial_loss 1.10, Flat_loss 0.15, Train_acc 91.96, Test_acc 54.52
2025-01-11 04:01:47,672 [podnet.py] => Task 5, Epoch 135/160 (LR 0.00590) => LSC_loss 0.36, Spatial_loss 1.10, Flat_loss 0.15, Train_acc 91.85, Test_acc 55.58
2025-01-11 04:01:55,071 [podnet.py] => Task 5, Epoch 136/160 (LR 0.00545) => LSC_loss 0.35, Spatial_loss 1.08, Flat_loss 0.15, Train_acc 92.01, Test_acc 55.87
2025-01-11 04:02:02,351 [podnet.py] => Task 5, Epoch 137/160 (LR 0.00501) => LSC_loss 0.34, Spatial_loss 1.07, Flat_loss 0.15, Train_acc 92.70, Test_acc 56.23
2025-01-11 04:02:09,600 [podnet.py] => Task 5, Epoch 138/160 (LR 0.00459) => LSC_loss 0.35, Spatial_loss 1.08, Flat_loss 0.15, Train_acc 92.40, Test_acc 55.28
2025-01-11 04:02:16,858 [podnet.py] => Task 5, Epoch 139/160 (LR 0.00419) => LSC_loss 0.34, Spatial_loss 1.06, Flat_loss 0.15, Train_acc 92.66, Test_acc 55.87
2025-01-11 04:02:24,126 [podnet.py] => Task 5, Epoch 140/160 (LR 0.00381) => LSC_loss 0.33, Spatial_loss 1.06, Flat_loss 0.15, Train_acc 92.76, Test_acc 55.77
2025-01-11 04:02:31,438 [podnet.py] => Task 5, Epoch 141/160 (LR 0.00344) => LSC_loss 0.33, Spatial_loss 1.03, Flat_loss 0.14, Train_acc 93.18, Test_acc 56.15
2025-01-11 04:02:38,552 [podnet.py] => Task 5, Epoch 142/160 (LR 0.00309) => LSC_loss 0.32, Spatial_loss 1.03, Flat_loss 0.14, Train_acc 93.35, Test_acc 55.98
2025-01-11 04:02:45,858 [podnet.py] => Task 5, Epoch 143/160 (LR 0.00276) => LSC_loss 0.33, Spatial_loss 1.03, Flat_loss 0.14, Train_acc 92.94, Test_acc 56.15
2025-01-11 04:02:52,986 [podnet.py] => Task 5, Epoch 144/160 (LR 0.00245) => LSC_loss 0.33, Spatial_loss 1.03, Flat_loss 0.14, Train_acc 92.90, Test_acc 56.18
2025-01-11 04:03:00,210 [podnet.py] => Task 5, Epoch 145/160 (LR 0.00215) => LSC_loss 0.32, Spatial_loss 1.03, Flat_loss 0.14, Train_acc 93.46, Test_acc 56.32
2025-01-11 04:03:07,678 [podnet.py] => Task 5, Epoch 146/160 (LR 0.00188) => LSC_loss 0.32, Spatial_loss 1.01, Flat_loss 0.14, Train_acc 93.40, Test_acc 56.15
2025-01-11 04:03:14,816 [podnet.py] => Task 5, Epoch 147/160 (LR 0.00162) => LSC_loss 0.32, Spatial_loss 1.01, Flat_loss 0.14, Train_acc 93.35, Test_acc 56.38
2025-01-11 04:03:22,007 [podnet.py] => Task 5, Epoch 148/160 (LR 0.00138) => LSC_loss 0.32, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 93.48, Test_acc 56.53
2025-01-11 04:03:29,223 [podnet.py] => Task 5, Epoch 149/160 (LR 0.00116) => LSC_loss 0.32, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 93.50, Test_acc 56.05
2025-01-11 04:03:36,415 [podnet.py] => Task 5, Epoch 150/160 (LR 0.00096) => LSC_loss 0.31, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 93.78, Test_acc 56.13
2025-01-11 04:03:43,779 [podnet.py] => Task 5, Epoch 151/160 (LR 0.00078) => LSC_loss 0.31, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 93.42, Test_acc 56.02
2025-01-11 04:03:51,147 [podnet.py] => Task 5, Epoch 152/160 (LR 0.00062) => LSC_loss 0.31, Spatial_loss 0.99, Flat_loss 0.14, Train_acc 93.97, Test_acc 56.28
2025-01-11 04:03:58,509 [podnet.py] => Task 5, Epoch 153/160 (LR 0.00047) => LSC_loss 0.31, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 93.65, Test_acc 56.47
2025-01-11 04:04:05,883 [podnet.py] => Task 5, Epoch 154/160 (LR 0.00035) => LSC_loss 0.30, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 94.28, Test_acc 56.22
2025-01-11 04:04:13,226 [podnet.py] => Task 5, Epoch 155/160 (LR 0.00024) => LSC_loss 0.30, Spatial_loss 0.97, Flat_loss 0.14, Train_acc 94.15, Test_acc 56.48
2025-01-11 04:04:20,595 [podnet.py] => Task 5, Epoch 156/160 (LR 0.00015) => LSC_loss 0.31, Spatial_loss 0.97, Flat_loss 0.14, Train_acc 93.75, Test_acc 56.40
2025-01-11 04:04:27,845 [podnet.py] => Task 5, Epoch 157/160 (LR 0.00009) => LSC_loss 0.31, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 93.88, Test_acc 56.55
2025-01-11 04:04:35,366 [podnet.py] => Task 5, Epoch 158/160 (LR 0.00004) => LSC_loss 0.30, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 94.01, Test_acc 56.23
2025-01-11 04:04:42,848 [podnet.py] => Task 5, Epoch 159/160 (LR 0.00001) => LSC_loss 0.30, Spatial_loss 0.98, Flat_loss 0.14, Train_acc 94.06, Test_acc 56.25
2025-01-11 04:04:50,243 [podnet.py] => Task 5, Epoch 160/160 (LR 0.00000) => LSC_loss 0.30, Spatial_loss 0.97, Flat_loss 0.14, Train_acc 93.83, Test_acc 56.30
2025-01-11 04:04:50,244 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 04:04:50,244 [base.py] => Reducing exemplars...(148 per classes)
2025-01-11 04:05:24,673 [base.py] => Constructing exemplars...(148 per classes)
2025-01-11 04:05:39,114 [podnet.py] => The size of finetune dataset: 8880
2025-01-11 04:05:45,348 [podnet.py] => Task 5, Epoch 1/20 (LR 0.00497) => LSC_loss 0.27, Spatial_loss 1.03, Flat_loss 0.11, Train_acc 94.49, Test_acc 55.77
2025-01-11 04:05:51,206 [podnet.py] => Task 5, Epoch 2/20 (LR 0.00488) => LSC_loss 0.27, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 94.63, Test_acc 56.32
2025-01-11 04:05:57,143 [podnet.py] => Task 5, Epoch 3/20 (LR 0.00473) => LSC_loss 0.27, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 94.37, Test_acc 55.92
2025-01-11 04:06:03,046 [podnet.py] => Task 5, Epoch 4/20 (LR 0.00452) => LSC_loss 0.27, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 94.47, Test_acc 56.28
2025-01-11 04:06:09,014 [podnet.py] => Task 5, Epoch 5/20 (LR 0.00427) => LSC_loss 0.26, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 94.38, Test_acc 56.68
2025-01-11 04:06:14,895 [podnet.py] => Task 5, Epoch 6/20 (LR 0.00397) => LSC_loss 0.25, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 94.82, Test_acc 55.98
2025-01-11 04:06:20,774 [podnet.py] => Task 5, Epoch 7/20 (LR 0.00363) => LSC_loss 0.25, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 94.70, Test_acc 56.82
2025-01-11 04:06:26,539 [podnet.py] => Task 5, Epoch 8/20 (LR 0.00327) => LSC_loss 0.25, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 94.98, Test_acc 56.07
2025-01-11 04:06:32,445 [podnet.py] => Task 5, Epoch 9/20 (LR 0.00289) => LSC_loss 0.24, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 95.07, Test_acc 56.13
2025-01-11 04:06:38,359 [podnet.py] => Task 5, Epoch 10/20 (LR 0.00250) => LSC_loss 0.25, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 94.77, Test_acc 55.97
2025-01-11 04:06:44,370 [podnet.py] => Task 5, Epoch 11/20 (LR 0.00211) => LSC_loss 0.24, Spatial_loss 0.96, Flat_loss 0.10, Train_acc 95.43, Test_acc 56.67
2025-01-11 04:06:50,415 [podnet.py] => Task 5, Epoch 12/20 (LR 0.00173) => LSC_loss 0.25, Spatial_loss 0.96, Flat_loss 0.10, Train_acc 95.01, Test_acc 56.45
2025-01-11 04:06:56,374 [podnet.py] => Task 5, Epoch 13/20 (LR 0.00137) => LSC_loss 0.24, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 95.53, Test_acc 56.92
2025-01-11 04:07:02,239 [podnet.py] => Task 5, Epoch 14/20 (LR 0.00103) => LSC_loss 0.23, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 95.61, Test_acc 56.58
2025-01-11 04:07:08,019 [podnet.py] => Task 5, Epoch 15/20 (LR 0.00073) => LSC_loss 0.23, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 95.61, Test_acc 56.72
2025-01-11 04:07:13,819 [podnet.py] => Task 5, Epoch 16/20 (LR 0.00048) => LSC_loss 0.24, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 95.69, Test_acc 56.62
2025-01-11 04:07:19,535 [podnet.py] => Task 5, Epoch 17/20 (LR 0.00027) => LSC_loss 0.22, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 96.06, Test_acc 56.77
2025-01-11 04:07:25,345 [podnet.py] => Task 5, Epoch 18/20 (LR 0.00012) => LSC_loss 0.23, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 95.77, Test_acc 56.72
2025-01-11 04:07:30,894 [podnet.py] => Task 5, Epoch 19/20 (LR 0.00003) => LSC_loss 0.23, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 95.73, Test_acc 56.68
2025-01-11 04:07:36,638 [podnet.py] => Task 5, Epoch 20/20 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 96.05, Test_acc 56.65
2025-01-11 04:07:36,639 [base.py] => Reducing exemplars...(123 per classes)
2025-01-11 04:08:09,058 [base.py] => Constructing exemplars...(123 per classes)
2025-01-11 04:08:25,970 [podnet.py] => Exemplar size: 7380
2025-01-11 04:08:25,970 [trainer.py] => CNN: {'total': np.float64(56.65), '00-09': np.float64(70.8), '10-19': np.float64(44.3), '20-29': np.float64(58.0), '30-39': np.float64(50.8), '40-49': np.float64(61.7), '50-59': np.float64(54.3), 'old': np.float64(57.12), 'new': np.float64(54.3)}
2025-01-11 04:08:25,970 [trainer.py] => NME: {'total': np.float64(55.12), '00-09': np.float64(72.5), '10-19': np.float64(42.9), '20-29': np.float64(56.4), '30-39': np.float64(49.1), '40-49': np.float64(58.2), '50-59': np.float64(51.6), 'old': np.float64(55.82), 'new': np.float64(51.6)}
2025-01-11 04:08:25,970 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42), np.float64(60.28), np.float64(56.65)]
2025-01-11 04:08:25,970 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82), np.float64(85.98), np.float64(83.17)]
2025-01-11 04:08:25,971 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38), np.float64(59.32), np.float64(55.12)]
2025-01-11 04:08:25,971 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02), np.float64(85.44), np.float64(82.3)]

2025-01-11 04:08:25,971 [trainer.py] => All params: 504657
2025-01-11 04:08:25,971 [trainer.py] => Trainable params: 504657
2025-01-11 04:08:25,972 [podnet.py] => Learning on 60-70
2025-01-11 04:08:26,088 [podnet.py] => Adaptive factor: 2.6457513110645907
2025-01-11 04:08:33,490 [podnet.py] => Task 6, Epoch 1/160 (LR 0.09999) => LSC_loss 2.42, Spatial_loss 2.86, Flat_loss 0.66, Train_acc 42.51, Test_acc 32.19
2025-01-11 04:08:40,781 [podnet.py] => Task 6, Epoch 2/160 (LR 0.09996) => LSC_loss 1.72, Spatial_loss 2.48, Flat_loss 0.38, Train_acc 53.81, Test_acc 40.24
2025-01-11 04:08:48,153 [podnet.py] => Task 6, Epoch 3/160 (LR 0.09991) => LSC_loss 1.57, Spatial_loss 2.30, Flat_loss 0.33, Train_acc 57.42, Test_acc 44.09
2025-01-11 04:08:55,776 [podnet.py] => Task 6, Epoch 4/160 (LR 0.09985) => LSC_loss 1.48, Spatial_loss 2.23, Flat_loss 0.31, Train_acc 59.94, Test_acc 40.86
2025-01-11 04:09:03,135 [podnet.py] => Task 6, Epoch 5/160 (LR 0.09976) => LSC_loss 1.42, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 61.24, Test_acc 36.86
2025-01-11 04:09:10,342 [podnet.py] => Task 6, Epoch 6/160 (LR 0.09965) => LSC_loss 1.40, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 61.32, Test_acc 42.73
2025-01-11 04:09:17,711 [podnet.py] => Task 6, Epoch 7/160 (LR 0.09953) => LSC_loss 1.35, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 63.13, Test_acc 40.34
2025-01-11 04:09:25,041 [podnet.py] => Task 6, Epoch 8/160 (LR 0.09938) => LSC_loss 1.34, Spatial_loss 2.13, Flat_loss 0.28, Train_acc 63.16, Test_acc 43.37
2025-01-11 04:09:32,404 [podnet.py] => Task 6, Epoch 9/160 (LR 0.09922) => LSC_loss 1.31, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 64.56, Test_acc 45.10
2025-01-11 04:09:39,976 [podnet.py] => Task 6, Epoch 10/160 (LR 0.09904) => LSC_loss 1.28, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 65.84, Test_acc 44.24
2025-01-11 04:09:47,341 [podnet.py] => Task 6, Epoch 11/160 (LR 0.09884) => LSC_loss 1.29, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 65.10, Test_acc 41.17
2025-01-11 04:09:54,604 [podnet.py] => Task 6, Epoch 12/160 (LR 0.09862) => LSC_loss 1.24, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 66.08, Test_acc 43.91
2025-01-11 04:10:01,965 [podnet.py] => Task 6, Epoch 13/160 (LR 0.09838) => LSC_loss 1.24, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 66.49, Test_acc 43.51
2025-01-11 04:10:09,305 [podnet.py] => Task 6, Epoch 14/160 (LR 0.09812) => LSC_loss 1.24, Spatial_loss 2.02, Flat_loss 0.27, Train_acc 66.46, Test_acc 45.01
2025-01-11 04:10:16,720 [podnet.py] => Task 6, Epoch 15/160 (LR 0.09785) => LSC_loss 1.22, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 66.66, Test_acc 41.54
2025-01-11 04:10:24,201 [podnet.py] => Task 6, Epoch 16/160 (LR 0.09755) => LSC_loss 1.22, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 66.49, Test_acc 46.20
2025-01-11 04:10:31,316 [podnet.py] => Task 6, Epoch 17/160 (LR 0.09724) => LSC_loss 1.20, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 66.95, Test_acc 42.96
2025-01-11 04:10:38,504 [podnet.py] => Task 6, Epoch 18/160 (LR 0.09691) => LSC_loss 1.18, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 67.16, Test_acc 47.33
2025-01-11 04:10:45,813 [podnet.py] => Task 6, Epoch 19/160 (LR 0.09656) => LSC_loss 1.20, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 67.21, Test_acc 40.50
2025-01-11 04:10:52,965 [podnet.py] => Task 6, Epoch 20/160 (LR 0.09619) => LSC_loss 1.18, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 67.42, Test_acc 40.19
2025-01-11 04:11:00,479 [podnet.py] => Task 6, Epoch 21/160 (LR 0.09581) => LSC_loss 1.17, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 67.76, Test_acc 44.70
2025-01-11 04:11:08,284 [podnet.py] => Task 6, Epoch 22/160 (LR 0.09541) => LSC_loss 1.16, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 68.02, Test_acc 40.76
2025-01-11 04:11:15,719 [podnet.py] => Task 6, Epoch 23/160 (LR 0.09499) => LSC_loss 1.17, Spatial_loss 2.06, Flat_loss 0.27, Train_acc 68.55, Test_acc 44.84
2025-01-11 04:11:23,141 [podnet.py] => Task 6, Epoch 24/160 (LR 0.09455) => LSC_loss 1.13, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 68.78, Test_acc 43.57
2025-01-11 04:11:30,749 [podnet.py] => Task 6, Epoch 25/160 (LR 0.09410) => LSC_loss 1.15, Spatial_loss 2.02, Flat_loss 0.27, Train_acc 68.52, Test_acc 45.93
2025-01-11 04:11:38,903 [podnet.py] => Task 6, Epoch 26/160 (LR 0.09362) => LSC_loss 1.13, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 69.34, Test_acc 39.21
2025-01-11 04:11:46,262 [podnet.py] => Task 6, Epoch 27/160 (LR 0.09314) => LSC_loss 1.11, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 69.61, Test_acc 42.97
2025-01-11 04:11:53,617 [podnet.py] => Task 6, Epoch 28/160 (LR 0.09263) => LSC_loss 1.13, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 68.89, Test_acc 43.99
2025-01-11 04:12:00,815 [podnet.py] => Task 6, Epoch 29/160 (LR 0.09211) => LSC_loss 1.13, Spatial_loss 2.03, Flat_loss 0.27, Train_acc 68.91, Test_acc 44.07
2025-01-11 04:12:08,336 [podnet.py] => Task 6, Epoch 30/160 (LR 0.09157) => LSC_loss 1.11, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 69.67, Test_acc 42.86
2025-01-11 04:12:15,556 [podnet.py] => Task 6, Epoch 31/160 (LR 0.09102) => LSC_loss 1.10, Spatial_loss 1.96, Flat_loss 0.26, Train_acc 70.31, Test_acc 45.83
2025-01-11 04:12:22,884 [podnet.py] => Task 6, Epoch 32/160 (LR 0.09045) => LSC_loss 1.08, Spatial_loss 1.96, Flat_loss 0.26, Train_acc 70.27, Test_acc 36.86
2025-01-11 04:12:30,268 [podnet.py] => Task 6, Epoch 33/160 (LR 0.08987) => LSC_loss 1.11, Spatial_loss 1.99, Flat_loss 0.27, Train_acc 69.10, Test_acc 39.83
2025-01-11 04:12:37,571 [podnet.py] => Task 6, Epoch 34/160 (LR 0.08927) => LSC_loss 1.08, Spatial_loss 1.99, Flat_loss 0.27, Train_acc 70.16, Test_acc 44.63
2025-01-11 04:12:44,922 [podnet.py] => Task 6, Epoch 35/160 (LR 0.08865) => LSC_loss 1.11, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 69.77, Test_acc 45.51
2025-01-11 04:12:52,276 [podnet.py] => Task 6, Epoch 36/160 (LR 0.08802) => LSC_loss 1.10, Spatial_loss 2.03, Flat_loss 0.27, Train_acc 70.28, Test_acc 39.37
2025-01-11 04:12:59,603 [podnet.py] => Task 6, Epoch 37/160 (LR 0.08738) => LSC_loss 1.07, Spatial_loss 1.96, Flat_loss 0.27, Train_acc 70.65, Test_acc 46.83
2025-01-11 04:13:07,070 [podnet.py] => Task 6, Epoch 38/160 (LR 0.08672) => LSC_loss 1.08, Spatial_loss 1.98, Flat_loss 0.27, Train_acc 69.89, Test_acc 41.17
2025-01-11 04:13:14,540 [podnet.py] => Task 6, Epoch 39/160 (LR 0.08604) => LSC_loss 1.09, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 70.00, Test_acc 43.73
2025-01-11 04:13:22,181 [podnet.py] => Task 6, Epoch 40/160 (LR 0.08536) => LSC_loss 1.05, Spatial_loss 1.94, Flat_loss 0.26, Train_acc 71.43, Test_acc 40.99
2025-01-11 04:13:29,437 [podnet.py] => Task 6, Epoch 41/160 (LR 0.08465) => LSC_loss 1.06, Spatial_loss 1.97, Flat_loss 0.27, Train_acc 70.89, Test_acc 45.83
2025-01-11 04:13:36,715 [podnet.py] => Task 6, Epoch 42/160 (LR 0.08394) => LSC_loss 1.05, Spatial_loss 1.92, Flat_loss 0.26, Train_acc 71.26, Test_acc 43.97
2025-01-11 04:13:43,864 [podnet.py] => Task 6, Epoch 43/160 (LR 0.08321) => LSC_loss 1.05, Spatial_loss 1.96, Flat_loss 0.26, Train_acc 71.82, Test_acc 45.76
2025-01-11 04:13:51,214 [podnet.py] => Task 6, Epoch 44/160 (LR 0.08247) => LSC_loss 1.06, Spatial_loss 1.95, Flat_loss 0.27, Train_acc 71.08, Test_acc 45.26
2025-01-11 04:13:58,540 [podnet.py] => Task 6, Epoch 45/160 (LR 0.08172) => LSC_loss 1.04, Spatial_loss 1.91, Flat_loss 0.26, Train_acc 71.72, Test_acc 46.17
2025-01-11 04:14:05,959 [podnet.py] => Task 6, Epoch 46/160 (LR 0.08095) => LSC_loss 1.04, Spatial_loss 1.98, Flat_loss 0.27, Train_acc 71.22, Test_acc 45.63
2025-01-11 04:14:13,433 [podnet.py] => Task 6, Epoch 47/160 (LR 0.08018) => LSC_loss 1.02, Spatial_loss 1.95, Flat_loss 0.26, Train_acc 71.99, Test_acc 47.63
2025-01-11 04:14:20,995 [podnet.py] => Task 6, Epoch 48/160 (LR 0.07939) => LSC_loss 1.02, Spatial_loss 1.91, Flat_loss 0.26, Train_acc 72.19, Test_acc 43.10
2025-01-11 04:14:28,183 [podnet.py] => Task 6, Epoch 49/160 (LR 0.07859) => LSC_loss 1.05, Spatial_loss 1.96, Flat_loss 0.27, Train_acc 70.69, Test_acc 43.03
2025-01-11 04:14:35,440 [podnet.py] => Task 6, Epoch 50/160 (LR 0.07778) => LSC_loss 1.02, Spatial_loss 1.89, Flat_loss 0.26, Train_acc 71.97, Test_acc 43.09
2025-01-11 04:14:42,806 [podnet.py] => Task 6, Epoch 51/160 (LR 0.07696) => LSC_loss 0.99, Spatial_loss 1.91, Flat_loss 0.26, Train_acc 72.76, Test_acc 45.94
2025-01-11 04:14:49,986 [podnet.py] => Task 6, Epoch 52/160 (LR 0.07612) => LSC_loss 1.01, Spatial_loss 1.90, Flat_loss 0.26, Train_acc 72.33, Test_acc 46.01
2025-01-11 04:14:57,223 [podnet.py] => Task 6, Epoch 53/160 (LR 0.07528) => LSC_loss 0.99, Spatial_loss 1.87, Flat_loss 0.25, Train_acc 73.04, Test_acc 45.61
2025-01-11 04:15:04,479 [podnet.py] => Task 6, Epoch 54/160 (LR 0.07443) => LSC_loss 0.99, Spatial_loss 1.92, Flat_loss 0.26, Train_acc 72.74, Test_acc 44.70
2025-01-11 04:15:11,801 [podnet.py] => Task 6, Epoch 55/160 (LR 0.07357) => LSC_loss 0.98, Spatial_loss 1.87, Flat_loss 0.26, Train_acc 73.14, Test_acc 45.61
2025-01-11 04:15:19,043 [podnet.py] => Task 6, Epoch 56/160 (LR 0.07270) => LSC_loss 0.97, Spatial_loss 1.90, Flat_loss 0.26, Train_acc 72.99, Test_acc 45.33
2025-01-11 04:15:26,359 [podnet.py] => Task 6, Epoch 57/160 (LR 0.07182) => LSC_loss 0.97, Spatial_loss 1.89, Flat_loss 0.26, Train_acc 73.42, Test_acc 46.87
2025-01-11 04:15:33,796 [podnet.py] => Task 6, Epoch 58/160 (LR 0.07093) => LSC_loss 0.97, Spatial_loss 1.91, Flat_loss 0.26, Train_acc 73.27, Test_acc 44.50
2025-01-11 04:15:41,344 [podnet.py] => Task 6, Epoch 59/160 (LR 0.07004) => LSC_loss 0.96, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 73.69, Test_acc 44.07
2025-01-11 04:15:48,974 [podnet.py] => Task 6, Epoch 60/160 (LR 0.06913) => LSC_loss 0.96, Spatial_loss 1.92, Flat_loss 0.26, Train_acc 73.38, Test_acc 46.39
2025-01-11 04:15:56,826 [podnet.py] => Task 6, Epoch 61/160 (LR 0.06822) => LSC_loss 0.96, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 73.44, Test_acc 46.37
2025-01-11 04:16:04,251 [podnet.py] => Task 6, Epoch 62/160 (LR 0.06731) => LSC_loss 0.93, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 74.52, Test_acc 45.13
2025-01-11 04:16:11,711 [podnet.py] => Task 6, Epoch 63/160 (LR 0.06638) => LSC_loss 0.92, Spatial_loss 1.83, Flat_loss 0.24, Train_acc 74.92, Test_acc 44.53
2025-01-11 04:16:18,984 [podnet.py] => Task 6, Epoch 64/160 (LR 0.06545) => LSC_loss 0.92, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 75.19, Test_acc 48.04
2025-01-11 04:16:26,316 [podnet.py] => Task 6, Epoch 65/160 (LR 0.06451) => LSC_loss 0.93, Spatial_loss 1.85, Flat_loss 0.25, Train_acc 74.09, Test_acc 43.23
2025-01-11 04:16:33,549 [podnet.py] => Task 6, Epoch 66/160 (LR 0.06357) => LSC_loss 0.92, Spatial_loss 1.83, Flat_loss 0.25, Train_acc 74.94, Test_acc 42.90
2025-01-11 04:16:40,840 [podnet.py] => Task 6, Epoch 67/160 (LR 0.06262) => LSC_loss 0.89, Spatial_loss 1.82, Flat_loss 0.25, Train_acc 75.48, Test_acc 47.47
2025-01-11 04:16:48,025 [podnet.py] => Task 6, Epoch 68/160 (LR 0.06167) => LSC_loss 0.88, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 76.04, Test_acc 44.17
2025-01-11 04:16:55,618 [podnet.py] => Task 6, Epoch 69/160 (LR 0.06072) => LSC_loss 0.90, Spatial_loss 1.83, Flat_loss 0.25, Train_acc 75.18, Test_acc 47.60
2025-01-11 04:17:02,975 [podnet.py] => Task 6, Epoch 70/160 (LR 0.05975) => LSC_loss 0.88, Spatial_loss 1.82, Flat_loss 0.24, Train_acc 76.08, Test_acc 47.84
2025-01-11 04:17:10,408 [podnet.py] => Task 6, Epoch 71/160 (LR 0.05879) => LSC_loss 0.86, Spatial_loss 1.76, Flat_loss 0.24, Train_acc 76.52, Test_acc 47.36
2025-01-11 04:17:17,643 [podnet.py] => Task 6, Epoch 72/160 (LR 0.05782) => LSC_loss 0.87, Spatial_loss 1.78, Flat_loss 0.24, Train_acc 76.11, Test_acc 47.07
2025-01-11 04:17:24,910 [podnet.py] => Task 6, Epoch 73/160 (LR 0.05685) => LSC_loss 0.86, Spatial_loss 1.77, Flat_loss 0.24, Train_acc 76.14, Test_acc 46.86
2025-01-11 04:17:32,249 [podnet.py] => Task 6, Epoch 74/160 (LR 0.05588) => LSC_loss 0.87, Spatial_loss 1.78, Flat_loss 0.24, Train_acc 76.37, Test_acc 45.79
2025-01-11 04:17:39,687 [podnet.py] => Task 6, Epoch 75/160 (LR 0.05490) => LSC_loss 0.86, Spatial_loss 1.77, Flat_loss 0.24, Train_acc 76.56, Test_acc 45.79
2025-01-11 04:17:46,891 [podnet.py] => Task 6, Epoch 76/160 (LR 0.05392) => LSC_loss 0.85, Spatial_loss 1.76, Flat_loss 0.24, Train_acc 76.64, Test_acc 46.10
2025-01-11 04:17:54,141 [podnet.py] => Task 6, Epoch 77/160 (LR 0.05294) => LSC_loss 0.84, Spatial_loss 1.76, Flat_loss 0.24, Train_acc 77.20, Test_acc 46.09
2025-01-11 04:18:01,557 [podnet.py] => Task 6, Epoch 78/160 (LR 0.05196) => LSC_loss 0.82, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 77.88, Test_acc 46.76
2025-01-11 04:18:08,865 [podnet.py] => Task 6, Epoch 79/160 (LR 0.05098) => LSC_loss 0.82, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 77.45, Test_acc 46.79
2025-01-11 04:18:16,255 [podnet.py] => Task 6, Epoch 80/160 (LR 0.05000) => LSC_loss 0.82, Spatial_loss 1.73, Flat_loss 0.24, Train_acc 77.62, Test_acc 44.80
2025-01-11 04:18:23,528 [podnet.py] => Task 6, Epoch 81/160 (LR 0.04902) => LSC_loss 0.77, Spatial_loss 1.69, Flat_loss 0.23, Train_acc 79.72, Test_acc 48.56
2025-01-11 04:18:30,993 [podnet.py] => Task 6, Epoch 82/160 (LR 0.04804) => LSC_loss 0.79, Spatial_loss 1.69, Flat_loss 0.23, Train_acc 78.26, Test_acc 47.87
2025-01-11 04:18:38,303 [podnet.py] => Task 6, Epoch 83/160 (LR 0.04706) => LSC_loss 0.80, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 78.34, Test_acc 44.41
2025-01-11 04:18:45,532 [podnet.py] => Task 6, Epoch 84/160 (LR 0.04608) => LSC_loss 0.78, Spatial_loss 1.70, Flat_loss 0.23, Train_acc 78.91, Test_acc 47.09
2025-01-11 04:18:52,811 [podnet.py] => Task 6, Epoch 85/160 (LR 0.04510) => LSC_loss 0.75, Spatial_loss 1.64, Flat_loss 0.22, Train_acc 79.61, Test_acc 49.01
2025-01-11 04:19:00,093 [podnet.py] => Task 6, Epoch 86/160 (LR 0.04412) => LSC_loss 0.75, Spatial_loss 1.66, Flat_loss 0.22, Train_acc 79.66, Test_acc 46.17
2025-01-11 04:19:07,400 [podnet.py] => Task 6, Epoch 87/160 (LR 0.04315) => LSC_loss 0.76, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 79.36, Test_acc 48.33
2025-01-11 04:19:14,803 [podnet.py] => Task 6, Epoch 88/160 (LR 0.04218) => LSC_loss 0.74, Spatial_loss 1.64, Flat_loss 0.22, Train_acc 79.78, Test_acc 45.44
2025-01-11 04:19:22,342 [podnet.py] => Task 6, Epoch 89/160 (LR 0.04121) => LSC_loss 0.74, Spatial_loss 1.65, Flat_loss 0.22, Train_acc 80.04, Test_acc 48.74
2025-01-11 04:19:29,722 [podnet.py] => Task 6, Epoch 90/160 (LR 0.04025) => LSC_loss 0.72, Spatial_loss 1.63, Flat_loss 0.22, Train_acc 80.68, Test_acc 48.20
2025-01-11 04:19:37,120 [podnet.py] => Task 6, Epoch 91/160 (LR 0.03928) => LSC_loss 0.72, Spatial_loss 1.62, Flat_loss 0.22, Train_acc 80.27, Test_acc 47.67
2025-01-11 04:19:44,446 [podnet.py] => Task 6, Epoch 92/160 (LR 0.03833) => LSC_loss 0.69, Spatial_loss 1.57, Flat_loss 0.21, Train_acc 81.28, Test_acc 48.03
2025-01-11 04:19:51,802 [podnet.py] => Task 6, Epoch 93/160 (LR 0.03738) => LSC_loss 0.70, Spatial_loss 1.58, Flat_loss 0.21, Train_acc 81.03, Test_acc 49.63
2025-01-11 04:19:59,131 [podnet.py] => Task 6, Epoch 94/160 (LR 0.03643) => LSC_loss 0.69, Spatial_loss 1.56, Flat_loss 0.21, Train_acc 81.64, Test_acc 49.17
2025-01-11 04:20:06,563 [podnet.py] => Task 6, Epoch 95/160 (LR 0.03549) => LSC_loss 0.69, Spatial_loss 1.58, Flat_loss 0.21, Train_acc 81.37, Test_acc 47.63
2025-01-11 04:20:13,818 [podnet.py] => Task 6, Epoch 96/160 (LR 0.03455) => LSC_loss 0.68, Spatial_loss 1.57, Flat_loss 0.21, Train_acc 81.32, Test_acc 48.06
2025-01-11 04:20:21,046 [podnet.py] => Task 6, Epoch 97/160 (LR 0.03362) => LSC_loss 0.65, Spatial_loss 1.53, Flat_loss 0.21, Train_acc 82.46, Test_acc 49.26
2025-01-11 04:20:28,286 [podnet.py] => Task 6, Epoch 98/160 (LR 0.03269) => LSC_loss 0.66, Spatial_loss 1.54, Flat_loss 0.20, Train_acc 82.72, Test_acc 48.63
2025-01-11 04:20:35,546 [podnet.py] => Task 6, Epoch 99/160 (LR 0.03178) => LSC_loss 0.66, Spatial_loss 1.53, Flat_loss 0.20, Train_acc 82.34, Test_acc 47.81
2025-01-11 04:20:42,825 [podnet.py] => Task 6, Epoch 100/160 (LR 0.03087) => LSC_loss 0.64, Spatial_loss 1.50, Flat_loss 0.20, Train_acc 83.41, Test_acc 50.76
2025-01-11 04:20:50,135 [podnet.py] => Task 6, Epoch 101/160 (LR 0.02996) => LSC_loss 0.63, Spatial_loss 1.52, Flat_loss 0.20, Train_acc 83.34, Test_acc 47.93
2025-01-11 04:20:57,530 [podnet.py] => Task 6, Epoch 102/160 (LR 0.02907) => LSC_loss 0.63, Spatial_loss 1.49, Flat_loss 0.20, Train_acc 83.19, Test_acc 45.66
2025-01-11 04:21:04,980 [podnet.py] => Task 6, Epoch 103/160 (LR 0.02818) => LSC_loss 0.61, Spatial_loss 1.47, Flat_loss 0.20, Train_acc 83.77, Test_acc 49.49
2025-01-11 04:21:12,621 [podnet.py] => Task 6, Epoch 104/160 (LR 0.02730) => LSC_loss 0.60, Spatial_loss 1.48, Flat_loss 0.20, Train_acc 84.31, Test_acc 50.16
2025-01-11 04:21:20,081 [podnet.py] => Task 6, Epoch 105/160 (LR 0.02643) => LSC_loss 0.60, Spatial_loss 1.46, Flat_loss 0.19, Train_acc 84.35, Test_acc 49.16
2025-01-11 04:21:27,561 [podnet.py] => Task 6, Epoch 106/160 (LR 0.02557) => LSC_loss 0.58, Spatial_loss 1.43, Flat_loss 0.19, Train_acc 85.11, Test_acc 48.06
2025-01-11 04:21:34,806 [podnet.py] => Task 6, Epoch 107/160 (LR 0.02472) => LSC_loss 0.57, Spatial_loss 1.41, Flat_loss 0.19, Train_acc 85.56, Test_acc 51.54
2025-01-11 04:21:42,195 [podnet.py] => Task 6, Epoch 108/160 (LR 0.02388) => LSC_loss 0.57, Spatial_loss 1.41, Flat_loss 0.19, Train_acc 85.54, Test_acc 50.77
2025-01-11 04:21:49,505 [podnet.py] => Task 6, Epoch 109/160 (LR 0.02304) => LSC_loss 0.56, Spatial_loss 1.41, Flat_loss 0.19, Train_acc 85.78, Test_acc 51.86
2025-01-11 04:21:56,783 [podnet.py] => Task 6, Epoch 110/160 (LR 0.02222) => LSC_loss 0.56, Spatial_loss 1.39, Flat_loss 0.18, Train_acc 85.53, Test_acc 51.46
2025-01-11 04:22:04,496 [podnet.py] => Task 6, Epoch 111/160 (LR 0.02141) => LSC_loss 0.55, Spatial_loss 1.37, Flat_loss 0.18, Train_acc 86.05, Test_acc 51.96
2025-01-11 04:22:11,892 [podnet.py] => Task 6, Epoch 112/160 (LR 0.02061) => LSC_loss 0.55, Spatial_loss 1.37, Flat_loss 0.18, Train_acc 85.98, Test_acc 49.77
2025-01-11 04:22:19,171 [podnet.py] => Task 6, Epoch 113/160 (LR 0.01982) => LSC_loss 0.53, Spatial_loss 1.37, Flat_loss 0.18, Train_acc 86.37, Test_acc 50.97
2025-01-11 04:22:26,516 [podnet.py] => Task 6, Epoch 114/160 (LR 0.01905) => LSC_loss 0.52, Spatial_loss 1.35, Flat_loss 0.18, Train_acc 86.69, Test_acc 50.53
2025-01-11 04:22:33,965 [podnet.py] => Task 6, Epoch 115/160 (LR 0.01828) => LSC_loss 0.52, Spatial_loss 1.33, Flat_loss 0.18, Train_acc 86.93, Test_acc 49.97
2025-01-11 04:22:41,183 [podnet.py] => Task 6, Epoch 116/160 (LR 0.01753) => LSC_loss 0.50, Spatial_loss 1.31, Flat_loss 0.17, Train_acc 87.63, Test_acc 49.96
2025-01-11 04:22:48,502 [podnet.py] => Task 6, Epoch 117/160 (LR 0.01679) => LSC_loss 0.49, Spatial_loss 1.30, Flat_loss 0.17, Train_acc 88.05, Test_acc 50.93
2025-01-11 04:22:55,884 [podnet.py] => Task 6, Epoch 118/160 (LR 0.01606) => LSC_loss 0.50, Spatial_loss 1.29, Flat_loss 0.17, Train_acc 87.45, Test_acc 50.41
2025-01-11 04:23:03,443 [podnet.py] => Task 6, Epoch 119/160 (LR 0.01535) => LSC_loss 0.48, Spatial_loss 1.27, Flat_loss 0.17, Train_acc 87.94, Test_acc 51.74
2025-01-11 04:23:10,996 [podnet.py] => Task 6, Epoch 120/160 (LR 0.01464) => LSC_loss 0.47, Spatial_loss 1.26, Flat_loss 0.17, Train_acc 88.59, Test_acc 51.87
2025-01-11 04:23:18,532 [podnet.py] => Task 6, Epoch 121/160 (LR 0.01396) => LSC_loss 0.46, Spatial_loss 1.24, Flat_loss 0.16, Train_acc 88.78, Test_acc 52.33
2025-01-11 04:23:25,905 [podnet.py] => Task 6, Epoch 122/160 (LR 0.01328) => LSC_loss 0.46, Spatial_loss 1.23, Flat_loss 0.16, Train_acc 88.83, Test_acc 51.50
2025-01-11 04:23:33,227 [podnet.py] => Task 6, Epoch 123/160 (LR 0.01262) => LSC_loss 0.44, Spatial_loss 1.23, Flat_loss 0.16, Train_acc 89.41, Test_acc 51.89
2025-01-11 04:23:40,841 [podnet.py] => Task 6, Epoch 124/160 (LR 0.01198) => LSC_loss 0.44, Spatial_loss 1.21, Flat_loss 0.16, Train_acc 89.31, Test_acc 51.30
2025-01-11 04:23:48,335 [podnet.py] => Task 6, Epoch 125/160 (LR 0.01135) => LSC_loss 0.44, Spatial_loss 1.19, Flat_loss 0.16, Train_acc 90.13, Test_acc 52.41
2025-01-11 04:23:55,805 [podnet.py] => Task 6, Epoch 126/160 (LR 0.01073) => LSC_loss 0.43, Spatial_loss 1.19, Flat_loss 0.16, Train_acc 90.05, Test_acc 52.11
2025-01-11 04:24:03,184 [podnet.py] => Task 6, Epoch 127/160 (LR 0.01013) => LSC_loss 0.43, Spatial_loss 1.18, Flat_loss 0.16, Train_acc 89.97, Test_acc 50.99
2025-01-11 04:24:10,599 [podnet.py] => Task 6, Epoch 128/160 (LR 0.00955) => LSC_loss 0.42, Spatial_loss 1.16, Flat_loss 0.15, Train_acc 90.15, Test_acc 52.03
2025-01-11 04:24:17,993 [podnet.py] => Task 6, Epoch 129/160 (LR 0.00898) => LSC_loss 0.41, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 90.56, Test_acc 52.67
2025-01-11 04:24:25,336 [podnet.py] => Task 6, Epoch 130/160 (LR 0.00843) => LSC_loss 0.40, Spatial_loss 1.13, Flat_loss 0.15, Train_acc 90.91, Test_acc 52.34
2025-01-11 04:24:32,748 [podnet.py] => Task 6, Epoch 131/160 (LR 0.00789) => LSC_loss 0.41, Spatial_loss 1.14, Flat_loss 0.15, Train_acc 90.54, Test_acc 53.66
2025-01-11 04:24:40,177 [podnet.py] => Task 6, Epoch 132/160 (LR 0.00737) => LSC_loss 0.39, Spatial_loss 1.12, Flat_loss 0.15, Train_acc 90.87, Test_acc 52.80
2025-01-11 04:24:47,504 [podnet.py] => Task 6, Epoch 133/160 (LR 0.00686) => LSC_loss 0.40, Spatial_loss 1.10, Flat_loss 0.15, Train_acc 91.01, Test_acc 53.33
2025-01-11 04:24:54,772 [podnet.py] => Task 6, Epoch 134/160 (LR 0.00638) => LSC_loss 0.39, Spatial_loss 1.11, Flat_loss 0.15, Train_acc 91.25, Test_acc 52.64
2025-01-11 04:25:02,254 [podnet.py] => Task 6, Epoch 135/160 (LR 0.00590) => LSC_loss 0.39, Spatial_loss 1.10, Flat_loss 0.15, Train_acc 91.32, Test_acc 53.20
2025-01-11 04:25:09,670 [podnet.py] => Task 6, Epoch 136/160 (LR 0.00545) => LSC_loss 0.38, Spatial_loss 1.09, Flat_loss 0.14, Train_acc 91.65, Test_acc 53.27
2025-01-11 04:25:17,010 [podnet.py] => Task 6, Epoch 137/160 (LR 0.00501) => LSC_loss 0.37, Spatial_loss 1.08, Flat_loss 0.14, Train_acc 91.86, Test_acc 53.64
2025-01-11 04:25:24,459 [podnet.py] => Task 6, Epoch 138/160 (LR 0.00459) => LSC_loss 0.37, Spatial_loss 1.07, Flat_loss 0.14, Train_acc 92.24, Test_acc 52.99
2025-01-11 04:25:31,871 [podnet.py] => Task 6, Epoch 139/160 (LR 0.00419) => LSC_loss 0.37, Spatial_loss 1.05, Flat_loss 0.14, Train_acc 92.07, Test_acc 53.34
2025-01-11 04:25:39,364 [podnet.py] => Task 6, Epoch 140/160 (LR 0.00381) => LSC_loss 0.36, Spatial_loss 1.05, Flat_loss 0.14, Train_acc 92.14, Test_acc 53.20
2025-01-11 04:25:46,757 [podnet.py] => Task 6, Epoch 141/160 (LR 0.00344) => LSC_loss 0.36, Spatial_loss 1.05, Flat_loss 0.14, Train_acc 92.04, Test_acc 53.83
2025-01-11 04:25:54,375 [podnet.py] => Task 6, Epoch 142/160 (LR 0.00309) => LSC_loss 0.36, Spatial_loss 1.05, Flat_loss 0.14, Train_acc 92.56, Test_acc 53.86
2025-01-11 04:26:01,670 [podnet.py] => Task 6, Epoch 143/160 (LR 0.00276) => LSC_loss 0.35, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 92.85, Test_acc 53.99
2025-01-11 04:26:08,951 [podnet.py] => Task 6, Epoch 144/160 (LR 0.00245) => LSC_loss 0.36, Spatial_loss 1.01, Flat_loss 0.14, Train_acc 92.43, Test_acc 53.61
2025-01-11 04:26:16,329 [podnet.py] => Task 6, Epoch 145/160 (LR 0.00215) => LSC_loss 0.35, Spatial_loss 1.01, Flat_loss 0.14, Train_acc 92.60, Test_acc 54.01
2025-01-11 04:26:24,029 [podnet.py] => Task 6, Epoch 146/160 (LR 0.00188) => LSC_loss 0.35, Spatial_loss 1.00, Flat_loss 0.14, Train_acc 92.83, Test_acc 53.69
2025-01-11 04:26:31,671 [podnet.py] => Task 6, Epoch 147/160 (LR 0.00162) => LSC_loss 0.35, Spatial_loss 1.02, Flat_loss 0.14, Train_acc 92.85, Test_acc 53.86
2025-01-11 04:26:38,980 [podnet.py] => Task 6, Epoch 148/160 (LR 0.00138) => LSC_loss 0.34, Spatial_loss 0.98, Flat_loss 0.13, Train_acc 92.94, Test_acc 53.91
2025-01-11 04:26:46,384 [podnet.py] => Task 6, Epoch 149/160 (LR 0.00116) => LSC_loss 0.34, Spatial_loss 0.98, Flat_loss 0.13, Train_acc 93.05, Test_acc 54.01
2025-01-11 04:26:53,868 [podnet.py] => Task 6, Epoch 150/160 (LR 0.00096) => LSC_loss 0.34, Spatial_loss 0.99, Flat_loss 0.13, Train_acc 93.10, Test_acc 54.20
2025-01-11 04:27:01,207 [podnet.py] => Task 6, Epoch 151/160 (LR 0.00078) => LSC_loss 0.34, Spatial_loss 0.99, Flat_loss 0.13, Train_acc 93.11, Test_acc 53.91
2025-01-11 04:27:08,503 [podnet.py] => Task 6, Epoch 152/160 (LR 0.00062) => LSC_loss 0.33, Spatial_loss 0.99, Flat_loss 0.13, Train_acc 93.57, Test_acc 54.23
2025-01-11 04:27:15,801 [podnet.py] => Task 6, Epoch 153/160 (LR 0.00047) => LSC_loss 0.34, Spatial_loss 0.98, Flat_loss 0.13, Train_acc 93.34, Test_acc 53.96
2025-01-11 04:27:23,033 [podnet.py] => Task 6, Epoch 154/160 (LR 0.00035) => LSC_loss 0.34, Spatial_loss 0.97, Flat_loss 0.13, Train_acc 93.20, Test_acc 53.83
2025-01-11 04:27:30,376 [podnet.py] => Task 6, Epoch 155/160 (LR 0.00024) => LSC_loss 0.33, Spatial_loss 0.98, Flat_loss 0.13, Train_acc 93.35, Test_acc 54.07
2025-01-11 04:27:37,690 [podnet.py] => Task 6, Epoch 156/160 (LR 0.00015) => LSC_loss 0.34, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 92.95, Test_acc 53.84
2025-01-11 04:27:44,972 [podnet.py] => Task 6, Epoch 157/160 (LR 0.00009) => LSC_loss 0.33, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 93.45, Test_acc 53.94
2025-01-11 04:27:52,222 [podnet.py] => Task 6, Epoch 158/160 (LR 0.00004) => LSC_loss 0.33, Spatial_loss 0.97, Flat_loss 0.13, Train_acc 93.12, Test_acc 54.07
2025-01-11 04:27:59,593 [podnet.py] => Task 6, Epoch 159/160 (LR 0.00001) => LSC_loss 0.34, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 93.04, Test_acc 53.87
2025-01-11 04:28:07,161 [podnet.py] => Task 6, Epoch 160/160 (LR 0.00000) => LSC_loss 0.33, Spatial_loss 0.96, Flat_loss 0.13, Train_acc 93.22, Test_acc 53.99
2025-01-11 04:28:07,163 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 04:28:07,163 [base.py] => Reducing exemplars...(123 per classes)
2025-01-11 04:28:44,416 [base.py] => Constructing exemplars...(123 per classes)
2025-01-11 04:28:57,940 [podnet.py] => The size of finetune dataset: 8610
2025-01-11 04:29:03,615 [podnet.py] => Task 6, Epoch 1/20 (LR 0.00497) => LSC_loss 0.30, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 93.74, Test_acc 53.27
2025-01-11 04:29:09,368 [podnet.py] => Task 6, Epoch 2/20 (LR 0.00488) => LSC_loss 0.29, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 93.89, Test_acc 53.89
2025-01-11 04:29:15,006 [podnet.py] => Task 6, Epoch 3/20 (LR 0.00473) => LSC_loss 0.29, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 94.01, Test_acc 54.34
2025-01-11 04:29:20,637 [podnet.py] => Task 6, Epoch 4/20 (LR 0.00452) => LSC_loss 0.28, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 94.43, Test_acc 54.49
2025-01-11 04:29:26,200 [podnet.py] => Task 6, Epoch 5/20 (LR 0.00427) => LSC_loss 0.28, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 94.12, Test_acc 53.60
2025-01-11 04:29:31,820 [podnet.py] => Task 6, Epoch 6/20 (LR 0.00397) => LSC_loss 0.28, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 94.27, Test_acc 54.07
2025-01-11 04:29:37,399 [podnet.py] => Task 6, Epoch 7/20 (LR 0.00363) => LSC_loss 0.28, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 94.61, Test_acc 54.54
2025-01-11 04:29:42,974 [podnet.py] => Task 6, Epoch 8/20 (LR 0.00327) => LSC_loss 0.27, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 94.81, Test_acc 54.31
2025-01-11 04:29:48,563 [podnet.py] => Task 6, Epoch 9/20 (LR 0.00289) => LSC_loss 0.27, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 94.74, Test_acc 54.70
2025-01-11 04:29:54,283 [podnet.py] => Task 6, Epoch 10/20 (LR 0.00250) => LSC_loss 0.27, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 94.60, Test_acc 54.66
2025-01-11 04:29:59,927 [podnet.py] => Task 6, Epoch 11/20 (LR 0.00211) => LSC_loss 0.27, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 94.56, Test_acc 54.34
2025-01-11 04:30:05,631 [podnet.py] => Task 6, Epoch 12/20 (LR 0.00173) => LSC_loss 0.27, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 94.74, Test_acc 54.67
2025-01-11 04:30:11,321 [podnet.py] => Task 6, Epoch 13/20 (LR 0.00137) => LSC_loss 0.26, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 95.03, Test_acc 54.87
2025-01-11 04:30:17,119 [podnet.py] => Task 6, Epoch 14/20 (LR 0.00103) => LSC_loss 0.26, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 95.34, Test_acc 54.84
2025-01-11 04:30:22,918 [podnet.py] => Task 6, Epoch 15/20 (LR 0.00073) => LSC_loss 0.25, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 95.41, Test_acc 54.79
2025-01-11 04:30:28,528 [podnet.py] => Task 6, Epoch 16/20 (LR 0.00048) => LSC_loss 0.26, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 95.06, Test_acc 55.00
2025-01-11 04:30:34,515 [podnet.py] => Task 6, Epoch 17/20 (LR 0.00027) => LSC_loss 0.26, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 95.16, Test_acc 54.70
2025-01-11 04:30:40,208 [podnet.py] => Task 6, Epoch 18/20 (LR 0.00012) => LSC_loss 0.25, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 94.88, Test_acc 54.73
2025-01-11 04:30:45,861 [podnet.py] => Task 6, Epoch 19/20 (LR 0.00003) => LSC_loss 0.26, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 95.08, Test_acc 54.79
2025-01-11 04:30:51,791 [podnet.py] => Task 6, Epoch 20/20 (LR 0.00000) => LSC_loss 0.25, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 95.31, Test_acc 54.80
2025-01-11 04:30:51,793 [base.py] => Reducing exemplars...(105 per classes)
2025-01-11 04:31:29,979 [base.py] => Constructing exemplars...(105 per classes)
2025-01-11 04:31:46,748 [podnet.py] => Exemplar size: 7350
2025-01-11 04:31:46,749 [trainer.py] => CNN: {'total': np.float64(54.8), '00-09': np.float64(67.9), '10-19': np.float64(41.5), '20-29': np.float64(57.5), '30-39': np.float64(46.5), '40-49': np.float64(59.6), '50-59': np.float64(52.0), '60-69': np.float64(58.6), 'old': np.float64(54.17), 'new': np.float64(58.6)}
2025-01-11 04:31:46,749 [trainer.py] => NME: {'total': np.float64(53.11), '00-09': np.float64(70.6), '10-19': np.float64(40.1), '20-29': np.float64(54.4), '30-39': np.float64(44.6), '40-49': np.float64(56.0), '50-59': np.float64(48.2), '60-69': np.float64(57.9), 'old': np.float64(52.32), 'new': np.float64(57.9)}
2025-01-11 04:31:46,749 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42), np.float64(60.28), np.float64(56.65), np.float64(54.8)]
2025-01-11 04:31:46,749 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82), np.float64(85.98), np.float64(83.17), np.float64(81.17)]
2025-01-11 04:31:46,749 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38), np.float64(59.32), np.float64(55.12), np.float64(53.11)]
2025-01-11 04:31:46,749 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02), np.float64(85.44), np.float64(82.3), np.float64(80.14)]

2025-01-11 04:31:46,749 [trainer.py] => All params: 511057
2025-01-11 04:31:46,750 [trainer.py] => Trainable params: 511057
2025-01-11 04:31:46,751 [podnet.py] => Learning on 70-80
2025-01-11 04:31:46,881 [podnet.py] => Adaptive factor: 2.8284271247461903
2025-01-11 04:31:54,425 [podnet.py] => Task 7, Epoch 1/160 (LR 0.09999) => LSC_loss 2.51, Spatial_loss 3.07, Flat_loss 0.74, Train_acc 41.19, Test_acc 29.15
2025-01-11 04:32:01,659 [podnet.py] => Task 7, Epoch 2/160 (LR 0.09996) => LSC_loss 1.78, Spatial_loss 2.60, Flat_loss 0.43, Train_acc 51.76, Test_acc 29.45
2025-01-11 04:32:09,387 [podnet.py] => Task 7, Epoch 3/160 (LR 0.09991) => LSC_loss 1.65, Spatial_loss 2.43, Flat_loss 0.37, Train_acc 55.02, Test_acc 35.88
2025-01-11 04:32:16,653 [podnet.py] => Task 7, Epoch 4/160 (LR 0.09985) => LSC_loss 1.56, Spatial_loss 2.31, Flat_loss 0.34, Train_acc 57.19, Test_acc 35.62
2025-01-11 04:32:24,053 [podnet.py] => Task 7, Epoch 5/160 (LR 0.09976) => LSC_loss 1.52, Spatial_loss 2.28, Flat_loss 0.32, Train_acc 58.40, Test_acc 40.88
2025-01-11 04:32:31,759 [podnet.py] => Task 7, Epoch 6/160 (LR 0.09965) => LSC_loss 1.49, Spatial_loss 2.26, Flat_loss 0.32, Train_acc 58.83, Test_acc 35.94
2025-01-11 04:32:39,348 [podnet.py] => Task 7, Epoch 7/160 (LR 0.09953) => LSC_loss 1.44, Spatial_loss 2.20, Flat_loss 0.30, Train_acc 60.64, Test_acc 39.70
2025-01-11 04:32:46,791 [podnet.py] => Task 7, Epoch 8/160 (LR 0.09938) => LSC_loss 1.44, Spatial_loss 2.24, Flat_loss 0.31, Train_acc 60.44, Test_acc 39.84
2025-01-11 04:32:54,196 [podnet.py] => Task 7, Epoch 9/160 (LR 0.09922) => LSC_loss 1.42, Spatial_loss 2.22, Flat_loss 0.31, Train_acc 60.27, Test_acc 36.80
2025-01-11 04:33:01,620 [podnet.py] => Task 7, Epoch 10/160 (LR 0.09904) => LSC_loss 1.43, Spatial_loss 2.22, Flat_loss 0.30, Train_acc 60.36, Test_acc 37.69
2025-01-11 04:33:09,108 [podnet.py] => Task 7, Epoch 11/160 (LR 0.09884) => LSC_loss 1.37, Spatial_loss 2.16, Flat_loss 0.29, Train_acc 61.94, Test_acc 39.21
2025-01-11 04:33:16,572 [podnet.py] => Task 7, Epoch 12/160 (LR 0.09862) => LSC_loss 1.36, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 62.40, Test_acc 38.30
2025-01-11 04:33:24,052 [podnet.py] => Task 7, Epoch 13/160 (LR 0.09838) => LSC_loss 1.36, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 61.96, Test_acc 43.10
2025-01-11 04:33:31,851 [podnet.py] => Task 7, Epoch 14/160 (LR 0.09812) => LSC_loss 1.34, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 62.69, Test_acc 36.89
2025-01-11 04:33:39,406 [podnet.py] => Task 7, Epoch 15/160 (LR 0.09785) => LSC_loss 1.35, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 62.17, Test_acc 38.96
2025-01-11 04:33:46,904 [podnet.py] => Task 7, Epoch 16/160 (LR 0.09755) => LSC_loss 1.33, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 62.74, Test_acc 39.41
2025-01-11 04:33:54,493 [podnet.py] => Task 7, Epoch 17/160 (LR 0.09724) => LSC_loss 1.32, Spatial_loss 2.11, Flat_loss 0.29, Train_acc 63.25, Test_acc 39.62
2025-01-11 04:34:02,112 [podnet.py] => Task 7, Epoch 18/160 (LR 0.09691) => LSC_loss 1.31, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 63.60, Test_acc 39.31
2025-01-11 04:34:09,976 [podnet.py] => Task 7, Epoch 19/160 (LR 0.09656) => LSC_loss 1.31, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 63.81, Test_acc 40.35
2025-01-11 04:34:17,361 [podnet.py] => Task 7, Epoch 20/160 (LR 0.09619) => LSC_loss 1.28, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 64.53, Test_acc 41.92
2025-01-11 04:34:24,937 [podnet.py] => Task 7, Epoch 21/160 (LR 0.09581) => LSC_loss 1.28, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 64.32, Test_acc 40.26
2025-01-11 04:34:32,392 [podnet.py] => Task 7, Epoch 22/160 (LR 0.09541) => LSC_loss 1.28, Spatial_loss 2.09, Flat_loss 0.29, Train_acc 64.29, Test_acc 37.11
2025-01-11 04:34:39,825 [podnet.py] => Task 7, Epoch 23/160 (LR 0.09499) => LSC_loss 1.25, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 64.56, Test_acc 41.49
2025-01-11 04:34:47,084 [podnet.py] => Task 7, Epoch 24/160 (LR 0.09455) => LSC_loss 1.26, Spatial_loss 2.09, Flat_loss 0.29, Train_acc 64.79, Test_acc 37.24
2025-01-11 04:34:54,503 [podnet.py] => Task 7, Epoch 25/160 (LR 0.09410) => LSC_loss 1.25, Spatial_loss 2.11, Flat_loss 0.29, Train_acc 65.17, Test_acc 37.16
2025-01-11 04:35:01,905 [podnet.py] => Task 7, Epoch 26/160 (LR 0.09362) => LSC_loss 1.26, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 64.70, Test_acc 36.74
2025-01-11 04:35:09,417 [podnet.py] => Task 7, Epoch 27/160 (LR 0.09314) => LSC_loss 1.27, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 64.55, Test_acc 40.02
2025-01-11 04:35:16,830 [podnet.py] => Task 7, Epoch 28/160 (LR 0.09263) => LSC_loss 1.23, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 65.74, Test_acc 42.32
2025-01-11 04:35:24,381 [podnet.py] => Task 7, Epoch 29/160 (LR 0.09211) => LSC_loss 1.24, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 65.42, Test_acc 40.08
2025-01-11 04:35:31,714 [podnet.py] => Task 7, Epoch 30/160 (LR 0.09157) => LSC_loss 1.21, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 66.06, Test_acc 40.64
2025-01-11 04:35:39,117 [podnet.py] => Task 7, Epoch 31/160 (LR 0.09102) => LSC_loss 1.24, Spatial_loss 2.10, Flat_loss 0.29, Train_acc 65.58, Test_acc 28.99
2025-01-11 04:35:46,730 [podnet.py] => Task 7, Epoch 32/160 (LR 0.09045) => LSC_loss 1.23, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 65.90, Test_acc 41.21
2025-01-11 04:35:54,163 [podnet.py] => Task 7, Epoch 33/160 (LR 0.08987) => LSC_loss 1.21, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 65.93, Test_acc 42.60
2025-01-11 04:36:01,582 [podnet.py] => Task 7, Epoch 34/160 (LR 0.08927) => LSC_loss 1.22, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 65.85, Test_acc 43.38
2025-01-11 04:36:09,068 [podnet.py] => Task 7, Epoch 35/160 (LR 0.08865) => LSC_loss 1.22, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 66.45, Test_acc 39.50
2025-01-11 04:36:16,436 [podnet.py] => Task 7, Epoch 36/160 (LR 0.08802) => LSC_loss 1.20, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 66.23, Test_acc 42.10
2025-01-11 04:36:23,854 [podnet.py] => Task 7, Epoch 37/160 (LR 0.08738) => LSC_loss 1.20, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 66.07, Test_acc 42.82
2025-01-11 04:36:31,152 [podnet.py] => Task 7, Epoch 38/160 (LR 0.08672) => LSC_loss 1.19, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 66.76, Test_acc 40.39
2025-01-11 04:36:38,506 [podnet.py] => Task 7, Epoch 39/160 (LR 0.08604) => LSC_loss 1.19, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 66.60, Test_acc 35.34
2025-01-11 04:36:46,137 [podnet.py] => Task 7, Epoch 40/160 (LR 0.08536) => LSC_loss 1.18, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 67.28, Test_acc 40.91
2025-01-11 04:36:53,722 [podnet.py] => Task 7, Epoch 41/160 (LR 0.08465) => LSC_loss 1.18, Spatial_loss 2.06, Flat_loss 0.27, Train_acc 67.30, Test_acc 41.22
2025-01-11 04:37:01,230 [podnet.py] => Task 7, Epoch 42/160 (LR 0.08394) => LSC_loss 1.19, Spatial_loss 2.05, Flat_loss 0.28, Train_acc 65.87, Test_acc 39.52
2025-01-11 04:37:08,704 [podnet.py] => Task 7, Epoch 43/160 (LR 0.08321) => LSC_loss 1.17, Spatial_loss 2.06, Flat_loss 0.28, Train_acc 67.13, Test_acc 39.54
2025-01-11 04:37:16,203 [podnet.py] => Task 7, Epoch 44/160 (LR 0.08247) => LSC_loss 1.17, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 67.32, Test_acc 39.33
2025-01-11 04:37:23,657 [podnet.py] => Task 7, Epoch 45/160 (LR 0.08172) => LSC_loss 1.14, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 68.19, Test_acc 38.46
2025-01-11 04:37:31,195 [podnet.py] => Task 7, Epoch 46/160 (LR 0.08095) => LSC_loss 1.15, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 67.91, Test_acc 43.41
2025-01-11 04:37:38,666 [podnet.py] => Task 7, Epoch 47/160 (LR 0.08018) => LSC_loss 1.13, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 68.04, Test_acc 41.39
2025-01-11 04:37:45,976 [podnet.py] => Task 7, Epoch 48/160 (LR 0.07939) => LSC_loss 1.14, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 67.77, Test_acc 41.28
2025-01-11 04:37:53,241 [podnet.py] => Task 7, Epoch 49/160 (LR 0.07859) => LSC_loss 1.13, Spatial_loss 1.99, Flat_loss 0.27, Train_acc 68.23, Test_acc 41.96
2025-01-11 04:38:00,557 [podnet.py] => Task 7, Epoch 50/160 (LR 0.07778) => LSC_loss 1.14, Spatial_loss 2.02, Flat_loss 0.27, Train_acc 68.05, Test_acc 39.62
2025-01-11 04:38:08,080 [podnet.py] => Task 7, Epoch 51/160 (LR 0.07696) => LSC_loss 1.11, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 68.57, Test_acc 39.64
2025-01-11 04:38:15,358 [podnet.py] => Task 7, Epoch 52/160 (LR 0.07612) => LSC_loss 1.14, Spatial_loss 2.03, Flat_loss 0.27, Train_acc 67.89, Test_acc 40.52
2025-01-11 04:38:22,841 [podnet.py] => Task 7, Epoch 53/160 (LR 0.07528) => LSC_loss 1.15, Spatial_loss 2.05, Flat_loss 0.28, Train_acc 67.90, Test_acc 40.99
2025-01-11 04:38:30,155 [podnet.py] => Task 7, Epoch 54/160 (LR 0.07443) => LSC_loss 1.14, Spatial_loss 2.02, Flat_loss 0.27, Train_acc 67.51, Test_acc 41.18
2025-01-11 04:38:37,484 [podnet.py] => Task 7, Epoch 55/160 (LR 0.07357) => LSC_loss 1.09, Spatial_loss 1.98, Flat_loss 0.27, Train_acc 69.28, Test_acc 40.98
2025-01-11 04:38:44,980 [podnet.py] => Task 7, Epoch 56/160 (LR 0.07270) => LSC_loss 1.10, Spatial_loss 1.99, Flat_loss 0.27, Train_acc 69.08, Test_acc 37.01
2025-01-11 04:38:52,321 [podnet.py] => Task 7, Epoch 57/160 (LR 0.07182) => LSC_loss 1.09, Spatial_loss 1.95, Flat_loss 0.26, Train_acc 69.65, Test_acc 41.11
2025-01-11 04:38:59,808 [podnet.py] => Task 7, Epoch 58/160 (LR 0.07093) => LSC_loss 1.09, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 69.13, Test_acc 43.09
2025-01-11 04:39:07,484 [podnet.py] => Task 7, Epoch 59/160 (LR 0.07004) => LSC_loss 1.08, Spatial_loss 1.94, Flat_loss 0.26, Train_acc 69.40, Test_acc 41.54
2025-01-11 04:39:14,853 [podnet.py] => Task 7, Epoch 60/160 (LR 0.06913) => LSC_loss 1.08, Spatial_loss 1.94, Flat_loss 0.26, Train_acc 69.68, Test_acc 39.50
2025-01-11 04:39:22,300 [podnet.py] => Task 7, Epoch 61/160 (LR 0.06822) => LSC_loss 1.07, Spatial_loss 1.91, Flat_loss 0.26, Train_acc 69.57, Test_acc 39.10
2025-01-11 04:39:29,774 [podnet.py] => Task 7, Epoch 62/160 (LR 0.06731) => LSC_loss 1.06, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 70.15, Test_acc 44.38
2025-01-11 04:39:37,112 [podnet.py] => Task 7, Epoch 63/160 (LR 0.06638) => LSC_loss 1.06, Spatial_loss 1.95, Flat_loss 0.26, Train_acc 69.83, Test_acc 42.59
2025-01-11 04:39:44,589 [podnet.py] => Task 7, Epoch 64/160 (LR 0.06545) => LSC_loss 1.06, Spatial_loss 1.94, Flat_loss 0.26, Train_acc 70.01, Test_acc 37.40
2025-01-11 04:39:52,478 [podnet.py] => Task 7, Epoch 65/160 (LR 0.06451) => LSC_loss 1.07, Spatial_loss 1.95, Flat_loss 0.26, Train_acc 69.79, Test_acc 41.92
2025-01-11 04:40:00,444 [podnet.py] => Task 7, Epoch 66/160 (LR 0.06357) => LSC_loss 1.04, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 70.61, Test_acc 39.17
2025-01-11 04:40:07,941 [podnet.py] => Task 7, Epoch 67/160 (LR 0.06262) => LSC_loss 1.05, Spatial_loss 1.94, Flat_loss 0.26, Train_acc 70.09, Test_acc 39.92
2025-01-11 04:40:15,293 [podnet.py] => Task 7, Epoch 68/160 (LR 0.06167) => LSC_loss 1.01, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 71.85, Test_acc 39.56
2025-01-11 04:40:22,608 [podnet.py] => Task 7, Epoch 69/160 (LR 0.06072) => LSC_loss 1.01, Spatial_loss 1.89, Flat_loss 0.25, Train_acc 71.30, Test_acc 40.70
2025-01-11 04:40:30,048 [podnet.py] => Task 7, Epoch 70/160 (LR 0.05975) => LSC_loss 1.03, Spatial_loss 1.88, Flat_loss 0.25, Train_acc 71.30, Test_acc 41.05
2025-01-11 04:40:37,458 [podnet.py] => Task 7, Epoch 71/160 (LR 0.05879) => LSC_loss 1.00, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 71.31, Test_acc 41.60
2025-01-11 04:40:44,837 [podnet.py] => Task 7, Epoch 72/160 (LR 0.05782) => LSC_loss 1.00, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 71.73, Test_acc 41.84
2025-01-11 04:40:52,336 [podnet.py] => Task 7, Epoch 73/160 (LR 0.05685) => LSC_loss 1.00, Spatial_loss 1.85, Flat_loss 0.25, Train_acc 71.25, Test_acc 43.79
2025-01-11 04:40:59,722 [podnet.py] => Task 7, Epoch 74/160 (LR 0.05588) => LSC_loss 1.01, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 71.00, Test_acc 42.64
2025-01-11 04:41:07,137 [podnet.py] => Task 7, Epoch 75/160 (LR 0.05490) => LSC_loss 0.96, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 72.61, Test_acc 44.30
2025-01-11 04:41:14,855 [podnet.py] => Task 7, Epoch 76/160 (LR 0.05392) => LSC_loss 0.98, Spatial_loss 1.80, Flat_loss 0.24, Train_acc 72.77, Test_acc 43.12
2025-01-11 04:41:22,211 [podnet.py] => Task 7, Epoch 77/160 (LR 0.05294) => LSC_loss 0.96, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 73.09, Test_acc 43.80
2025-01-11 04:41:29,913 [podnet.py] => Task 7, Epoch 78/160 (LR 0.05196) => LSC_loss 0.96, Spatial_loss 1.81, Flat_loss 0.23, Train_acc 73.04, Test_acc 42.25
2025-01-11 04:41:38,187 [podnet.py] => Task 7, Epoch 79/160 (LR 0.05098) => LSC_loss 0.95, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 73.47, Test_acc 41.09
2025-01-11 04:41:45,649 [podnet.py] => Task 7, Epoch 80/160 (LR 0.05000) => LSC_loss 0.94, Spatial_loss 1.78, Flat_loss 0.23, Train_acc 73.61, Test_acc 41.66
2025-01-11 04:41:53,098 [podnet.py] => Task 7, Epoch 81/160 (LR 0.04902) => LSC_loss 0.95, Spatial_loss 1.79, Flat_loss 0.23, Train_acc 73.37, Test_acc 44.11
2025-01-11 04:42:00,545 [podnet.py] => Task 7, Epoch 82/160 (LR 0.04804) => LSC_loss 0.94, Spatial_loss 1.80, Flat_loss 0.23, Train_acc 73.43, Test_acc 42.44
2025-01-11 04:42:08,456 [podnet.py] => Task 7, Epoch 83/160 (LR 0.04706) => LSC_loss 0.90, Spatial_loss 1.76, Flat_loss 0.23, Train_acc 73.98, Test_acc 44.61
2025-01-11 04:42:15,947 [podnet.py] => Task 7, Epoch 84/160 (LR 0.04608) => LSC_loss 0.90, Spatial_loss 1.76, Flat_loss 0.23, Train_acc 74.77, Test_acc 44.95
2025-01-11 04:42:23,602 [podnet.py] => Task 7, Epoch 85/160 (LR 0.04510) => LSC_loss 0.89, Spatial_loss 1.72, Flat_loss 0.23, Train_acc 75.20, Test_acc 47.20
2025-01-11 04:42:30,888 [podnet.py] => Task 7, Epoch 86/160 (LR 0.04412) => LSC_loss 0.88, Spatial_loss 1.73, Flat_loss 0.22, Train_acc 75.21, Test_acc 42.40
2025-01-11 04:42:38,161 [podnet.py] => Task 7, Epoch 87/160 (LR 0.04315) => LSC_loss 0.87, Spatial_loss 1.74, Flat_loss 0.22, Train_acc 75.37, Test_acc 43.92
2025-01-11 04:42:45,688 [podnet.py] => Task 7, Epoch 88/160 (LR 0.04218) => LSC_loss 0.86, Spatial_loss 1.70, Flat_loss 0.22, Train_acc 76.02, Test_acc 44.50
2025-01-11 04:42:53,280 [podnet.py] => Task 7, Epoch 89/160 (LR 0.04121) => LSC_loss 0.86, Spatial_loss 1.71, Flat_loss 0.22, Train_acc 75.59, Test_acc 44.66
2025-01-11 04:43:00,753 [podnet.py] => Task 7, Epoch 90/160 (LR 0.04025) => LSC_loss 0.85, Spatial_loss 1.67, Flat_loss 0.22, Train_acc 76.00, Test_acc 44.16
2025-01-11 04:43:08,044 [podnet.py] => Task 7, Epoch 91/160 (LR 0.03928) => LSC_loss 0.85, Spatial_loss 1.70, Flat_loss 0.22, Train_acc 75.69, Test_acc 42.91
2025-01-11 04:43:15,348 [podnet.py] => Task 7, Epoch 92/160 (LR 0.03833) => LSC_loss 0.84, Spatial_loss 1.64, Flat_loss 0.21, Train_acc 76.31, Test_acc 45.50
2025-01-11 04:43:22,845 [podnet.py] => Task 7, Epoch 93/160 (LR 0.03738) => LSC_loss 0.82, Spatial_loss 1.65, Flat_loss 0.21, Train_acc 77.14, Test_acc 44.46
2025-01-11 04:43:30,044 [podnet.py] => Task 7, Epoch 94/160 (LR 0.03643) => LSC_loss 0.82, Spatial_loss 1.66, Flat_loss 0.21, Train_acc 77.08, Test_acc 44.94
2025-01-11 04:43:37,614 [podnet.py] => Task 7, Epoch 95/160 (LR 0.03549) => LSC_loss 0.80, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 77.54, Test_acc 44.05
2025-01-11 04:43:45,074 [podnet.py] => Task 7, Epoch 96/160 (LR 0.03455) => LSC_loss 0.81, Spatial_loss 1.62, Flat_loss 0.21, Train_acc 77.51, Test_acc 42.02
2025-01-11 04:43:52,470 [podnet.py] => Task 7, Epoch 97/160 (LR 0.03362) => LSC_loss 0.80, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 77.61, Test_acc 47.14
2025-01-11 04:43:59,980 [podnet.py] => Task 7, Epoch 98/160 (LR 0.03269) => LSC_loss 0.79, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 78.36, Test_acc 46.42
2025-01-11 04:44:07,261 [podnet.py] => Task 7, Epoch 99/160 (LR 0.03178) => LSC_loss 0.79, Spatial_loss 1.59, Flat_loss 0.20, Train_acc 77.82, Test_acc 46.00
2025-01-11 04:44:14,815 [podnet.py] => Task 7, Epoch 100/160 (LR 0.03087) => LSC_loss 0.77, Spatial_loss 1.58, Flat_loss 0.20, Train_acc 78.31, Test_acc 45.30
2025-01-11 04:44:22,181 [podnet.py] => Task 7, Epoch 101/160 (LR 0.02996) => LSC_loss 0.76, Spatial_loss 1.59, Flat_loss 0.20, Train_acc 78.49, Test_acc 44.76
2025-01-11 04:44:29,939 [podnet.py] => Task 7, Epoch 102/160 (LR 0.02907) => LSC_loss 0.74, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 79.87, Test_acc 45.62
2025-01-11 04:44:37,381 [podnet.py] => Task 7, Epoch 103/160 (LR 0.02818) => LSC_loss 0.72, Spatial_loss 1.51, Flat_loss 0.19, Train_acc 79.69, Test_acc 42.40
2025-01-11 04:44:44,726 [podnet.py] => Task 7, Epoch 104/160 (LR 0.02730) => LSC_loss 0.73, Spatial_loss 1.51, Flat_loss 0.19, Train_acc 79.69, Test_acc 44.46
2025-01-11 04:44:52,161 [podnet.py] => Task 7, Epoch 105/160 (LR 0.02643) => LSC_loss 0.71, Spatial_loss 1.50, Flat_loss 0.19, Train_acc 80.28, Test_acc 45.89
2025-01-11 04:44:59,699 [podnet.py] => Task 7, Epoch 106/160 (LR 0.02557) => LSC_loss 0.72, Spatial_loss 1.50, Flat_loss 0.19, Train_acc 80.02, Test_acc 46.56
2025-01-11 04:45:07,151 [podnet.py] => Task 7, Epoch 107/160 (LR 0.02472) => LSC_loss 0.71, Spatial_loss 1.50, Flat_loss 0.19, Train_acc 80.13, Test_acc 45.58
2025-01-11 04:45:14,546 [podnet.py] => Task 7, Epoch 108/160 (LR 0.02388) => LSC_loss 0.68, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 81.43, Test_acc 43.28
2025-01-11 04:45:22,073 [podnet.py] => Task 7, Epoch 109/160 (LR 0.02304) => LSC_loss 0.67, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 81.83, Test_acc 46.34
2025-01-11 04:45:29,580 [podnet.py] => Task 7, Epoch 110/160 (LR 0.02222) => LSC_loss 0.67, Spatial_loss 1.43, Flat_loss 0.18, Train_acc 81.44, Test_acc 44.95
2025-01-11 04:45:37,164 [podnet.py] => Task 7, Epoch 111/160 (LR 0.02141) => LSC_loss 0.65, Spatial_loss 1.42, Flat_loss 0.18, Train_acc 82.37, Test_acc 45.39
2025-01-11 04:45:44,845 [podnet.py] => Task 7, Epoch 112/160 (LR 0.02061) => LSC_loss 0.64, Spatial_loss 1.40, Flat_loss 0.18, Train_acc 82.49, Test_acc 45.92
2025-01-11 04:45:52,326 [podnet.py] => Task 7, Epoch 113/160 (LR 0.01982) => LSC_loss 0.65, Spatial_loss 1.37, Flat_loss 0.17, Train_acc 82.16, Test_acc 47.14
2025-01-11 04:45:59,615 [podnet.py] => Task 7, Epoch 114/160 (LR 0.01905) => LSC_loss 0.64, Spatial_loss 1.39, Flat_loss 0.17, Train_acc 82.56, Test_acc 47.20
2025-01-11 04:46:07,185 [podnet.py] => Task 7, Epoch 115/160 (LR 0.01828) => LSC_loss 0.63, Spatial_loss 1.34, Flat_loss 0.17, Train_acc 82.94, Test_acc 47.56
2025-01-11 04:46:14,606 [podnet.py] => Task 7, Epoch 116/160 (LR 0.01753) => LSC_loss 0.62, Spatial_loss 1.34, Flat_loss 0.17, Train_acc 83.82, Test_acc 48.01
2025-01-11 04:46:22,174 [podnet.py] => Task 7, Epoch 117/160 (LR 0.01679) => LSC_loss 0.62, Spatial_loss 1.33, Flat_loss 0.16, Train_acc 83.35, Test_acc 47.06
2025-01-11 04:46:29,613 [podnet.py] => Task 7, Epoch 118/160 (LR 0.01606) => LSC_loss 0.59, Spatial_loss 1.31, Flat_loss 0.16, Train_acc 84.49, Test_acc 47.48
2025-01-11 04:46:37,160 [podnet.py] => Task 7, Epoch 119/160 (LR 0.01535) => LSC_loss 0.60, Spatial_loss 1.34, Flat_loss 0.16, Train_acc 83.85, Test_acc 47.82
2025-01-11 04:46:44,674 [podnet.py] => Task 7, Epoch 120/160 (LR 0.01464) => LSC_loss 0.59, Spatial_loss 1.28, Flat_loss 0.16, Train_acc 84.28, Test_acc 46.58
2025-01-11 04:46:52,015 [podnet.py] => Task 7, Epoch 121/160 (LR 0.01396) => LSC_loss 0.58, Spatial_loss 1.29, Flat_loss 0.16, Train_acc 84.19, Test_acc 47.94
2025-01-11 04:46:59,268 [podnet.py] => Task 7, Epoch 122/160 (LR 0.01328) => LSC_loss 0.57, Spatial_loss 1.25, Flat_loss 0.16, Train_acc 84.95, Test_acc 47.91
2025-01-11 04:47:07,051 [podnet.py] => Task 7, Epoch 123/160 (LR 0.01262) => LSC_loss 0.58, Spatial_loss 1.27, Flat_loss 0.16, Train_acc 84.58, Test_acc 47.54
2025-01-11 04:47:14,525 [podnet.py] => Task 7, Epoch 124/160 (LR 0.01198) => LSC_loss 0.56, Spatial_loss 1.24, Flat_loss 0.15, Train_acc 85.47, Test_acc 47.80
2025-01-11 04:47:21,934 [podnet.py] => Task 7, Epoch 125/160 (LR 0.01135) => LSC_loss 0.55, Spatial_loss 1.22, Flat_loss 0.15, Train_acc 85.86, Test_acc 48.74
2025-01-11 04:47:29,588 [podnet.py] => Task 7, Epoch 126/160 (LR 0.01073) => LSC_loss 0.54, Spatial_loss 1.23, Flat_loss 0.15, Train_acc 86.08, Test_acc 47.71
2025-01-11 04:47:36,897 [podnet.py] => Task 7, Epoch 127/160 (LR 0.01013) => LSC_loss 0.55, Spatial_loss 1.19, Flat_loss 0.15, Train_acc 85.78, Test_acc 45.72
2025-01-11 04:47:44,272 [podnet.py] => Task 7, Epoch 128/160 (LR 0.00955) => LSC_loss 0.53, Spatial_loss 1.21, Flat_loss 0.15, Train_acc 85.92, Test_acc 48.84
2025-01-11 04:47:51,736 [podnet.py] => Task 7, Epoch 129/160 (LR 0.00898) => LSC_loss 0.53, Spatial_loss 1.17, Flat_loss 0.15, Train_acc 86.41, Test_acc 48.28
2025-01-11 04:47:59,087 [podnet.py] => Task 7, Epoch 130/160 (LR 0.00843) => LSC_loss 0.52, Spatial_loss 1.16, Flat_loss 0.14, Train_acc 86.70, Test_acc 49.29
2025-01-11 04:48:06,454 [podnet.py] => Task 7, Epoch 131/160 (LR 0.00789) => LSC_loss 0.51, Spatial_loss 1.15, Flat_loss 0.14, Train_acc 86.87, Test_acc 49.36
2025-01-11 04:48:13,742 [podnet.py] => Task 7, Epoch 132/160 (LR 0.00737) => LSC_loss 0.52, Spatial_loss 1.17, Flat_loss 0.14, Train_acc 86.73, Test_acc 49.10
2025-01-11 04:48:21,291 [podnet.py] => Task 7, Epoch 133/160 (LR 0.00686) => LSC_loss 0.51, Spatial_loss 1.15, Flat_loss 0.14, Train_acc 87.35, Test_acc 49.51
2025-01-11 04:48:28,583 [podnet.py] => Task 7, Epoch 134/160 (LR 0.00638) => LSC_loss 0.50, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 87.41, Test_acc 49.31
2025-01-11 04:48:36,219 [podnet.py] => Task 7, Epoch 135/160 (LR 0.00590) => LSC_loss 0.50, Spatial_loss 1.09, Flat_loss 0.14, Train_acc 87.34, Test_acc 49.09
2025-01-11 04:48:43,693 [podnet.py] => Task 7, Epoch 136/160 (LR 0.00545) => LSC_loss 0.50, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 87.39, Test_acc 48.82
2025-01-11 04:48:50,979 [podnet.py] => Task 7, Epoch 137/160 (LR 0.00501) => LSC_loss 0.49, Spatial_loss 1.09, Flat_loss 0.13, Train_acc 87.85, Test_acc 49.30
2025-01-11 04:48:58,488 [podnet.py] => Task 7, Epoch 138/160 (LR 0.00459) => LSC_loss 0.49, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 87.38, Test_acc 49.45
2025-01-11 04:49:05,959 [podnet.py] => Task 7, Epoch 139/160 (LR 0.00419) => LSC_loss 0.49, Spatial_loss 1.08, Flat_loss 0.13, Train_acc 87.68, Test_acc 49.31
2025-01-11 04:49:13,280 [podnet.py] => Task 7, Epoch 140/160 (LR 0.00381) => LSC_loss 0.48, Spatial_loss 1.06, Flat_loss 0.13, Train_acc 88.42, Test_acc 49.86
2025-01-11 04:49:20,694 [podnet.py] => Task 7, Epoch 141/160 (LR 0.00344) => LSC_loss 0.47, Spatial_loss 1.05, Flat_loss 0.13, Train_acc 88.45, Test_acc 49.50
2025-01-11 04:49:28,026 [podnet.py] => Task 7, Epoch 142/160 (LR 0.00309) => LSC_loss 0.47, Spatial_loss 1.05, Flat_loss 0.13, Train_acc 88.41, Test_acc 49.10
2025-01-11 04:49:35,634 [podnet.py] => Task 7, Epoch 143/160 (LR 0.00276) => LSC_loss 0.48, Spatial_loss 1.04, Flat_loss 0.13, Train_acc 88.11, Test_acc 48.99
2025-01-11 04:49:42,944 [podnet.py] => Task 7, Epoch 144/160 (LR 0.00245) => LSC_loss 0.46, Spatial_loss 1.02, Flat_loss 0.13, Train_acc 88.73, Test_acc 49.94
2025-01-11 04:49:50,442 [podnet.py] => Task 7, Epoch 145/160 (LR 0.00215) => LSC_loss 0.46, Spatial_loss 1.02, Flat_loss 0.13, Train_acc 88.86, Test_acc 49.34
2025-01-11 04:49:57,736 [podnet.py] => Task 7, Epoch 146/160 (LR 0.00188) => LSC_loss 0.45, Spatial_loss 1.03, Flat_loss 0.13, Train_acc 88.95, Test_acc 49.95
2025-01-11 04:50:05,039 [podnet.py] => Task 7, Epoch 147/160 (LR 0.00162) => LSC_loss 0.46, Spatial_loss 1.01, Flat_loss 0.13, Train_acc 89.15, Test_acc 49.22
2025-01-11 04:50:12,692 [podnet.py] => Task 7, Epoch 148/160 (LR 0.00138) => LSC_loss 0.46, Spatial_loss 1.00, Flat_loss 0.12, Train_acc 88.84, Test_acc 49.74
2025-01-11 04:50:19,932 [podnet.py] => Task 7, Epoch 149/160 (LR 0.00116) => LSC_loss 0.46, Spatial_loss 1.00, Flat_loss 0.12, Train_acc 88.84, Test_acc 49.98
2025-01-11 04:50:27,584 [podnet.py] => Task 7, Epoch 150/160 (LR 0.00096) => LSC_loss 0.44, Spatial_loss 1.00, Flat_loss 0.12, Train_acc 89.28, Test_acc 50.12
2025-01-11 04:50:35,137 [podnet.py] => Task 7, Epoch 151/160 (LR 0.00078) => LSC_loss 0.46, Spatial_loss 0.99, Flat_loss 0.12, Train_acc 89.23, Test_acc 49.79
2025-01-11 04:50:42,380 [podnet.py] => Task 7, Epoch 152/160 (LR 0.00062) => LSC_loss 0.44, Spatial_loss 1.00, Flat_loss 0.12, Train_acc 89.30, Test_acc 50.15
2025-01-11 04:50:49,711 [podnet.py] => Task 7, Epoch 153/160 (LR 0.00047) => LSC_loss 0.45, Spatial_loss 0.98, Flat_loss 0.12, Train_acc 89.36, Test_acc 50.31
2025-01-11 04:50:57,228 [podnet.py] => Task 7, Epoch 154/160 (LR 0.00035) => LSC_loss 0.44, Spatial_loss 0.99, Flat_loss 0.12, Train_acc 89.23, Test_acc 50.14
2025-01-11 04:51:04,919 [podnet.py] => Task 7, Epoch 155/160 (LR 0.00024) => LSC_loss 0.45, Spatial_loss 0.98, Flat_loss 0.12, Train_acc 89.45, Test_acc 50.26
2025-01-11 04:51:12,369 [podnet.py] => Task 7, Epoch 156/160 (LR 0.00015) => LSC_loss 0.45, Spatial_loss 0.97, Flat_loss 0.12, Train_acc 88.93, Test_acc 50.12
2025-01-11 04:51:19,737 [podnet.py] => Task 7, Epoch 157/160 (LR 0.00009) => LSC_loss 0.44, Spatial_loss 0.98, Flat_loss 0.12, Train_acc 89.30, Test_acc 49.98
2025-01-11 04:51:27,303 [podnet.py] => Task 7, Epoch 158/160 (LR 0.00004) => LSC_loss 0.44, Spatial_loss 0.96, Flat_loss 0.12, Train_acc 89.57, Test_acc 49.86
2025-01-11 04:51:35,203 [podnet.py] => Task 7, Epoch 159/160 (LR 0.00001) => LSC_loss 0.44, Spatial_loss 0.97, Flat_loss 0.12, Train_acc 89.44, Test_acc 49.89
2025-01-11 04:51:42,807 [podnet.py] => Task 7, Epoch 160/160 (LR 0.00000) => LSC_loss 0.44, Spatial_loss 0.97, Flat_loss 0.12, Train_acc 89.86, Test_acc 49.96
2025-01-11 04:51:42,808 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 04:51:42,808 [base.py] => Reducing exemplars...(105 per classes)
2025-01-11 04:52:25,201 [base.py] => Constructing exemplars...(105 per classes)
2025-01-11 04:52:38,728 [podnet.py] => The size of finetune dataset: 8400
2025-01-11 04:52:44,597 [podnet.py] => Task 7, Epoch 1/20 (LR 0.00497) => LSC_loss 0.36, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 91.60, Test_acc 50.34
2025-01-11 04:52:50,228 [podnet.py] => Task 7, Epoch 2/20 (LR 0.00488) => LSC_loss 0.35, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 91.86, Test_acc 50.59
2025-01-11 04:52:55,866 [podnet.py] => Task 7, Epoch 3/20 (LR 0.00473) => LSC_loss 0.35, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 92.04, Test_acc 51.25
2025-01-11 04:53:01,526 [podnet.py] => Task 7, Epoch 4/20 (LR 0.00452) => LSC_loss 0.34, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 92.56, Test_acc 51.00
2025-01-11 04:53:07,078 [podnet.py] => Task 7, Epoch 5/20 (LR 0.00427) => LSC_loss 0.35, Spatial_loss 1.01, Flat_loss 0.08, Train_acc 92.26, Test_acc 51.20
2025-01-11 04:53:12,752 [podnet.py] => Task 7, Epoch 6/20 (LR 0.00397) => LSC_loss 0.33, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 92.57, Test_acc 50.95
2025-01-11 04:53:18,209 [podnet.py] => Task 7, Epoch 7/20 (LR 0.00363) => LSC_loss 0.33, Spatial_loss 1.02, Flat_loss 0.08, Train_acc 93.01, Test_acc 51.19
2025-01-11 04:53:23,829 [podnet.py] => Task 7, Epoch 8/20 (LR 0.00327) => LSC_loss 0.33, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 92.90, Test_acc 50.66
2025-01-11 04:53:29,333 [podnet.py] => Task 7, Epoch 9/20 (LR 0.00289) => LSC_loss 0.33, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 92.68, Test_acc 50.90
2025-01-11 04:53:35,031 [podnet.py] => Task 7, Epoch 10/20 (LR 0.00250) => LSC_loss 0.32, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 92.80, Test_acc 50.56
2025-01-11 04:53:41,102 [podnet.py] => Task 7, Epoch 11/20 (LR 0.00211) => LSC_loss 0.33, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 92.80, Test_acc 51.32
2025-01-11 04:53:47,079 [podnet.py] => Task 7, Epoch 12/20 (LR 0.00173) => LSC_loss 0.32, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 92.99, Test_acc 51.16
2025-01-11 04:53:53,265 [podnet.py] => Task 7, Epoch 13/20 (LR 0.00137) => LSC_loss 0.33, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 92.74, Test_acc 51.58
2025-01-11 04:53:59,291 [podnet.py] => Task 7, Epoch 14/20 (LR 0.00103) => LSC_loss 0.31, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 93.08, Test_acc 51.12
2025-01-11 04:54:05,398 [podnet.py] => Task 7, Epoch 15/20 (LR 0.00073) => LSC_loss 0.33, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 92.79, Test_acc 51.45
2025-01-11 04:54:11,520 [podnet.py] => Task 7, Epoch 16/20 (LR 0.00048) => LSC_loss 0.31, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 93.30, Test_acc 51.60
2025-01-11 04:54:17,466 [podnet.py] => Task 7, Epoch 17/20 (LR 0.00027) => LSC_loss 0.31, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 93.54, Test_acc 51.61
2025-01-11 04:54:23,763 [podnet.py] => Task 7, Epoch 18/20 (LR 0.00012) => LSC_loss 0.32, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 93.04, Test_acc 51.61
2025-01-11 04:54:29,675 [podnet.py] => Task 7, Epoch 19/20 (LR 0.00003) => LSC_loss 0.30, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 93.85, Test_acc 51.42
2025-01-11 04:54:36,162 [podnet.py] => Task 7, Epoch 20/20 (LR 0.00000) => LSC_loss 0.31, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 93.51, Test_acc 51.38
2025-01-11 04:54:36,164 [base.py] => Reducing exemplars...(92 per classes)
2025-01-11 04:55:21,875 [base.py] => Constructing exemplars...(92 per classes)
2025-01-11 04:55:38,938 [podnet.py] => Exemplar size: 7360
2025-01-11 04:55:38,938 [trainer.py] => CNN: {'total': np.float64(51.38), '00-09': np.float64(64.8), '10-19': np.float64(38.7), '20-29': np.float64(55.3), '30-39': np.float64(44.6), '40-49': np.float64(55.1), '50-59': np.float64(47.4), '60-69': np.float64(56.2), '70-79': np.float64(48.9), 'old': np.float64(51.73), 'new': np.float64(48.9)}
2025-01-11 04:55:38,938 [trainer.py] => NME: {'total': np.float64(49.86), '00-09': np.float64(69.0), '10-19': np.float64(38.0), '20-29': np.float64(53.6), '30-39': np.float64(43.2), '40-49': np.float64(52.3), '50-59': np.float64(42.4), '60-69': np.float64(53.5), '70-79': np.float64(46.9), 'old': np.float64(50.29), 'new': np.float64(46.9)}
2025-01-11 04:55:38,938 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42), np.float64(60.28), np.float64(56.65), np.float64(54.8), np.float64(51.38)]
2025-01-11 04:55:38,939 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82), np.float64(85.98), np.float64(83.17), np.float64(81.17), np.float64(79.08)]
2025-01-11 04:55:38,939 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38), np.float64(59.32), np.float64(55.12), np.float64(53.11), np.float64(49.86)]
2025-01-11 04:55:38,939 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02), np.float64(85.44), np.float64(82.3), np.float64(80.14), np.float64(78.16)]

2025-01-11 04:55:38,939 [trainer.py] => All params: 517457
2025-01-11 04:55:38,939 [trainer.py] => Trainable params: 517457
2025-01-11 04:55:38,940 [podnet.py] => Learning on 80-90
2025-01-11 04:55:39,107 [podnet.py] => Adaptive factor: 3.0
2025-01-11 04:55:46,597 [podnet.py] => Task 8, Epoch 1/160 (LR 0.09999) => LSC_loss 2.56, Spatial_loss 3.08, Flat_loss 0.79, Train_acc 40.78, Test_acc 30.38
2025-01-11 04:55:54,293 [podnet.py] => Task 8, Epoch 2/160 (LR 0.09996) => LSC_loss 1.89, Spatial_loss 2.69, Flat_loss 0.48, Train_acc 49.74, Test_acc 34.42
2025-01-11 04:56:02,120 [podnet.py] => Task 8, Epoch 3/160 (LR 0.09991) => LSC_loss 1.74, Spatial_loss 2.55, Flat_loss 0.42, Train_acc 53.24, Test_acc 37.49
2025-01-11 04:56:09,750 [podnet.py] => Task 8, Epoch 4/160 (LR 0.09985) => LSC_loss 1.63, Spatial_loss 2.42, Flat_loss 0.38, Train_acc 56.76, Test_acc 36.19
2025-01-11 04:56:17,307 [podnet.py] => Task 8, Epoch 5/160 (LR 0.09976) => LSC_loss 1.59, Spatial_loss 2.39, Flat_loss 0.37, Train_acc 57.39, Test_acc 38.42
2025-01-11 04:56:25,351 [podnet.py] => Task 8, Epoch 6/160 (LR 0.09965) => LSC_loss 1.54, Spatial_loss 2.33, Flat_loss 0.36, Train_acc 57.53, Test_acc 35.64
2025-01-11 04:56:32,911 [podnet.py] => Task 8, Epoch 7/160 (LR 0.09953) => LSC_loss 1.51, Spatial_loss 2.29, Flat_loss 0.35, Train_acc 59.42, Test_acc 39.68
2025-01-11 04:56:40,770 [podnet.py] => Task 8, Epoch 8/160 (LR 0.09938) => LSC_loss 1.50, Spatial_loss 2.30, Flat_loss 0.35, Train_acc 59.57, Test_acc 35.02
2025-01-11 04:56:48,523 [podnet.py] => Task 8, Epoch 9/160 (LR 0.09922) => LSC_loss 1.47, Spatial_loss 2.26, Flat_loss 0.34, Train_acc 60.38, Test_acc 39.60
2025-01-11 04:56:56,051 [podnet.py] => Task 8, Epoch 10/160 (LR 0.09904) => LSC_loss 1.46, Spatial_loss 2.28, Flat_loss 0.35, Train_acc 60.23, Test_acc 36.66
2025-01-11 04:57:03,774 [podnet.py] => Task 8, Epoch 11/160 (LR 0.09884) => LSC_loss 1.43, Spatial_loss 2.24, Flat_loss 0.34, Train_acc 61.30, Test_acc 39.63
2025-01-11 04:57:11,330 [podnet.py] => Task 8, Epoch 12/160 (LR 0.09862) => LSC_loss 1.40, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 62.39, Test_acc 36.62
2025-01-11 04:57:18,984 [podnet.py] => Task 8, Epoch 13/160 (LR 0.09838) => LSC_loss 1.42, Spatial_loss 2.24, Flat_loss 0.34, Train_acc 61.46, Test_acc 35.87
2025-01-11 04:57:26,682 [podnet.py] => Task 8, Epoch 14/160 (LR 0.09812) => LSC_loss 1.40, Spatial_loss 2.25, Flat_loss 0.34, Train_acc 62.16, Test_acc 35.02
2025-01-11 04:57:34,141 [podnet.py] => Task 8, Epoch 15/160 (LR 0.09785) => LSC_loss 1.38, Spatial_loss 2.26, Flat_loss 0.34, Train_acc 62.34, Test_acc 37.86
2025-01-11 04:57:41,781 [podnet.py] => Task 8, Epoch 16/160 (LR 0.09755) => LSC_loss 1.39, Spatial_loss 2.26, Flat_loss 0.34, Train_acc 62.24, Test_acc 38.73
2025-01-11 04:57:49,637 [podnet.py] => Task 8, Epoch 17/160 (LR 0.09724) => LSC_loss 1.35, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 62.97, Test_acc 35.87
2025-01-11 04:57:57,247 [podnet.py] => Task 8, Epoch 18/160 (LR 0.09691) => LSC_loss 1.35, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 63.38, Test_acc 35.70
2025-01-11 04:58:04,806 [podnet.py] => Task 8, Epoch 19/160 (LR 0.09656) => LSC_loss 1.35, Spatial_loss 2.23, Flat_loss 0.34, Train_acc 63.05, Test_acc 30.88
2025-01-11 04:58:12,369 [podnet.py] => Task 8, Epoch 20/160 (LR 0.09619) => LSC_loss 1.32, Spatial_loss 2.21, Flat_loss 0.34, Train_acc 64.12, Test_acc 38.57
2025-01-11 04:58:20,194 [podnet.py] => Task 8, Epoch 21/160 (LR 0.09581) => LSC_loss 1.33, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 63.82, Test_acc 35.51
2025-01-11 04:58:27,779 [podnet.py] => Task 8, Epoch 22/160 (LR 0.09541) => LSC_loss 1.31, Spatial_loss 2.23, Flat_loss 0.34, Train_acc 64.37, Test_acc 37.64
2025-01-11 04:58:35,546 [podnet.py] => Task 8, Epoch 23/160 (LR 0.09499) => LSC_loss 1.30, Spatial_loss 2.21, Flat_loss 0.33, Train_acc 64.61, Test_acc 39.69
2025-01-11 04:58:43,135 [podnet.py] => Task 8, Epoch 24/160 (LR 0.09455) => LSC_loss 1.33, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 63.53, Test_acc 34.00
2025-01-11 04:58:51,007 [podnet.py] => Task 8, Epoch 25/160 (LR 0.09410) => LSC_loss 1.28, Spatial_loss 2.22, Flat_loss 0.33, Train_acc 64.96, Test_acc 35.79
2025-01-11 04:58:58,805 [podnet.py] => Task 8, Epoch 26/160 (LR 0.09362) => LSC_loss 1.29, Spatial_loss 2.24, Flat_loss 0.34, Train_acc 64.67, Test_acc 36.90
2025-01-11 04:59:06,653 [podnet.py] => Task 8, Epoch 27/160 (LR 0.09314) => LSC_loss 1.30, Spatial_loss 2.20, Flat_loss 0.34, Train_acc 64.73, Test_acc 38.23
2025-01-11 04:59:14,447 [podnet.py] => Task 8, Epoch 28/160 (LR 0.09263) => LSC_loss 1.28, Spatial_loss 2.20, Flat_loss 0.33, Train_acc 65.07, Test_acc 38.69
2025-01-11 04:59:22,241 [podnet.py] => Task 8, Epoch 29/160 (LR 0.09211) => LSC_loss 1.27, Spatial_loss 2.18, Flat_loss 0.33, Train_acc 65.53, Test_acc 39.98
2025-01-11 04:59:30,067 [podnet.py] => Task 8, Epoch 30/160 (LR 0.09157) => LSC_loss 1.28, Spatial_loss 2.16, Flat_loss 0.33, Train_acc 65.43, Test_acc 36.76
2025-01-11 04:59:37,631 [podnet.py] => Task 8, Epoch 31/160 (LR 0.09102) => LSC_loss 1.27, Spatial_loss 2.18, Flat_loss 0.33, Train_acc 65.37, Test_acc 36.54
2025-01-11 04:59:45,251 [podnet.py] => Task 8, Epoch 32/160 (LR 0.09045) => LSC_loss 1.27, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 65.14, Test_acc 38.64
2025-01-11 04:59:52,784 [podnet.py] => Task 8, Epoch 33/160 (LR 0.08987) => LSC_loss 1.27, Spatial_loss 2.22, Flat_loss 0.33, Train_acc 65.53, Test_acc 38.04
2025-01-11 05:00:00,408 [podnet.py] => Task 8, Epoch 34/160 (LR 0.08927) => LSC_loss 1.26, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 65.28, Test_acc 40.13
2025-01-11 05:00:08,062 [podnet.py] => Task 8, Epoch 35/160 (LR 0.08865) => LSC_loss 1.24, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 65.87, Test_acc 39.19
2025-01-11 05:00:15,846 [podnet.py] => Task 8, Epoch 36/160 (LR 0.08802) => LSC_loss 1.22, Spatial_loss 2.16, Flat_loss 0.33, Train_acc 66.73, Test_acc 40.47
2025-01-11 05:00:23,464 [podnet.py] => Task 8, Epoch 37/160 (LR 0.08738) => LSC_loss 1.24, Spatial_loss 2.16, Flat_loss 0.33, Train_acc 66.15, Test_acc 40.07
2025-01-11 05:00:31,010 [podnet.py] => Task 8, Epoch 38/160 (LR 0.08672) => LSC_loss 1.23, Spatial_loss 2.15, Flat_loss 0.32, Train_acc 66.42, Test_acc 37.92
2025-01-11 05:00:38,605 [podnet.py] => Task 8, Epoch 39/160 (LR 0.08604) => LSC_loss 1.23, Spatial_loss 2.19, Flat_loss 0.33, Train_acc 66.58, Test_acc 37.41
2025-01-11 05:00:46,157 [podnet.py] => Task 8, Epoch 40/160 (LR 0.08536) => LSC_loss 1.22, Spatial_loss 2.14, Flat_loss 0.33, Train_acc 66.72, Test_acc 40.69
2025-01-11 05:00:53,785 [podnet.py] => Task 8, Epoch 41/160 (LR 0.08465) => LSC_loss 1.20, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 66.90, Test_acc 36.12
2025-01-11 05:01:01,354 [podnet.py] => Task 8, Epoch 42/160 (LR 0.08394) => LSC_loss 1.20, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 67.45, Test_acc 37.97
2025-01-11 05:01:09,341 [podnet.py] => Task 8, Epoch 43/160 (LR 0.08321) => LSC_loss 1.20, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 67.10, Test_acc 37.87
2025-01-11 05:01:16,823 [podnet.py] => Task 8, Epoch 44/160 (LR 0.08247) => LSC_loss 1.21, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 67.06, Test_acc 38.31
2025-01-11 05:01:24,692 [podnet.py] => Task 8, Epoch 45/160 (LR 0.08172) => LSC_loss 1.22, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 66.51, Test_acc 39.22
2025-01-11 05:01:32,233 [podnet.py] => Task 8, Epoch 46/160 (LR 0.08095) => LSC_loss 1.16, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 68.75, Test_acc 38.01
2025-01-11 05:01:39,824 [podnet.py] => Task 8, Epoch 47/160 (LR 0.08018) => LSC_loss 1.18, Spatial_loss 2.07, Flat_loss 0.32, Train_acc 67.99, Test_acc 38.31
2025-01-11 05:01:47,356 [podnet.py] => Task 8, Epoch 48/160 (LR 0.07939) => LSC_loss 1.16, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 68.13, Test_acc 38.27
2025-01-11 05:01:55,058 [podnet.py] => Task 8, Epoch 49/160 (LR 0.07859) => LSC_loss 1.17, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 67.39, Test_acc 37.02
2025-01-11 05:02:02,743 [podnet.py] => Task 8, Epoch 50/160 (LR 0.07778) => LSC_loss 1.16, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 67.99, Test_acc 41.84
2025-01-11 05:02:10,446 [podnet.py] => Task 8, Epoch 51/160 (LR 0.07696) => LSC_loss 1.16, Spatial_loss 2.09, Flat_loss 0.32, Train_acc 68.16, Test_acc 37.92
2025-01-11 05:02:18,348 [podnet.py] => Task 8, Epoch 52/160 (LR 0.07612) => LSC_loss 1.17, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 67.61, Test_acc 41.16
2025-01-11 05:02:26,346 [podnet.py] => Task 8, Epoch 53/160 (LR 0.07528) => LSC_loss 1.14, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 69.15, Test_acc 39.72
2025-01-11 05:02:34,159 [podnet.py] => Task 8, Epoch 54/160 (LR 0.07443) => LSC_loss 1.14, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 68.42, Test_acc 39.46
2025-01-11 05:02:41,773 [podnet.py] => Task 8, Epoch 55/160 (LR 0.07357) => LSC_loss 1.13, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 69.49, Test_acc 36.93
2025-01-11 05:02:49,291 [podnet.py] => Task 8, Epoch 56/160 (LR 0.07270) => LSC_loss 1.13, Spatial_loss 2.03, Flat_loss 0.31, Train_acc 68.58, Test_acc 41.57
2025-01-11 05:02:56,894 [podnet.py] => Task 8, Epoch 57/160 (LR 0.07182) => LSC_loss 1.12, Spatial_loss 2.04, Flat_loss 0.31, Train_acc 69.38, Test_acc 37.49
2025-01-11 05:03:04,788 [podnet.py] => Task 8, Epoch 58/160 (LR 0.07093) => LSC_loss 1.12, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 68.66, Test_acc 37.69
2025-01-11 05:03:12,355 [podnet.py] => Task 8, Epoch 59/160 (LR 0.07004) => LSC_loss 1.11, Spatial_loss 2.05, Flat_loss 0.31, Train_acc 69.17, Test_acc 35.99
2025-01-11 05:03:19,949 [podnet.py] => Task 8, Epoch 60/160 (LR 0.06913) => LSC_loss 1.11, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 69.77, Test_acc 36.94
2025-01-11 05:03:27,481 [podnet.py] => Task 8, Epoch 61/160 (LR 0.06822) => LSC_loss 1.12, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 69.23, Test_acc 39.33
2025-01-11 05:03:34,968 [podnet.py] => Task 8, Epoch 62/160 (LR 0.06731) => LSC_loss 1.08, Spatial_loss 2.01, Flat_loss 0.30, Train_acc 70.72, Test_acc 37.06
2025-01-11 05:03:42,417 [podnet.py] => Task 8, Epoch 63/160 (LR 0.06638) => LSC_loss 1.08, Spatial_loss 2.03, Flat_loss 0.30, Train_acc 70.39, Test_acc 41.12
2025-01-11 05:03:50,037 [podnet.py] => Task 8, Epoch 64/160 (LR 0.06545) => LSC_loss 1.06, Spatial_loss 1.99, Flat_loss 0.30, Train_acc 70.74, Test_acc 40.49
2025-01-11 05:03:57,471 [podnet.py] => Task 8, Epoch 65/160 (LR 0.06451) => LSC_loss 1.07, Spatial_loss 2.01, Flat_loss 0.30, Train_acc 70.40, Test_acc 37.47
2025-01-11 05:04:05,092 [podnet.py] => Task 8, Epoch 66/160 (LR 0.06357) => LSC_loss 1.05, Spatial_loss 1.95, Flat_loss 0.30, Train_acc 71.53, Test_acc 39.22
2025-01-11 05:04:12,540 [podnet.py] => Task 8, Epoch 67/160 (LR 0.06262) => LSC_loss 1.06, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 70.81, Test_acc 36.37
2025-01-11 05:04:20,330 [podnet.py] => Task 8, Epoch 68/160 (LR 0.06167) => LSC_loss 1.05, Spatial_loss 1.99, Flat_loss 0.30, Train_acc 71.08, Test_acc 38.67
2025-01-11 05:04:28,109 [podnet.py] => Task 8, Epoch 69/160 (LR 0.06072) => LSC_loss 1.04, Spatial_loss 1.99, Flat_loss 0.30, Train_acc 71.37, Test_acc 40.90
2025-01-11 05:04:35,725 [podnet.py] => Task 8, Epoch 70/160 (LR 0.05975) => LSC_loss 1.01, Spatial_loss 1.97, Flat_loss 0.29, Train_acc 72.12, Test_acc 36.09
2025-01-11 05:04:43,740 [podnet.py] => Task 8, Epoch 71/160 (LR 0.05879) => LSC_loss 1.03, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 71.90, Test_acc 38.73
2025-01-11 05:04:51,859 [podnet.py] => Task 8, Epoch 72/160 (LR 0.05782) => LSC_loss 1.04, Spatial_loss 1.99, Flat_loss 0.30, Train_acc 71.33, Test_acc 38.07
2025-01-11 05:04:59,563 [podnet.py] => Task 8, Epoch 73/160 (LR 0.05685) => LSC_loss 1.01, Spatial_loss 1.96, Flat_loss 0.29, Train_acc 72.09, Test_acc 38.47
2025-01-11 05:05:07,416 [podnet.py] => Task 8, Epoch 74/160 (LR 0.05588) => LSC_loss 1.02, Spatial_loss 1.98, Flat_loss 0.29, Train_acc 72.29, Test_acc 33.44
2025-01-11 05:05:14,915 [podnet.py] => Task 8, Epoch 75/160 (LR 0.05490) => LSC_loss 0.98, Spatial_loss 1.94, Flat_loss 0.29, Train_acc 73.49, Test_acc 39.30
2025-01-11 05:05:22,628 [podnet.py] => Task 8, Epoch 76/160 (LR 0.05392) => LSC_loss 0.98, Spatial_loss 1.90, Flat_loss 0.29, Train_acc 73.11, Test_acc 37.98
2025-01-11 05:05:30,299 [podnet.py] => Task 8, Epoch 77/160 (LR 0.05294) => LSC_loss 0.96, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 74.05, Test_acc 38.38
2025-01-11 05:05:38,027 [podnet.py] => Task 8, Epoch 78/160 (LR 0.05196) => LSC_loss 0.97, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 73.06, Test_acc 36.48
2025-01-11 05:05:45,723 [podnet.py] => Task 8, Epoch 79/160 (LR 0.05098) => LSC_loss 0.95, Spatial_loss 1.87, Flat_loss 0.28, Train_acc 73.95, Test_acc 39.24
2025-01-11 05:05:53,410 [podnet.py] => Task 8, Epoch 80/160 (LR 0.05000) => LSC_loss 0.96, Spatial_loss 1.88, Flat_loss 0.28, Train_acc 73.57, Test_acc 38.84
2025-01-11 05:06:01,022 [podnet.py] => Task 8, Epoch 81/160 (LR 0.04902) => LSC_loss 0.94, Spatial_loss 1.87, Flat_loss 0.28, Train_acc 74.58, Test_acc 38.18
2025-01-11 05:06:08,575 [podnet.py] => Task 8, Epoch 82/160 (LR 0.04804) => LSC_loss 0.96, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 73.53, Test_acc 40.88
2025-01-11 05:06:15,968 [podnet.py] => Task 8, Epoch 83/160 (LR 0.04706) => LSC_loss 0.93, Spatial_loss 1.88, Flat_loss 0.28, Train_acc 74.24, Test_acc 39.29
2025-01-11 05:06:23,400 [podnet.py] => Task 8, Epoch 84/160 (LR 0.04608) => LSC_loss 0.91, Spatial_loss 1.83, Flat_loss 0.27, Train_acc 75.02, Test_acc 41.20
2025-01-11 05:06:31,161 [podnet.py] => Task 8, Epoch 85/160 (LR 0.04510) => LSC_loss 0.90, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 75.35, Test_acc 40.47
2025-01-11 05:06:38,778 [podnet.py] => Task 8, Epoch 86/160 (LR 0.04412) => LSC_loss 0.90, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 74.98, Test_acc 40.50
2025-01-11 05:06:46,315 [podnet.py] => Task 8, Epoch 87/160 (LR 0.04315) => LSC_loss 0.88, Spatial_loss 1.80, Flat_loss 0.27, Train_acc 75.86, Test_acc 40.38
2025-01-11 05:06:54,004 [podnet.py] => Task 8, Epoch 88/160 (LR 0.04218) => LSC_loss 0.87, Spatial_loss 1.80, Flat_loss 0.26, Train_acc 76.58, Test_acc 43.29
2025-01-11 05:07:01,559 [podnet.py] => Task 8, Epoch 89/160 (LR 0.04121) => LSC_loss 0.89, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 75.61, Test_acc 42.84
2025-01-11 05:07:09,095 [podnet.py] => Task 8, Epoch 90/160 (LR 0.04025) => LSC_loss 0.86, Spatial_loss 1.79, Flat_loss 0.26, Train_acc 76.38, Test_acc 41.13
2025-01-11 05:07:16,651 [podnet.py] => Task 8, Epoch 91/160 (LR 0.03928) => LSC_loss 0.84, Spatial_loss 1.76, Flat_loss 0.26, Train_acc 76.98, Test_acc 40.79
2025-01-11 05:07:24,342 [podnet.py] => Task 8, Epoch 92/160 (LR 0.03833) => LSC_loss 0.85, Spatial_loss 1.76, Flat_loss 0.26, Train_acc 76.70, Test_acc 41.36
2025-01-11 05:07:31,938 [podnet.py] => Task 8, Epoch 93/160 (LR 0.03738) => LSC_loss 0.82, Spatial_loss 1.72, Flat_loss 0.25, Train_acc 77.80, Test_acc 42.42
2025-01-11 05:07:39,507 [podnet.py] => Task 8, Epoch 94/160 (LR 0.03643) => LSC_loss 0.81, Spatial_loss 1.72, Flat_loss 0.26, Train_acc 78.20, Test_acc 42.82
2025-01-11 05:07:47,188 [podnet.py] => Task 8, Epoch 95/160 (LR 0.03549) => LSC_loss 0.82, Spatial_loss 1.73, Flat_loss 0.25, Train_acc 77.78, Test_acc 40.83
2025-01-11 05:07:54,835 [podnet.py] => Task 8, Epoch 96/160 (LR 0.03455) => LSC_loss 0.80, Spatial_loss 1.70, Flat_loss 0.25, Train_acc 78.30, Test_acc 40.39
2025-01-11 05:08:02,347 [podnet.py] => Task 8, Epoch 97/160 (LR 0.03362) => LSC_loss 0.79, Spatial_loss 1.69, Flat_loss 0.24, Train_acc 78.90, Test_acc 43.60
2025-01-11 05:08:09,811 [podnet.py] => Task 8, Epoch 98/160 (LR 0.03269) => LSC_loss 0.77, Spatial_loss 1.66, Flat_loss 0.24, Train_acc 79.18, Test_acc 43.19
2025-01-11 05:08:17,291 [podnet.py] => Task 8, Epoch 99/160 (LR 0.03178) => LSC_loss 0.78, Spatial_loss 1.65, Flat_loss 0.24, Train_acc 78.83, Test_acc 41.37
2025-01-11 05:08:25,512 [podnet.py] => Task 8, Epoch 100/160 (LR 0.03087) => LSC_loss 0.76, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 80.28, Test_acc 41.56
2025-01-11 05:08:33,174 [podnet.py] => Task 8, Epoch 101/160 (LR 0.02996) => LSC_loss 0.74, Spatial_loss 1.62, Flat_loss 0.24, Train_acc 80.45, Test_acc 42.83
2025-01-11 05:08:40,626 [podnet.py] => Task 8, Epoch 102/160 (LR 0.02907) => LSC_loss 0.74, Spatial_loss 1.61, Flat_loss 0.24, Train_acc 80.55, Test_acc 44.42
2025-01-11 05:08:48,138 [podnet.py] => Task 8, Epoch 103/160 (LR 0.02818) => LSC_loss 0.74, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 80.13, Test_acc 42.16
2025-01-11 05:08:55,821 [podnet.py] => Task 8, Epoch 104/160 (LR 0.02730) => LSC_loss 0.73, Spatial_loss 1.60, Flat_loss 0.23, Train_acc 80.39, Test_acc 42.82
2025-01-11 05:09:03,785 [podnet.py] => Task 8, Epoch 105/160 (LR 0.02643) => LSC_loss 0.70, Spatial_loss 1.56, Flat_loss 0.23, Train_acc 81.89, Test_acc 41.97
2025-01-11 05:09:11,531 [podnet.py] => Task 8, Epoch 106/160 (LR 0.02557) => LSC_loss 0.70, Spatial_loss 1.57, Flat_loss 0.23, Train_acc 81.05, Test_acc 44.14
2025-01-11 05:09:19,080 [podnet.py] => Task 8, Epoch 107/160 (LR 0.02472) => LSC_loss 0.70, Spatial_loss 1.57, Flat_loss 0.23, Train_acc 81.30, Test_acc 44.00
2025-01-11 05:09:26,724 [podnet.py] => Task 8, Epoch 108/160 (LR 0.02388) => LSC_loss 0.68, Spatial_loss 1.54, Flat_loss 0.22, Train_acc 82.20, Test_acc 40.40
2025-01-11 05:09:34,240 [podnet.py] => Task 8, Epoch 109/160 (LR 0.02304) => LSC_loss 0.68, Spatial_loss 1.53, Flat_loss 0.22, Train_acc 81.96, Test_acc 43.99
2025-01-11 05:09:41,966 [podnet.py] => Task 8, Epoch 110/160 (LR 0.02222) => LSC_loss 0.69, Spatial_loss 1.53, Flat_loss 0.22, Train_acc 81.70, Test_acc 43.07
2025-01-11 05:09:49,476 [podnet.py] => Task 8, Epoch 111/160 (LR 0.02141) => LSC_loss 0.66, Spatial_loss 1.53, Flat_loss 0.22, Train_acc 82.56, Test_acc 41.94
2025-01-11 05:09:56,941 [podnet.py] => Task 8, Epoch 112/160 (LR 0.02061) => LSC_loss 0.64, Spatial_loss 1.47, Flat_loss 0.21, Train_acc 83.50, Test_acc 44.80
2025-01-11 05:10:04,521 [podnet.py] => Task 8, Epoch 113/160 (LR 0.01982) => LSC_loss 0.64, Spatial_loss 1.45, Flat_loss 0.21, Train_acc 83.51, Test_acc 44.23
2025-01-11 05:10:12,096 [podnet.py] => Task 8, Epoch 114/160 (LR 0.01905) => LSC_loss 0.63, Spatial_loss 1.45, Flat_loss 0.21, Train_acc 83.50, Test_acc 44.53
2025-01-11 05:10:19,641 [podnet.py] => Task 8, Epoch 115/160 (LR 0.01828) => LSC_loss 0.62, Spatial_loss 1.44, Flat_loss 0.21, Train_acc 83.81, Test_acc 44.72
2025-01-11 05:10:27,211 [podnet.py] => Task 8, Epoch 116/160 (LR 0.01753) => LSC_loss 0.62, Spatial_loss 1.44, Flat_loss 0.21, Train_acc 84.11, Test_acc 45.97
2025-01-11 05:10:34,909 [podnet.py] => Task 8, Epoch 117/160 (LR 0.01679) => LSC_loss 0.60, Spatial_loss 1.41, Flat_loss 0.20, Train_acc 85.12, Test_acc 46.17
2025-01-11 05:10:42,654 [podnet.py] => Task 8, Epoch 118/160 (LR 0.01606) => LSC_loss 0.61, Spatial_loss 1.40, Flat_loss 0.20, Train_acc 84.43, Test_acc 43.97
2025-01-11 05:10:50,005 [podnet.py] => Task 8, Epoch 119/160 (LR 0.01535) => LSC_loss 0.58, Spatial_loss 1.36, Flat_loss 0.20, Train_acc 85.68, Test_acc 44.87
2025-01-11 05:10:57,745 [podnet.py] => Task 8, Epoch 120/160 (LR 0.01464) => LSC_loss 0.57, Spatial_loss 1.37, Flat_loss 0.20, Train_acc 85.67, Test_acc 45.46
2025-01-11 05:11:05,346 [podnet.py] => Task 8, Epoch 121/160 (LR 0.01396) => LSC_loss 0.58, Spatial_loss 1.35, Flat_loss 0.20, Train_acc 85.74, Test_acc 46.53
2025-01-11 05:11:13,090 [podnet.py] => Task 8, Epoch 122/160 (LR 0.01328) => LSC_loss 0.56, Spatial_loss 1.34, Flat_loss 0.19, Train_acc 85.82, Test_acc 46.26
2025-01-11 05:11:20,661 [podnet.py] => Task 8, Epoch 123/160 (LR 0.01262) => LSC_loss 0.55, Spatial_loss 1.33, Flat_loss 0.19, Train_acc 86.44, Test_acc 45.47
2025-01-11 05:11:28,239 [podnet.py] => Task 8, Epoch 124/160 (LR 0.01198) => LSC_loss 0.54, Spatial_loss 1.32, Flat_loss 0.19, Train_acc 86.97, Test_acc 44.68
2025-01-11 05:11:35,730 [podnet.py] => Task 8, Epoch 125/160 (LR 0.01135) => LSC_loss 0.54, Spatial_loss 1.34, Flat_loss 0.19, Train_acc 86.39, Test_acc 45.19
2025-01-11 05:11:43,456 [podnet.py] => Task 8, Epoch 126/160 (LR 0.01073) => LSC_loss 0.52, Spatial_loss 1.30, Flat_loss 0.19, Train_acc 87.10, Test_acc 44.51
2025-01-11 05:11:51,112 [podnet.py] => Task 8, Epoch 127/160 (LR 0.01013) => LSC_loss 0.53, Spatial_loss 1.28, Flat_loss 0.18, Train_acc 87.24, Test_acc 45.46
2025-01-11 05:11:58,643 [podnet.py] => Task 8, Epoch 128/160 (LR 0.00955) => LSC_loss 0.51, Spatial_loss 1.26, Flat_loss 0.18, Train_acc 87.81, Test_acc 45.09
2025-01-11 05:12:06,463 [podnet.py] => Task 8, Epoch 129/160 (LR 0.00898) => LSC_loss 0.52, Spatial_loss 1.26, Flat_loss 0.18, Train_acc 87.66, Test_acc 46.09
2025-01-11 05:12:14,184 [podnet.py] => Task 8, Epoch 130/160 (LR 0.00843) => LSC_loss 0.51, Spatial_loss 1.23, Flat_loss 0.18, Train_acc 87.70, Test_acc 46.13
2025-01-11 05:12:21,654 [podnet.py] => Task 8, Epoch 131/160 (LR 0.00789) => LSC_loss 0.50, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 88.11, Test_acc 46.77
2025-01-11 05:12:29,434 [podnet.py] => Task 8, Epoch 132/160 (LR 0.00737) => LSC_loss 0.48, Spatial_loss 1.21, Flat_loss 0.17, Train_acc 88.62, Test_acc 46.34
2025-01-11 05:12:36,904 [podnet.py] => Task 8, Epoch 133/160 (LR 0.00686) => LSC_loss 0.50, Spatial_loss 1.21, Flat_loss 0.17, Train_acc 88.06, Test_acc 46.76
2025-01-11 05:12:44,533 [podnet.py] => Task 8, Epoch 134/160 (LR 0.00638) => LSC_loss 0.48, Spatial_loss 1.19, Flat_loss 0.17, Train_acc 88.75, Test_acc 46.17
2025-01-11 05:12:52,029 [podnet.py] => Task 8, Epoch 135/160 (LR 0.00590) => LSC_loss 0.48, Spatial_loss 1.18, Flat_loss 0.17, Train_acc 89.07, Test_acc 46.53
2025-01-11 05:12:59,579 [podnet.py] => Task 8, Epoch 136/160 (LR 0.00545) => LSC_loss 0.47, Spatial_loss 1.17, Flat_loss 0.17, Train_acc 89.30, Test_acc 46.67
2025-01-11 05:13:07,226 [podnet.py] => Task 8, Epoch 137/160 (LR 0.00501) => LSC_loss 0.46, Spatial_loss 1.16, Flat_loss 0.17, Train_acc 89.56, Test_acc 46.12
2025-01-11 05:13:14,931 [podnet.py] => Task 8, Epoch 138/160 (LR 0.00459) => LSC_loss 0.46, Spatial_loss 1.17, Flat_loss 0.17, Train_acc 89.54, Test_acc 46.67
2025-01-11 05:13:22,490 [podnet.py] => Task 8, Epoch 139/160 (LR 0.00419) => LSC_loss 0.46, Spatial_loss 1.14, Flat_loss 0.17, Train_acc 89.60, Test_acc 47.34
2025-01-11 05:13:30,014 [podnet.py] => Task 8, Epoch 140/160 (LR 0.00381) => LSC_loss 0.45, Spatial_loss 1.14, Flat_loss 0.16, Train_acc 89.97, Test_acc 47.01
2025-01-11 05:13:37,732 [podnet.py] => Task 8, Epoch 141/160 (LR 0.00344) => LSC_loss 0.45, Spatial_loss 1.12, Flat_loss 0.16, Train_acc 90.08, Test_acc 46.89
2025-01-11 05:13:45,306 [podnet.py] => Task 8, Epoch 142/160 (LR 0.00309) => LSC_loss 0.44, Spatial_loss 1.12, Flat_loss 0.16, Train_acc 90.16, Test_acc 45.88
2025-01-11 05:13:52,784 [podnet.py] => Task 8, Epoch 143/160 (LR 0.00276) => LSC_loss 0.44, Spatial_loss 1.10, Flat_loss 0.16, Train_acc 90.61, Test_acc 47.36
2025-01-11 05:14:00,338 [podnet.py] => Task 8, Epoch 144/160 (LR 0.00245) => LSC_loss 0.44, Spatial_loss 1.08, Flat_loss 0.16, Train_acc 90.44, Test_acc 47.00
2025-01-11 05:14:08,054 [podnet.py] => Task 8, Epoch 145/160 (LR 0.00215) => LSC_loss 0.44, Spatial_loss 1.08, Flat_loss 0.16, Train_acc 90.61, Test_acc 46.99
2025-01-11 05:14:15,832 [podnet.py] => Task 8, Epoch 146/160 (LR 0.00188) => LSC_loss 0.43, Spatial_loss 1.08, Flat_loss 0.16, Train_acc 90.19, Test_acc 47.37
2025-01-11 05:14:23,423 [podnet.py] => Task 8, Epoch 147/160 (LR 0.00162) => LSC_loss 0.44, Spatial_loss 1.08, Flat_loss 0.16, Train_acc 90.45, Test_acc 47.10
2025-01-11 05:14:30,950 [podnet.py] => Task 8, Epoch 148/160 (LR 0.00138) => LSC_loss 0.43, Spatial_loss 1.07, Flat_loss 0.16, Train_acc 90.70, Test_acc 47.42
2025-01-11 05:14:38,332 [podnet.py] => Task 8, Epoch 149/160 (LR 0.00116) => LSC_loss 0.43, Spatial_loss 1.05, Flat_loss 0.16, Train_acc 90.95, Test_acc 47.08
2025-01-11 05:14:46,075 [podnet.py] => Task 8, Epoch 150/160 (LR 0.00096) => LSC_loss 0.42, Spatial_loss 1.07, Flat_loss 0.16, Train_acc 90.80, Test_acc 47.21
2025-01-11 05:14:53,613 [podnet.py] => Task 8, Epoch 151/160 (LR 0.00078) => LSC_loss 0.42, Spatial_loss 1.06, Flat_loss 0.16, Train_acc 90.99, Test_acc 47.53
2025-01-11 05:15:01,115 [podnet.py] => Task 8, Epoch 152/160 (LR 0.00062) => LSC_loss 0.42, Spatial_loss 1.05, Flat_loss 0.15, Train_acc 91.11, Test_acc 47.26
2025-01-11 05:15:08,650 [podnet.py] => Task 8, Epoch 153/160 (LR 0.00047) => LSC_loss 0.42, Spatial_loss 1.05, Flat_loss 0.16, Train_acc 91.00, Test_acc 47.58
2025-01-11 05:15:16,182 [podnet.py] => Task 8, Epoch 154/160 (LR 0.00035) => LSC_loss 0.42, Spatial_loss 1.03, Flat_loss 0.15, Train_acc 90.97, Test_acc 47.19
2025-01-11 05:15:23,674 [podnet.py] => Task 8, Epoch 155/160 (LR 0.00024) => LSC_loss 0.42, Spatial_loss 1.05, Flat_loss 0.15, Train_acc 90.67, Test_acc 47.43
2025-01-11 05:15:31,165 [podnet.py] => Task 8, Epoch 156/160 (LR 0.00015) => LSC_loss 0.42, Spatial_loss 1.04, Flat_loss 0.15, Train_acc 90.91, Test_acc 47.32
2025-01-11 05:15:38,608 [podnet.py] => Task 8, Epoch 157/160 (LR 0.00009) => LSC_loss 0.42, Spatial_loss 1.04, Flat_loss 0.16, Train_acc 90.78, Test_acc 47.58
2025-01-11 05:15:46,133 [podnet.py] => Task 8, Epoch 158/160 (LR 0.00004) => LSC_loss 0.41, Spatial_loss 1.04, Flat_loss 0.15, Train_acc 91.41, Test_acc 47.48
2025-01-11 05:15:53,693 [podnet.py] => Task 8, Epoch 159/160 (LR 0.00001) => LSC_loss 0.41, Spatial_loss 1.03, Flat_loss 0.15, Train_acc 91.21, Test_acc 47.31
2025-01-11 05:16:01,349 [podnet.py] => Task 8, Epoch 160/160 (LR 0.00000) => LSC_loss 0.42, Spatial_loss 1.04, Flat_loss 0.15, Train_acc 90.95, Test_acc 47.69
2025-01-11 05:16:01,350 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 05:16:01,350 [base.py] => Reducing exemplars...(92 per classes)
2025-01-11 05:16:50,331 [base.py] => Constructing exemplars...(92 per classes)
2025-01-11 05:17:03,612 [podnet.py] => The size of finetune dataset: 8280
2025-01-11 05:17:09,356 [podnet.py] => Task 8, Epoch 1/20 (LR 0.00497) => LSC_loss 0.37, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 91.67, Test_acc 48.44
2025-01-11 05:17:14,988 [podnet.py] => Task 8, Epoch 2/20 (LR 0.00488) => LSC_loss 0.36, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 92.25, Test_acc 48.70
2025-01-11 05:17:20,859 [podnet.py] => Task 8, Epoch 3/20 (LR 0.00473) => LSC_loss 0.37, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 91.63, Test_acc 48.32
2025-01-11 05:17:26,564 [podnet.py] => Task 8, Epoch 4/20 (LR 0.00452) => LSC_loss 0.36, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 92.19, Test_acc 47.74
2025-01-11 05:17:32,196 [podnet.py] => Task 8, Epoch 5/20 (LR 0.00427) => LSC_loss 0.36, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 91.88, Test_acc 47.78
2025-01-11 05:17:37,989 [podnet.py] => Task 8, Epoch 6/20 (LR 0.00397) => LSC_loss 0.35, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 92.40, Test_acc 48.92
2025-01-11 05:17:43,595 [podnet.py] => Task 8, Epoch 7/20 (LR 0.00363) => LSC_loss 0.34, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 92.79, Test_acc 49.07
2025-01-11 05:17:49,215 [podnet.py] => Task 8, Epoch 8/20 (LR 0.00327) => LSC_loss 0.34, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 92.67, Test_acc 48.62
2025-01-11 05:17:54,752 [podnet.py] => Task 8, Epoch 9/20 (LR 0.00289) => LSC_loss 0.34, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 92.92, Test_acc 48.87
2025-01-11 05:18:00,450 [podnet.py] => Task 8, Epoch 10/20 (LR 0.00250) => LSC_loss 0.34, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 92.95, Test_acc 48.23
2025-01-11 05:18:06,261 [podnet.py] => Task 8, Epoch 11/20 (LR 0.00211) => LSC_loss 0.34, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 93.16, Test_acc 48.96
2025-01-11 05:18:11,999 [podnet.py] => Task 8, Epoch 12/20 (LR 0.00173) => LSC_loss 0.34, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 92.83, Test_acc 48.47
2025-01-11 05:18:17,687 [podnet.py] => Task 8, Epoch 13/20 (LR 0.00137) => LSC_loss 0.33, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 93.12, Test_acc 48.40
2025-01-11 05:18:23,486 [podnet.py] => Task 8, Epoch 14/20 (LR 0.00103) => LSC_loss 0.33, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 93.47, Test_acc 48.87
2025-01-11 05:18:29,379 [podnet.py] => Task 8, Epoch 15/20 (LR 0.00073) => LSC_loss 0.32, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 93.47, Test_acc 48.84
2025-01-11 05:18:35,294 [podnet.py] => Task 8, Epoch 16/20 (LR 0.00048) => LSC_loss 0.32, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 93.74, Test_acc 49.07
2025-01-11 05:18:41,062 [podnet.py] => Task 8, Epoch 17/20 (LR 0.00027) => LSC_loss 0.32, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 93.76, Test_acc 49.17
2025-01-11 05:18:46,882 [podnet.py] => Task 8, Epoch 18/20 (LR 0.00012) => LSC_loss 0.32, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 93.72, Test_acc 49.22
2025-01-11 05:18:52,815 [podnet.py] => Task 8, Epoch 19/20 (LR 0.00003) => LSC_loss 0.32, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 93.51, Test_acc 49.26
2025-01-11 05:18:58,853 [podnet.py] => Task 8, Epoch 20/20 (LR 0.00000) => LSC_loss 0.32, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 93.57, Test_acc 49.08
2025-01-11 05:18:58,855 [base.py] => Reducing exemplars...(82 per classes)
2025-01-11 05:19:47,766 [base.py] => Constructing exemplars...(82 per classes)
2025-01-11 05:20:04,249 [podnet.py] => Exemplar size: 7380
2025-01-11 05:20:04,249 [trainer.py] => CNN: {'total': np.float64(49.08), '00-09': np.float64(63.8), '10-19': np.float64(35.0), '20-29': np.float64(50.5), '30-39': np.float64(41.9), '40-49': np.float64(52.8), '50-59': np.float64(43.1), '60-69': np.float64(51.5), '70-79': np.float64(48.9), '80-89': np.float64(54.2), 'old': np.float64(48.44), 'new': np.float64(54.2)}
2025-01-11 05:20:04,249 [trainer.py] => NME: {'total': np.float64(47.64), '00-09': np.float64(66.9), '10-19': np.float64(33.3), '20-29': np.float64(50.5), '30-39': np.float64(41.4), '40-49': np.float64(50.9), '50-59': np.float64(40.4), '60-69': np.float64(50.3), '70-79': np.float64(43.5), '80-89': np.float64(51.6), 'old': np.float64(47.15), 'new': np.float64(51.6)}
2025-01-11 05:20:04,249 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42), np.float64(60.28), np.float64(56.65), np.float64(54.8), np.float64(51.38), np.float64(49.08)]
2025-01-11 05:20:04,249 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82), np.float64(85.98), np.float64(83.17), np.float64(81.17), np.float64(79.08), np.float64(76.79)]
2025-01-11 05:20:04,249 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38), np.float64(59.32), np.float64(55.12), np.float64(53.11), np.float64(49.86), np.float64(47.64)]
2025-01-11 05:20:04,250 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02), np.float64(85.44), np.float64(82.3), np.float64(80.14), np.float64(78.16), np.float64(75.96)]

2025-01-11 05:20:04,250 [trainer.py] => All params: 523857
2025-01-11 05:20:04,250 [trainer.py] => Trainable params: 523857
2025-01-11 05:20:04,251 [podnet.py] => Learning on 90-100
2025-01-11 05:20:04,376 [podnet.py] => Adaptive factor: 3.1622776601683795
2025-01-11 05:20:12,033 [podnet.py] => Task 9, Epoch 1/160 (LR 0.09999) => LSC_loss 2.74, Spatial_loss 3.32, Flat_loss 0.89, Train_acc 37.68, Test_acc 31.30
2025-01-11 05:20:19,437 [podnet.py] => Task 9, Epoch 2/160 (LR 0.09996) => LSC_loss 1.95, Spatial_loss 2.83, Flat_loss 0.56, Train_acc 49.11, Test_acc 26.33
2025-01-11 05:20:26,927 [podnet.py] => Task 9, Epoch 3/160 (LR 0.09991) => LSC_loss 1.79, Spatial_loss 2.67, Flat_loss 0.48, Train_acc 52.84, Test_acc 32.37
2025-01-11 05:20:34,315 [podnet.py] => Task 9, Epoch 4/160 (LR 0.09985) => LSC_loss 1.72, Spatial_loss 2.61, Flat_loss 0.46, Train_acc 54.67, Test_acc 34.33
2025-01-11 05:20:42,034 [podnet.py] => Task 9, Epoch 5/160 (LR 0.09976) => LSC_loss 1.63, Spatial_loss 2.55, Flat_loss 0.43, Train_acc 56.41, Test_acc 36.20
2025-01-11 05:20:49,541 [podnet.py] => Task 9, Epoch 6/160 (LR 0.09965) => LSC_loss 1.65, Spatial_loss 2.59, Flat_loss 0.43, Train_acc 56.13, Test_acc 34.48
2025-01-11 05:20:57,144 [podnet.py] => Task 9, Epoch 7/160 (LR 0.09953) => LSC_loss 1.57, Spatial_loss 2.48, Flat_loss 0.42, Train_acc 58.34, Test_acc 31.10
2025-01-11 05:21:04,913 [podnet.py] => Task 9, Epoch 8/160 (LR 0.09938) => LSC_loss 1.56, Spatial_loss 2.41, Flat_loss 0.41, Train_acc 58.20, Test_acc 36.34
2025-01-11 05:21:12,678 [podnet.py] => Task 9, Epoch 9/160 (LR 0.09922) => LSC_loss 1.53, Spatial_loss 2.42, Flat_loss 0.41, Train_acc 58.98, Test_acc 31.51
2025-01-11 05:21:20,557 [podnet.py] => Task 9, Epoch 10/160 (LR 0.09904) => LSC_loss 1.52, Spatial_loss 2.41, Flat_loss 0.40, Train_acc 59.94, Test_acc 35.16
2025-01-11 05:21:28,398 [podnet.py] => Task 9, Epoch 11/160 (LR 0.09884) => LSC_loss 1.48, Spatial_loss 2.40, Flat_loss 0.40, Train_acc 60.49, Test_acc 38.52
2025-01-11 05:21:36,061 [podnet.py] => Task 9, Epoch 12/160 (LR 0.09862) => LSC_loss 1.52, Spatial_loss 2.46, Flat_loss 0.41, Train_acc 58.84, Test_acc 35.27
2025-01-11 05:21:43,751 [podnet.py] => Task 9, Epoch 13/160 (LR 0.09838) => LSC_loss 1.48, Spatial_loss 2.39, Flat_loss 0.40, Train_acc 60.95, Test_acc 35.61
2025-01-11 05:21:51,386 [podnet.py] => Task 9, Epoch 14/160 (LR 0.09812) => LSC_loss 1.45, Spatial_loss 2.38, Flat_loss 0.40, Train_acc 61.29, Test_acc 33.78
2025-01-11 05:21:59,184 [podnet.py] => Task 9, Epoch 15/160 (LR 0.09785) => LSC_loss 1.42, Spatial_loss 2.36, Flat_loss 0.40, Train_acc 61.60, Test_acc 36.62
2025-01-11 05:22:06,928 [podnet.py] => Task 9, Epoch 16/160 (LR 0.09755) => LSC_loss 1.43, Spatial_loss 2.36, Flat_loss 0.40, Train_acc 61.96, Test_acc 33.30
2025-01-11 05:22:14,607 [podnet.py] => Task 9, Epoch 17/160 (LR 0.09724) => LSC_loss 1.43, Spatial_loss 2.37, Flat_loss 0.40, Train_acc 61.83, Test_acc 34.98
2025-01-11 05:22:22,135 [podnet.py] => Task 9, Epoch 18/160 (LR 0.09691) => LSC_loss 1.43, Spatial_loss 2.37, Flat_loss 0.40, Train_acc 61.50, Test_acc 34.92
2025-01-11 05:22:29,777 [podnet.py] => Task 9, Epoch 19/160 (LR 0.09656) => LSC_loss 1.42, Spatial_loss 2.38, Flat_loss 0.40, Train_acc 61.91, Test_acc 31.81
2025-01-11 05:22:37,336 [podnet.py] => Task 9, Epoch 20/160 (LR 0.09619) => LSC_loss 1.39, Spatial_loss 2.35, Flat_loss 0.39, Train_acc 62.76, Test_acc 32.60
2025-01-11 05:22:45,094 [podnet.py] => Task 9, Epoch 21/160 (LR 0.09581) => LSC_loss 1.38, Spatial_loss 2.35, Flat_loss 0.39, Train_acc 62.79, Test_acc 33.69
2025-01-11 05:22:52,695 [podnet.py] => Task 9, Epoch 22/160 (LR 0.09541) => LSC_loss 1.38, Spatial_loss 2.35, Flat_loss 0.39, Train_acc 62.40, Test_acc 33.62
2025-01-11 05:23:00,444 [podnet.py] => Task 9, Epoch 23/160 (LR 0.09499) => LSC_loss 1.39, Spatial_loss 2.37, Flat_loss 0.40, Train_acc 62.23, Test_acc 34.74
2025-01-11 05:23:08,516 [podnet.py] => Task 9, Epoch 24/160 (LR 0.09455) => LSC_loss 1.35, Spatial_loss 2.36, Flat_loss 0.39, Train_acc 63.40, Test_acc 40.38
2025-01-11 05:23:16,426 [podnet.py] => Task 9, Epoch 25/160 (LR 0.09410) => LSC_loss 1.35, Spatial_loss 2.33, Flat_loss 0.39, Train_acc 63.51, Test_acc 36.72
2025-01-11 05:23:24,022 [podnet.py] => Task 9, Epoch 26/160 (LR 0.09362) => LSC_loss 1.36, Spatial_loss 2.33, Flat_loss 0.39, Train_acc 63.13, Test_acc 28.70
2025-01-11 05:23:31,717 [podnet.py] => Task 9, Epoch 27/160 (LR 0.09314) => LSC_loss 1.36, Spatial_loss 2.38, Flat_loss 0.40, Train_acc 63.51, Test_acc 32.37
2025-01-11 05:23:39,406 [podnet.py] => Task 9, Epoch 28/160 (LR 0.09263) => LSC_loss 1.36, Spatial_loss 2.37, Flat_loss 0.39, Train_acc 63.25, Test_acc 35.92
2025-01-11 05:23:46,923 [podnet.py] => Task 9, Epoch 29/160 (LR 0.09211) => LSC_loss 1.31, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 64.83, Test_acc 38.76
2025-01-11 05:23:54,634 [podnet.py] => Task 9, Epoch 30/160 (LR 0.09157) => LSC_loss 1.34, Spatial_loss 2.36, Flat_loss 0.40, Train_acc 64.10, Test_acc 33.68
2025-01-11 05:24:02,406 [podnet.py] => Task 9, Epoch 31/160 (LR 0.09102) => LSC_loss 1.33, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 64.05, Test_acc 35.86
2025-01-11 05:24:10,323 [podnet.py] => Task 9, Epoch 32/160 (LR 0.09045) => LSC_loss 1.30, Spatial_loss 2.30, Flat_loss 0.39, Train_acc 64.82, Test_acc 30.03
2025-01-11 05:24:17,895 [podnet.py] => Task 9, Epoch 33/160 (LR 0.08987) => LSC_loss 1.33, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 64.18, Test_acc 38.63
2025-01-11 05:24:25,574 [podnet.py] => Task 9, Epoch 34/160 (LR 0.08927) => LSC_loss 1.32, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 64.43, Test_acc 30.69
2025-01-11 05:24:33,329 [podnet.py] => Task 9, Epoch 35/160 (LR 0.08865) => LSC_loss 1.31, Spatial_loss 2.34, Flat_loss 0.39, Train_acc 64.91, Test_acc 37.81
2025-01-11 05:24:41,007 [podnet.py] => Task 9, Epoch 36/160 (LR 0.08802) => LSC_loss 1.31, Spatial_loss 2.31, Flat_loss 0.38, Train_acc 64.81, Test_acc 32.94
2025-01-11 05:24:48,582 [podnet.py] => Task 9, Epoch 37/160 (LR 0.08738) => LSC_loss 1.32, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 64.40, Test_acc 35.45
2025-01-11 05:24:56,157 [podnet.py] => Task 9, Epoch 38/160 (LR 0.08672) => LSC_loss 1.29, Spatial_loss 2.29, Flat_loss 0.38, Train_acc 65.58, Test_acc 34.82
2025-01-11 05:25:04,024 [podnet.py] => Task 9, Epoch 39/160 (LR 0.08604) => LSC_loss 1.28, Spatial_loss 2.29, Flat_loss 0.38, Train_acc 65.19, Test_acc 36.33
2025-01-11 05:25:11,635 [podnet.py] => Task 9, Epoch 40/160 (LR 0.08536) => LSC_loss 1.26, Spatial_loss 2.26, Flat_loss 0.38, Train_acc 65.94, Test_acc 34.55
2025-01-11 05:25:19,207 [podnet.py] => Task 9, Epoch 41/160 (LR 0.08465) => LSC_loss 1.26, Spatial_loss 2.30, Flat_loss 0.38, Train_acc 65.59, Test_acc 35.05
2025-01-11 05:25:26,851 [podnet.py] => Task 9, Epoch 42/160 (LR 0.08394) => LSC_loss 1.27, Spatial_loss 2.27, Flat_loss 0.38, Train_acc 65.72, Test_acc 37.55
2025-01-11 05:25:34,287 [podnet.py] => Task 9, Epoch 43/160 (LR 0.08321) => LSC_loss 1.26, Spatial_loss 2.29, Flat_loss 0.38, Train_acc 65.70, Test_acc 32.26
2025-01-11 05:25:41,894 [podnet.py] => Task 9, Epoch 44/160 (LR 0.08247) => LSC_loss 1.26, Spatial_loss 2.30, Flat_loss 0.38, Train_acc 65.48, Test_acc 35.10
2025-01-11 05:25:49,694 [podnet.py] => Task 9, Epoch 45/160 (LR 0.08172) => LSC_loss 1.25, Spatial_loss 2.23, Flat_loss 0.38, Train_acc 65.89, Test_acc 36.63
2025-01-11 05:25:57,319 [podnet.py] => Task 9, Epoch 46/160 (LR 0.08095) => LSC_loss 1.25, Spatial_loss 2.26, Flat_loss 0.38, Train_acc 66.06, Test_acc 37.88
2025-01-11 05:26:04,900 [podnet.py] => Task 9, Epoch 47/160 (LR 0.08018) => LSC_loss 1.21, Spatial_loss 2.24, Flat_loss 0.37, Train_acc 67.12, Test_acc 38.40
2025-01-11 05:26:12,664 [podnet.py] => Task 9, Epoch 48/160 (LR 0.07939) => LSC_loss 1.25, Spatial_loss 2.25, Flat_loss 0.38, Train_acc 66.19, Test_acc 37.85
2025-01-11 05:26:20,260 [podnet.py] => Task 9, Epoch 49/160 (LR 0.07859) => LSC_loss 1.22, Spatial_loss 2.23, Flat_loss 0.37, Train_acc 67.37, Test_acc 29.84
2025-01-11 05:26:27,894 [podnet.py] => Task 9, Epoch 50/160 (LR 0.07778) => LSC_loss 1.23, Spatial_loss 2.24, Flat_loss 0.37, Train_acc 66.70, Test_acc 39.55
2025-01-11 05:26:35,504 [podnet.py] => Task 9, Epoch 51/160 (LR 0.07696) => LSC_loss 1.20, Spatial_loss 2.23, Flat_loss 0.37, Train_acc 67.82, Test_acc 36.49
2025-01-11 05:26:43,281 [podnet.py] => Task 9, Epoch 52/160 (LR 0.07612) => LSC_loss 1.22, Spatial_loss 2.25, Flat_loss 0.37, Train_acc 66.93, Test_acc 37.03
2025-01-11 05:26:50,958 [podnet.py] => Task 9, Epoch 53/160 (LR 0.07528) => LSC_loss 1.19, Spatial_loss 2.24, Flat_loss 0.37, Train_acc 67.58, Test_acc 36.12
2025-01-11 05:26:58,511 [podnet.py] => Task 9, Epoch 54/160 (LR 0.07443) => LSC_loss 1.20, Spatial_loss 2.25, Flat_loss 0.37, Train_acc 67.55, Test_acc 38.69
2025-01-11 05:27:06,313 [podnet.py] => Task 9, Epoch 55/160 (LR 0.07357) => LSC_loss 1.18, Spatial_loss 2.21, Flat_loss 0.36, Train_acc 67.78, Test_acc 35.15
2025-01-11 05:27:14,114 [podnet.py] => Task 9, Epoch 56/160 (LR 0.07270) => LSC_loss 1.18, Spatial_loss 2.25, Flat_loss 0.37, Train_acc 67.88, Test_acc 35.73
2025-01-11 05:27:21,885 [podnet.py] => Task 9, Epoch 57/160 (LR 0.07182) => LSC_loss 1.19, Spatial_loss 2.24, Flat_loss 0.37, Train_acc 67.42, Test_acc 36.94
2025-01-11 05:27:29,571 [podnet.py] => Task 9, Epoch 58/160 (LR 0.07093) => LSC_loss 1.16, Spatial_loss 2.17, Flat_loss 0.36, Train_acc 68.60, Test_acc 36.69
2025-01-11 05:27:37,129 [podnet.py] => Task 9, Epoch 59/160 (LR 0.07004) => LSC_loss 1.17, Spatial_loss 2.22, Flat_loss 0.36, Train_acc 68.58, Test_acc 36.63
2025-01-11 05:27:44,886 [podnet.py] => Task 9, Epoch 60/160 (LR 0.06913) => LSC_loss 1.16, Spatial_loss 2.20, Flat_loss 0.36, Train_acc 68.81, Test_acc 38.35
2025-01-11 05:27:52,583 [podnet.py] => Task 9, Epoch 61/160 (LR 0.06822) => LSC_loss 1.16, Spatial_loss 2.19, Flat_loss 0.36, Train_acc 68.60, Test_acc 36.13
2025-01-11 05:28:00,040 [podnet.py] => Task 9, Epoch 62/160 (LR 0.06731) => LSC_loss 1.13, Spatial_loss 2.16, Flat_loss 0.36, Train_acc 69.99, Test_acc 37.37
2025-01-11 05:28:07,844 [podnet.py] => Task 9, Epoch 63/160 (LR 0.06638) => LSC_loss 1.15, Spatial_loss 2.21, Flat_loss 0.36, Train_acc 68.87, Test_acc 34.13
2025-01-11 05:28:15,538 [podnet.py] => Task 9, Epoch 64/160 (LR 0.06545) => LSC_loss 1.14, Spatial_loss 2.15, Flat_loss 0.35, Train_acc 69.01, Test_acc 34.74
2025-01-11 05:28:23,048 [podnet.py] => Task 9, Epoch 65/160 (LR 0.06451) => LSC_loss 1.11, Spatial_loss 2.16, Flat_loss 0.35, Train_acc 70.13, Test_acc 38.84
2025-01-11 05:28:30,561 [podnet.py] => Task 9, Epoch 66/160 (LR 0.06357) => LSC_loss 1.12, Spatial_loss 2.16, Flat_loss 0.35, Train_acc 69.68, Test_acc 37.22
2025-01-11 05:28:38,376 [podnet.py] => Task 9, Epoch 67/160 (LR 0.06262) => LSC_loss 1.11, Spatial_loss 2.12, Flat_loss 0.35, Train_acc 69.61, Test_acc 37.78
2025-01-11 05:28:45,942 [podnet.py] => Task 9, Epoch 68/160 (LR 0.06167) => LSC_loss 1.09, Spatial_loss 2.13, Flat_loss 0.35, Train_acc 70.61, Test_acc 35.50
2025-01-11 05:28:53,865 [podnet.py] => Task 9, Epoch 69/160 (LR 0.06072) => LSC_loss 1.08, Spatial_loss 2.11, Flat_loss 0.34, Train_acc 70.78, Test_acc 35.59
2025-01-11 05:29:01,486 [podnet.py] => Task 9, Epoch 70/160 (LR 0.05975) => LSC_loss 1.11, Spatial_loss 2.14, Flat_loss 0.35, Train_acc 69.47, Test_acc 37.40
2025-01-11 05:29:09,188 [podnet.py] => Task 9, Epoch 71/160 (LR 0.05879) => LSC_loss 1.07, Spatial_loss 2.07, Flat_loss 0.34, Train_acc 70.82, Test_acc 37.20
2025-01-11 05:29:16,857 [podnet.py] => Task 9, Epoch 72/160 (LR 0.05782) => LSC_loss 1.06, Spatial_loss 2.07, Flat_loss 0.34, Train_acc 71.35, Test_acc 39.68
2025-01-11 05:29:24,411 [podnet.py] => Task 9, Epoch 73/160 (LR 0.05685) => LSC_loss 1.07, Spatial_loss 2.06, Flat_loss 0.34, Train_acc 70.80, Test_acc 35.30
2025-01-11 05:29:32,155 [podnet.py] => Task 9, Epoch 74/160 (LR 0.05588) => LSC_loss 1.06, Spatial_loss 2.08, Flat_loss 0.34, Train_acc 71.31, Test_acc 39.91
2025-01-11 05:29:39,789 [podnet.py] => Task 9, Epoch 75/160 (LR 0.05490) => LSC_loss 1.05, Spatial_loss 2.07, Flat_loss 0.33, Train_acc 71.56, Test_acc 38.53
2025-01-11 05:29:47,416 [podnet.py] => Task 9, Epoch 76/160 (LR 0.05392) => LSC_loss 1.04, Spatial_loss 2.04, Flat_loss 0.33, Train_acc 71.87, Test_acc 40.30
2025-01-11 05:29:55,153 [podnet.py] => Task 9, Epoch 77/160 (LR 0.05294) => LSC_loss 1.01, Spatial_loss 2.03, Flat_loss 0.33, Train_acc 72.64, Test_acc 39.90
2025-01-11 05:30:03,014 [podnet.py] => Task 9, Epoch 78/160 (LR 0.05196) => LSC_loss 1.04, Spatial_loss 2.05, Flat_loss 0.33, Train_acc 71.87, Test_acc 39.38
2025-01-11 05:30:10,604 [podnet.py] => Task 9, Epoch 79/160 (LR 0.05098) => LSC_loss 1.02, Spatial_loss 2.02, Flat_loss 0.33, Train_acc 72.71, Test_acc 37.17
2025-01-11 05:30:18,239 [podnet.py] => Task 9, Epoch 80/160 (LR 0.05000) => LSC_loss 1.00, Spatial_loss 2.01, Flat_loss 0.33, Train_acc 72.86, Test_acc 41.32
2025-01-11 05:30:25,942 [podnet.py] => Task 9, Epoch 81/160 (LR 0.04902) => LSC_loss 1.00, Spatial_loss 1.99, Flat_loss 0.32, Train_acc 73.06, Test_acc 39.87
2025-01-11 05:30:33,669 [podnet.py] => Task 9, Epoch 82/160 (LR 0.04804) => LSC_loss 0.97, Spatial_loss 1.97, Flat_loss 0.32, Train_acc 73.42, Test_acc 39.99
2025-01-11 05:30:41,191 [podnet.py] => Task 9, Epoch 83/160 (LR 0.04706) => LSC_loss 1.00, Spatial_loss 1.98, Flat_loss 0.32, Train_acc 72.80, Test_acc 38.25
2025-01-11 05:30:48,885 [podnet.py] => Task 9, Epoch 84/160 (LR 0.04608) => LSC_loss 1.00, Spatial_loss 1.98, Flat_loss 0.32, Train_acc 73.13, Test_acc 41.16
2025-01-11 05:30:56,761 [podnet.py] => Task 9, Epoch 85/160 (LR 0.04510) => LSC_loss 0.96, Spatial_loss 1.93, Flat_loss 0.31, Train_acc 74.36, Test_acc 40.02
2025-01-11 05:31:04,276 [podnet.py] => Task 9, Epoch 86/160 (LR 0.04412) => LSC_loss 0.95, Spatial_loss 1.92, Flat_loss 0.31, Train_acc 74.60, Test_acc 40.36
2025-01-11 05:31:11,961 [podnet.py] => Task 9, Epoch 87/160 (LR 0.04315) => LSC_loss 0.94, Spatial_loss 1.90, Flat_loss 0.31, Train_acc 74.67, Test_acc 38.98
2025-01-11 05:31:19,844 [podnet.py] => Task 9, Epoch 88/160 (LR 0.04218) => LSC_loss 0.91, Spatial_loss 1.90, Flat_loss 0.31, Train_acc 75.77, Test_acc 39.70
2025-01-11 05:31:27,548 [podnet.py] => Task 9, Epoch 89/160 (LR 0.04121) => LSC_loss 0.90, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 75.72, Test_acc 39.01
2025-01-11 05:31:35,255 [podnet.py] => Task 9, Epoch 90/160 (LR 0.04025) => LSC_loss 0.92, Spatial_loss 1.91, Flat_loss 0.31, Train_acc 75.12, Test_acc 41.20
2025-01-11 05:31:42,815 [podnet.py] => Task 9, Epoch 91/160 (LR 0.03928) => LSC_loss 0.93, Spatial_loss 1.89, Flat_loss 0.31, Train_acc 75.15, Test_acc 41.08
2025-01-11 05:31:50,586 [podnet.py] => Task 9, Epoch 92/160 (LR 0.03833) => LSC_loss 0.89, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 75.88, Test_acc 39.20
2025-01-11 05:31:58,240 [podnet.py] => Task 9, Epoch 93/160 (LR 0.03738) => LSC_loss 0.88, Spatial_loss 1.86, Flat_loss 0.30, Train_acc 76.14, Test_acc 37.98
2025-01-11 05:32:06,048 [podnet.py] => Task 9, Epoch 94/160 (LR 0.03643) => LSC_loss 0.88, Spatial_loss 1.86, Flat_loss 0.30, Train_acc 76.82, Test_acc 38.29
2025-01-11 05:32:14,316 [podnet.py] => Task 9, Epoch 95/160 (LR 0.03549) => LSC_loss 0.87, Spatial_loss 1.84, Flat_loss 0.30, Train_acc 76.54, Test_acc 37.96
2025-01-11 05:32:22,230 [podnet.py] => Task 9, Epoch 96/160 (LR 0.03455) => LSC_loss 0.87, Spatial_loss 1.84, Flat_loss 0.30, Train_acc 76.41, Test_acc 40.19
2025-01-11 05:32:30,343 [podnet.py] => Task 9, Epoch 97/160 (LR 0.03362) => LSC_loss 0.85, Spatial_loss 1.83, Flat_loss 0.30, Train_acc 77.41, Test_acc 40.80
2025-01-11 05:32:38,301 [podnet.py] => Task 9, Epoch 98/160 (LR 0.03269) => LSC_loss 0.81, Spatial_loss 1.79, Flat_loss 0.29, Train_acc 78.30, Test_acc 39.30
2025-01-11 05:32:45,915 [podnet.py] => Task 9, Epoch 99/160 (LR 0.03178) => LSC_loss 0.81, Spatial_loss 1.78, Flat_loss 0.28, Train_acc 78.48, Test_acc 41.29
2025-01-11 05:32:53,491 [podnet.py] => Task 9, Epoch 100/160 (LR 0.03087) => LSC_loss 0.82, Spatial_loss 1.77, Flat_loss 0.28, Train_acc 78.00, Test_acc 41.17
2025-01-11 05:33:00,931 [podnet.py] => Task 9, Epoch 101/160 (LR 0.02996) => LSC_loss 0.81, Spatial_loss 1.78, Flat_loss 0.28, Train_acc 78.69, Test_acc 40.07
2025-01-11 05:33:08,589 [podnet.py] => Task 9, Epoch 102/160 (LR 0.02907) => LSC_loss 0.77, Spatial_loss 1.72, Flat_loss 0.28, Train_acc 79.75, Test_acc 38.89
2025-01-11 05:33:16,374 [podnet.py] => Task 9, Epoch 103/160 (LR 0.02818) => LSC_loss 0.78, Spatial_loss 1.72, Flat_loss 0.27, Train_acc 79.23, Test_acc 40.12
2025-01-11 05:33:24,029 [podnet.py] => Task 9, Epoch 104/160 (LR 0.02730) => LSC_loss 0.78, Spatial_loss 1.74, Flat_loss 0.27, Train_acc 79.35, Test_acc 38.13
2025-01-11 05:33:31,701 [podnet.py] => Task 9, Epoch 105/160 (LR 0.02643) => LSC_loss 0.76, Spatial_loss 1.73, Flat_loss 0.27, Train_acc 80.27, Test_acc 42.41
2025-01-11 05:33:39,622 [podnet.py] => Task 9, Epoch 106/160 (LR 0.02557) => LSC_loss 0.76, Spatial_loss 1.70, Flat_loss 0.27, Train_acc 79.86, Test_acc 40.18
2025-01-11 05:33:47,351 [podnet.py] => Task 9, Epoch 107/160 (LR 0.02472) => LSC_loss 0.74, Spatial_loss 1.66, Flat_loss 0.27, Train_acc 80.37, Test_acc 42.21
2025-01-11 05:33:55,255 [podnet.py] => Task 9, Epoch 108/160 (LR 0.02388) => LSC_loss 0.73, Spatial_loss 1.65, Flat_loss 0.26, Train_acc 81.34, Test_acc 42.07
2025-01-11 05:34:02,984 [podnet.py] => Task 9, Epoch 109/160 (LR 0.02304) => LSC_loss 0.72, Spatial_loss 1.66, Flat_loss 0.26, Train_acc 81.48, Test_acc 41.19
2025-01-11 05:34:10,736 [podnet.py] => Task 9, Epoch 110/160 (LR 0.02222) => LSC_loss 0.71, Spatial_loss 1.62, Flat_loss 0.26, Train_acc 81.43, Test_acc 40.10
2025-01-11 05:34:18,578 [podnet.py] => Task 9, Epoch 111/160 (LR 0.02141) => LSC_loss 0.70, Spatial_loss 1.61, Flat_loss 0.26, Train_acc 81.66, Test_acc 41.99
2025-01-11 05:34:26,302 [podnet.py] => Task 9, Epoch 112/160 (LR 0.02061) => LSC_loss 0.69, Spatial_loss 1.58, Flat_loss 0.25, Train_acc 82.07, Test_acc 41.88
2025-01-11 05:34:34,288 [podnet.py] => Task 9, Epoch 113/160 (LR 0.01982) => LSC_loss 0.68, Spatial_loss 1.60, Flat_loss 0.25, Train_acc 82.53, Test_acc 42.02
2025-01-11 05:34:42,108 [podnet.py] => Task 9, Epoch 114/160 (LR 0.01905) => LSC_loss 0.68, Spatial_loss 1.59, Flat_loss 0.25, Train_acc 82.74, Test_acc 41.56
2025-01-11 05:34:49,848 [podnet.py] => Task 9, Epoch 115/160 (LR 0.01828) => LSC_loss 0.65, Spatial_loss 1.56, Flat_loss 0.25, Train_acc 83.37, Test_acc 43.22
2025-01-11 05:34:57,399 [podnet.py] => Task 9, Epoch 116/160 (LR 0.01753) => LSC_loss 0.66, Spatial_loss 1.55, Flat_loss 0.25, Train_acc 83.22, Test_acc 39.87
2025-01-11 05:35:05,034 [podnet.py] => Task 9, Epoch 117/160 (LR 0.01679) => LSC_loss 0.64, Spatial_loss 1.55, Flat_loss 0.24, Train_acc 84.04, Test_acc 42.68
2025-01-11 05:35:12,545 [podnet.py] => Task 9, Epoch 118/160 (LR 0.01606) => LSC_loss 0.63, Spatial_loss 1.48, Flat_loss 0.24, Train_acc 84.01, Test_acc 42.31
2025-01-11 05:35:20,032 [podnet.py] => Task 9, Epoch 119/160 (LR 0.01535) => LSC_loss 0.63, Spatial_loss 1.50, Flat_loss 0.24, Train_acc 84.14, Test_acc 42.11
2025-01-11 05:35:27,612 [podnet.py] => Task 9, Epoch 120/160 (LR 0.01464) => LSC_loss 0.62, Spatial_loss 1.51, Flat_loss 0.23, Train_acc 84.59, Test_acc 42.28
2025-01-11 05:35:35,356 [podnet.py] => Task 9, Epoch 121/160 (LR 0.01396) => LSC_loss 0.60, Spatial_loss 1.50, Flat_loss 0.23, Train_acc 85.31, Test_acc 41.72
2025-01-11 05:35:43,098 [podnet.py] => Task 9, Epoch 122/160 (LR 0.01328) => LSC_loss 0.59, Spatial_loss 1.43, Flat_loss 0.23, Train_acc 85.15, Test_acc 42.20
2025-01-11 05:35:50,843 [podnet.py] => Task 9, Epoch 123/160 (LR 0.01262) => LSC_loss 0.58, Spatial_loss 1.46, Flat_loss 0.23, Train_acc 85.86, Test_acc 42.72
2025-01-11 05:35:58,423 [podnet.py] => Task 9, Epoch 124/160 (LR 0.01198) => LSC_loss 0.56, Spatial_loss 1.41, Flat_loss 0.22, Train_acc 86.52, Test_acc 43.33
2025-01-11 05:36:06,131 [podnet.py] => Task 9, Epoch 125/160 (LR 0.01135) => LSC_loss 0.57, Spatial_loss 1.42, Flat_loss 0.22, Train_acc 86.00, Test_acc 43.61
2025-01-11 05:36:13,695 [podnet.py] => Task 9, Epoch 126/160 (LR 0.01073) => LSC_loss 0.56, Spatial_loss 1.41, Flat_loss 0.22, Train_acc 86.56, Test_acc 41.40
2025-01-11 05:36:21,396 [podnet.py] => Task 9, Epoch 127/160 (LR 0.01013) => LSC_loss 0.55, Spatial_loss 1.40, Flat_loss 0.22, Train_acc 86.83, Test_acc 43.40
2025-01-11 05:36:29,131 [podnet.py] => Task 9, Epoch 128/160 (LR 0.00955) => LSC_loss 0.55, Spatial_loss 1.35, Flat_loss 0.22, Train_acc 86.75, Test_acc 44.11
2025-01-11 05:36:36,945 [podnet.py] => Task 9, Epoch 129/160 (LR 0.00898) => LSC_loss 0.54, Spatial_loss 1.37, Flat_loss 0.21, Train_acc 87.28, Test_acc 43.48
2025-01-11 05:36:44,463 [podnet.py] => Task 9, Epoch 130/160 (LR 0.00843) => LSC_loss 0.52, Spatial_loss 1.34, Flat_loss 0.21, Train_acc 87.68, Test_acc 44.83
2025-01-11 05:36:52,243 [podnet.py] => Task 9, Epoch 131/160 (LR 0.00789) => LSC_loss 0.53, Spatial_loss 1.33, Flat_loss 0.21, Train_acc 87.46, Test_acc 43.15
2025-01-11 05:36:59,810 [podnet.py] => Task 9, Epoch 132/160 (LR 0.00737) => LSC_loss 0.52, Spatial_loss 1.32, Flat_loss 0.21, Train_acc 87.53, Test_acc 44.71
2025-01-11 05:37:07,925 [podnet.py] => Task 9, Epoch 133/160 (LR 0.00686) => LSC_loss 0.51, Spatial_loss 1.31, Flat_loss 0.21, Train_acc 87.93, Test_acc 44.04
2025-01-11 05:37:15,795 [podnet.py] => Task 9, Epoch 134/160 (LR 0.00638) => LSC_loss 0.51, Spatial_loss 1.30, Flat_loss 0.20, Train_acc 87.93, Test_acc 44.12
2025-01-11 05:37:23,504 [podnet.py] => Task 9, Epoch 135/160 (LR 0.00590) => LSC_loss 0.50, Spatial_loss 1.29, Flat_loss 0.20, Train_acc 88.32, Test_acc 44.75
2025-01-11 05:37:31,408 [podnet.py] => Task 9, Epoch 136/160 (LR 0.00545) => LSC_loss 0.50, Spatial_loss 1.29, Flat_loss 0.20, Train_acc 88.26, Test_acc 45.11
2025-01-11 05:37:39,202 [podnet.py] => Task 9, Epoch 137/160 (LR 0.00501) => LSC_loss 0.49, Spatial_loss 1.27, Flat_loss 0.20, Train_acc 88.70, Test_acc 43.54
2025-01-11 05:37:47,070 [podnet.py] => Task 9, Epoch 138/160 (LR 0.00459) => LSC_loss 0.49, Spatial_loss 1.27, Flat_loss 0.20, Train_acc 88.79, Test_acc 44.28
2025-01-11 05:37:54,902 [podnet.py] => Task 9, Epoch 139/160 (LR 0.00419) => LSC_loss 0.48, Spatial_loss 1.25, Flat_loss 0.20, Train_acc 88.65, Test_acc 44.45
2025-01-11 05:38:02,689 [podnet.py] => Task 9, Epoch 140/160 (LR 0.00381) => LSC_loss 0.48, Spatial_loss 1.23, Flat_loss 0.20, Train_acc 89.04, Test_acc 44.58
2025-01-11 05:38:10,536 [podnet.py] => Task 9, Epoch 141/160 (LR 0.00344) => LSC_loss 0.47, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 90.08, Test_acc 45.44
2025-01-11 05:38:18,455 [podnet.py] => Task 9, Epoch 142/160 (LR 0.00309) => LSC_loss 0.46, Spatial_loss 1.19, Flat_loss 0.19, Train_acc 89.56, Test_acc 44.90
2025-01-11 05:38:26,171 [podnet.py] => Task 9, Epoch 143/160 (LR 0.00276) => LSC_loss 0.47, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 90.04, Test_acc 44.20
2025-01-11 05:38:33,911 [podnet.py] => Task 9, Epoch 144/160 (LR 0.00245) => LSC_loss 0.46, Spatial_loss 1.22, Flat_loss 0.19, Train_acc 89.78, Test_acc 44.54
2025-01-11 05:38:41,626 [podnet.py] => Task 9, Epoch 145/160 (LR 0.00215) => LSC_loss 0.45, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 89.89, Test_acc 45.22
2025-01-11 05:38:49,221 [podnet.py] => Task 9, Epoch 146/160 (LR 0.00188) => LSC_loss 0.45, Spatial_loss 1.19, Flat_loss 0.19, Train_acc 89.92, Test_acc 44.84
2025-01-11 05:38:57,061 [podnet.py] => Task 9, Epoch 147/160 (LR 0.00162) => LSC_loss 0.44, Spatial_loss 1.18, Flat_loss 0.19, Train_acc 90.19, Test_acc 45.46
2025-01-11 05:39:05,463 [podnet.py] => Task 9, Epoch 148/160 (LR 0.00138) => LSC_loss 0.45, Spatial_loss 1.16, Flat_loss 0.18, Train_acc 89.98, Test_acc 45.42
2025-01-11 05:39:13,126 [podnet.py] => Task 9, Epoch 149/160 (LR 0.00116) => LSC_loss 0.44, Spatial_loss 1.16, Flat_loss 0.19, Train_acc 90.76, Test_acc 45.28
2025-01-11 05:39:20,839 [podnet.py] => Task 9, Epoch 150/160 (LR 0.00096) => LSC_loss 0.44, Spatial_loss 1.15, Flat_loss 0.18, Train_acc 90.22, Test_acc 45.32
2025-01-11 05:39:28,557 [podnet.py] => Task 9, Epoch 151/160 (LR 0.00078) => LSC_loss 0.44, Spatial_loss 1.13, Flat_loss 0.18, Train_acc 91.11, Test_acc 45.41
2025-01-11 05:39:36,322 [podnet.py] => Task 9, Epoch 152/160 (LR 0.00062) => LSC_loss 0.44, Spatial_loss 1.16, Flat_loss 0.18, Train_acc 90.65, Test_acc 45.37
2025-01-11 05:39:44,045 [podnet.py] => Task 9, Epoch 153/160 (LR 0.00047) => LSC_loss 0.44, Spatial_loss 1.15, Flat_loss 0.18, Train_acc 90.23, Test_acc 45.19
2025-01-11 05:39:51,717 [podnet.py] => Task 9, Epoch 154/160 (LR 0.00035) => LSC_loss 0.44, Spatial_loss 1.15, Flat_loss 0.18, Train_acc 90.61, Test_acc 45.54
2025-01-11 05:39:59,451 [podnet.py] => Task 9, Epoch 155/160 (LR 0.00024) => LSC_loss 0.43, Spatial_loss 1.12, Flat_loss 0.18, Train_acc 90.72, Test_acc 45.38
2025-01-11 05:40:07,043 [podnet.py] => Task 9, Epoch 156/160 (LR 0.00015) => LSC_loss 0.43, Spatial_loss 1.14, Flat_loss 0.18, Train_acc 90.90, Test_acc 45.32
2025-01-11 05:40:15,017 [podnet.py] => Task 9, Epoch 157/160 (LR 0.00009) => LSC_loss 0.43, Spatial_loss 1.12, Flat_loss 0.18, Train_acc 90.89, Test_acc 45.61
2025-01-11 05:40:22,740 [podnet.py] => Task 9, Epoch 158/160 (LR 0.00004) => LSC_loss 0.43, Spatial_loss 1.14, Flat_loss 0.18, Train_acc 91.00, Test_acc 45.57
2025-01-11 05:40:30,409 [podnet.py] => Task 9, Epoch 159/160 (LR 0.00001) => LSC_loss 0.43, Spatial_loss 1.13, Flat_loss 0.18, Train_acc 90.56, Test_acc 45.44
2025-01-11 05:40:38,130 [podnet.py] => Task 9, Epoch 160/160 (LR 0.00000) => LSC_loss 0.44, Spatial_loss 1.13, Flat_loss 0.18, Train_acc 90.61, Test_acc 45.45
2025-01-11 05:40:38,131 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-11 05:40:38,131 [base.py] => Reducing exemplars...(82 per classes)
2025-01-11 05:41:34,029 [base.py] => Constructing exemplars...(82 per classes)
2025-01-11 05:41:48,021 [podnet.py] => The size of finetune dataset: 8200
2025-01-11 05:41:53,839 [podnet.py] => Task 9, Epoch 1/20 (LR 0.00497) => LSC_loss 0.39, Spatial_loss 1.24, Flat_loss 0.14, Train_acc 91.80, Test_acc 44.97
2025-01-11 05:41:59,689 [podnet.py] => Task 9, Epoch 2/20 (LR 0.00488) => LSC_loss 0.41, Spatial_loss 1.22, Flat_loss 0.14, Train_acc 91.39, Test_acc 46.10
2025-01-11 05:42:05,610 [podnet.py] => Task 9, Epoch 3/20 (LR 0.00473) => LSC_loss 0.40, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 91.95, Test_acc 45.35
2025-01-11 05:42:11,474 [podnet.py] => Task 9, Epoch 4/20 (LR 0.00452) => LSC_loss 0.39, Spatial_loss 1.20, Flat_loss 0.13, Train_acc 92.04, Test_acc 45.62
2025-01-11 05:42:17,313 [podnet.py] => Task 9, Epoch 5/20 (LR 0.00427) => LSC_loss 0.39, Spatial_loss 1.19, Flat_loss 0.13, Train_acc 91.87, Test_acc 44.89
2025-01-11 05:42:23,014 [podnet.py] => Task 9, Epoch 6/20 (LR 0.00397) => LSC_loss 0.38, Spatial_loss 1.21, Flat_loss 0.13, Train_acc 92.43, Test_acc 45.92
2025-01-11 05:42:28,749 [podnet.py] => Task 9, Epoch 7/20 (LR 0.00363) => LSC_loss 0.38, Spatial_loss 1.19, Flat_loss 0.12, Train_acc 92.21, Test_acc 45.46
2025-01-11 05:42:34,578 [podnet.py] => Task 9, Epoch 8/20 (LR 0.00327) => LSC_loss 0.39, Spatial_loss 1.18, Flat_loss 0.13, Train_acc 92.43, Test_acc 45.89
2025-01-11 05:42:40,436 [podnet.py] => Task 9, Epoch 9/20 (LR 0.00289) => LSC_loss 0.38, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 92.26, Test_acc 46.00
2025-01-11 05:42:46,209 [podnet.py] => Task 9, Epoch 10/20 (LR 0.00250) => LSC_loss 0.38, Spatial_loss 1.16, Flat_loss 0.13, Train_acc 92.48, Test_acc 46.15
2025-01-11 05:42:51,874 [podnet.py] => Task 9, Epoch 11/20 (LR 0.00211) => LSC_loss 0.37, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 93.16, Test_acc 46.25
2025-01-11 05:42:57,557 [podnet.py] => Task 9, Epoch 12/20 (LR 0.00173) => LSC_loss 0.37, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 93.04, Test_acc 45.87
2025-01-11 05:43:03,624 [podnet.py] => Task 9, Epoch 13/20 (LR 0.00137) => LSC_loss 0.37, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 92.61, Test_acc 45.93
2025-01-11 05:43:09,680 [podnet.py] => Task 9, Epoch 14/20 (LR 0.00103) => LSC_loss 0.36, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 93.23, Test_acc 46.47
2025-01-11 05:43:15,492 [podnet.py] => Task 9, Epoch 15/20 (LR 0.00073) => LSC_loss 0.36, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 93.32, Test_acc 46.30
2025-01-11 05:43:21,311 [podnet.py] => Task 9, Epoch 16/20 (LR 0.00048) => LSC_loss 0.35, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 93.20, Test_acc 46.23
2025-01-11 05:43:27,179 [podnet.py] => Task 9, Epoch 17/20 (LR 0.00027) => LSC_loss 0.34, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 93.76, Test_acc 45.91
2025-01-11 05:43:33,007 [podnet.py] => Task 9, Epoch 18/20 (LR 0.00012) => LSC_loss 0.34, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 93.52, Test_acc 46.41
2025-01-11 05:43:39,408 [podnet.py] => Task 9, Epoch 19/20 (LR 0.00003) => LSC_loss 0.34, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 94.00, Test_acc 46.40
2025-01-11 05:43:45,413 [podnet.py] => Task 9, Epoch 20/20 (LR 0.00000) => LSC_loss 0.34, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 93.76, Test_acc 46.32
2025-01-11 05:43:45,415 [base.py] => Reducing exemplars...(74 per classes)
2025-01-11 05:44:40,456 [base.py] => Constructing exemplars...(74 per classes)
2025-01-11 05:44:57,681 [podnet.py] => Exemplar size: 7400
2025-01-11 05:44:57,682 [trainer.py] => CNN: {'total': np.float64(46.32), '00-09': np.float64(59.5), '10-19': np.float64(32.4), '20-29': np.float64(48.3), '30-39': np.float64(40.0), '40-49': np.float64(49.9), '50-59': np.float64(40.4), '60-69': np.float64(46.8), '70-79': np.float64(43.1), '80-89': np.float64(50.4), '90-99': np.float64(52.4), 'old': np.float64(45.64), 'new': np.float64(52.4)}
2025-01-11 05:44:57,682 [trainer.py] => NME: {'total': np.float64(44.9), '00-09': np.float64(64.7), '10-19': np.float64(32.5), '20-29': np.float64(48.5), '30-39': np.float64(38.9), '40-49': np.float64(48.6), '50-59': np.float64(37.9), '60-69': np.float64(47.3), '70-79': np.float64(40.5), '80-89': np.float64(45.8), '90-99': np.float64(44.3), 'old': np.float64(44.97), 'new': np.float64(44.3)}
2025-01-11 05:44:57,682 [trainer.py] => CNN top1 curve: [np.float64(90.1), np.float64(72.3), np.float64(68.93), np.float64(63.42), np.float64(60.28), np.float64(56.65), np.float64(54.8), np.float64(51.38), np.float64(49.08), np.float64(46.32)]
2025-01-11 05:44:57,682 [trainer.py] => CNN top5 curve: [np.float64(99.4), np.float64(94.05), np.float64(91.37), np.float64(87.82), np.float64(85.98), np.float64(83.17), np.float64(81.17), np.float64(79.08), np.float64(76.79), np.float64(74.57)]
2025-01-11 05:44:57,682 [trainer.py] => NME top1 curve: [np.float64(90.0), np.float64(72.3), np.float64(68.67), np.float64(61.38), np.float64(59.32), np.float64(55.12), np.float64(53.11), np.float64(49.86), np.float64(47.64), np.float64(44.9)]
2025-01-11 05:44:57,682 [trainer.py] => NME top5 curve: [np.float64(99.4), np.float64(93.7), np.float64(90.37), np.float64(87.02), np.float64(85.44), np.float64(82.3), np.float64(80.14), np.float64(78.16), np.float64(75.96), np.float64(73.88)]

2025-01-11 05:44:57,682 [trainer.py] => End Time:1736570697.6825764
