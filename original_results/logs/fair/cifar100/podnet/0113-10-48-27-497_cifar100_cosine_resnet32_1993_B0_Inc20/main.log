2025-01-13 10:48:27,542 [trainer.py] => Time Str >>> 0113-10-48-27-497
2025-01-13 10:48:27,744 [trainer.py] => prefix: fair
2025-01-13 10:48:27,745 [trainer.py] => dataset: cifar100
2025-01-13 10:48:27,745 [trainer.py] => memory_size: 4414
2025-01-13 10:48:27,745 [trainer.py] => memory_per_class: 20
2025-01-13 10:48:27,745 [trainer.py] => fixed_memory: False
2025-01-13 10:48:27,745 [trainer.py] => shuffle: True
2025-01-13 10:48:27,745 [trainer.py] => init_cls: 20
2025-01-13 10:48:27,745 [trainer.py] => increment: 20
2025-01-13 10:48:27,746 [trainer.py] => model_name: podnet
2025-01-13 10:48:27,746 [trainer.py] => convnet_type: cosine_resnet32
2025-01-13 10:48:27,746 [trainer.py] => device: [device(type='cuda', index=3)]
2025-01-13 10:48:27,746 [trainer.py] => seed: 1993
2025-01-13 10:48:27,746 [trainer.py] => debug: False
2025-01-13 10:48:27,746 [trainer.py] => skip: False
2025-01-13 10:48:27,746 [trainer.py] => config: ./exps/podnet.json
2025-01-13 10:48:27,746 [trainer.py] => time_str: 0113-10-48-27-497
2025-01-13 10:48:27,746 [trainer.py] => exp_name: 0113-10-48-27-497_cifar100_cosine_resnet32_1993_B0_Inc20
2025-01-13 10:48:27,746 [trainer.py] => logfilename: logs/fair/cifar100/podnet/0113-10-48-27-497_cifar100_cosine_resnet32_1993_B0_Inc20
2025-01-13 10:48:27,747 [trainer.py] => csv_name: cifar100_1993_cosine_resnet32_B0_Inc20
2025-01-13 10:48:33,145 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-01-13 10:48:33,778 [trainer.py] => Start time:1736761713.7785153
2025-01-13 10:48:33,779 [trainer.py] => All params: 466256
2025-01-13 10:48:33,780 [trainer.py] => Trainable params: 466256
2025-01-13 10:48:33,780 [podnet.py] => Learning on 0-20
2025-01-13 10:48:33,815 [podnet.py] => Adaptive factor: 0
2025-01-13 10:48:48,338 [podnet.py] => Task 0, Epoch 1/160 (LR 0.09999) => LSC_loss 2.97, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 9.43, Test_acc 11.60
2025-01-13 10:48:52,638 [podnet.py] => Task 0, Epoch 2/160 (LR 0.09996) => LSC_loss 2.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 17.96, Test_acc 21.55
2025-01-13 10:48:56,784 [podnet.py] => Task 0, Epoch 3/160 (LR 0.09991) => LSC_loss 2.47, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 26.06, Test_acc 26.70
2025-01-13 10:49:00,990 [podnet.py] => Task 0, Epoch 4/160 (LR 0.09985) => LSC_loss 2.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 31.73, Test_acc 30.40
2025-01-13 10:49:05,135 [podnet.py] => Task 0, Epoch 5/160 (LR 0.09976) => LSC_loss 2.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 34.54, Test_acc 35.90
2025-01-13 10:49:09,365 [podnet.py] => Task 0, Epoch 6/160 (LR 0.09965) => LSC_loss 2.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 39.49, Test_acc 33.40
2025-01-13 10:49:13,710 [podnet.py] => Task 0, Epoch 7/160 (LR 0.09953) => LSC_loss 1.96, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.65, Test_acc 35.65
2025-01-13 10:49:17,819 [podnet.py] => Task 0, Epoch 8/160 (LR 0.09938) => LSC_loss 1.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 45.66, Test_acc 34.50
2025-01-13 10:49:21,872 [podnet.py] => Task 0, Epoch 9/160 (LR 0.09922) => LSC_loss 1.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 48.29, Test_acc 46.35
2025-01-13 10:49:26,045 [podnet.py] => Task 0, Epoch 10/160 (LR 0.09904) => LSC_loss 1.63, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 51.74, Test_acc 46.45
2025-01-13 10:49:30,185 [podnet.py] => Task 0, Epoch 11/160 (LR 0.09884) => LSC_loss 1.54, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 54.54, Test_acc 44.10
2025-01-13 10:49:34,332 [podnet.py] => Task 0, Epoch 12/160 (LR 0.09862) => LSC_loss 1.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 56.61, Test_acc 50.80
2025-01-13 10:49:38,479 [podnet.py] => Task 0, Epoch 13/160 (LR 0.09838) => LSC_loss 1.42, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 58.24, Test_acc 54.90
2025-01-13 10:49:42,495 [podnet.py] => Task 0, Epoch 14/160 (LR 0.09812) => LSC_loss 1.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 58.32, Test_acc 54.65
2025-01-13 10:49:46,551 [podnet.py] => Task 0, Epoch 15/160 (LR 0.09785) => LSC_loss 1.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 60.55, Test_acc 54.60
2025-01-13 10:49:50,785 [podnet.py] => Task 0, Epoch 16/160 (LR 0.09755) => LSC_loss 1.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 61.45, Test_acc 52.10
2025-01-13 10:49:54,932 [podnet.py] => Task 0, Epoch 17/160 (LR 0.09724) => LSC_loss 1.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 63.16, Test_acc 51.95
2025-01-13 10:49:58,947 [podnet.py] => Task 0, Epoch 18/160 (LR 0.09691) => LSC_loss 1.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 64.46, Test_acc 57.15
2025-01-13 10:50:03,032 [podnet.py] => Task 0, Epoch 19/160 (LR 0.09656) => LSC_loss 1.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.76, Test_acc 53.55
2025-01-13 10:50:07,015 [podnet.py] => Task 0, Epoch 20/160 (LR 0.09619) => LSC_loss 1.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 66.45, Test_acc 59.15
2025-01-13 10:50:11,175 [podnet.py] => Task 0, Epoch 21/160 (LR 0.09581) => LSC_loss 1.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.79, Test_acc 60.80
2025-01-13 10:50:15,413 [podnet.py] => Task 0, Epoch 22/160 (LR 0.09541) => LSC_loss 1.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 67.50, Test_acc 49.15
2025-01-13 10:50:19,476 [podnet.py] => Task 0, Epoch 23/160 (LR 0.09499) => LSC_loss 1.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.01, Test_acc 59.50
2025-01-13 10:50:23,306 [podnet.py] => Task 0, Epoch 24/160 (LR 0.09455) => LSC_loss 1.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 69.03, Test_acc 59.40
2025-01-13 10:50:27,564 [podnet.py] => Task 0, Epoch 25/160 (LR 0.09410) => LSC_loss 1.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.50, Test_acc 60.70
2025-01-13 10:50:31,498 [podnet.py] => Task 0, Epoch 26/160 (LR 0.09362) => LSC_loss 1.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.28, Test_acc 57.50
2025-01-13 10:50:35,751 [podnet.py] => Task 0, Epoch 27/160 (LR 0.09314) => LSC_loss 0.98, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 71.70, Test_acc 53.15
2025-01-13 10:50:39,872 [podnet.py] => Task 0, Epoch 28/160 (LR 0.09263) => LSC_loss 0.95, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.13, Test_acc 57.90
2025-01-13 10:50:44,330 [podnet.py] => Task 0, Epoch 29/160 (LR 0.09211) => LSC_loss 0.93, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.95, Test_acc 59.15
2025-01-13 10:50:48,606 [podnet.py] => Task 0, Epoch 30/160 (LR 0.09157) => LSC_loss 0.92, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 72.86, Test_acc 64.50
2025-01-13 10:50:53,194 [podnet.py] => Task 0, Epoch 31/160 (LR 0.09102) => LSC_loss 0.90, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.07, Test_acc 64.30
2025-01-13 10:50:57,474 [podnet.py] => Task 0, Epoch 32/160 (LR 0.09045) => LSC_loss 0.86, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.29, Test_acc 65.45
2025-01-13 10:51:01,798 [podnet.py] => Task 0, Epoch 33/160 (LR 0.08987) => LSC_loss 0.85, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.16, Test_acc 68.00
2025-01-13 10:51:06,407 [podnet.py] => Task 0, Epoch 34/160 (LR 0.08927) => LSC_loss 0.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.65, Test_acc 63.90
2025-01-13 10:51:10,819 [podnet.py] => Task 0, Epoch 35/160 (LR 0.08865) => LSC_loss 0.81, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 76.46, Test_acc 62.25
2025-01-13 10:51:15,260 [podnet.py] => Task 0, Epoch 36/160 (LR 0.08802) => LSC_loss 0.83, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.72, Test_acc 58.85
2025-01-13 10:51:19,559 [podnet.py] => Task 0, Epoch 37/160 (LR 0.08738) => LSC_loss 0.77, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.42, Test_acc 66.20
2025-01-13 10:51:23,883 [podnet.py] => Task 0, Epoch 38/160 (LR 0.08672) => LSC_loss 0.79, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.51, Test_acc 53.05
2025-01-13 10:51:27,906 [podnet.py] => Task 0, Epoch 39/160 (LR 0.08604) => LSC_loss 0.84, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.82, Test_acc 64.00
2025-01-13 10:51:31,889 [podnet.py] => Task 0, Epoch 40/160 (LR 0.08536) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.35, Test_acc 68.30
2025-01-13 10:51:36,016 [podnet.py] => Task 0, Epoch 41/160 (LR 0.08465) => LSC_loss 0.73, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 78.92, Test_acc 67.45
2025-01-13 10:51:40,286 [podnet.py] => Task 0, Epoch 42/160 (LR 0.08394) => LSC_loss 0.75, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 77.80, Test_acc 67.40
2025-01-13 10:51:44,344 [podnet.py] => Task 0, Epoch 43/160 (LR 0.08321) => LSC_loss 0.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.83, Test_acc 70.10
2025-01-13 10:51:48,464 [podnet.py] => Task 0, Epoch 44/160 (LR 0.08247) => LSC_loss 0.69, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.75, Test_acc 67.00
2025-01-13 10:51:52,722 [podnet.py] => Task 0, Epoch 45/160 (LR 0.08172) => LSC_loss 0.66, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.82, Test_acc 67.60
2025-01-13 10:51:56,797 [podnet.py] => Task 0, Epoch 46/160 (LR 0.08095) => LSC_loss 0.70, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.55, Test_acc 68.10
2025-01-13 10:52:00,915 [podnet.py] => Task 0, Epoch 47/160 (LR 0.08018) => LSC_loss 0.66, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.85, Test_acc 70.50
2025-01-13 10:52:05,134 [podnet.py] => Task 0, Epoch 48/160 (LR 0.07939) => LSC_loss 0.65, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.85, Test_acc 73.55
2025-01-13 10:52:09,166 [podnet.py] => Task 0, Epoch 49/160 (LR 0.07859) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.78, Test_acc 69.95
2025-01-13 10:52:13,157 [podnet.py] => Task 0, Epoch 50/160 (LR 0.07778) => LSC_loss 0.65, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.97, Test_acc 66.30
2025-01-13 10:52:17,268 [podnet.py] => Task 0, Epoch 51/160 (LR 0.07696) => LSC_loss 0.60, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.45, Test_acc 70.70
2025-01-13 10:52:21,807 [podnet.py] => Task 0, Epoch 52/160 (LR 0.07612) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.14, Test_acc 69.60
2025-01-13 10:52:25,898 [podnet.py] => Task 0, Epoch 53/160 (LR 0.07528) => LSC_loss 0.64, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 81.21, Test_acc 66.40
2025-01-13 10:52:29,835 [podnet.py] => Task 0, Epoch 54/160 (LR 0.07443) => LSC_loss 0.59, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.82, Test_acc 68.55
2025-01-13 10:52:33,915 [podnet.py] => Task 0, Epoch 55/160 (LR 0.07357) => LSC_loss 0.58, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.87, Test_acc 69.90
2025-01-13 10:52:37,965 [podnet.py] => Task 0, Epoch 56/160 (LR 0.07270) => LSC_loss 0.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 82.21, Test_acc 72.75
2025-01-13 10:52:41,837 [podnet.py] => Task 0, Epoch 57/160 (LR 0.07182) => LSC_loss 0.55, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.46, Test_acc 73.70
2025-01-13 10:52:45,765 [podnet.py] => Task 0, Epoch 58/160 (LR 0.07093) => LSC_loss 0.53, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.69, Test_acc 70.35
2025-01-13 10:52:49,937 [podnet.py] => Task 0, Epoch 59/160 (LR 0.07004) => LSC_loss 0.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.27, Test_acc 71.00
2025-01-13 10:52:54,010 [podnet.py] => Task 0, Epoch 60/160 (LR 0.06913) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.40, Test_acc 68.20
2025-01-13 10:52:58,222 [podnet.py] => Task 0, Epoch 61/160 (LR 0.06822) => LSC_loss 0.56, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.38, Test_acc 72.15
2025-01-13 10:53:02,275 [podnet.py] => Task 0, Epoch 62/160 (LR 0.06731) => LSC_loss 0.52, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.58, Test_acc 70.80
2025-01-13 10:53:06,511 [podnet.py] => Task 0, Epoch 63/160 (LR 0.06638) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.64, Test_acc 73.20
2025-01-13 10:53:10,735 [podnet.py] => Task 0, Epoch 64/160 (LR 0.06545) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.70, Test_acc 66.35
2025-01-13 10:53:15,058 [podnet.py] => Task 0, Epoch 65/160 (LR 0.06451) => LSC_loss 0.49, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.95, Test_acc 73.75
2025-01-13 10:53:19,366 [podnet.py] => Task 0, Epoch 66/160 (LR 0.06357) => LSC_loss 0.50, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 85.48, Test_acc 72.50
2025-01-13 10:53:23,682 [podnet.py] => Task 0, Epoch 67/160 (LR 0.06262) => LSC_loss 0.46, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.65, Test_acc 67.40
2025-01-13 10:53:27,915 [podnet.py] => Task 0, Epoch 68/160 (LR 0.06167) => LSC_loss 0.46, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.15, Test_acc 72.85
2025-01-13 10:53:32,128 [podnet.py] => Task 0, Epoch 69/160 (LR 0.06072) => LSC_loss 0.43, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.69, Test_acc 72.80
2025-01-13 10:53:36,418 [podnet.py] => Task 0, Epoch 70/160 (LR 0.05975) => LSC_loss 0.39, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.97, Test_acc 74.05
2025-01-13 10:53:40,704 [podnet.py] => Task 0, Epoch 71/160 (LR 0.05879) => LSC_loss 0.44, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.97, Test_acc 71.50
2025-01-13 10:53:44,964 [podnet.py] => Task 0, Epoch 72/160 (LR 0.05782) => LSC_loss 0.42, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.97, Test_acc 71.15
2025-01-13 10:53:49,190 [podnet.py] => Task 0, Epoch 73/160 (LR 0.05685) => LSC_loss 0.44, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 87.18, Test_acc 70.60
2025-01-13 10:53:53,323 [podnet.py] => Task 0, Epoch 74/160 (LR 0.05588) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.22, Test_acc 73.60
2025-01-13 10:53:57,768 [podnet.py] => Task 0, Epoch 75/160 (LR 0.05490) => LSC_loss 0.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.77, Test_acc 70.90
2025-01-13 10:54:02,102 [podnet.py] => Task 0, Epoch 76/160 (LR 0.05392) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.00, Test_acc 71.25
2025-01-13 10:54:06,416 [podnet.py] => Task 0, Epoch 77/160 (LR 0.05294) => LSC_loss 0.38, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.13, Test_acc 71.60
2025-01-13 10:54:10,512 [podnet.py] => Task 0, Epoch 78/160 (LR 0.05196) => LSC_loss 0.37, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.23, Test_acc 68.75
2025-01-13 10:54:14,632 [podnet.py] => Task 0, Epoch 79/160 (LR 0.05098) => LSC_loss 0.38, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.05, Test_acc 72.20
2025-01-13 10:54:18,827 [podnet.py] => Task 0, Epoch 80/160 (LR 0.05000) => LSC_loss 0.40, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.27, Test_acc 72.95
2025-01-13 10:54:23,025 [podnet.py] => Task 0, Epoch 81/160 (LR 0.04902) => LSC_loss 0.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.03, Test_acc 71.75
2025-01-13 10:54:27,430 [podnet.py] => Task 0, Epoch 82/160 (LR 0.04804) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.13, Test_acc 72.70
2025-01-13 10:54:31,517 [podnet.py] => Task 0, Epoch 83/160 (LR 0.04706) => LSC_loss 0.34, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.27, Test_acc 75.00
2025-01-13 10:54:35,862 [podnet.py] => Task 0, Epoch 84/160 (LR 0.04608) => LSC_loss 0.36, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.72, Test_acc 75.00
2025-01-13 10:54:40,416 [podnet.py] => Task 0, Epoch 85/160 (LR 0.04510) => LSC_loss 0.30, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 90.95, Test_acc 74.40
2025-01-13 10:54:44,721 [podnet.py] => Task 0, Epoch 86/160 (LR 0.04412) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.40, Test_acc 72.80
2025-01-13 10:54:48,946 [podnet.py] => Task 0, Epoch 87/160 (LR 0.04315) => LSC_loss 0.28, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.68, Test_acc 69.35
2025-01-13 10:54:53,379 [podnet.py] => Task 0, Epoch 88/160 (LR 0.04218) => LSC_loss 0.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.02, Test_acc 73.00
2025-01-13 10:54:57,530 [podnet.py] => Task 0, Epoch 89/160 (LR 0.04121) => LSC_loss 0.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.94, Test_acc 73.95
2025-01-13 10:55:01,897 [podnet.py] => Task 0, Epoch 90/160 (LR 0.04025) => LSC_loss 0.26, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.24, Test_acc 74.55
2025-01-13 10:55:06,030 [podnet.py] => Task 0, Epoch 91/160 (LR 0.03928) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.11, Test_acc 76.75
2025-01-13 10:55:10,417 [podnet.py] => Task 0, Epoch 92/160 (LR 0.03833) => LSC_loss 0.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.66, Test_acc 75.35
2025-01-13 10:55:14,550 [podnet.py] => Task 0, Epoch 93/160 (LR 0.03738) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.19, Test_acc 73.70
2025-01-13 10:55:18,828 [podnet.py] => Task 0, Epoch 94/160 (LR 0.03643) => LSC_loss 0.22, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.70, Test_acc 76.30
2025-01-13 10:55:22,980 [podnet.py] => Task 0, Epoch 95/160 (LR 0.03549) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.82, Test_acc 75.70
2025-01-13 10:55:27,223 [podnet.py] => Task 0, Epoch 96/160 (LR 0.03455) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.44, Test_acc 76.85
2025-01-13 10:55:31,544 [podnet.py] => Task 0, Epoch 97/160 (LR 0.03362) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.05, Test_acc 74.75
2025-01-13 10:55:36,186 [podnet.py] => Task 0, Epoch 98/160 (LR 0.03269) => LSC_loss 0.21, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.06, Test_acc 76.40
2025-01-13 10:55:40,461 [podnet.py] => Task 0, Epoch 99/160 (LR 0.03178) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.67, Test_acc 75.60
2025-01-13 10:55:44,788 [podnet.py] => Task 0, Epoch 100/160 (LR 0.03087) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.01, Test_acc 77.05
2025-01-13 10:55:49,186 [podnet.py] => Task 0, Epoch 101/160 (LR 0.02996) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.99, Test_acc 76.00
2025-01-13 10:55:53,290 [podnet.py] => Task 0, Epoch 102/160 (LR 0.02907) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.16, Test_acc 77.15
2025-01-13 10:55:57,596 [podnet.py] => Task 0, Epoch 103/160 (LR 0.02818) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.00, Test_acc 79.50
2025-01-13 10:56:01,677 [podnet.py] => Task 0, Epoch 104/160 (LR 0.02730) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.05, Test_acc 77.55
2025-01-13 10:56:05,977 [podnet.py] => Task 0, Epoch 105/160 (LR 0.02643) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.48, Test_acc 78.70
2025-01-13 10:56:10,247 [podnet.py] => Task 0, Epoch 106/160 (LR 0.02557) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.23, Test_acc 79.05
2025-01-13 10:56:14,342 [podnet.py] => Task 0, Epoch 107/160 (LR 0.02472) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.07, Test_acc 75.45
2025-01-13 10:56:18,589 [podnet.py] => Task 0, Epoch 108/160 (LR 0.02388) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.62, Test_acc 77.20
2025-01-13 10:56:22,800 [podnet.py] => Task 0, Epoch 109/160 (LR 0.02304) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.25, Test_acc 78.35
2025-01-13 10:56:26,961 [podnet.py] => Task 0, Epoch 110/160 (LR 0.02222) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 77.45
2025-01-13 10:56:31,265 [podnet.py] => Task 0, Epoch 111/160 (LR 0.02141) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.09, Test_acc 80.25
2025-01-13 10:56:35,462 [podnet.py] => Task 0, Epoch 112/160 (LR 0.02061) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.29, Test_acc 79.40
2025-01-13 10:56:39,347 [podnet.py] => Task 0, Epoch 113/160 (LR 0.01982) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 78.90
2025-01-13 10:56:44,161 [podnet.py] => Task 0, Epoch 114/160 (LR 0.01905) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.28, Test_acc 78.45
2025-01-13 10:56:48,579 [podnet.py] => Task 0, Epoch 115/160 (LR 0.01828) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.42, Test_acc 77.85
2025-01-13 10:56:52,810 [podnet.py] => Task 0, Epoch 116/160 (LR 0.01753) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.40, Test_acc 79.00
2025-01-13 10:56:57,119 [podnet.py] => Task 0, Epoch 117/160 (LR 0.01679) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 80.35
2025-01-13 10:57:01,342 [podnet.py] => Task 0, Epoch 118/160 (LR 0.01606) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 79.45
2025-01-13 10:57:05,464 [podnet.py] => Task 0, Epoch 119/160 (LR 0.01535) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.81, Test_acc 80.15
2025-01-13 10:57:09,980 [podnet.py] => Task 0, Epoch 120/160 (LR 0.01464) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.01, Test_acc 79.40
2025-01-13 10:57:14,626 [podnet.py] => Task 0, Epoch 121/160 (LR 0.01396) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 80.15
2025-01-13 10:57:19,207 [podnet.py] => Task 0, Epoch 122/160 (LR 0.01328) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.21, Test_acc 80.30
2025-01-13 10:57:23,535 [podnet.py] => Task 0, Epoch 123/160 (LR 0.01262) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.20, Test_acc 81.35
2025-01-13 10:57:27,834 [podnet.py] => Task 0, Epoch 124/160 (LR 0.01198) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.23, Test_acc 80.50
2025-01-13 10:57:32,172 [podnet.py] => Task 0, Epoch 125/160 (LR 0.01135) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.06, Test_acc 80.90
2025-01-13 10:57:36,268 [podnet.py] => Task 0, Epoch 126/160 (LR 0.01073) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 80.25
2025-01-13 10:57:40,355 [podnet.py] => Task 0, Epoch 127/160 (LR 0.01013) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.75, Test_acc 80.90
2025-01-13 10:57:44,411 [podnet.py] => Task 0, Epoch 128/160 (LR 0.00955) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.57, Test_acc 81.80
2025-01-13 10:57:48,419 [podnet.py] => Task 0, Epoch 129/160 (LR 0.00898) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.74, Test_acc 81.25
2025-01-13 10:57:52,654 [podnet.py] => Task 0, Epoch 130/160 (LR 0.00843) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.62, Test_acc 80.90
2025-01-13 10:57:57,129 [podnet.py] => Task 0, Epoch 131/160 (LR 0.00789) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 81.35
2025-01-13 10:58:01,507 [podnet.py] => Task 0, Epoch 132/160 (LR 0.00737) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.73, Test_acc 81.45
2025-01-13 10:58:06,240 [podnet.py] => Task 0, Epoch 133/160 (LR 0.00686) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 81.55
2025-01-13 10:58:10,659 [podnet.py] => Task 0, Epoch 134/160 (LR 0.00638) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.77, Test_acc 81.75
2025-01-13 10:58:15,236 [podnet.py] => Task 0, Epoch 135/160 (LR 0.00590) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 81.70
2025-01-13 10:58:19,512 [podnet.py] => Task 0, Epoch 136/160 (LR 0.00545) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.81, Test_acc 81.65
2025-01-13 10:58:24,159 [podnet.py] => Task 0, Epoch 137/160 (LR 0.00501) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 82.10
2025-01-13 10:58:28,529 [podnet.py] => Task 0, Epoch 138/160 (LR 0.00459) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.86, Test_acc 82.10
2025-01-13 10:58:32,784 [podnet.py] => Task 0, Epoch 139/160 (LR 0.00419) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 82.45
2025-01-13 10:58:37,330 [podnet.py] => Task 0, Epoch 140/160 (LR 0.00381) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 82.30
2025-01-13 10:58:41,817 [podnet.py] => Task 0, Epoch 141/160 (LR 0.00344) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 81.30
2025-01-13 10:58:46,066 [podnet.py] => Task 0, Epoch 142/160 (LR 0.00309) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 81.65
2025-01-13 10:58:50,243 [podnet.py] => Task 0, Epoch 143/160 (LR 0.00276) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 81.30
2025-01-13 10:58:54,419 [podnet.py] => Task 0, Epoch 144/160 (LR 0.00245) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.83, Test_acc 81.75
2025-01-13 10:58:58,899 [podnet.py] => Task 0, Epoch 145/160 (LR 0.00215) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.85, Test_acc 82.20
2025-01-13 10:59:03,199 [podnet.py] => Task 0, Epoch 146/160 (LR 0.00188) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 82.05
2025-01-13 10:59:07,522 [podnet.py] => Task 0, Epoch 147/160 (LR 0.00162) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 81.95
2025-01-13 10:59:11,785 [podnet.py] => Task 0, Epoch 148/160 (LR 0.00138) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 81.80
2025-01-13 10:59:16,337 [podnet.py] => Task 0, Epoch 149/160 (LR 0.00116) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 81.95
2025-01-13 10:59:20,981 [podnet.py] => Task 0, Epoch 150/160 (LR 0.00096) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 81.95
2025-01-13 10:59:25,306 [podnet.py] => Task 0, Epoch 151/160 (LR 0.00078) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 81.95
2025-01-13 10:59:29,672 [podnet.py] => Task 0, Epoch 152/160 (LR 0.00062) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 82.05
2025-01-13 10:59:33,687 [podnet.py] => Task 0, Epoch 153/160 (LR 0.00047) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.93, Test_acc 81.80
2025-01-13 10:59:37,934 [podnet.py] => Task 0, Epoch 154/160 (LR 0.00035) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 82.25
2025-01-13 10:59:42,044 [podnet.py] => Task 0, Epoch 155/160 (LR 0.00024) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.94, Test_acc 82.30
2025-01-13 10:59:46,063 [podnet.py] => Task 0, Epoch 156/160 (LR 0.00015) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 82.20
2025-01-13 10:59:50,074 [podnet.py] => Task 0, Epoch 157/160 (LR 0.00009) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.91, Test_acc 82.30
2025-01-13 10:59:54,086 [podnet.py] => Task 0, Epoch 158/160 (LR 0.00004) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.97, Test_acc 82.45
2025-01-13 10:59:58,253 [podnet.py] => Task 0, Epoch 159/160 (LR 0.00001) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 82.45
2025-01-13 11:00:02,246 [podnet.py] => Task 0, Epoch 160/160 (LR 0.00000) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.95, Test_acc 82.45
2025-01-13 11:00:02,246 [base.py] => Reducing exemplars...(220 per classes)
2025-01-13 11:00:02,247 [base.py] => Constructing exemplars...(220 per classes)
2025-01-13 11:00:40,604 [podnet.py] => Exemplar size: 4400
2025-01-13 11:00:40,607 [trainer.py] => CNN: {'total': np.float64(82.45), '00-09': np.float64(85.3), '10-19': np.float64(79.6), 'old': 0, 'new': np.float64(82.45)}
2025-01-13 11:00:40,608 [trainer.py] => NME: {'total': np.float64(82.05), '00-09': np.float64(84.9), '10-19': np.float64(79.2), 'old': 0, 'new': np.float64(82.05)}
2025-01-13 11:00:40,608 [trainer.py] => CNN top1 curve: [np.float64(82.45)]
2025-01-13 11:00:40,608 [trainer.py] => CNN top5 curve: [np.float64(96.4)]
2025-01-13 11:00:40,608 [trainer.py] => NME top1 curve: [np.float64(82.05)]
2025-01-13 11:00:40,608 [trainer.py] => NME top5 curve: [np.float64(96.2)]

2025-01-13 11:00:40,608 [trainer.py] => All params: 479057
2025-01-13 11:00:40,608 [trainer.py] => Trainable params: 479057
2025-01-13 11:00:40,609 [podnet.py] => Learning on 20-40
2025-01-13 11:00:40,738 [podnet.py] => Adaptive factor: 1.4142135623730951
2025-01-13 11:00:48,839 [podnet.py] => Task 1, Epoch 1/160 (LR 0.09999) => LSC_loss 1.94, Spatial_loss 2.18, Flat_loss 0.47, Train_acc 47.65, Test_acc 44.32
2025-01-13 11:00:56,264 [podnet.py] => Task 1, Epoch 2/160 (LR 0.09996) => LSC_loss 1.36, Spatial_loss 2.02, Flat_loss 0.37, Train_acc 59.94, Test_acc 50.32
2025-01-13 11:01:04,014 [podnet.py] => Task 1, Epoch 3/160 (LR 0.09991) => LSC_loss 1.23, Spatial_loss 1.93, Flat_loss 0.35, Train_acc 63.83, Test_acc 54.90
2025-01-13 11:01:11,546 [podnet.py] => Task 1, Epoch 4/160 (LR 0.09985) => LSC_loss 1.16, Spatial_loss 1.86, Flat_loss 0.33, Train_acc 65.93, Test_acc 55.48
2025-01-13 11:01:18,818 [podnet.py] => Task 1, Epoch 5/160 (LR 0.09976) => LSC_loss 1.08, Spatial_loss 1.83, Flat_loss 0.32, Train_acc 68.78, Test_acc 57.12
2025-01-13 11:01:26,332 [podnet.py] => Task 1, Epoch 6/160 (LR 0.09965) => LSC_loss 1.04, Spatial_loss 1.80, Flat_loss 0.32, Train_acc 69.76, Test_acc 55.75
2025-01-13 11:01:33,907 [podnet.py] => Task 1, Epoch 7/160 (LR 0.09953) => LSC_loss 0.99, Spatial_loss 1.79, Flat_loss 0.32, Train_acc 71.24, Test_acc 56.58
2025-01-13 11:01:41,213 [podnet.py] => Task 1, Epoch 8/160 (LR 0.09938) => LSC_loss 0.99, Spatial_loss 1.79, Flat_loss 0.32, Train_acc 71.12, Test_acc 52.32
2025-01-13 11:01:48,686 [podnet.py] => Task 1, Epoch 9/160 (LR 0.09922) => LSC_loss 0.94, Spatial_loss 1.75, Flat_loss 0.32, Train_acc 72.70, Test_acc 55.88
2025-01-13 11:01:56,353 [podnet.py] => Task 1, Epoch 10/160 (LR 0.09904) => LSC_loss 0.92, Spatial_loss 1.75, Flat_loss 0.32, Train_acc 73.06, Test_acc 58.50
2025-01-13 11:02:03,805 [podnet.py] => Task 1, Epoch 11/160 (LR 0.09884) => LSC_loss 0.91, Spatial_loss 1.77, Flat_loss 0.33, Train_acc 73.11, Test_acc 59.12
2025-01-13 11:02:11,584 [podnet.py] => Task 1, Epoch 12/160 (LR 0.09862) => LSC_loss 0.89, Spatial_loss 1.76, Flat_loss 0.32, Train_acc 74.12, Test_acc 59.88
2025-01-13 11:02:19,182 [podnet.py] => Task 1, Epoch 13/160 (LR 0.09838) => LSC_loss 0.87, Spatial_loss 1.74, Flat_loss 0.32, Train_acc 74.38, Test_acc 56.58
2025-01-13 11:02:26,753 [podnet.py] => Task 1, Epoch 14/160 (LR 0.09812) => LSC_loss 0.84, Spatial_loss 1.74, Flat_loss 0.32, Train_acc 75.16, Test_acc 60.05
2025-01-13 11:02:34,249 [podnet.py] => Task 1, Epoch 15/160 (LR 0.09785) => LSC_loss 0.84, Spatial_loss 1.74, Flat_loss 0.32, Train_acc 75.51, Test_acc 58.08
2025-01-13 11:02:41,808 [podnet.py] => Task 1, Epoch 16/160 (LR 0.09755) => LSC_loss 0.85, Spatial_loss 1.75, Flat_loss 0.33, Train_acc 75.26, Test_acc 58.80
2025-01-13 11:02:48,675 [podnet.py] => Task 1, Epoch 17/160 (LR 0.09724) => LSC_loss 0.83, Spatial_loss 1.75, Flat_loss 0.32, Train_acc 75.85, Test_acc 62.00
2025-01-13 11:02:56,179 [podnet.py] => Task 1, Epoch 18/160 (LR 0.09691) => LSC_loss 0.81, Spatial_loss 1.73, Flat_loss 0.32, Train_acc 76.11, Test_acc 60.65
2025-01-13 11:03:03,677 [podnet.py] => Task 1, Epoch 19/160 (LR 0.09656) => LSC_loss 0.80, Spatial_loss 1.73, Flat_loss 0.32, Train_acc 76.39, Test_acc 58.15
2025-01-13 11:03:11,245 [podnet.py] => Task 1, Epoch 20/160 (LR 0.09619) => LSC_loss 0.79, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 76.83, Test_acc 60.02
2025-01-13 11:03:18,354 [podnet.py] => Task 1, Epoch 21/160 (LR 0.09581) => LSC_loss 0.79, Spatial_loss 1.75, Flat_loss 0.33, Train_acc 76.72, Test_acc 58.48
2025-01-13 11:03:25,484 [podnet.py] => Task 1, Epoch 22/160 (LR 0.09541) => LSC_loss 0.79, Spatial_loss 1.75, Flat_loss 0.33, Train_acc 76.74, Test_acc 57.65
2025-01-13 11:03:32,601 [podnet.py] => Task 1, Epoch 23/160 (LR 0.09499) => LSC_loss 0.77, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 77.42, Test_acc 59.20
2025-01-13 11:03:40,293 [podnet.py] => Task 1, Epoch 24/160 (LR 0.09455) => LSC_loss 0.75, Spatial_loss 1.74, Flat_loss 0.33, Train_acc 78.01, Test_acc 62.45
2025-01-13 11:03:47,831 [podnet.py] => Task 1, Epoch 25/160 (LR 0.09410) => LSC_loss 0.75, Spatial_loss 1.74, Flat_loss 0.33, Train_acc 78.05, Test_acc 59.28
2025-01-13 11:03:55,425 [podnet.py] => Task 1, Epoch 26/160 (LR 0.09362) => LSC_loss 0.75, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 78.22, Test_acc 59.95
2025-01-13 11:04:03,018 [podnet.py] => Task 1, Epoch 27/160 (LR 0.09314) => LSC_loss 0.75, Spatial_loss 1.74, Flat_loss 0.33, Train_acc 77.92, Test_acc 58.85
2025-01-13 11:04:10,767 [podnet.py] => Task 1, Epoch 28/160 (LR 0.09263) => LSC_loss 0.74, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 78.21, Test_acc 63.90
2025-01-13 11:04:17,893 [podnet.py] => Task 1, Epoch 29/160 (LR 0.09211) => LSC_loss 0.74, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 78.10, Test_acc 61.55
2025-01-13 11:04:25,555 [podnet.py] => Task 1, Epoch 30/160 (LR 0.09157) => LSC_loss 0.73, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 78.59, Test_acc 58.25
2025-01-13 11:04:33,390 [podnet.py] => Task 1, Epoch 31/160 (LR 0.09102) => LSC_loss 0.71, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 78.97, Test_acc 54.68
2025-01-13 11:04:40,973 [podnet.py] => Task 1, Epoch 32/160 (LR 0.09045) => LSC_loss 0.71, Spatial_loss 1.74, Flat_loss 0.33, Train_acc 79.67, Test_acc 60.30
2025-01-13 11:04:48,505 [podnet.py] => Task 1, Epoch 33/160 (LR 0.08987) => LSC_loss 0.70, Spatial_loss 1.72, Flat_loss 0.33, Train_acc 79.70, Test_acc 65.47
2025-01-13 11:04:56,041 [podnet.py] => Task 1, Epoch 34/160 (LR 0.08927) => LSC_loss 0.71, Spatial_loss 1.72, Flat_loss 0.33, Train_acc 79.76, Test_acc 61.72
2025-01-13 11:05:03,754 [podnet.py] => Task 1, Epoch 35/160 (LR 0.08865) => LSC_loss 0.70, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 79.60, Test_acc 59.42
2025-01-13 11:05:11,448 [podnet.py] => Task 1, Epoch 36/160 (LR 0.08802) => LSC_loss 0.69, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 79.60, Test_acc 57.88
2025-01-13 11:05:19,008 [podnet.py] => Task 1, Epoch 37/160 (LR 0.08738) => LSC_loss 0.69, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 80.20, Test_acc 62.45
2025-01-13 11:05:26,763 [podnet.py] => Task 1, Epoch 38/160 (LR 0.08672) => LSC_loss 0.69, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 79.72, Test_acc 53.55
2025-01-13 11:05:34,400 [podnet.py] => Task 1, Epoch 39/160 (LR 0.08604) => LSC_loss 0.69, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 79.76, Test_acc 60.85
2025-01-13 11:05:41,915 [podnet.py] => Task 1, Epoch 40/160 (LR 0.08536) => LSC_loss 0.68, Spatial_loss 1.72, Flat_loss 0.33, Train_acc 80.65, Test_acc 61.88
2025-01-13 11:05:49,481 [podnet.py] => Task 1, Epoch 41/160 (LR 0.08465) => LSC_loss 0.67, Spatial_loss 1.72, Flat_loss 0.33, Train_acc 80.50, Test_acc 56.52
2025-01-13 11:05:57,264 [podnet.py] => Task 1, Epoch 42/160 (LR 0.08394) => LSC_loss 0.65, Spatial_loss 1.72, Flat_loss 0.33, Train_acc 80.71, Test_acc 58.80
2025-01-13 11:06:04,914 [podnet.py] => Task 1, Epoch 43/160 (LR 0.08321) => LSC_loss 0.67, Spatial_loss 1.73, Flat_loss 0.33, Train_acc 80.28, Test_acc 63.40
2025-01-13 11:06:12,603 [podnet.py] => Task 1, Epoch 44/160 (LR 0.08247) => LSC_loss 0.66, Spatial_loss 1.71, Flat_loss 0.33, Train_acc 80.54, Test_acc 61.60
2025-01-13 11:06:20,148 [podnet.py] => Task 1, Epoch 45/160 (LR 0.08172) => LSC_loss 0.67, Spatial_loss 1.71, Flat_loss 0.33, Train_acc 80.74, Test_acc 60.80
2025-01-13 11:06:27,806 [podnet.py] => Task 1, Epoch 46/160 (LR 0.08095) => LSC_loss 0.65, Spatial_loss 1.71, Flat_loss 0.33, Train_acc 80.67, Test_acc 61.78
2025-01-13 11:06:35,384 [podnet.py] => Task 1, Epoch 47/160 (LR 0.08018) => LSC_loss 0.64, Spatial_loss 1.71, Flat_loss 0.33, Train_acc 81.47, Test_acc 61.20
2025-01-13 11:06:42,685 [podnet.py] => Task 1, Epoch 48/160 (LR 0.07939) => LSC_loss 0.64, Spatial_loss 1.71, Flat_loss 0.33, Train_acc 80.92, Test_acc 61.42
2025-01-13 11:06:50,020 [podnet.py] => Task 1, Epoch 49/160 (LR 0.07859) => LSC_loss 0.63, Spatial_loss 1.70, Flat_loss 0.33, Train_acc 81.65, Test_acc 61.28
2025-01-13 11:06:57,484 [podnet.py] => Task 1, Epoch 50/160 (LR 0.07778) => LSC_loss 0.63, Spatial_loss 1.70, Flat_loss 0.33, Train_acc 81.59, Test_acc 61.05
2025-01-13 11:07:05,185 [podnet.py] => Task 1, Epoch 51/160 (LR 0.07696) => LSC_loss 0.61, Spatial_loss 1.69, Flat_loss 0.33, Train_acc 82.28, Test_acc 62.08
2025-01-13 11:07:12,652 [podnet.py] => Task 1, Epoch 52/160 (LR 0.07612) => LSC_loss 0.61, Spatial_loss 1.67, Flat_loss 0.32, Train_acc 81.90, Test_acc 63.15
2025-01-13 11:07:19,928 [podnet.py] => Task 1, Epoch 53/160 (LR 0.07528) => LSC_loss 0.63, Spatial_loss 1.69, Flat_loss 0.33, Train_acc 81.34, Test_acc 56.02
2025-01-13 11:07:27,651 [podnet.py] => Task 1, Epoch 54/160 (LR 0.07443) => LSC_loss 0.61, Spatial_loss 1.69, Flat_loss 0.33, Train_acc 82.24, Test_acc 62.92
2025-01-13 11:07:35,280 [podnet.py] => Task 1, Epoch 55/160 (LR 0.07357) => LSC_loss 0.59, Spatial_loss 1.67, Flat_loss 0.32, Train_acc 82.56, Test_acc 64.05
2025-01-13 11:07:43,087 [podnet.py] => Task 1, Epoch 56/160 (LR 0.07270) => LSC_loss 0.60, Spatial_loss 1.68, Flat_loss 0.32, Train_acc 82.66, Test_acc 63.42
2025-01-13 11:07:50,614 [podnet.py] => Task 1, Epoch 57/160 (LR 0.07182) => LSC_loss 0.59, Spatial_loss 1.67, Flat_loss 0.32, Train_acc 82.89, Test_acc 66.30
2025-01-13 11:07:58,278 [podnet.py] => Task 1, Epoch 58/160 (LR 0.07093) => LSC_loss 0.58, Spatial_loss 1.69, Flat_loss 0.33, Train_acc 83.53, Test_acc 60.38
2025-01-13 11:08:06,007 [podnet.py] => Task 1, Epoch 59/160 (LR 0.07004) => LSC_loss 0.58, Spatial_loss 1.67, Flat_loss 0.32, Train_acc 83.19, Test_acc 59.42
2025-01-13 11:08:13,612 [podnet.py] => Task 1, Epoch 60/160 (LR 0.06913) => LSC_loss 0.57, Spatial_loss 1.66, Flat_loss 0.32, Train_acc 83.62, Test_acc 63.45
2025-01-13 11:08:21,458 [podnet.py] => Task 1, Epoch 61/160 (LR 0.06822) => LSC_loss 0.55, Spatial_loss 1.66, Flat_loss 0.32, Train_acc 84.18, Test_acc 64.92
2025-01-13 11:08:29,065 [podnet.py] => Task 1, Epoch 62/160 (LR 0.06731) => LSC_loss 0.58, Spatial_loss 1.65, Flat_loss 0.32, Train_acc 83.17, Test_acc 58.62
2025-01-13 11:08:36,810 [podnet.py] => Task 1, Epoch 63/160 (LR 0.06638) => LSC_loss 0.55, Spatial_loss 1.64, Flat_loss 0.32, Train_acc 84.03, Test_acc 65.00
2025-01-13 11:08:44,476 [podnet.py] => Task 1, Epoch 64/160 (LR 0.06545) => LSC_loss 0.55, Spatial_loss 1.64, Flat_loss 0.32, Train_acc 84.20, Test_acc 62.82
2025-01-13 11:08:51,974 [podnet.py] => Task 1, Epoch 65/160 (LR 0.06451) => LSC_loss 0.54, Spatial_loss 1.63, Flat_loss 0.32, Train_acc 84.51, Test_acc 62.18
2025-01-13 11:08:59,695 [podnet.py] => Task 1, Epoch 66/160 (LR 0.06357) => LSC_loss 0.55, Spatial_loss 1.64, Flat_loss 0.32, Train_acc 84.21, Test_acc 63.02
2025-01-13 11:09:07,377 [podnet.py] => Task 1, Epoch 67/160 (LR 0.06262) => LSC_loss 0.56, Spatial_loss 1.63, Flat_loss 0.32, Train_acc 83.88, Test_acc 59.75
2025-01-13 11:09:14,935 [podnet.py] => Task 1, Epoch 68/160 (LR 0.06167) => LSC_loss 0.54, Spatial_loss 1.64, Flat_loss 0.32, Train_acc 84.09, Test_acc 65.55
2025-01-13 11:09:22,685 [podnet.py] => Task 1, Epoch 69/160 (LR 0.06072) => LSC_loss 0.53, Spatial_loss 1.63, Flat_loss 0.32, Train_acc 84.57, Test_acc 65.40
2025-01-13 11:09:30,115 [podnet.py] => Task 1, Epoch 70/160 (LR 0.05975) => LSC_loss 0.52, Spatial_loss 1.62, Flat_loss 0.32, Train_acc 85.03, Test_acc 63.02
2025-01-13 11:09:37,712 [podnet.py] => Task 1, Epoch 71/160 (LR 0.05879) => LSC_loss 0.53, Spatial_loss 1.62, Flat_loss 0.32, Train_acc 84.53, Test_acc 64.42
2025-01-13 11:09:45,386 [podnet.py] => Task 1, Epoch 72/160 (LR 0.05782) => LSC_loss 0.51, Spatial_loss 1.61, Flat_loss 0.31, Train_acc 85.19, Test_acc 64.35
2025-01-13 11:09:52,811 [podnet.py] => Task 1, Epoch 73/160 (LR 0.05685) => LSC_loss 0.50, Spatial_loss 1.61, Flat_loss 0.32, Train_acc 85.44, Test_acc 65.22
2025-01-13 11:10:00,527 [podnet.py] => Task 1, Epoch 74/160 (LR 0.05588) => LSC_loss 0.51, Spatial_loss 1.61, Flat_loss 0.32, Train_acc 85.29, Test_acc 64.03
2025-01-13 11:10:08,082 [podnet.py] => Task 1, Epoch 75/160 (LR 0.05490) => LSC_loss 0.49, Spatial_loss 1.60, Flat_loss 0.31, Train_acc 85.59, Test_acc 64.62
2025-01-13 11:10:15,444 [podnet.py] => Task 1, Epoch 76/160 (LR 0.05392) => LSC_loss 0.50, Spatial_loss 1.61, Flat_loss 0.31, Train_acc 85.62, Test_acc 65.65
2025-01-13 11:10:22,927 [podnet.py] => Task 1, Epoch 77/160 (LR 0.05294) => LSC_loss 0.48, Spatial_loss 1.58, Flat_loss 0.31, Train_acc 85.93, Test_acc 63.95
2025-01-13 11:10:30,368 [podnet.py] => Task 1, Epoch 78/160 (LR 0.05196) => LSC_loss 0.49, Spatial_loss 1.60, Flat_loss 0.32, Train_acc 85.30, Test_acc 62.88
2025-01-13 11:10:38,057 [podnet.py] => Task 1, Epoch 79/160 (LR 0.05098) => LSC_loss 0.47, Spatial_loss 1.58, Flat_loss 0.31, Train_acc 86.60, Test_acc 65.65
2025-01-13 11:10:45,464 [podnet.py] => Task 1, Epoch 80/160 (LR 0.05000) => LSC_loss 0.45, Spatial_loss 1.57, Flat_loss 0.31, Train_acc 87.17, Test_acc 65.30
2025-01-13 11:10:53,122 [podnet.py] => Task 1, Epoch 81/160 (LR 0.04902) => LSC_loss 0.45, Spatial_loss 1.56, Flat_loss 0.31, Train_acc 87.19, Test_acc 64.78
2025-01-13 11:11:00,798 [podnet.py] => Task 1, Epoch 82/160 (LR 0.04804) => LSC_loss 0.45, Spatial_loss 1.55, Flat_loss 0.31, Train_acc 87.07, Test_acc 64.47
2025-01-13 11:11:08,323 [podnet.py] => Task 1, Epoch 83/160 (LR 0.04706) => LSC_loss 0.44, Spatial_loss 1.56, Flat_loss 0.31, Train_acc 87.19, Test_acc 65.90
2025-01-13 11:11:15,939 [podnet.py] => Task 1, Epoch 84/160 (LR 0.04608) => LSC_loss 0.43, Spatial_loss 1.54, Flat_loss 0.30, Train_acc 87.54, Test_acc 64.78
2025-01-13 11:11:23,385 [podnet.py] => Task 1, Epoch 85/160 (LR 0.04510) => LSC_loss 0.44, Spatial_loss 1.54, Flat_loss 0.30, Train_acc 87.20, Test_acc 64.90
2025-01-13 11:11:31,111 [podnet.py] => Task 1, Epoch 86/160 (LR 0.04412) => LSC_loss 0.44, Spatial_loss 1.54, Flat_loss 0.30, Train_acc 87.10, Test_acc 63.95
2025-01-13 11:11:38,556 [podnet.py] => Task 1, Epoch 87/160 (LR 0.04315) => LSC_loss 0.43, Spatial_loss 1.55, Flat_loss 0.31, Train_acc 87.73, Test_acc 64.03
2025-01-13 11:11:46,074 [podnet.py] => Task 1, Epoch 88/160 (LR 0.04218) => LSC_loss 0.41, Spatial_loss 1.53, Flat_loss 0.30, Train_acc 88.18, Test_acc 66.85
2025-01-13 11:11:53,895 [podnet.py] => Task 1, Epoch 89/160 (LR 0.04121) => LSC_loss 0.40, Spatial_loss 1.52, Flat_loss 0.30, Train_acc 88.73, Test_acc 66.78
2025-01-13 11:12:01,444 [podnet.py] => Task 1, Epoch 90/160 (LR 0.04025) => LSC_loss 0.41, Spatial_loss 1.52, Flat_loss 0.30, Train_acc 88.42, Test_acc 63.55
2025-01-13 11:12:09,063 [podnet.py] => Task 1, Epoch 91/160 (LR 0.03928) => LSC_loss 0.39, Spatial_loss 1.51, Flat_loss 0.30, Train_acc 88.79, Test_acc 65.70
2025-01-13 11:12:16,680 [podnet.py] => Task 1, Epoch 92/160 (LR 0.03833) => LSC_loss 0.40, Spatial_loss 1.50, Flat_loss 0.30, Train_acc 88.72, Test_acc 66.32
2025-01-13 11:12:24,654 [podnet.py] => Task 1, Epoch 93/160 (LR 0.03738) => LSC_loss 0.39, Spatial_loss 1.50, Flat_loss 0.30, Train_acc 88.62, Test_acc 66.03
2025-01-13 11:12:32,277 [podnet.py] => Task 1, Epoch 94/160 (LR 0.03643) => LSC_loss 0.38, Spatial_loss 1.50, Flat_loss 0.30, Train_acc 89.12, Test_acc 65.78
2025-01-13 11:12:39,914 [podnet.py] => Task 1, Epoch 95/160 (LR 0.03549) => LSC_loss 0.38, Spatial_loss 1.48, Flat_loss 0.29, Train_acc 89.13, Test_acc 66.25
2025-01-13 11:12:47,399 [podnet.py] => Task 1, Epoch 96/160 (LR 0.03455) => LSC_loss 0.36, Spatial_loss 1.46, Flat_loss 0.29, Train_acc 90.13, Test_acc 67.08
2025-01-13 11:12:55,118 [podnet.py] => Task 1, Epoch 97/160 (LR 0.03362) => LSC_loss 0.36, Spatial_loss 1.46, Flat_loss 0.29, Train_acc 89.50, Test_acc 67.18
2025-01-13 11:13:02,814 [podnet.py] => Task 1, Epoch 98/160 (LR 0.03269) => LSC_loss 0.35, Spatial_loss 1.46, Flat_loss 0.29, Train_acc 90.09, Test_acc 67.12
2025-01-13 11:13:10,190 [podnet.py] => Task 1, Epoch 99/160 (LR 0.03178) => LSC_loss 0.34, Spatial_loss 1.44, Flat_loss 0.29, Train_acc 90.58, Test_acc 67.47
2025-01-13 11:13:17,737 [podnet.py] => Task 1, Epoch 100/160 (LR 0.03087) => LSC_loss 0.34, Spatial_loss 1.43, Flat_loss 0.28, Train_acc 90.89, Test_acc 67.32
2025-01-13 11:13:25,266 [podnet.py] => Task 1, Epoch 101/160 (LR 0.02996) => LSC_loss 0.33, Spatial_loss 1.44, Flat_loss 0.29, Train_acc 90.74, Test_acc 66.78
2025-01-13 11:13:32,964 [podnet.py] => Task 1, Epoch 102/160 (LR 0.02907) => LSC_loss 0.32, Spatial_loss 1.41, Flat_loss 0.28, Train_acc 90.98, Test_acc 67.82
2025-01-13 11:13:40,685 [podnet.py] => Task 1, Epoch 103/160 (LR 0.02818) => LSC_loss 0.32, Spatial_loss 1.42, Flat_loss 0.28, Train_acc 91.10, Test_acc 68.03
2025-01-13 11:13:48,284 [podnet.py] => Task 1, Epoch 104/160 (LR 0.02730) => LSC_loss 0.31, Spatial_loss 1.41, Flat_loss 0.28, Train_acc 91.38, Test_acc 66.28
2025-01-13 11:13:55,939 [podnet.py] => Task 1, Epoch 105/160 (LR 0.02643) => LSC_loss 0.31, Spatial_loss 1.40, Flat_loss 0.28, Train_acc 91.41, Test_acc 66.75
2025-01-13 11:14:03,557 [podnet.py] => Task 1, Epoch 106/160 (LR 0.02557) => LSC_loss 0.29, Spatial_loss 1.39, Flat_loss 0.28, Train_acc 91.85, Test_acc 67.80
2025-01-13 11:14:11,018 [podnet.py] => Task 1, Epoch 107/160 (LR 0.02472) => LSC_loss 0.30, Spatial_loss 1.39, Flat_loss 0.28, Train_acc 91.87, Test_acc 67.88
2025-01-13 11:14:18,653 [podnet.py] => Task 1, Epoch 108/160 (LR 0.02388) => LSC_loss 0.29, Spatial_loss 1.38, Flat_loss 0.28, Train_acc 92.32, Test_acc 67.95
2025-01-13 11:14:26,523 [podnet.py] => Task 1, Epoch 109/160 (LR 0.02304) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.27, Train_acc 92.50, Test_acc 67.72
2025-01-13 11:14:34,384 [podnet.py] => Task 1, Epoch 110/160 (LR 0.02222) => LSC_loss 0.29, Spatial_loss 1.37, Flat_loss 0.27, Train_acc 92.05, Test_acc 67.62
2025-01-13 11:14:41,999 [podnet.py] => Task 1, Epoch 111/160 (LR 0.02141) => LSC_loss 0.28, Spatial_loss 1.36, Flat_loss 0.27, Train_acc 92.70, Test_acc 67.92
2025-01-13 11:14:49,577 [podnet.py] => Task 1, Epoch 112/160 (LR 0.02061) => LSC_loss 0.26, Spatial_loss 1.34, Flat_loss 0.27, Train_acc 92.92, Test_acc 67.60
2025-01-13 11:14:57,339 [podnet.py] => Task 1, Epoch 113/160 (LR 0.01982) => LSC_loss 0.26, Spatial_loss 1.33, Flat_loss 0.27, Train_acc 93.20, Test_acc 68.20
2025-01-13 11:15:05,059 [podnet.py] => Task 1, Epoch 114/160 (LR 0.01905) => LSC_loss 0.25, Spatial_loss 1.31, Flat_loss 0.27, Train_acc 93.37, Test_acc 68.53
2025-01-13 11:15:12,760 [podnet.py] => Task 1, Epoch 115/160 (LR 0.01828) => LSC_loss 0.25, Spatial_loss 1.31, Flat_loss 0.26, Train_acc 93.57, Test_acc 67.82
2025-01-13 11:15:20,308 [podnet.py] => Task 1, Epoch 116/160 (LR 0.01753) => LSC_loss 0.25, Spatial_loss 1.29, Flat_loss 0.26, Train_acc 93.58, Test_acc 67.35
2025-01-13 11:15:27,756 [podnet.py] => Task 1, Epoch 117/160 (LR 0.01679) => LSC_loss 0.24, Spatial_loss 1.30, Flat_loss 0.26, Train_acc 93.72, Test_acc 64.47
2025-01-13 11:15:35,618 [podnet.py] => Task 1, Epoch 118/160 (LR 0.01606) => LSC_loss 0.24, Spatial_loss 1.29, Flat_loss 0.26, Train_acc 93.87, Test_acc 69.20
2025-01-13 11:15:43,147 [podnet.py] => Task 1, Epoch 119/160 (LR 0.01535) => LSC_loss 0.24, Spatial_loss 1.29, Flat_loss 0.26, Train_acc 94.00, Test_acc 67.75
2025-01-13 11:15:50,812 [podnet.py] => Task 1, Epoch 120/160 (LR 0.01464) => LSC_loss 0.23, Spatial_loss 1.27, Flat_loss 0.26, Train_acc 94.14, Test_acc 68.45
2025-01-13 11:15:58,514 [podnet.py] => Task 1, Epoch 121/160 (LR 0.01396) => LSC_loss 0.24, Spatial_loss 1.27, Flat_loss 0.26, Train_acc 94.15, Test_acc 67.97
2025-01-13 11:16:06,082 [podnet.py] => Task 1, Epoch 122/160 (LR 0.01328) => LSC_loss 0.22, Spatial_loss 1.26, Flat_loss 0.26, Train_acc 94.41, Test_acc 68.30
2025-01-13 11:16:13,808 [podnet.py] => Task 1, Epoch 123/160 (LR 0.01262) => LSC_loss 0.22, Spatial_loss 1.24, Flat_loss 0.25, Train_acc 94.60, Test_acc 68.92
2025-01-13 11:16:21,447 [podnet.py] => Task 1, Epoch 124/160 (LR 0.01198) => LSC_loss 0.21, Spatial_loss 1.24, Flat_loss 0.25, Train_acc 94.78, Test_acc 67.82
2025-01-13 11:16:29,330 [podnet.py] => Task 1, Epoch 125/160 (LR 0.01135) => LSC_loss 0.20, Spatial_loss 1.23, Flat_loss 0.25, Train_acc 95.24, Test_acc 68.65
2025-01-13 11:16:37,066 [podnet.py] => Task 1, Epoch 126/160 (LR 0.01073) => LSC_loss 0.21, Spatial_loss 1.22, Flat_loss 0.25, Train_acc 95.04, Test_acc 69.35
2025-01-13 11:16:44,865 [podnet.py] => Task 1, Epoch 127/160 (LR 0.01013) => LSC_loss 0.21, Spatial_loss 1.23, Flat_loss 0.25, Train_acc 95.08, Test_acc 68.45
2025-01-13 11:16:52,763 [podnet.py] => Task 1, Epoch 128/160 (LR 0.00955) => LSC_loss 0.20, Spatial_loss 1.20, Flat_loss 0.24, Train_acc 95.34, Test_acc 69.55
2025-01-13 11:17:00,572 [podnet.py] => Task 1, Epoch 129/160 (LR 0.00898) => LSC_loss 0.19, Spatial_loss 1.21, Flat_loss 0.24, Train_acc 95.54, Test_acc 69.45
2025-01-13 11:17:07,790 [podnet.py] => Task 1, Epoch 130/160 (LR 0.00843) => LSC_loss 0.20, Spatial_loss 1.19, Flat_loss 0.24, Train_acc 95.45, Test_acc 69.82
2025-01-13 11:17:15,332 [podnet.py] => Task 1, Epoch 131/160 (LR 0.00789) => LSC_loss 0.19, Spatial_loss 1.20, Flat_loss 0.24, Train_acc 95.65, Test_acc 69.15
2025-01-13 11:17:22,953 [podnet.py] => Task 1, Epoch 132/160 (LR 0.00737) => LSC_loss 0.19, Spatial_loss 1.19, Flat_loss 0.24, Train_acc 95.43, Test_acc 69.22
2025-01-13 11:17:30,591 [podnet.py] => Task 1, Epoch 133/160 (LR 0.00686) => LSC_loss 0.19, Spatial_loss 1.17, Flat_loss 0.24, Train_acc 95.74, Test_acc 69.92
2025-01-13 11:17:38,208 [podnet.py] => Task 1, Epoch 134/160 (LR 0.00638) => LSC_loss 0.19, Spatial_loss 1.17, Flat_loss 0.24, Train_acc 95.76, Test_acc 69.75
2025-01-13 11:17:46,078 [podnet.py] => Task 1, Epoch 135/160 (LR 0.00590) => LSC_loss 0.18, Spatial_loss 1.16, Flat_loss 0.24, Train_acc 95.99, Test_acc 69.47
2025-01-13 11:17:53,701 [podnet.py] => Task 1, Epoch 136/160 (LR 0.00545) => LSC_loss 0.18, Spatial_loss 1.16, Flat_loss 0.24, Train_acc 96.17, Test_acc 69.50
2025-01-13 11:18:00,981 [podnet.py] => Task 1, Epoch 137/160 (LR 0.00501) => LSC_loss 0.18, Spatial_loss 1.16, Flat_loss 0.24, Train_acc 96.03, Test_acc 70.30
2025-01-13 11:18:08,625 [podnet.py] => Task 1, Epoch 138/160 (LR 0.00459) => LSC_loss 0.18, Spatial_loss 1.15, Flat_loss 0.24, Train_acc 96.12, Test_acc 70.12
2025-01-13 11:18:16,121 [podnet.py] => Task 1, Epoch 139/160 (LR 0.00419) => LSC_loss 0.17, Spatial_loss 1.14, Flat_loss 0.23, Train_acc 96.42, Test_acc 70.12
2025-01-13 11:18:23,724 [podnet.py] => Task 1, Epoch 140/160 (LR 0.00381) => LSC_loss 0.17, Spatial_loss 1.13, Flat_loss 0.23, Train_acc 96.32, Test_acc 70.03
2025-01-13 11:18:31,582 [podnet.py] => Task 1, Epoch 141/160 (LR 0.00344) => LSC_loss 0.17, Spatial_loss 1.13, Flat_loss 0.23, Train_acc 96.38, Test_acc 69.88
2025-01-13 11:18:39,290 [podnet.py] => Task 1, Epoch 142/160 (LR 0.00309) => LSC_loss 0.16, Spatial_loss 1.13, Flat_loss 0.23, Train_acc 96.72, Test_acc 70.00
2025-01-13 11:18:46,916 [podnet.py] => Task 1, Epoch 143/160 (LR 0.00276) => LSC_loss 0.17, Spatial_loss 1.12, Flat_loss 0.23, Train_acc 96.62, Test_acc 69.82
2025-01-13 11:18:54,718 [podnet.py] => Task 1, Epoch 144/160 (LR 0.00245) => LSC_loss 0.17, Spatial_loss 1.11, Flat_loss 0.23, Train_acc 96.50, Test_acc 69.88
2025-01-13 11:19:02,466 [podnet.py] => Task 1, Epoch 145/160 (LR 0.00215) => LSC_loss 0.16, Spatial_loss 1.11, Flat_loss 0.23, Train_acc 96.89, Test_acc 70.18
2025-01-13 11:19:10,120 [podnet.py] => Task 1, Epoch 146/160 (LR 0.00188) => LSC_loss 0.16, Spatial_loss 1.10, Flat_loss 0.23, Train_acc 96.96, Test_acc 69.80
2025-01-13 11:19:17,736 [podnet.py] => Task 1, Epoch 147/160 (LR 0.00162) => LSC_loss 0.16, Spatial_loss 1.10, Flat_loss 0.23, Train_acc 96.90, Test_acc 70.35
2025-01-13 11:19:25,333 [podnet.py] => Task 1, Epoch 148/160 (LR 0.00138) => LSC_loss 0.16, Spatial_loss 1.11, Flat_loss 0.23, Train_acc 96.83, Test_acc 69.95
2025-01-13 11:19:32,964 [podnet.py] => Task 1, Epoch 149/160 (LR 0.00116) => LSC_loss 0.15, Spatial_loss 1.10, Flat_loss 0.23, Train_acc 97.05, Test_acc 69.95
2025-01-13 11:19:40,526 [podnet.py] => Task 1, Epoch 150/160 (LR 0.00096) => LSC_loss 0.16, Spatial_loss 1.10, Flat_loss 0.22, Train_acc 96.90, Test_acc 70.32
2025-01-13 11:19:48,255 [podnet.py] => Task 1, Epoch 151/160 (LR 0.00078) => LSC_loss 0.15, Spatial_loss 1.09, Flat_loss 0.23, Train_acc 97.24, Test_acc 70.50
2025-01-13 11:19:55,733 [podnet.py] => Task 1, Epoch 152/160 (LR 0.00062) => LSC_loss 0.16, Spatial_loss 1.09, Flat_loss 0.23, Train_acc 96.82, Test_acc 70.38
2025-01-13 11:20:03,234 [podnet.py] => Task 1, Epoch 153/160 (LR 0.00047) => LSC_loss 0.16, Spatial_loss 1.09, Flat_loss 0.22, Train_acc 96.92, Test_acc 70.18
2025-01-13 11:20:10,827 [podnet.py] => Task 1, Epoch 154/160 (LR 0.00035) => LSC_loss 0.15, Spatial_loss 1.08, Flat_loss 0.22, Train_acc 96.96, Test_acc 70.45
2025-01-13 11:20:18,531 [podnet.py] => Task 1, Epoch 155/160 (LR 0.00024) => LSC_loss 0.15, Spatial_loss 1.09, Flat_loss 0.23, Train_acc 97.07, Test_acc 70.03
2025-01-13 11:20:26,124 [podnet.py] => Task 1, Epoch 156/160 (LR 0.00015) => LSC_loss 0.16, Spatial_loss 1.09, Flat_loss 0.22, Train_acc 96.85, Test_acc 70.30
2025-01-13 11:20:33,777 [podnet.py] => Task 1, Epoch 157/160 (LR 0.00009) => LSC_loss 0.15, Spatial_loss 1.08, Flat_loss 0.23, Train_acc 97.18, Test_acc 70.28
2025-01-13 11:20:41,261 [podnet.py] => Task 1, Epoch 158/160 (LR 0.00004) => LSC_loss 0.15, Spatial_loss 1.08, Flat_loss 0.22, Train_acc 97.16, Test_acc 70.45
2025-01-13 11:20:48,858 [podnet.py] => Task 1, Epoch 159/160 (LR 0.00001) => LSC_loss 0.15, Spatial_loss 1.09, Flat_loss 0.22, Train_acc 97.09, Test_acc 70.50
2025-01-13 11:20:56,383 [podnet.py] => Task 1, Epoch 160/160 (LR 0.00000) => LSC_loss 0.16, Spatial_loss 1.08, Flat_loss 0.22, Train_acc 97.01, Test_acc 69.90
2025-01-13 11:20:56,384 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-13 11:20:56,384 [base.py] => Reducing exemplars...(220 per classes)
2025-01-13 11:21:13,069 [base.py] => Constructing exemplars...(220 per classes)
2025-01-13 11:21:49,364 [podnet.py] => The size of finetune dataset: 8800
2025-01-13 11:21:54,562 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.14, Spatial_loss 1.07, Flat_loss 0.19, Train_acc 97.27, Test_acc 69.20
2025-01-13 11:21:59,952 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.14, Spatial_loss 1.08, Flat_loss 0.19, Train_acc 96.88, Test_acc 69.08
2025-01-13 11:22:05,330 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.15, Spatial_loss 1.08, Flat_loss 0.19, Train_acc 96.49, Test_acc 69.08
2025-01-13 11:22:11,001 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.15, Spatial_loss 1.08, Flat_loss 0.19, Train_acc 96.70, Test_acc 69.40
2025-01-13 11:22:16,468 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.14, Spatial_loss 1.08, Flat_loss 0.18, Train_acc 96.95, Test_acc 69.40
2025-01-13 11:22:22,017 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.14, Spatial_loss 1.07, Flat_loss 0.18, Train_acc 96.83, Test_acc 69.62
2025-01-13 11:22:27,304 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.14, Spatial_loss 1.08, Flat_loss 0.18, Train_acc 97.18, Test_acc 69.08
2025-01-13 11:22:32,613 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.12, Spatial_loss 1.07, Flat_loss 0.18, Train_acc 97.56, Test_acc 69.75
2025-01-13 11:22:38,061 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.14, Spatial_loss 1.06, Flat_loss 0.18, Train_acc 96.97, Test_acc 69.95
2025-01-13 11:22:43,511 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.13, Spatial_loss 1.05, Flat_loss 0.18, Train_acc 97.24, Test_acc 69.15
2025-01-13 11:22:49,093 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.13, Spatial_loss 1.05, Flat_loss 0.18, Train_acc 97.30, Test_acc 69.10
2025-01-13 11:22:54,519 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.13, Spatial_loss 1.04, Flat_loss 0.18, Train_acc 97.19, Test_acc 69.42
2025-01-13 11:22:59,847 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.13, Spatial_loss 1.04, Flat_loss 0.18, Train_acc 97.23, Test_acc 69.58
2025-01-13 11:23:05,271 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.12, Spatial_loss 1.03, Flat_loss 0.18, Train_acc 97.64, Test_acc 69.60
2025-01-13 11:23:10,766 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.13, Spatial_loss 1.04, Flat_loss 0.18, Train_acc 97.40, Test_acc 69.60
2025-01-13 11:23:16,409 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.12, Spatial_loss 1.03, Flat_loss 0.18, Train_acc 97.61, Test_acc 69.82
2025-01-13 11:23:21,810 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.12, Spatial_loss 1.03, Flat_loss 0.18, Train_acc 97.75, Test_acc 69.68
2025-01-13 11:23:27,366 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.11, Spatial_loss 1.03, Flat_loss 0.18, Train_acc 97.89, Test_acc 69.68
2025-01-13 11:23:32,906 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.12, Spatial_loss 1.03, Flat_loss 0.18, Train_acc 97.75, Test_acc 69.62
2025-01-13 11:23:38,171 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.12, Spatial_loss 1.02, Flat_loss 0.18, Train_acc 97.72, Test_acc 69.32
2025-01-13 11:23:38,172 [base.py] => Reducing exemplars...(110 per classes)
2025-01-13 11:23:55,641 [base.py] => Constructing exemplars...(110 per classes)
2025-01-13 11:24:33,852 [podnet.py] => Exemplar size: 4400
2025-01-13 11:24:33,852 [trainer.py] => CNN: {'total': np.float64(69.32), '00-09': np.float64(79.7), '10-19': np.float64(69.1), '20-29': np.float64(65.7), '30-39': np.float64(62.8), 'old': np.float64(74.4), 'new': np.float64(64.25)}
2025-01-13 11:24:33,852 [trainer.py] => NME: {'total': np.float64(69.08), '00-09': np.float64(79.4), '10-19': np.float64(68.5), '20-29': np.float64(66.3), '30-39': np.float64(62.1), 'old': np.float64(73.95), 'new': np.float64(64.2)}
2025-01-13 11:24:33,852 [trainer.py] => CNN top1 curve: [np.float64(82.45), np.float64(69.32)]
2025-01-13 11:24:33,852 [trainer.py] => CNN top5 curve: [np.float64(96.4), np.float64(91.95)]
2025-01-13 11:24:33,852 [trainer.py] => NME top1 curve: [np.float64(82.05), np.float64(69.08)]
2025-01-13 11:24:33,852 [trainer.py] => NME top5 curve: [np.float64(96.2), np.float64(91.62)]

2025-01-13 11:24:33,853 [trainer.py] => All params: 491857
2025-01-13 11:24:33,853 [trainer.py] => Trainable params: 491857
2025-01-13 11:24:33,854 [podnet.py] => Learning on 40-60
2025-01-13 11:24:33,987 [podnet.py] => Adaptive factor: 1.7320508075688772
2025-01-13 11:24:41,890 [podnet.py] => Task 2, Epoch 1/160 (LR 0.09999) => LSC_loss 2.18, Spatial_loss 2.57, Flat_loss 0.65, Train_acc 46.12, Test_acc 42.17
2025-01-13 11:24:49,695 [podnet.py] => Task 2, Epoch 2/160 (LR 0.09996) => LSC_loss 1.46, Spatial_loss 2.18, Flat_loss 0.40, Train_acc 59.33, Test_acc 47.73
2025-01-13 11:24:57,422 [podnet.py] => Task 2, Epoch 3/160 (LR 0.09991) => LSC_loss 1.32, Spatial_loss 2.06, Flat_loss 0.34, Train_acc 63.49, Test_acc 48.42
2025-01-13 11:25:04,948 [podnet.py] => Task 2, Epoch 4/160 (LR 0.09985) => LSC_loss 1.25, Spatial_loss 2.03, Flat_loss 0.33, Train_acc 65.16, Test_acc 51.98
2025-01-13 11:25:12,581 [podnet.py] => Task 2, Epoch 5/160 (LR 0.09976) => LSC_loss 1.19, Spatial_loss 1.97, Flat_loss 0.32, Train_acc 66.65, Test_acc 48.72
2025-01-13 11:25:20,284 [podnet.py] => Task 2, Epoch 6/160 (LR 0.09965) => LSC_loss 1.15, Spatial_loss 1.92, Flat_loss 0.31, Train_acc 68.10, Test_acc 48.82
2025-01-13 11:25:27,864 [podnet.py] => Task 2, Epoch 7/160 (LR 0.09953) => LSC_loss 1.11, Spatial_loss 1.92, Flat_loss 0.30, Train_acc 68.91, Test_acc 48.90
2025-01-13 11:25:35,488 [podnet.py] => Task 2, Epoch 8/160 (LR 0.09938) => LSC_loss 1.11, Spatial_loss 1.91, Flat_loss 0.30, Train_acc 68.44, Test_acc 51.42
2025-01-13 11:25:43,300 [podnet.py] => Task 2, Epoch 9/160 (LR 0.09922) => LSC_loss 1.07, Spatial_loss 1.91, Flat_loss 0.30, Train_acc 70.32, Test_acc 48.12
2025-01-13 11:25:50,818 [podnet.py] => Task 2, Epoch 10/160 (LR 0.09904) => LSC_loss 1.04, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 70.95, Test_acc 50.38
2025-01-13 11:25:58,297 [podnet.py] => Task 2, Epoch 11/160 (LR 0.09884) => LSC_loss 1.02, Spatial_loss 1.89, Flat_loss 0.30, Train_acc 71.36, Test_acc 51.48
2025-01-13 11:26:05,982 [podnet.py] => Task 2, Epoch 12/160 (LR 0.09862) => LSC_loss 1.00, Spatial_loss 1.90, Flat_loss 0.30, Train_acc 71.77, Test_acc 49.12
2025-01-13 11:26:13,793 [podnet.py] => Task 2, Epoch 13/160 (LR 0.09838) => LSC_loss 0.99, Spatial_loss 1.89, Flat_loss 0.30, Train_acc 72.26, Test_acc 51.40
2025-01-13 11:26:21,454 [podnet.py] => Task 2, Epoch 14/160 (LR 0.09812) => LSC_loss 0.98, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 72.25, Test_acc 53.32
2025-01-13 11:26:29,394 [podnet.py] => Task 2, Epoch 15/160 (LR 0.09785) => LSC_loss 0.97, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 73.01, Test_acc 54.92
2025-01-13 11:26:37,032 [podnet.py] => Task 2, Epoch 16/160 (LR 0.09755) => LSC_loss 0.96, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 72.95, Test_acc 50.22
2025-01-13 11:26:44,988 [podnet.py] => Task 2, Epoch 17/160 (LR 0.09724) => LSC_loss 0.95, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 73.14, Test_acc 50.80
2025-01-13 11:26:52,785 [podnet.py] => Task 2, Epoch 18/160 (LR 0.09691) => LSC_loss 0.93, Spatial_loss 1.86, Flat_loss 0.30, Train_acc 73.88, Test_acc 46.78
2025-01-13 11:26:59,890 [podnet.py] => Task 2, Epoch 19/160 (LR 0.09656) => LSC_loss 0.93, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 73.73, Test_acc 48.95
2025-01-13 11:27:07,562 [podnet.py] => Task 2, Epoch 20/160 (LR 0.09619) => LSC_loss 0.92, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 73.97, Test_acc 48.93
2025-01-13 11:27:15,246 [podnet.py] => Task 2, Epoch 21/160 (LR 0.09581) => LSC_loss 0.92, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 74.26, Test_acc 52.13
2025-01-13 11:27:23,069 [podnet.py] => Task 2, Epoch 22/160 (LR 0.09541) => LSC_loss 0.92, Spatial_loss 1.89, Flat_loss 0.31, Train_acc 74.08, Test_acc 50.97
2025-01-13 11:27:30,749 [podnet.py] => Task 2, Epoch 23/160 (LR 0.09499) => LSC_loss 0.88, Spatial_loss 1.86, Flat_loss 0.30, Train_acc 75.14, Test_acc 53.58
2025-01-13 11:27:38,477 [podnet.py] => Task 2, Epoch 24/160 (LR 0.09455) => LSC_loss 0.88, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 74.94, Test_acc 53.13
2025-01-13 11:27:46,214 [podnet.py] => Task 2, Epoch 25/160 (LR 0.09410) => LSC_loss 0.88, Spatial_loss 1.88, Flat_loss 0.31, Train_acc 75.17, Test_acc 48.53
2025-01-13 11:27:53,892 [podnet.py] => Task 2, Epoch 26/160 (LR 0.09362) => LSC_loss 0.87, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 75.15, Test_acc 48.37
2025-01-13 11:28:01,645 [podnet.py] => Task 2, Epoch 27/160 (LR 0.09314) => LSC_loss 0.88, Spatial_loss 1.87, Flat_loss 0.31, Train_acc 74.77, Test_acc 54.43
2025-01-13 11:28:08,839 [podnet.py] => Task 2, Epoch 28/160 (LR 0.09263) => LSC_loss 0.87, Spatial_loss 1.88, Flat_loss 0.31, Train_acc 75.12, Test_acc 52.07
2025-01-13 11:28:16,919 [podnet.py] => Task 2, Epoch 29/160 (LR 0.09211) => LSC_loss 0.87, Spatial_loss 1.88, Flat_loss 0.30, Train_acc 75.18, Test_acc 52.73
2025-01-13 11:28:24,186 [podnet.py] => Task 2, Epoch 30/160 (LR 0.09157) => LSC_loss 0.86, Spatial_loss 1.87, Flat_loss 0.31, Train_acc 75.56, Test_acc 52.93
2025-01-13 11:28:31,945 [podnet.py] => Task 2, Epoch 31/160 (LR 0.09102) => LSC_loss 0.86, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 75.42, Test_acc 53.83
2025-01-13 11:28:39,792 [podnet.py] => Task 2, Epoch 32/160 (LR 0.09045) => LSC_loss 0.85, Spatial_loss 1.88, Flat_loss 0.31, Train_acc 75.99, Test_acc 51.62
2025-01-13 11:28:47,383 [podnet.py] => Task 2, Epoch 33/160 (LR 0.08987) => LSC_loss 0.85, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 75.79, Test_acc 52.78
2025-01-13 11:28:55,262 [podnet.py] => Task 2, Epoch 34/160 (LR 0.08927) => LSC_loss 0.85, Spatial_loss 1.86, Flat_loss 0.30, Train_acc 76.13, Test_acc 50.72
2025-01-13 11:29:02,919 [podnet.py] => Task 2, Epoch 35/160 (LR 0.08865) => LSC_loss 0.81, Spatial_loss 1.86, Flat_loss 0.30, Train_acc 77.03, Test_acc 51.13
2025-01-13 11:29:10,806 [podnet.py] => Task 2, Epoch 36/160 (LR 0.08802) => LSC_loss 0.82, Spatial_loss 1.85, Flat_loss 0.30, Train_acc 77.09, Test_acc 51.02
2025-01-13 11:29:18,610 [podnet.py] => Task 2, Epoch 37/160 (LR 0.08738) => LSC_loss 0.85, Spatial_loss 1.88, Flat_loss 0.31, Train_acc 75.87, Test_acc 47.63
2025-01-13 11:29:26,460 [podnet.py] => Task 2, Epoch 38/160 (LR 0.08672) => LSC_loss 0.82, Spatial_loss 1.87, Flat_loss 0.30, Train_acc 76.99, Test_acc 52.05
2025-01-13 11:29:34,157 [podnet.py] => Task 2, Epoch 39/160 (LR 0.08604) => LSC_loss 0.80, Spatial_loss 1.85, Flat_loss 0.30, Train_acc 77.49, Test_acc 51.20
2025-01-13 11:29:41,948 [podnet.py] => Task 2, Epoch 40/160 (LR 0.08536) => LSC_loss 0.80, Spatial_loss 1.83, Flat_loss 0.30, Train_acc 77.35, Test_acc 51.55
2025-01-13 11:29:49,713 [podnet.py] => Task 2, Epoch 41/160 (LR 0.08465) => LSC_loss 0.81, Spatial_loss 1.86, Flat_loss 0.31, Train_acc 76.92, Test_acc 50.62
2025-01-13 11:29:57,455 [podnet.py] => Task 2, Epoch 42/160 (LR 0.08394) => LSC_loss 0.81, Spatial_loss 1.86, Flat_loss 0.31, Train_acc 77.26, Test_acc 53.15
2025-01-13 11:30:05,214 [podnet.py] => Task 2, Epoch 43/160 (LR 0.08321) => LSC_loss 0.78, Spatial_loss 1.83, Flat_loss 0.30, Train_acc 77.98, Test_acc 54.73
2025-01-13 11:30:13,113 [podnet.py] => Task 2, Epoch 44/160 (LR 0.08247) => LSC_loss 0.79, Spatial_loss 1.85, Flat_loss 0.31, Train_acc 77.49, Test_acc 51.27
2025-01-13 11:30:21,175 [podnet.py] => Task 2, Epoch 45/160 (LR 0.08172) => LSC_loss 0.78, Spatial_loss 1.85, Flat_loss 0.30, Train_acc 77.88, Test_acc 51.00
2025-01-13 11:30:28,966 [podnet.py] => Task 2, Epoch 46/160 (LR 0.08095) => LSC_loss 0.78, Spatial_loss 1.85, Flat_loss 0.30, Train_acc 78.01, Test_acc 53.35
2025-01-13 11:30:36,670 [podnet.py] => Task 2, Epoch 47/160 (LR 0.08018) => LSC_loss 0.78, Spatial_loss 1.83, Flat_loss 0.30, Train_acc 78.15, Test_acc 51.35
2025-01-13 11:30:44,570 [podnet.py] => Task 2, Epoch 48/160 (LR 0.07939) => LSC_loss 0.77, Spatial_loss 1.83, Flat_loss 0.30, Train_acc 78.34, Test_acc 55.48
2025-01-13 11:30:52,319 [podnet.py] => Task 2, Epoch 49/160 (LR 0.07859) => LSC_loss 0.76, Spatial_loss 1.82, Flat_loss 0.30, Train_acc 78.49, Test_acc 50.32
2025-01-13 11:31:00,004 [podnet.py] => Task 2, Epoch 50/160 (LR 0.07778) => LSC_loss 0.75, Spatial_loss 1.82, Flat_loss 0.30, Train_acc 78.96, Test_acc 55.42
2025-01-13 11:31:07,802 [podnet.py] => Task 2, Epoch 51/160 (LR 0.07696) => LSC_loss 0.77, Spatial_loss 1.85, Flat_loss 0.30, Train_acc 78.25, Test_acc 56.22
2025-01-13 11:31:15,529 [podnet.py] => Task 2, Epoch 52/160 (LR 0.07612) => LSC_loss 0.73, Spatial_loss 1.80, Flat_loss 0.30, Train_acc 79.31, Test_acc 52.82
2025-01-13 11:31:23,193 [podnet.py] => Task 2, Epoch 53/160 (LR 0.07528) => LSC_loss 0.74, Spatial_loss 1.81, Flat_loss 0.30, Train_acc 78.70, Test_acc 55.10
2025-01-13 11:31:30,800 [podnet.py] => Task 2, Epoch 54/160 (LR 0.07443) => LSC_loss 0.73, Spatial_loss 1.78, Flat_loss 0.30, Train_acc 79.19, Test_acc 52.13
2025-01-13 11:31:38,671 [podnet.py] => Task 2, Epoch 55/160 (LR 0.07357) => LSC_loss 0.74, Spatial_loss 1.81, Flat_loss 0.30, Train_acc 79.35, Test_acc 54.82
2025-01-13 11:31:46,406 [podnet.py] => Task 2, Epoch 56/160 (LR 0.07270) => LSC_loss 0.73, Spatial_loss 1.81, Flat_loss 0.30, Train_acc 79.33, Test_acc 54.12
2025-01-13 11:31:54,514 [podnet.py] => Task 2, Epoch 57/160 (LR 0.07182) => LSC_loss 0.70, Spatial_loss 1.79, Flat_loss 0.30, Train_acc 80.06, Test_acc 42.62
2025-01-13 11:32:02,387 [podnet.py] => Task 2, Epoch 58/160 (LR 0.07093) => LSC_loss 0.74, Spatial_loss 1.82, Flat_loss 0.30, Train_acc 78.91, Test_acc 52.85
2025-01-13 11:32:10,181 [podnet.py] => Task 2, Epoch 59/160 (LR 0.07004) => LSC_loss 0.71, Spatial_loss 1.80, Flat_loss 0.30, Train_acc 79.92, Test_acc 53.93
2025-01-13 11:32:17,832 [podnet.py] => Task 2, Epoch 60/160 (LR 0.06913) => LSC_loss 0.71, Spatial_loss 1.79, Flat_loss 0.30, Train_acc 80.04, Test_acc 54.22
2025-01-13 11:32:25,797 [podnet.py] => Task 2, Epoch 61/160 (LR 0.06822) => LSC_loss 0.70, Spatial_loss 1.79, Flat_loss 0.30, Train_acc 79.97, Test_acc 49.77
2025-01-13 11:32:33,661 [podnet.py] => Task 2, Epoch 62/160 (LR 0.06731) => LSC_loss 0.70, Spatial_loss 1.78, Flat_loss 0.30, Train_acc 79.96, Test_acc 57.67
2025-01-13 11:32:41,479 [podnet.py] => Task 2, Epoch 63/160 (LR 0.06638) => LSC_loss 0.69, Spatial_loss 1.77, Flat_loss 0.30, Train_acc 80.23, Test_acc 53.07
2025-01-13 11:32:49,406 [podnet.py] => Task 2, Epoch 64/160 (LR 0.06545) => LSC_loss 0.66, Spatial_loss 1.77, Flat_loss 0.29, Train_acc 81.50, Test_acc 54.10
2025-01-13 11:32:57,236 [podnet.py] => Task 2, Epoch 65/160 (LR 0.06451) => LSC_loss 0.67, Spatial_loss 1.76, Flat_loss 0.29, Train_acc 80.74, Test_acc 55.98
2025-01-13 11:33:05,093 [podnet.py] => Task 2, Epoch 66/160 (LR 0.06357) => LSC_loss 0.67, Spatial_loss 1.75, Flat_loss 0.29, Train_acc 80.90, Test_acc 53.28
2025-01-13 11:33:12,747 [podnet.py] => Task 2, Epoch 67/160 (LR 0.06262) => LSC_loss 0.68, Spatial_loss 1.76, Flat_loss 0.29, Train_acc 80.86, Test_acc 56.12
2025-01-13 11:33:20,591 [podnet.py] => Task 2, Epoch 68/160 (LR 0.06167) => LSC_loss 0.66, Spatial_loss 1.77, Flat_loss 0.29, Train_acc 81.29, Test_acc 51.17
2025-01-13 11:33:28,548 [podnet.py] => Task 2, Epoch 69/160 (LR 0.06072) => LSC_loss 0.67, Spatial_loss 1.76, Flat_loss 0.29, Train_acc 81.01, Test_acc 53.27
2025-01-13 11:33:36,484 [podnet.py] => Task 2, Epoch 70/160 (LR 0.05975) => LSC_loss 0.63, Spatial_loss 1.73, Flat_loss 0.29, Train_acc 81.99, Test_acc 56.02
2025-01-13 11:33:44,230 [podnet.py] => Task 2, Epoch 71/160 (LR 0.05879) => LSC_loss 0.63, Spatial_loss 1.75, Flat_loss 0.29, Train_acc 82.00, Test_acc 50.05
2025-01-13 11:33:51,945 [podnet.py] => Task 2, Epoch 72/160 (LR 0.05782) => LSC_loss 0.63, Spatial_loss 1.72, Flat_loss 0.29, Train_acc 82.42, Test_acc 55.73
2025-01-13 11:33:59,596 [podnet.py] => Task 2, Epoch 73/160 (LR 0.05685) => LSC_loss 0.62, Spatial_loss 1.72, Flat_loss 0.29, Train_acc 82.29, Test_acc 55.45
2025-01-13 11:34:07,104 [podnet.py] => Task 2, Epoch 74/160 (LR 0.05588) => LSC_loss 0.62, Spatial_loss 1.73, Flat_loss 0.29, Train_acc 82.49, Test_acc 56.53
2025-01-13 11:34:14,966 [podnet.py] => Task 2, Epoch 75/160 (LR 0.05490) => LSC_loss 0.60, Spatial_loss 1.70, Flat_loss 0.29, Train_acc 83.11, Test_acc 55.50
2025-01-13 11:34:22,697 [podnet.py] => Task 2, Epoch 76/160 (LR 0.05392) => LSC_loss 0.62, Spatial_loss 1.73, Flat_loss 0.29, Train_acc 82.53, Test_acc 52.30
2025-01-13 11:34:30,406 [podnet.py] => Task 2, Epoch 77/160 (LR 0.05294) => LSC_loss 0.61, Spatial_loss 1.73, Flat_loss 0.29, Train_acc 82.89, Test_acc 54.70
2025-01-13 11:34:38,097 [podnet.py] => Task 2, Epoch 78/160 (LR 0.05196) => LSC_loss 0.62, Spatial_loss 1.70, Flat_loss 0.29, Train_acc 82.49, Test_acc 54.98
2025-01-13 11:34:45,820 [podnet.py] => Task 2, Epoch 79/160 (LR 0.05098) => LSC_loss 0.58, Spatial_loss 1.68, Flat_loss 0.28, Train_acc 83.66, Test_acc 56.07
2025-01-13 11:34:53,420 [podnet.py] => Task 2, Epoch 80/160 (LR 0.05000) => LSC_loss 0.59, Spatial_loss 1.68, Flat_loss 0.28, Train_acc 83.03, Test_acc 55.95
2025-01-13 11:35:01,274 [podnet.py] => Task 2, Epoch 81/160 (LR 0.04902) => LSC_loss 0.57, Spatial_loss 1.67, Flat_loss 0.28, Train_acc 84.03, Test_acc 54.25
2025-01-13 11:35:08,985 [podnet.py] => Task 2, Epoch 82/160 (LR 0.04804) => LSC_loss 0.56, Spatial_loss 1.66, Flat_loss 0.28, Train_acc 84.33, Test_acc 56.70
2025-01-13 11:35:16,684 [podnet.py] => Task 2, Epoch 83/160 (LR 0.04706) => LSC_loss 0.58, Spatial_loss 1.67, Flat_loss 0.28, Train_acc 83.33, Test_acc 54.78
2025-01-13 11:35:24,259 [podnet.py] => Task 2, Epoch 84/160 (LR 0.04608) => LSC_loss 0.57, Spatial_loss 1.66, Flat_loss 0.28, Train_acc 83.69, Test_acc 54.45
2025-01-13 11:35:32,183 [podnet.py] => Task 2, Epoch 85/160 (LR 0.04510) => LSC_loss 0.56, Spatial_loss 1.67, Flat_loss 0.28, Train_acc 84.03, Test_acc 54.25
2025-01-13 11:35:39,882 [podnet.py] => Task 2, Epoch 86/160 (LR 0.04412) => LSC_loss 0.55, Spatial_loss 1.65, Flat_loss 0.28, Train_acc 84.47, Test_acc 52.88
2025-01-13 11:35:47,671 [podnet.py] => Task 2, Epoch 87/160 (LR 0.04315) => LSC_loss 0.54, Spatial_loss 1.64, Flat_loss 0.27, Train_acc 84.80, Test_acc 55.55
2025-01-13 11:35:55,464 [podnet.py] => Task 2, Epoch 88/160 (LR 0.04218) => LSC_loss 0.54, Spatial_loss 1.64, Flat_loss 0.27, Train_acc 84.74, Test_acc 55.75
2025-01-13 11:36:03,131 [podnet.py] => Task 2, Epoch 89/160 (LR 0.04121) => LSC_loss 0.52, Spatial_loss 1.63, Flat_loss 0.27, Train_acc 85.47, Test_acc 59.03
2025-01-13 11:36:10,504 [podnet.py] => Task 2, Epoch 90/160 (LR 0.04025) => LSC_loss 0.52, Spatial_loss 1.62, Flat_loss 0.27, Train_acc 85.76, Test_acc 56.18
2025-01-13 11:36:18,298 [podnet.py] => Task 2, Epoch 91/160 (LR 0.03928) => LSC_loss 0.51, Spatial_loss 1.61, Flat_loss 0.27, Train_acc 85.97, Test_acc 57.20
2025-01-13 11:36:26,000 [podnet.py] => Task 2, Epoch 92/160 (LR 0.03833) => LSC_loss 0.51, Spatial_loss 1.61, Flat_loss 0.27, Train_acc 85.91, Test_acc 56.68
2025-01-13 11:36:33,896 [podnet.py] => Task 2, Epoch 93/160 (LR 0.03738) => LSC_loss 0.51, Spatial_loss 1.59, Flat_loss 0.27, Train_acc 85.80, Test_acc 57.02
2025-01-13 11:36:41,906 [podnet.py] => Task 2, Epoch 94/160 (LR 0.03643) => LSC_loss 0.50, Spatial_loss 1.59, Flat_loss 0.27, Train_acc 86.12, Test_acc 56.43
2025-01-13 11:36:49,606 [podnet.py] => Task 2, Epoch 95/160 (LR 0.03549) => LSC_loss 0.48, Spatial_loss 1.56, Flat_loss 0.26, Train_acc 87.17, Test_acc 57.92
2025-01-13 11:36:57,422 [podnet.py] => Task 2, Epoch 96/160 (LR 0.03455) => LSC_loss 0.46, Spatial_loss 1.55, Flat_loss 0.26, Train_acc 87.18, Test_acc 57.40
2025-01-13 11:37:04,942 [podnet.py] => Task 2, Epoch 97/160 (LR 0.03362) => LSC_loss 0.47, Spatial_loss 1.55, Flat_loss 0.26, Train_acc 87.06, Test_acc 57.88
2025-01-13 11:37:12,726 [podnet.py] => Task 2, Epoch 98/160 (LR 0.03269) => LSC_loss 0.46, Spatial_loss 1.55, Flat_loss 0.26, Train_acc 87.53, Test_acc 57.32
2025-01-13 11:37:20,709 [podnet.py] => Task 2, Epoch 99/160 (LR 0.03178) => LSC_loss 0.46, Spatial_loss 1.53, Flat_loss 0.26, Train_acc 87.84, Test_acc 58.92
2025-01-13 11:37:28,526 [podnet.py] => Task 2, Epoch 100/160 (LR 0.03087) => LSC_loss 0.45, Spatial_loss 1.52, Flat_loss 0.26, Train_acc 87.86, Test_acc 59.22
2025-01-13 11:37:36,324 [podnet.py] => Task 2, Epoch 101/160 (LR 0.02996) => LSC_loss 0.44, Spatial_loss 1.51, Flat_loss 0.25, Train_acc 88.15, Test_acc 59.07
2025-01-13 11:37:44,280 [podnet.py] => Task 2, Epoch 102/160 (LR 0.02907) => LSC_loss 0.44, Spatial_loss 1.52, Flat_loss 0.26, Train_acc 87.91, Test_acc 57.40
2025-01-13 11:37:51,888 [podnet.py] => Task 2, Epoch 103/160 (LR 0.02818) => LSC_loss 0.44, Spatial_loss 1.50, Flat_loss 0.25, Train_acc 87.73, Test_acc 57.97
2025-01-13 11:37:59,681 [podnet.py] => Task 2, Epoch 104/160 (LR 0.02730) => LSC_loss 0.42, Spatial_loss 1.50, Flat_loss 0.25, Train_acc 88.79, Test_acc 58.73
2025-01-13 11:38:07,445 [podnet.py] => Task 2, Epoch 105/160 (LR 0.02643) => LSC_loss 0.43, Spatial_loss 1.50, Flat_loss 0.25, Train_acc 88.31, Test_acc 59.28
2025-01-13 11:38:14,928 [podnet.py] => Task 2, Epoch 106/160 (LR 0.02557) => LSC_loss 0.42, Spatial_loss 1.47, Flat_loss 0.25, Train_acc 88.69, Test_acc 59.38
2025-01-13 11:38:22,460 [podnet.py] => Task 2, Epoch 107/160 (LR 0.02472) => LSC_loss 0.40, Spatial_loss 1.46, Flat_loss 0.25, Train_acc 89.17, Test_acc 58.90
2025-01-13 11:38:30,108 [podnet.py] => Task 2, Epoch 108/160 (LR 0.02388) => LSC_loss 0.39, Spatial_loss 1.46, Flat_loss 0.25, Train_acc 89.85, Test_acc 60.02
2025-01-13 11:38:38,023 [podnet.py] => Task 2, Epoch 109/160 (LR 0.02304) => LSC_loss 0.39, Spatial_loss 1.44, Flat_loss 0.24, Train_acc 89.80, Test_acc 58.65
2025-01-13 11:38:45,636 [podnet.py] => Task 2, Epoch 110/160 (LR 0.02222) => LSC_loss 0.38, Spatial_loss 1.44, Flat_loss 0.24, Train_acc 89.83, Test_acc 59.88
2025-01-13 11:38:53,414 [podnet.py] => Task 2, Epoch 111/160 (LR 0.02141) => LSC_loss 0.38, Spatial_loss 1.44, Flat_loss 0.24, Train_acc 89.98, Test_acc 59.62
2025-01-13 11:39:01,231 [podnet.py] => Task 2, Epoch 112/160 (LR 0.02061) => LSC_loss 0.38, Spatial_loss 1.43, Flat_loss 0.24, Train_acc 89.99, Test_acc 57.60
2025-01-13 11:39:09,156 [podnet.py] => Task 2, Epoch 113/160 (LR 0.01982) => LSC_loss 0.37, Spatial_loss 1.43, Flat_loss 0.24, Train_acc 90.10, Test_acc 59.30
2025-01-13 11:39:16,433 [podnet.py] => Task 2, Epoch 114/160 (LR 0.01905) => LSC_loss 0.35, Spatial_loss 1.40, Flat_loss 0.24, Train_acc 90.92, Test_acc 59.10
2025-01-13 11:39:23,881 [podnet.py] => Task 2, Epoch 115/160 (LR 0.01828) => LSC_loss 0.35, Spatial_loss 1.39, Flat_loss 0.24, Train_acc 90.86, Test_acc 60.23
2025-01-13 11:39:31,074 [podnet.py] => Task 2, Epoch 116/160 (LR 0.01753) => LSC_loss 0.35, Spatial_loss 1.39, Flat_loss 0.23, Train_acc 91.05, Test_acc 60.90
2025-01-13 11:39:38,787 [podnet.py] => Task 2, Epoch 117/160 (LR 0.01679) => LSC_loss 0.34, Spatial_loss 1.37, Flat_loss 0.23, Train_acc 91.49, Test_acc 60.55
2025-01-13 11:39:46,348 [podnet.py] => Task 2, Epoch 118/160 (LR 0.01606) => LSC_loss 0.34, Spatial_loss 1.36, Flat_loss 0.23, Train_acc 91.49, Test_acc 60.55
2025-01-13 11:39:54,041 [podnet.py] => Task 2, Epoch 119/160 (LR 0.01535) => LSC_loss 0.33, Spatial_loss 1.35, Flat_loss 0.23, Train_acc 91.65, Test_acc 60.63
2025-01-13 11:40:01,633 [podnet.py] => Task 2, Epoch 120/160 (LR 0.01464) => LSC_loss 0.32, Spatial_loss 1.36, Flat_loss 0.23, Train_acc 92.30, Test_acc 61.53
2025-01-13 11:40:09,378 [podnet.py] => Task 2, Epoch 121/160 (LR 0.01396) => LSC_loss 0.32, Spatial_loss 1.34, Flat_loss 0.23, Train_acc 92.21, Test_acc 60.50
2025-01-13 11:40:16,564 [podnet.py] => Task 2, Epoch 122/160 (LR 0.01328) => LSC_loss 0.33, Spatial_loss 1.32, Flat_loss 0.23, Train_acc 91.77, Test_acc 60.62
2025-01-13 11:40:24,203 [podnet.py] => Task 2, Epoch 123/160 (LR 0.01262) => LSC_loss 0.31, Spatial_loss 1.33, Flat_loss 0.23, Train_acc 92.29, Test_acc 61.40
2025-01-13 11:40:31,793 [podnet.py] => Task 2, Epoch 124/160 (LR 0.01198) => LSC_loss 0.30, Spatial_loss 1.31, Flat_loss 0.22, Train_acc 92.55, Test_acc 61.62
2025-01-13 11:40:39,514 [podnet.py] => Task 2, Epoch 125/160 (LR 0.01135) => LSC_loss 0.30, Spatial_loss 1.30, Flat_loss 0.22, Train_acc 93.12, Test_acc 61.82
2025-01-13 11:40:47,311 [podnet.py] => Task 2, Epoch 126/160 (LR 0.01073) => LSC_loss 0.30, Spatial_loss 1.30, Flat_loss 0.22, Train_acc 92.90, Test_acc 61.47
2025-01-13 11:40:55,031 [podnet.py] => Task 2, Epoch 127/160 (LR 0.01013) => LSC_loss 0.29, Spatial_loss 1.28, Flat_loss 0.22, Train_acc 93.13, Test_acc 61.97
2025-01-13 11:41:02,932 [podnet.py] => Task 2, Epoch 128/160 (LR 0.00955) => LSC_loss 0.29, Spatial_loss 1.28, Flat_loss 0.22, Train_acc 93.30, Test_acc 62.25
2025-01-13 11:41:10,651 [podnet.py] => Task 2, Epoch 129/160 (LR 0.00898) => LSC_loss 0.29, Spatial_loss 1.27, Flat_loss 0.22, Train_acc 93.29, Test_acc 61.30
2025-01-13 11:41:18,451 [podnet.py] => Task 2, Epoch 130/160 (LR 0.00843) => LSC_loss 0.28, Spatial_loss 1.25, Flat_loss 0.21, Train_acc 93.42, Test_acc 62.07
2025-01-13 11:41:26,292 [podnet.py] => Task 2, Epoch 131/160 (LR 0.00789) => LSC_loss 0.27, Spatial_loss 1.25, Flat_loss 0.21, Train_acc 93.82, Test_acc 61.60
2025-01-13 11:41:34,099 [podnet.py] => Task 2, Epoch 132/160 (LR 0.00737) => LSC_loss 0.27, Spatial_loss 1.23, Flat_loss 0.21, Train_acc 93.51, Test_acc 61.63
2025-01-13 11:41:41,744 [podnet.py] => Task 2, Epoch 133/160 (LR 0.00686) => LSC_loss 0.27, Spatial_loss 1.23, Flat_loss 0.21, Train_acc 94.12, Test_acc 60.83
2025-01-13 11:41:49,419 [podnet.py] => Task 2, Epoch 134/160 (LR 0.00638) => LSC_loss 0.27, Spatial_loss 1.23, Flat_loss 0.21, Train_acc 93.71, Test_acc 62.23
2025-01-13 11:41:57,502 [podnet.py] => Task 2, Epoch 135/160 (LR 0.00590) => LSC_loss 0.26, Spatial_loss 1.22, Flat_loss 0.21, Train_acc 94.03, Test_acc 61.70
2025-01-13 11:42:05,397 [podnet.py] => Task 2, Epoch 136/160 (LR 0.00545) => LSC_loss 0.25, Spatial_loss 1.21, Flat_loss 0.21, Train_acc 94.72, Test_acc 62.35
2025-01-13 11:42:13,419 [podnet.py] => Task 2, Epoch 137/160 (LR 0.00501) => LSC_loss 0.26, Spatial_loss 1.21, Flat_loss 0.21, Train_acc 94.51, Test_acc 62.73
2025-01-13 11:42:21,458 [podnet.py] => Task 2, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 1.20, Flat_loss 0.21, Train_acc 94.42, Test_acc 61.90
2025-01-13 11:42:28,526 [podnet.py] => Task 2, Epoch 139/160 (LR 0.00419) => LSC_loss 0.25, Spatial_loss 1.19, Flat_loss 0.20, Train_acc 94.55, Test_acc 61.72
2025-01-13 11:42:36,601 [podnet.py] => Task 2, Epoch 140/160 (LR 0.00381) => LSC_loss 0.25, Spatial_loss 1.19, Flat_loss 0.21, Train_acc 94.78, Test_acc 61.73
2025-01-13 11:42:44,243 [podnet.py] => Task 2, Epoch 141/160 (LR 0.00344) => LSC_loss 0.24, Spatial_loss 1.18, Flat_loss 0.20, Train_acc 94.77, Test_acc 62.32
2025-01-13 11:42:51,972 [podnet.py] => Task 2, Epoch 142/160 (LR 0.00309) => LSC_loss 0.23, Spatial_loss 1.16, Flat_loss 0.20, Train_acc 95.32, Test_acc 62.13
2025-01-13 11:43:00,016 [podnet.py] => Task 2, Epoch 143/160 (LR 0.00276) => LSC_loss 0.24, Spatial_loss 1.17, Flat_loss 0.20, Train_acc 94.98, Test_acc 61.97
2025-01-13 11:43:08,081 [podnet.py] => Task 2, Epoch 144/160 (LR 0.00245) => LSC_loss 0.23, Spatial_loss 1.16, Flat_loss 0.20, Train_acc 95.14, Test_acc 62.45
2025-01-13 11:43:16,285 [podnet.py] => Task 2, Epoch 145/160 (LR 0.00215) => LSC_loss 0.23, Spatial_loss 1.16, Flat_loss 0.20, Train_acc 95.22, Test_acc 62.38
2025-01-13 11:43:24,398 [podnet.py] => Task 2, Epoch 146/160 (LR 0.00188) => LSC_loss 0.23, Spatial_loss 1.15, Flat_loss 0.20, Train_acc 95.39, Test_acc 62.42
2025-01-13 11:43:32,915 [podnet.py] => Task 2, Epoch 147/160 (LR 0.00162) => LSC_loss 0.23, Spatial_loss 1.15, Flat_loss 0.20, Train_acc 95.40, Test_acc 62.40
2025-01-13 11:43:40,984 [podnet.py] => Task 2, Epoch 148/160 (LR 0.00138) => LSC_loss 0.23, Spatial_loss 1.15, Flat_loss 0.20, Train_acc 95.45, Test_acc 62.17
2025-01-13 11:43:48,675 [podnet.py] => Task 2, Epoch 149/160 (LR 0.00116) => LSC_loss 0.23, Spatial_loss 1.14, Flat_loss 0.20, Train_acc 95.40, Test_acc 62.38
2025-01-13 11:43:55,902 [podnet.py] => Task 2, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 1.14, Flat_loss 0.20, Train_acc 95.56, Test_acc 62.15
2025-01-13 11:44:03,955 [podnet.py] => Task 2, Epoch 151/160 (LR 0.00078) => LSC_loss 0.23, Spatial_loss 1.13, Flat_loss 0.20, Train_acc 95.49, Test_acc 62.43
2025-01-13 11:44:11,956 [podnet.py] => Task 2, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 1.13, Flat_loss 0.20, Train_acc 95.97, Test_acc 62.33
2025-01-13 11:44:20,030 [podnet.py] => Task 2, Epoch 153/160 (LR 0.00047) => LSC_loss 0.23, Spatial_loss 1.13, Flat_loss 0.20, Train_acc 95.50, Test_acc 62.63
2025-01-13 11:44:27,757 [podnet.py] => Task 2, Epoch 154/160 (LR 0.00035) => LSC_loss 0.22, Spatial_loss 1.13, Flat_loss 0.20, Train_acc 96.03, Test_acc 62.68
2025-01-13 11:44:34,871 [podnet.py] => Task 2, Epoch 155/160 (LR 0.00024) => LSC_loss 0.22, Spatial_loss 1.12, Flat_loss 0.20, Train_acc 95.79, Test_acc 62.45
2025-01-13 11:44:42,816 [podnet.py] => Task 2, Epoch 156/160 (LR 0.00015) => LSC_loss 0.22, Spatial_loss 1.12, Flat_loss 0.20, Train_acc 95.83, Test_acc 62.58
2025-01-13 11:44:50,498 [podnet.py] => Task 2, Epoch 157/160 (LR 0.00009) => LSC_loss 0.22, Spatial_loss 1.12, Flat_loss 0.20, Train_acc 95.90, Test_acc 62.53
2025-01-13 11:44:58,037 [podnet.py] => Task 2, Epoch 158/160 (LR 0.00004) => LSC_loss 0.22, Spatial_loss 1.12, Flat_loss 0.19, Train_acc 95.76, Test_acc 62.52
2025-01-13 11:45:05,569 [podnet.py] => Task 2, Epoch 159/160 (LR 0.00001) => LSC_loss 0.22, Spatial_loss 1.12, Flat_loss 0.20, Train_acc 95.87, Test_acc 62.68
2025-01-13 11:45:13,387 [podnet.py] => Task 2, Epoch 160/160 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 1.12, Flat_loss 0.20, Train_acc 95.97, Test_acc 62.57
2025-01-13 11:45:13,387 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-13 11:45:13,387 [base.py] => Reducing exemplars...(110 per classes)
2025-01-13 11:45:45,564 [base.py] => Constructing exemplars...(110 per classes)
2025-01-13 11:46:21,421 [podnet.py] => The size of finetune dataset: 6600
2025-01-13 11:46:26,146 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.20, Spatial_loss 1.12, Flat_loss 0.15, Train_acc 95.76, Test_acc 62.48
2025-01-13 11:46:30,873 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.20, Spatial_loss 1.14, Flat_loss 0.14, Train_acc 95.89, Test_acc 62.85
2025-01-13 11:46:35,561 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.19, Spatial_loss 1.13, Flat_loss 0.14, Train_acc 96.08, Test_acc 62.80
2025-01-13 11:46:40,150 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.19, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 95.77, Test_acc 62.55
2025-01-13 11:46:44,661 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.19, Spatial_loss 1.10, Flat_loss 0.14, Train_acc 96.12, Test_acc 62.18
2025-01-13 11:46:49,554 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.18, Spatial_loss 1.12, Flat_loss 0.14, Train_acc 96.20, Test_acc 62.72
2025-01-13 11:46:54,101 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.18, Spatial_loss 1.10, Flat_loss 0.14, Train_acc 96.38, Test_acc 62.52
2025-01-13 11:46:58,909 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.18, Spatial_loss 1.10, Flat_loss 0.14, Train_acc 96.29, Test_acc 62.68
2025-01-13 11:47:03,639 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.17, Spatial_loss 1.09, Flat_loss 0.14, Train_acc 96.59, Test_acc 62.60
2025-01-13 11:47:08,254 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.17, Spatial_loss 1.10, Flat_loss 0.14, Train_acc 96.79, Test_acc 62.60
2025-01-13 11:47:12,900 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.17, Spatial_loss 1.07, Flat_loss 0.13, Train_acc 96.83, Test_acc 62.50
2025-01-13 11:47:17,617 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.17, Spatial_loss 1.06, Flat_loss 0.13, Train_acc 97.03, Test_acc 62.60
2025-01-13 11:47:22,494 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.17, Spatial_loss 1.07, Flat_loss 0.13, Train_acc 96.89, Test_acc 62.33
2025-01-13 11:47:27,340 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.16, Spatial_loss 1.06, Flat_loss 0.13, Train_acc 97.18, Test_acc 63.07
2025-01-13 11:47:32,175 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.15, Spatial_loss 1.06, Flat_loss 0.13, Train_acc 97.36, Test_acc 62.88
2025-01-13 11:47:36,832 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.15, Spatial_loss 1.06, Flat_loss 0.13, Train_acc 97.38, Test_acc 62.95
2025-01-13 11:47:41,460 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.16, Spatial_loss 1.06, Flat_loss 0.13, Train_acc 97.30, Test_acc 62.68
2025-01-13 11:47:46,244 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.15, Spatial_loss 1.05, Flat_loss 0.13, Train_acc 97.27, Test_acc 62.60
2025-01-13 11:47:50,991 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.16, Spatial_loss 1.05, Flat_loss 0.13, Train_acc 97.45, Test_acc 62.73
2025-01-13 11:47:55,664 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.16, Spatial_loss 1.05, Flat_loss 0.13, Train_acc 97.32, Test_acc 62.65
2025-01-13 11:47:55,665 [base.py] => Reducing exemplars...(73 per classes)
2025-01-13 11:48:27,423 [base.py] => Constructing exemplars...(73 per classes)
2025-01-13 11:49:06,003 [podnet.py] => Exemplar size: 4380
2025-01-13 11:49:06,003 [trainer.py] => CNN: {'total': np.float64(62.65), '00-09': np.float64(73.2), '10-19': np.float64(59.2), '20-29': np.float64(62.5), '30-39': np.float64(55.2), '40-49': np.float64(66.9), '50-59': np.float64(58.9), 'old': np.float64(62.52), 'new': np.float64(62.9)}
2025-01-13 11:49:06,003 [trainer.py] => NME: {'total': np.float64(61.6), '00-09': np.float64(74.3), '10-19': np.float64(61.8), '20-29': np.float64(60.6), '30-39': np.float64(51.1), '40-49': np.float64(65.5), '50-59': np.float64(56.3), 'old': np.float64(61.95), 'new': np.float64(60.9)}
2025-01-13 11:49:06,003 [trainer.py] => CNN top1 curve: [np.float64(82.45), np.float64(69.32), np.float64(62.65)]
2025-01-13 11:49:06,003 [trainer.py] => CNN top5 curve: [np.float64(96.4), np.float64(91.95), np.float64(88.07)]
2025-01-13 11:49:06,003 [trainer.py] => NME top1 curve: [np.float64(82.05), np.float64(69.08), np.float64(61.6)]
2025-01-13 11:49:06,003 [trainer.py] => NME top5 curve: [np.float64(96.2), np.float64(91.62), np.float64(87.72)]

2025-01-13 11:49:06,004 [trainer.py] => All params: 504657
2025-01-13 11:49:06,004 [trainer.py] => Trainable params: 504657
2025-01-13 11:49:06,005 [podnet.py] => Learning on 60-80
2025-01-13 11:49:06,145 [podnet.py] => Adaptive factor: 2.0
2025-01-13 11:49:14,103 [podnet.py] => Task 3, Epoch 1/160 (LR 0.09999) => LSC_loss 2.33, Spatial_loss 2.75, Flat_loss 0.72, Train_acc 44.71, Test_acc 31.75
2025-01-13 11:49:22,033 [podnet.py] => Task 3, Epoch 2/160 (LR 0.09996) => LSC_loss 1.56, Spatial_loss 2.33, Flat_loss 0.43, Train_acc 56.82, Test_acc 44.09
2025-01-13 11:49:29,964 [podnet.py] => Task 3, Epoch 3/160 (LR 0.09991) => LSC_loss 1.42, Spatial_loss 2.21, Flat_loss 0.35, Train_acc 60.46, Test_acc 39.25
2025-01-13 11:49:38,018 [podnet.py] => Task 3, Epoch 4/160 (LR 0.09985) => LSC_loss 1.36, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 62.02, Test_acc 33.50
2025-01-13 11:49:46,059 [podnet.py] => Task 3, Epoch 5/160 (LR 0.09976) => LSC_loss 1.28, Spatial_loss 2.07, Flat_loss 0.30, Train_acc 64.40, Test_acc 45.00
2025-01-13 11:49:54,058 [podnet.py] => Task 3, Epoch 6/160 (LR 0.09965) => LSC_loss 1.26, Spatial_loss 2.07, Flat_loss 0.30, Train_acc 64.69, Test_acc 45.65
2025-01-13 11:50:02,098 [podnet.py] => Task 3, Epoch 7/160 (LR 0.09953) => LSC_loss 1.22, Spatial_loss 2.04, Flat_loss 0.29, Train_acc 65.78, Test_acc 43.45
2025-01-13 11:50:10,114 [podnet.py] => Task 3, Epoch 8/160 (LR 0.09938) => LSC_loss 1.21, Spatial_loss 2.02, Flat_loss 0.29, Train_acc 66.20, Test_acc 44.58
2025-01-13 11:50:17,770 [podnet.py] => Task 3, Epoch 9/160 (LR 0.09922) => LSC_loss 1.19, Spatial_loss 2.05, Flat_loss 0.29, Train_acc 66.72, Test_acc 37.41
2025-01-13 11:50:25,715 [podnet.py] => Task 3, Epoch 10/160 (LR 0.09904) => LSC_loss 1.16, Spatial_loss 2.00, Flat_loss 0.28, Train_acc 67.82, Test_acc 45.10
2025-01-13 11:50:33,684 [podnet.py] => Task 3, Epoch 11/160 (LR 0.09884) => LSC_loss 1.15, Spatial_loss 2.01, Flat_loss 0.28, Train_acc 67.68, Test_acc 40.79
2025-01-13 11:50:41,815 [podnet.py] => Task 3, Epoch 12/160 (LR 0.09862) => LSC_loss 1.13, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 68.48, Test_acc 47.48
2025-01-13 11:50:49,733 [podnet.py] => Task 3, Epoch 13/160 (LR 0.09838) => LSC_loss 1.10, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 69.07, Test_acc 47.39
2025-01-13 11:50:57,793 [podnet.py] => Task 3, Epoch 14/160 (LR 0.09812) => LSC_loss 1.10, Spatial_loss 1.99, Flat_loss 0.28, Train_acc 68.88, Test_acc 46.61
2025-01-13 11:51:05,972 [podnet.py] => Task 3, Epoch 15/160 (LR 0.09785) => LSC_loss 1.10, Spatial_loss 2.01, Flat_loss 0.28, Train_acc 69.06, Test_acc 45.60
2025-01-13 11:51:14,031 [podnet.py] => Task 3, Epoch 16/160 (LR 0.09755) => LSC_loss 1.07, Spatial_loss 2.00, Flat_loss 0.28, Train_acc 69.61, Test_acc 36.01
2025-01-13 11:51:21,954 [podnet.py] => Task 3, Epoch 17/160 (LR 0.09724) => LSC_loss 1.07, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 69.62, Test_acc 45.12
2025-01-13 11:51:29,871 [podnet.py] => Task 3, Epoch 18/160 (LR 0.09691) => LSC_loss 1.07, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 69.94, Test_acc 47.25
2025-01-13 11:51:37,698 [podnet.py] => Task 3, Epoch 19/160 (LR 0.09656) => LSC_loss 1.06, Spatial_loss 2.00, Flat_loss 0.28, Train_acc 70.59, Test_acc 46.34
2025-01-13 11:51:45,588 [podnet.py] => Task 3, Epoch 20/160 (LR 0.09619) => LSC_loss 1.03, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 70.85, Test_acc 43.42
2025-01-13 11:51:53,580 [podnet.py] => Task 3, Epoch 21/160 (LR 0.09581) => LSC_loss 1.04, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 70.33, Test_acc 47.06
2025-01-13 11:52:01,553 [podnet.py] => Task 3, Epoch 22/160 (LR 0.09541) => LSC_loss 1.03, Spatial_loss 1.99, Flat_loss 0.28, Train_acc 71.06, Test_acc 42.60
2025-01-13 11:52:09,568 [podnet.py] => Task 3, Epoch 23/160 (LR 0.09499) => LSC_loss 1.03, Spatial_loss 1.99, Flat_loss 0.28, Train_acc 70.57, Test_acc 46.62
2025-01-13 11:52:17,395 [podnet.py] => Task 3, Epoch 24/160 (LR 0.09455) => LSC_loss 1.00, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 71.84, Test_acc 42.00
2025-01-13 11:52:25,456 [podnet.py] => Task 3, Epoch 25/160 (LR 0.09410) => LSC_loss 1.02, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 71.06, Test_acc 42.91
2025-01-13 11:52:33,331 [podnet.py] => Task 3, Epoch 26/160 (LR 0.09362) => LSC_loss 1.00, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 71.67, Test_acc 45.88
2025-01-13 11:52:41,182 [podnet.py] => Task 3, Epoch 27/160 (LR 0.09314) => LSC_loss 0.99, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 72.10, Test_acc 43.50
2025-01-13 11:52:48,985 [podnet.py] => Task 3, Epoch 28/160 (LR 0.09263) => LSC_loss 1.01, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 71.93, Test_acc 47.91
2025-01-13 11:52:56,690 [podnet.py] => Task 3, Epoch 29/160 (LR 0.09211) => LSC_loss 1.00, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 71.84, Test_acc 42.60
2025-01-13 11:53:04,631 [podnet.py] => Task 3, Epoch 30/160 (LR 0.09157) => LSC_loss 0.98, Spatial_loss 1.99, Flat_loss 0.28, Train_acc 72.36, Test_acc 46.00
2025-01-13 11:53:12,712 [podnet.py] => Task 3, Epoch 31/160 (LR 0.09102) => LSC_loss 0.98, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 71.79, Test_acc 47.32
2025-01-13 11:53:20,432 [podnet.py] => Task 3, Epoch 32/160 (LR 0.09045) => LSC_loss 0.96, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 72.48, Test_acc 41.75
2025-01-13 11:53:28,284 [podnet.py] => Task 3, Epoch 33/160 (LR 0.08987) => LSC_loss 0.97, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 72.88, Test_acc 43.35
2025-01-13 11:53:36,040 [podnet.py] => Task 3, Epoch 34/160 (LR 0.08927) => LSC_loss 0.98, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 72.30, Test_acc 46.10
2025-01-13 11:53:43,796 [podnet.py] => Task 3, Epoch 35/160 (LR 0.08865) => LSC_loss 0.96, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 72.91, Test_acc 44.74
2025-01-13 11:53:51,691 [podnet.py] => Task 3, Epoch 36/160 (LR 0.08802) => LSC_loss 0.97, Spatial_loss 1.98, Flat_loss 0.29, Train_acc 72.05, Test_acc 45.58
2025-01-13 11:53:59,710 [podnet.py] => Task 3, Epoch 37/160 (LR 0.08738) => LSC_loss 0.94, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 73.57, Test_acc 46.19
2025-01-13 11:54:07,594 [podnet.py] => Task 3, Epoch 38/160 (LR 0.08672) => LSC_loss 0.95, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 72.89, Test_acc 42.34
2025-01-13 11:54:14,995 [podnet.py] => Task 3, Epoch 39/160 (LR 0.08604) => LSC_loss 0.94, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 73.71, Test_acc 45.55
2025-01-13 11:54:22,922 [podnet.py] => Task 3, Epoch 40/160 (LR 0.08536) => LSC_loss 0.91, Spatial_loss 1.91, Flat_loss 0.28, Train_acc 74.45, Test_acc 46.44
2025-01-13 11:54:30,879 [podnet.py] => Task 3, Epoch 41/160 (LR 0.08465) => LSC_loss 0.93, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 73.49, Test_acc 48.35
2025-01-13 11:54:38,814 [podnet.py] => Task 3, Epoch 42/160 (LR 0.08394) => LSC_loss 0.94, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 73.30, Test_acc 45.99
2025-01-13 11:54:46,847 [podnet.py] => Task 3, Epoch 43/160 (LR 0.08321) => LSC_loss 0.92, Spatial_loss 1.93, Flat_loss 0.28, Train_acc 74.29, Test_acc 50.31
2025-01-13 11:54:55,014 [podnet.py] => Task 3, Epoch 44/160 (LR 0.08247) => LSC_loss 0.92, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 73.85, Test_acc 43.78
2025-01-13 11:55:02,937 [podnet.py] => Task 3, Epoch 45/160 (LR 0.08172) => LSC_loss 0.91, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 74.11, Test_acc 46.86
2025-01-13 11:55:11,038 [podnet.py] => Task 3, Epoch 46/160 (LR 0.08095) => LSC_loss 0.89, Spatial_loss 1.93, Flat_loss 0.27, Train_acc 74.75, Test_acc 46.94
2025-01-13 11:55:19,128 [podnet.py] => Task 3, Epoch 47/160 (LR 0.08018) => LSC_loss 0.90, Spatial_loss 1.93, Flat_loss 0.28, Train_acc 74.37, Test_acc 45.74
2025-01-13 11:55:27,143 [podnet.py] => Task 3, Epoch 48/160 (LR 0.07939) => LSC_loss 0.89, Spatial_loss 1.91, Flat_loss 0.28, Train_acc 74.14, Test_acc 49.72
2025-01-13 11:55:35,193 [podnet.py] => Task 3, Epoch 49/160 (LR 0.07859) => LSC_loss 0.90, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 74.43, Test_acc 45.00
2025-01-13 11:55:43,239 [podnet.py] => Task 3, Epoch 50/160 (LR 0.07778) => LSC_loss 0.88, Spatial_loss 1.93, Flat_loss 0.28, Train_acc 74.57, Test_acc 46.85
2025-01-13 11:55:50,975 [podnet.py] => Task 3, Epoch 51/160 (LR 0.07696) => LSC_loss 0.88, Spatial_loss 1.90, Flat_loss 0.27, Train_acc 75.06, Test_acc 47.55
2025-01-13 11:55:58,916 [podnet.py] => Task 3, Epoch 52/160 (LR 0.07612) => LSC_loss 0.89, Spatial_loss 1.93, Flat_loss 0.28, Train_acc 74.73, Test_acc 43.80
2025-01-13 11:56:06,601 [podnet.py] => Task 3, Epoch 53/160 (LR 0.07528) => LSC_loss 0.87, Spatial_loss 1.90, Flat_loss 0.27, Train_acc 75.17, Test_acc 46.01
2025-01-13 11:56:14,531 [podnet.py] => Task 3, Epoch 54/160 (LR 0.07443) => LSC_loss 0.88, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 75.08, Test_acc 46.46
2025-01-13 11:56:22,654 [podnet.py] => Task 3, Epoch 55/160 (LR 0.07357) => LSC_loss 0.86, Spatial_loss 1.90, Flat_loss 0.27, Train_acc 75.60, Test_acc 50.24
2025-01-13 11:56:30,662 [podnet.py] => Task 3, Epoch 56/160 (LR 0.07270) => LSC_loss 0.84, Spatial_loss 1.88, Flat_loss 0.27, Train_acc 76.27, Test_acc 43.86
2025-01-13 11:56:38,631 [podnet.py] => Task 3, Epoch 57/160 (LR 0.07182) => LSC_loss 0.84, Spatial_loss 1.89, Flat_loss 0.27, Train_acc 76.02, Test_acc 43.45
2025-01-13 11:56:46,553 [podnet.py] => Task 3, Epoch 58/160 (LR 0.07093) => LSC_loss 0.83, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 76.86, Test_acc 47.36
2025-01-13 11:56:54,583 [podnet.py] => Task 3, Epoch 59/160 (LR 0.07004) => LSC_loss 0.84, Spatial_loss 1.86, Flat_loss 0.27, Train_acc 76.24, Test_acc 49.62
2025-01-13 11:57:02,606 [podnet.py] => Task 3, Epoch 60/160 (LR 0.06913) => LSC_loss 0.83, Spatial_loss 1.87, Flat_loss 0.27, Train_acc 76.54, Test_acc 46.01
2025-01-13 11:57:10,659 [podnet.py] => Task 3, Epoch 61/160 (LR 0.06822) => LSC_loss 0.84, Spatial_loss 1.88, Flat_loss 0.27, Train_acc 76.50, Test_acc 48.25
2025-01-13 11:57:18,678 [podnet.py] => Task 3, Epoch 62/160 (LR 0.06731) => LSC_loss 0.83, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 76.72, Test_acc 48.86
2025-01-13 11:57:26,761 [podnet.py] => Task 3, Epoch 63/160 (LR 0.06638) => LSC_loss 0.82, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 76.64, Test_acc 47.34
2025-01-13 11:57:34,823 [podnet.py] => Task 3, Epoch 64/160 (LR 0.06545) => LSC_loss 0.81, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 76.92, Test_acc 49.64
2025-01-13 11:57:42,748 [podnet.py] => Task 3, Epoch 65/160 (LR 0.06451) => LSC_loss 0.79, Spatial_loss 1.86, Flat_loss 0.27, Train_acc 77.47, Test_acc 48.02
2025-01-13 11:57:50,813 [podnet.py] => Task 3, Epoch 66/160 (LR 0.06357) => LSC_loss 0.81, Spatial_loss 1.85, Flat_loss 0.27, Train_acc 77.32, Test_acc 48.70
2025-01-13 11:57:58,871 [podnet.py] => Task 3, Epoch 67/160 (LR 0.06262) => LSC_loss 0.81, Spatial_loss 1.86, Flat_loss 0.27, Train_acc 77.03, Test_acc 46.25
2025-01-13 11:58:06,832 [podnet.py] => Task 3, Epoch 68/160 (LR 0.06167) => LSC_loss 0.78, Spatial_loss 1.82, Flat_loss 0.26, Train_acc 78.12, Test_acc 49.28
2025-01-13 11:58:13,686 [podnet.py] => Task 3, Epoch 69/160 (LR 0.06072) => LSC_loss 0.79, Spatial_loss 1.84, Flat_loss 0.27, Train_acc 77.65, Test_acc 47.12
2025-01-13 11:58:21,662 [podnet.py] => Task 3, Epoch 70/160 (LR 0.05975) => LSC_loss 0.78, Spatial_loss 1.83, Flat_loss 0.26, Train_acc 77.89, Test_acc 44.81
2025-01-13 11:58:29,807 [podnet.py] => Task 3, Epoch 71/160 (LR 0.05879) => LSC_loss 0.76, Spatial_loss 1.81, Flat_loss 0.26, Train_acc 78.43, Test_acc 47.98
2025-01-13 11:58:37,850 [podnet.py] => Task 3, Epoch 72/160 (LR 0.05782) => LSC_loss 0.78, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 78.00, Test_acc 48.12
2025-01-13 11:58:45,811 [podnet.py] => Task 3, Epoch 73/160 (LR 0.05685) => LSC_loss 0.75, Spatial_loss 1.81, Flat_loss 0.26, Train_acc 78.76, Test_acc 46.15
2025-01-13 11:58:54,013 [podnet.py] => Task 3, Epoch 74/160 (LR 0.05588) => LSC_loss 0.74, Spatial_loss 1.79, Flat_loss 0.26, Train_acc 79.03, Test_acc 49.02
2025-01-13 11:59:01,892 [podnet.py] => Task 3, Epoch 75/160 (LR 0.05490) => LSC_loss 0.73, Spatial_loss 1.78, Flat_loss 0.26, Train_acc 79.53, Test_acc 49.49
2025-01-13 11:59:09,940 [podnet.py] => Task 3, Epoch 76/160 (LR 0.05392) => LSC_loss 0.74, Spatial_loss 1.78, Flat_loss 0.26, Train_acc 78.92, Test_acc 47.81
2025-01-13 11:59:17,859 [podnet.py] => Task 3, Epoch 77/160 (LR 0.05294) => LSC_loss 0.73, Spatial_loss 1.78, Flat_loss 0.26, Train_acc 79.16, Test_acc 44.75
2025-01-13 11:59:25,575 [podnet.py] => Task 3, Epoch 78/160 (LR 0.05196) => LSC_loss 0.72, Spatial_loss 1.79, Flat_loss 0.26, Train_acc 79.69, Test_acc 49.21
2025-01-13 11:59:32,975 [podnet.py] => Task 3, Epoch 79/160 (LR 0.05098) => LSC_loss 0.72, Spatial_loss 1.77, Flat_loss 0.26, Train_acc 79.51, Test_acc 48.09
2025-01-13 11:59:40,874 [podnet.py] => Task 3, Epoch 80/160 (LR 0.05000) => LSC_loss 0.73, Spatial_loss 1.78, Flat_loss 0.26, Train_acc 78.87, Test_acc 43.56
2025-01-13 11:59:48,695 [podnet.py] => Task 3, Epoch 81/160 (LR 0.04902) => LSC_loss 0.70, Spatial_loss 1.75, Flat_loss 0.25, Train_acc 80.15, Test_acc 46.31
2025-01-13 11:59:56,611 [podnet.py] => Task 3, Epoch 82/160 (LR 0.04804) => LSC_loss 0.70, Spatial_loss 1.75, Flat_loss 0.25, Train_acc 80.19, Test_acc 47.79
2025-01-13 12:00:04,687 [podnet.py] => Task 3, Epoch 83/160 (LR 0.04706) => LSC_loss 0.72, Spatial_loss 1.77, Flat_loss 0.26, Train_acc 79.59, Test_acc 47.95
2025-01-13 12:00:12,766 [podnet.py] => Task 3, Epoch 84/160 (LR 0.04608) => LSC_loss 0.68, Spatial_loss 1.73, Flat_loss 0.25, Train_acc 80.77, Test_acc 48.71
2025-01-13 12:00:20,799 [podnet.py] => Task 3, Epoch 85/160 (LR 0.04510) => LSC_loss 0.67, Spatial_loss 1.71, Flat_loss 0.25, Train_acc 81.50, Test_acc 49.05
2025-01-13 12:00:28,737 [podnet.py] => Task 3, Epoch 86/160 (LR 0.04412) => LSC_loss 0.67, Spatial_loss 1.71, Flat_loss 0.25, Train_acc 80.97, Test_acc 47.16
2025-01-13 12:00:36,631 [podnet.py] => Task 3, Epoch 87/160 (LR 0.04315) => LSC_loss 0.65, Spatial_loss 1.70, Flat_loss 0.25, Train_acc 81.59, Test_acc 49.02
2025-01-13 12:00:44,759 [podnet.py] => Task 3, Epoch 88/160 (LR 0.04218) => LSC_loss 0.65, Spatial_loss 1.71, Flat_loss 0.25, Train_acc 81.82, Test_acc 49.62
2025-01-13 12:00:52,714 [podnet.py] => Task 3, Epoch 89/160 (LR 0.04121) => LSC_loss 0.63, Spatial_loss 1.69, Flat_loss 0.25, Train_acc 82.11, Test_acc 47.91
2025-01-13 12:01:00,570 [podnet.py] => Task 3, Epoch 90/160 (LR 0.04025) => LSC_loss 0.64, Spatial_loss 1.68, Flat_loss 0.24, Train_acc 82.38, Test_acc 48.52
2025-01-13 12:01:09,079 [podnet.py] => Task 3, Epoch 91/160 (LR 0.03928) => LSC_loss 0.64, Spatial_loss 1.67, Flat_loss 0.24, Train_acc 82.48, Test_acc 51.84
2025-01-13 12:01:17,298 [podnet.py] => Task 3, Epoch 92/160 (LR 0.03833) => LSC_loss 0.62, Spatial_loss 1.65, Flat_loss 0.24, Train_acc 82.84, Test_acc 46.39
2025-01-13 12:01:25,275 [podnet.py] => Task 3, Epoch 93/160 (LR 0.03738) => LSC_loss 0.62, Spatial_loss 1.67, Flat_loss 0.24, Train_acc 82.44, Test_acc 51.85
2025-01-13 12:01:33,121 [podnet.py] => Task 3, Epoch 94/160 (LR 0.03643) => LSC_loss 0.62, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 82.74, Test_acc 48.12
2025-01-13 12:01:41,184 [podnet.py] => Task 3, Epoch 95/160 (LR 0.03549) => LSC_loss 0.62, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 82.67, Test_acc 47.36
2025-01-13 12:01:49,118 [podnet.py] => Task 3, Epoch 96/160 (LR 0.03455) => LSC_loss 0.61, Spatial_loss 1.65, Flat_loss 0.24, Train_acc 82.84, Test_acc 49.59
2025-01-13 12:01:57,054 [podnet.py] => Task 3, Epoch 97/160 (LR 0.03362) => LSC_loss 0.59, Spatial_loss 1.63, Flat_loss 0.24, Train_acc 83.43, Test_acc 51.48
2025-01-13 12:02:05,094 [podnet.py] => Task 3, Epoch 98/160 (LR 0.03269) => LSC_loss 0.59, Spatial_loss 1.61, Flat_loss 0.24, Train_acc 83.59, Test_acc 50.52
2025-01-13 12:02:13,159 [podnet.py] => Task 3, Epoch 99/160 (LR 0.03178) => LSC_loss 0.57, Spatial_loss 1.58, Flat_loss 0.23, Train_acc 84.15, Test_acc 50.94
2025-01-13 12:02:21,243 [podnet.py] => Task 3, Epoch 100/160 (LR 0.03087) => LSC_loss 0.57, Spatial_loss 1.59, Flat_loss 0.23, Train_acc 83.87, Test_acc 47.95
2025-01-13 12:02:29,185 [podnet.py] => Task 3, Epoch 101/160 (LR 0.02996) => LSC_loss 0.56, Spatial_loss 1.58, Flat_loss 0.23, Train_acc 84.43, Test_acc 49.70
2025-01-13 12:02:37,020 [podnet.py] => Task 3, Epoch 102/160 (LR 0.02907) => LSC_loss 0.56, Spatial_loss 1.58, Flat_loss 0.23, Train_acc 84.74, Test_acc 52.81
2025-01-13 12:02:44,871 [podnet.py] => Task 3, Epoch 103/160 (LR 0.02818) => LSC_loss 0.55, Spatial_loss 1.58, Flat_loss 0.23, Train_acc 85.03, Test_acc 50.82
2025-01-13 12:02:52,792 [podnet.py] => Task 3, Epoch 104/160 (LR 0.02730) => LSC_loss 0.53, Spatial_loss 1.56, Flat_loss 0.23, Train_acc 85.63, Test_acc 52.08
2025-01-13 12:03:00,706 [podnet.py] => Task 3, Epoch 105/160 (LR 0.02643) => LSC_loss 0.52, Spatial_loss 1.54, Flat_loss 0.22, Train_acc 85.69, Test_acc 52.20
2025-01-13 12:03:08,757 [podnet.py] => Task 3, Epoch 106/160 (LR 0.02557) => LSC_loss 0.52, Spatial_loss 1.53, Flat_loss 0.22, Train_acc 86.15, Test_acc 50.46
2025-01-13 12:03:16,680 [podnet.py] => Task 3, Epoch 107/160 (LR 0.02472) => LSC_loss 0.52, Spatial_loss 1.52, Flat_loss 0.22, Train_acc 85.74, Test_acc 51.99
2025-01-13 12:03:24,632 [podnet.py] => Task 3, Epoch 108/160 (LR 0.02388) => LSC_loss 0.50, Spatial_loss 1.52, Flat_loss 0.22, Train_acc 86.69, Test_acc 50.80
2025-01-13 12:03:32,730 [podnet.py] => Task 3, Epoch 109/160 (LR 0.02304) => LSC_loss 0.51, Spatial_loss 1.51, Flat_loss 0.22, Train_acc 86.38, Test_acc 52.04
2025-01-13 12:03:41,047 [podnet.py] => Task 3, Epoch 110/160 (LR 0.02222) => LSC_loss 0.50, Spatial_loss 1.49, Flat_loss 0.22, Train_acc 86.50, Test_acc 50.11
2025-01-13 12:03:49,195 [podnet.py] => Task 3, Epoch 111/160 (LR 0.02141) => LSC_loss 0.50, Spatial_loss 1.47, Flat_loss 0.22, Train_acc 86.67, Test_acc 52.95
2025-01-13 12:03:57,054 [podnet.py] => Task 3, Epoch 112/160 (LR 0.02061) => LSC_loss 0.48, Spatial_loss 1.44, Flat_loss 0.21, Train_acc 87.34, Test_acc 53.78
2025-01-13 12:04:05,118 [podnet.py] => Task 3, Epoch 113/160 (LR 0.01982) => LSC_loss 0.46, Spatial_loss 1.44, Flat_loss 0.21, Train_acc 88.14, Test_acc 52.52
2025-01-13 12:04:13,074 [podnet.py] => Task 3, Epoch 114/160 (LR 0.01905) => LSC_loss 0.47, Spatial_loss 1.45, Flat_loss 0.21, Train_acc 87.75, Test_acc 51.01
2025-01-13 12:04:21,128 [podnet.py] => Task 3, Epoch 115/160 (LR 0.01828) => LSC_loss 0.46, Spatial_loss 1.44, Flat_loss 0.21, Train_acc 87.68, Test_acc 53.54
2025-01-13 12:04:28,985 [podnet.py] => Task 3, Epoch 116/160 (LR 0.01753) => LSC_loss 0.44, Spatial_loss 1.41, Flat_loss 0.21, Train_acc 88.67, Test_acc 53.25
2025-01-13 12:04:37,057 [podnet.py] => Task 3, Epoch 117/160 (LR 0.01679) => LSC_loss 0.44, Spatial_loss 1.40, Flat_loss 0.21, Train_acc 88.69, Test_acc 53.91
2025-01-13 12:04:44,941 [podnet.py] => Task 3, Epoch 118/160 (LR 0.01606) => LSC_loss 0.45, Spatial_loss 1.41, Flat_loss 0.21, Train_acc 88.44, Test_acc 53.46
2025-01-13 12:04:52,953 [podnet.py] => Task 3, Epoch 119/160 (LR 0.01535) => LSC_loss 0.44, Spatial_loss 1.39, Flat_loss 0.21, Train_acc 88.96, Test_acc 53.65
2025-01-13 12:05:00,912 [podnet.py] => Task 3, Epoch 120/160 (LR 0.01464) => LSC_loss 0.43, Spatial_loss 1.38, Flat_loss 0.20, Train_acc 88.86, Test_acc 53.08
2025-01-13 12:05:08,984 [podnet.py] => Task 3, Epoch 121/160 (LR 0.01396) => LSC_loss 0.42, Spatial_loss 1.37, Flat_loss 0.20, Train_acc 89.29, Test_acc 54.04
2025-01-13 12:05:16,915 [podnet.py] => Task 3, Epoch 122/160 (LR 0.01328) => LSC_loss 0.42, Spatial_loss 1.38, Flat_loss 0.20, Train_acc 89.32, Test_acc 53.48
2025-01-13 12:05:24,111 [podnet.py] => Task 3, Epoch 123/160 (LR 0.01262) => LSC_loss 0.41, Spatial_loss 1.36, Flat_loss 0.20, Train_acc 89.52, Test_acc 52.09
2025-01-13 12:05:31,921 [podnet.py] => Task 3, Epoch 124/160 (LR 0.01198) => LSC_loss 0.40, Spatial_loss 1.35, Flat_loss 0.20, Train_acc 90.11, Test_acc 53.76
2025-01-13 12:05:39,915 [podnet.py] => Task 3, Epoch 125/160 (LR 0.01135) => LSC_loss 0.41, Spatial_loss 1.36, Flat_loss 0.20, Train_acc 89.85, Test_acc 54.34
2025-01-13 12:05:48,032 [podnet.py] => Task 3, Epoch 126/160 (LR 0.01073) => LSC_loss 0.40, Spatial_loss 1.34, Flat_loss 0.20, Train_acc 90.08, Test_acc 54.35
2025-01-13 12:05:56,014 [podnet.py] => Task 3, Epoch 127/160 (LR 0.01013) => LSC_loss 0.40, Spatial_loss 1.34, Flat_loss 0.19, Train_acc 90.44, Test_acc 53.41
2025-01-13 12:06:04,120 [podnet.py] => Task 3, Epoch 128/160 (LR 0.00955) => LSC_loss 0.38, Spatial_loss 1.31, Flat_loss 0.19, Train_acc 90.61, Test_acc 53.82
2025-01-13 12:06:12,154 [podnet.py] => Task 3, Epoch 129/160 (LR 0.00898) => LSC_loss 0.37, Spatial_loss 1.28, Flat_loss 0.19, Train_acc 91.56, Test_acc 53.82
2025-01-13 12:06:20,258 [podnet.py] => Task 3, Epoch 130/160 (LR 0.00843) => LSC_loss 0.37, Spatial_loss 1.28, Flat_loss 0.19, Train_acc 91.25, Test_acc 55.08
2025-01-13 12:06:28,103 [podnet.py] => Task 3, Epoch 131/160 (LR 0.00789) => LSC_loss 0.37, Spatial_loss 1.28, Flat_loss 0.19, Train_acc 91.43, Test_acc 53.69
2025-01-13 12:06:36,293 [podnet.py] => Task 3, Epoch 132/160 (LR 0.00737) => LSC_loss 0.37, Spatial_loss 1.27, Flat_loss 0.19, Train_acc 91.52, Test_acc 54.09
2025-01-13 12:06:44,489 [podnet.py] => Task 3, Epoch 133/160 (LR 0.00686) => LSC_loss 0.36, Spatial_loss 1.27, Flat_loss 0.19, Train_acc 91.47, Test_acc 54.36
2025-01-13 12:06:52,399 [podnet.py] => Task 3, Epoch 134/160 (LR 0.00638) => LSC_loss 0.35, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 91.98, Test_acc 53.50
2025-01-13 12:07:00,208 [podnet.py] => Task 3, Epoch 135/160 (LR 0.00590) => LSC_loss 0.35, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 91.81, Test_acc 54.54
2025-01-13 12:07:08,243 [podnet.py] => Task 3, Epoch 136/160 (LR 0.00545) => LSC_loss 0.34, Spatial_loss 1.23, Flat_loss 0.18, Train_acc 92.09, Test_acc 55.10
2025-01-13 12:07:16,207 [podnet.py] => Task 3, Epoch 137/160 (LR 0.00501) => LSC_loss 0.35, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 92.09, Test_acc 54.58
2025-01-13 12:07:24,025 [podnet.py] => Task 3, Epoch 138/160 (LR 0.00459) => LSC_loss 0.34, Spatial_loss 1.23, Flat_loss 0.18, Train_acc 92.29, Test_acc 54.22
2025-01-13 12:07:31,817 [podnet.py] => Task 3, Epoch 139/160 (LR 0.00419) => LSC_loss 0.33, Spatial_loss 1.22, Flat_loss 0.18, Train_acc 92.77, Test_acc 55.02
2025-01-13 12:07:39,830 [podnet.py] => Task 3, Epoch 140/160 (LR 0.00381) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 92.67, Test_acc 55.00
2025-01-13 12:07:47,847 [podnet.py] => Task 3, Epoch 141/160 (LR 0.00344) => LSC_loss 0.32, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 93.02, Test_acc 55.05
2025-01-13 12:07:56,008 [podnet.py] => Task 3, Epoch 142/160 (LR 0.00309) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 92.59, Test_acc 55.08
2025-01-13 12:08:03,904 [podnet.py] => Task 3, Epoch 143/160 (LR 0.00276) => LSC_loss 0.33, Spatial_loss 1.19, Flat_loss 0.18, Train_acc 92.93, Test_acc 55.20
2025-01-13 12:08:11,866 [podnet.py] => Task 3, Epoch 144/160 (LR 0.00245) => LSC_loss 0.33, Spatial_loss 1.18, Flat_loss 0.18, Train_acc 92.89, Test_acc 55.59
2025-01-13 12:08:20,010 [podnet.py] => Task 3, Epoch 145/160 (LR 0.00215) => LSC_loss 0.32, Spatial_loss 1.19, Flat_loss 0.18, Train_acc 93.40, Test_acc 55.14
2025-01-13 12:08:27,990 [podnet.py] => Task 3, Epoch 146/160 (LR 0.00188) => LSC_loss 0.31, Spatial_loss 1.18, Flat_loss 0.18, Train_acc 93.39, Test_acc 55.46
2025-01-13 12:08:36,015 [podnet.py] => Task 3, Epoch 147/160 (LR 0.00162) => LSC_loss 0.31, Spatial_loss 1.16, Flat_loss 0.18, Train_acc 93.43, Test_acc 54.94
2025-01-13 12:08:44,002 [podnet.py] => Task 3, Epoch 148/160 (LR 0.00138) => LSC_loss 0.32, Spatial_loss 1.17, Flat_loss 0.18, Train_acc 93.56, Test_acc 55.54
2025-01-13 12:08:51,947 [podnet.py] => Task 3, Epoch 149/160 (LR 0.00116) => LSC_loss 0.31, Spatial_loss 1.16, Flat_loss 0.17, Train_acc 93.66, Test_acc 55.05
2025-01-13 12:09:00,094 [podnet.py] => Task 3, Epoch 150/160 (LR 0.00096) => LSC_loss 0.31, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 93.64, Test_acc 55.38
2025-01-13 12:09:08,036 [podnet.py] => Task 3, Epoch 151/160 (LR 0.00078) => LSC_loss 0.31, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 93.78, Test_acc 55.42
2025-01-13 12:09:15,102 [podnet.py] => Task 3, Epoch 152/160 (LR 0.00062) => LSC_loss 0.31, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 94.02, Test_acc 55.48
2025-01-13 12:09:22,569 [podnet.py] => Task 3, Epoch 153/160 (LR 0.00047) => LSC_loss 0.30, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 93.99, Test_acc 55.24
2025-01-13 12:09:30,402 [podnet.py] => Task 3, Epoch 154/160 (LR 0.00035) => LSC_loss 0.30, Spatial_loss 1.14, Flat_loss 0.17, Train_acc 94.03, Test_acc 55.50
2025-01-13 12:09:37,971 [podnet.py] => Task 3, Epoch 155/160 (LR 0.00024) => LSC_loss 0.31, Spatial_loss 1.14, Flat_loss 0.17, Train_acc 93.89, Test_acc 55.25
2025-01-13 12:09:46,112 [podnet.py] => Task 3, Epoch 156/160 (LR 0.00015) => LSC_loss 0.31, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 93.71, Test_acc 55.44
2025-01-13 12:09:54,057 [podnet.py] => Task 3, Epoch 157/160 (LR 0.00009) => LSC_loss 0.30, Spatial_loss 1.14, Flat_loss 0.17, Train_acc 93.85, Test_acc 55.38
2025-01-13 12:10:01,951 [podnet.py] => Task 3, Epoch 158/160 (LR 0.00004) => LSC_loss 0.30, Spatial_loss 1.14, Flat_loss 0.17, Train_acc 93.89, Test_acc 55.50
2025-01-13 12:10:08,936 [podnet.py] => Task 3, Epoch 159/160 (LR 0.00001) => LSC_loss 0.31, Spatial_loss 1.14, Flat_loss 0.17, Train_acc 93.78, Test_acc 55.60
2025-01-13 12:10:16,848 [podnet.py] => Task 3, Epoch 160/160 (LR 0.00000) => LSC_loss 0.30, Spatial_loss 1.15, Flat_loss 0.17, Train_acc 93.92, Test_acc 55.30
2025-01-13 12:10:16,848 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-13 12:10:16,848 [base.py] => Reducing exemplars...(73 per classes)
2025-01-13 12:11:07,503 [base.py] => Constructing exemplars...(73 per classes)
2025-01-13 12:11:44,605 [podnet.py] => The size of finetune dataset: 5840
2025-01-13 12:11:49,425 [podnet.py] => Task 3, Epoch 1/20 (LR 0.00497) => LSC_loss 0.27, Spatial_loss 1.15, Flat_loss 0.13, Train_acc 94.37, Test_acc 56.76
2025-01-13 12:11:54,188 [podnet.py] => Task 3, Epoch 2/20 (LR 0.00488) => LSC_loss 0.25, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 94.64, Test_acc 56.66
2025-01-13 12:11:58,935 [podnet.py] => Task 3, Epoch 3/20 (LR 0.00473) => LSC_loss 0.24, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 94.90, Test_acc 56.55
2025-01-13 12:12:03,458 [podnet.py] => Task 3, Epoch 4/20 (LR 0.00452) => LSC_loss 0.25, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 94.64, Test_acc 56.70
2025-01-13 12:12:08,258 [podnet.py] => Task 3, Epoch 5/20 (LR 0.00427) => LSC_loss 0.23, Spatial_loss 1.13, Flat_loss 0.12, Train_acc 95.02, Test_acc 57.19
2025-01-13 12:12:12,921 [podnet.py] => Task 3, Epoch 6/20 (LR 0.00397) => LSC_loss 0.22, Spatial_loss 1.11, Flat_loss 0.12, Train_acc 95.74, Test_acc 56.48
2025-01-13 12:12:17,659 [podnet.py] => Task 3, Epoch 7/20 (LR 0.00363) => LSC_loss 0.23, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 95.53, Test_acc 56.89
2025-01-13 12:12:22,146 [podnet.py] => Task 3, Epoch 8/20 (LR 0.00327) => LSC_loss 0.22, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 95.98, Test_acc 56.88
2025-01-13 12:12:26,601 [podnet.py] => Task 3, Epoch 9/20 (LR 0.00289) => LSC_loss 0.22, Spatial_loss 1.11, Flat_loss 0.12, Train_acc 96.03, Test_acc 56.96
2025-01-13 12:12:31,048 [podnet.py] => Task 3, Epoch 10/20 (LR 0.00250) => LSC_loss 0.21, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 95.87, Test_acc 56.75
2025-01-13 12:12:35,926 [podnet.py] => Task 3, Epoch 11/20 (LR 0.00211) => LSC_loss 0.21, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 95.98, Test_acc 57.19
2025-01-13 12:12:40,353 [podnet.py] => Task 3, Epoch 12/20 (LR 0.00173) => LSC_loss 0.21, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 96.37, Test_acc 56.98
2025-01-13 12:12:44,946 [podnet.py] => Task 3, Epoch 13/20 (LR 0.00137) => LSC_loss 0.21, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 96.40, Test_acc 57.14
2025-01-13 12:12:49,701 [podnet.py] => Task 3, Epoch 14/20 (LR 0.00103) => LSC_loss 0.20, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 96.58, Test_acc 57.04
2025-01-13 12:12:54,101 [podnet.py] => Task 3, Epoch 15/20 (LR 0.00073) => LSC_loss 0.21, Spatial_loss 1.06, Flat_loss 0.11, Train_acc 96.03, Test_acc 57.15
2025-01-13 12:12:58,756 [podnet.py] => Task 3, Epoch 16/20 (LR 0.00048) => LSC_loss 0.20, Spatial_loss 1.06, Flat_loss 0.11, Train_acc 96.42, Test_acc 57.21
2025-01-13 12:13:03,500 [podnet.py] => Task 3, Epoch 17/20 (LR 0.00027) => LSC_loss 0.20, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 96.71, Test_acc 57.12
2025-01-13 12:13:08,210 [podnet.py] => Task 3, Epoch 18/20 (LR 0.00012) => LSC_loss 0.20, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 96.52, Test_acc 57.19
2025-01-13 12:13:12,890 [podnet.py] => Task 3, Epoch 19/20 (LR 0.00003) => LSC_loss 0.21, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 96.61, Test_acc 57.08
2025-01-13 12:13:17,429 [podnet.py] => Task 3, Epoch 20/20 (LR 0.00000) => LSC_loss 0.20, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 96.73, Test_acc 57.09
2025-01-13 12:13:17,430 [base.py] => Reducing exemplars...(55 per classes)
2025-01-13 12:14:07,622 [base.py] => Constructing exemplars...(55 per classes)
2025-01-13 12:14:44,980 [podnet.py] => Exemplar size: 4400
2025-01-13 12:14:44,980 [trainer.py] => CNN: {'total': np.float64(57.09), '00-09': np.float64(69.4), '10-19': np.float64(52.6), '20-29': np.float64(58.5), '30-39': np.float64(49.4), '40-49': np.float64(59.1), '50-59': np.float64(49.5), '60-69': np.float64(63.3), '70-79': np.float64(54.9), 'old': np.float64(56.42), 'new': np.float64(59.1)}
2025-01-13 12:14:44,981 [trainer.py] => NME: {'total': np.float64(55.65), '00-09': np.float64(70.1), '10-19': np.float64(56.4), '20-29': np.float64(57.1), '30-39': np.float64(46.4), '40-49': np.float64(57.1), '50-59': np.float64(45.0), '60-69': np.float64(61.5), '70-79': np.float64(51.6), 'old': np.float64(55.35), 'new': np.float64(56.55)}
2025-01-13 12:14:44,981 [trainer.py] => CNN top1 curve: [np.float64(82.45), np.float64(69.32), np.float64(62.65), np.float64(57.09)]
2025-01-13 12:14:44,981 [trainer.py] => CNN top5 curve: [np.float64(96.4), np.float64(91.95), np.float64(88.07), np.float64(84.09)]
2025-01-13 12:14:44,981 [trainer.py] => NME top1 curve: [np.float64(82.05), np.float64(69.08), np.float64(61.6), np.float64(55.65)]
2025-01-13 12:14:44,981 [trainer.py] => NME top5 curve: [np.float64(96.2), np.float64(91.62), np.float64(87.72), np.float64(83.79)]

2025-01-13 12:14:44,981 [trainer.py] => All params: 517457
2025-01-13 12:14:44,981 [trainer.py] => Trainable params: 517457
2025-01-13 12:14:44,982 [podnet.py] => Learning on 80-100
2025-01-13 12:14:45,115 [podnet.py] => Adaptive factor: 2.23606797749979
2025-01-13 12:14:53,035 [podnet.py] => Task 4, Epoch 1/160 (LR 0.09999) => LSC_loss 2.51, Spatial_loss 2.97, Flat_loss 0.85, Train_acc 43.38, Test_acc 32.78
2025-01-13 12:15:01,051 [podnet.py] => Task 4, Epoch 2/160 (LR 0.09996) => LSC_loss 1.66, Spatial_loss 2.54, Flat_loss 0.49, Train_acc 56.56, Test_acc 32.04
2025-01-13 12:15:09,109 [podnet.py] => Task 4, Epoch 3/160 (LR 0.09991) => LSC_loss 1.51, Spatial_loss 2.40, Flat_loss 0.41, Train_acc 59.67, Test_acc 29.84
2025-01-13 12:15:17,211 [podnet.py] => Task 4, Epoch 4/160 (LR 0.09985) => LSC_loss 1.43, Spatial_loss 2.29, Flat_loss 0.37, Train_acc 62.35, Test_acc 40.04
2025-01-13 12:15:25,488 [podnet.py] => Task 4, Epoch 5/160 (LR 0.09976) => LSC_loss 1.36, Spatial_loss 2.24, Flat_loss 0.35, Train_acc 64.27, Test_acc 37.80
2025-01-13 12:15:33,663 [podnet.py] => Task 4, Epoch 6/160 (LR 0.09965) => LSC_loss 1.33, Spatial_loss 2.24, Flat_loss 0.34, Train_acc 64.49, Test_acc 40.06
2025-01-13 12:15:41,795 [podnet.py] => Task 4, Epoch 7/160 (LR 0.09953) => LSC_loss 1.29, Spatial_loss 2.18, Flat_loss 0.33, Train_acc 65.32, Test_acc 41.71
2025-01-13 12:15:49,842 [podnet.py] => Task 4, Epoch 8/160 (LR 0.09938) => LSC_loss 1.27, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 65.94, Test_acc 38.31
2025-01-13 12:15:57,865 [podnet.py] => Task 4, Epoch 9/160 (LR 0.09922) => LSC_loss 1.24, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 66.76, Test_acc 39.33
2025-01-13 12:16:06,256 [podnet.py] => Task 4, Epoch 10/160 (LR 0.09904) => LSC_loss 1.24, Spatial_loss 2.16, Flat_loss 0.32, Train_acc 66.94, Test_acc 39.55
2025-01-13 12:16:14,900 [podnet.py] => Task 4, Epoch 11/160 (LR 0.09884) => LSC_loss 1.21, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 67.50, Test_acc 38.23
2025-01-13 12:16:23,237 [podnet.py] => Task 4, Epoch 12/160 (LR 0.09862) => LSC_loss 1.19, Spatial_loss 2.15, Flat_loss 0.32, Train_acc 68.22, Test_acc 41.19
2025-01-13 12:16:31,328 [podnet.py] => Task 4, Epoch 13/160 (LR 0.09838) => LSC_loss 1.18, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 68.49, Test_acc 40.52
2025-01-13 12:16:39,456 [podnet.py] => Task 4, Epoch 14/160 (LR 0.09812) => LSC_loss 1.14, Spatial_loss 2.12, Flat_loss 0.31, Train_acc 69.46, Test_acc 38.78
2025-01-13 12:16:47,364 [podnet.py] => Task 4, Epoch 15/160 (LR 0.09785) => LSC_loss 1.14, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 69.42, Test_acc 42.05
2025-01-13 12:16:55,611 [podnet.py] => Task 4, Epoch 16/160 (LR 0.09755) => LSC_loss 1.15, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 69.45, Test_acc 41.79
2025-01-13 12:17:03,758 [podnet.py] => Task 4, Epoch 17/160 (LR 0.09724) => LSC_loss 1.13, Spatial_loss 2.10, Flat_loss 0.31, Train_acc 69.61, Test_acc 39.74
2025-01-13 12:17:11,716 [podnet.py] => Task 4, Epoch 18/160 (LR 0.09691) => LSC_loss 1.11, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 70.69, Test_acc 36.62
2025-01-13 12:17:19,734 [podnet.py] => Task 4, Epoch 19/160 (LR 0.09656) => LSC_loss 1.11, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 69.80, Test_acc 41.03
2025-01-13 12:17:27,850 [podnet.py] => Task 4, Epoch 20/160 (LR 0.09619) => LSC_loss 1.11, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 70.10, Test_acc 39.38
2025-01-13 12:17:35,884 [podnet.py] => Task 4, Epoch 21/160 (LR 0.09581) => LSC_loss 1.11, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 70.03, Test_acc 42.50
2025-01-13 12:17:44,035 [podnet.py] => Task 4, Epoch 22/160 (LR 0.09541) => LSC_loss 1.10, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 70.67, Test_acc 41.88
2025-01-13 12:17:52,205 [podnet.py] => Task 4, Epoch 23/160 (LR 0.09499) => LSC_loss 1.08, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 70.76, Test_acc 42.83
2025-01-13 12:18:00,308 [podnet.py] => Task 4, Epoch 24/160 (LR 0.09455) => LSC_loss 1.07, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 71.10, Test_acc 40.88
2025-01-13 12:18:08,173 [podnet.py] => Task 4, Epoch 25/160 (LR 0.09410) => LSC_loss 1.07, Spatial_loss 2.09, Flat_loss 0.31, Train_acc 70.96, Test_acc 38.08
2025-01-13 12:18:16,052 [podnet.py] => Task 4, Epoch 26/160 (LR 0.09362) => LSC_loss 1.06, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 71.39, Test_acc 39.77
2025-01-13 12:18:24,204 [podnet.py] => Task 4, Epoch 27/160 (LR 0.09314) => LSC_loss 1.04, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 71.85, Test_acc 44.18
2025-01-13 12:18:32,341 [podnet.py] => Task 4, Epoch 28/160 (LR 0.09263) => LSC_loss 1.05, Spatial_loss 2.09, Flat_loss 0.31, Train_acc 72.24, Test_acc 42.05
2025-01-13 12:18:40,227 [podnet.py] => Task 4, Epoch 29/160 (LR 0.09211) => LSC_loss 1.05, Spatial_loss 2.09, Flat_loss 0.32, Train_acc 72.08, Test_acc 41.01
2025-01-13 12:18:48,216 [podnet.py] => Task 4, Epoch 30/160 (LR 0.09157) => LSC_loss 1.02, Spatial_loss 2.08, Flat_loss 0.31, Train_acc 72.35, Test_acc 39.80
2025-01-13 12:18:56,505 [podnet.py] => Task 4, Epoch 31/160 (LR 0.09102) => LSC_loss 1.05, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 71.60, Test_acc 40.12
2025-01-13 12:19:04,485 [podnet.py] => Task 4, Epoch 32/160 (LR 0.09045) => LSC_loss 1.03, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 72.44, Test_acc 34.87
2025-01-13 12:19:12,442 [podnet.py] => Task 4, Epoch 33/160 (LR 0.08987) => LSC_loss 1.02, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 72.78, Test_acc 41.51
2025-01-13 12:19:20,545 [podnet.py] => Task 4, Epoch 34/160 (LR 0.08927) => LSC_loss 1.02, Spatial_loss 2.09, Flat_loss 0.32, Train_acc 72.47, Test_acc 38.95
2025-01-13 12:19:28,648 [podnet.py] => Task 4, Epoch 35/160 (LR 0.08865) => LSC_loss 1.01, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 72.92, Test_acc 41.24
2025-01-13 12:19:36,757 [podnet.py] => Task 4, Epoch 36/160 (LR 0.08802) => LSC_loss 1.01, Spatial_loss 2.09, Flat_loss 0.31, Train_acc 72.65, Test_acc 39.01
2025-01-13 12:19:45,028 [podnet.py] => Task 4, Epoch 37/160 (LR 0.08738) => LSC_loss 1.02, Spatial_loss 2.09, Flat_loss 0.32, Train_acc 72.61, Test_acc 39.84
2025-01-13 12:19:53,361 [podnet.py] => Task 4, Epoch 38/160 (LR 0.08672) => LSC_loss 1.02, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 72.72, Test_acc 42.70
2025-01-13 12:20:01,418 [podnet.py] => Task 4, Epoch 39/160 (LR 0.08604) => LSC_loss 1.00, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 73.16, Test_acc 42.01
2025-01-13 12:20:09,347 [podnet.py] => Task 4, Epoch 40/160 (LR 0.08536) => LSC_loss 0.99, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 73.46, Test_acc 40.67
2025-01-13 12:20:16,989 [podnet.py] => Task 4, Epoch 41/160 (LR 0.08465) => LSC_loss 0.98, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 73.42, Test_acc 35.29
2025-01-13 12:20:25,222 [podnet.py] => Task 4, Epoch 42/160 (LR 0.08394) => LSC_loss 0.99, Spatial_loss 2.10, Flat_loss 0.31, Train_acc 73.51, Test_acc 42.00
2025-01-13 12:20:33,437 [podnet.py] => Task 4, Epoch 43/160 (LR 0.08321) => LSC_loss 0.98, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 73.08, Test_acc 40.77
2025-01-13 12:20:41,453 [podnet.py] => Task 4, Epoch 44/160 (LR 0.08247) => LSC_loss 0.98, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 74.01, Test_acc 39.73
2025-01-13 12:20:49,636 [podnet.py] => Task 4, Epoch 45/160 (LR 0.08172) => LSC_loss 0.95, Spatial_loss 2.03, Flat_loss 0.31, Train_acc 73.82, Test_acc 41.81
2025-01-13 12:20:57,727 [podnet.py] => Task 4, Epoch 46/160 (LR 0.08095) => LSC_loss 0.95, Spatial_loss 2.04, Flat_loss 0.31, Train_acc 74.36, Test_acc 39.17
2025-01-13 12:21:05,789 [podnet.py] => Task 4, Epoch 47/160 (LR 0.08018) => LSC_loss 0.96, Spatial_loss 2.05, Flat_loss 0.31, Train_acc 74.09, Test_acc 39.43
2025-01-13 12:21:13,820 [podnet.py] => Task 4, Epoch 48/160 (LR 0.07939) => LSC_loss 0.97, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 73.74, Test_acc 43.69
2025-01-13 12:21:22,075 [podnet.py] => Task 4, Epoch 49/160 (LR 0.07859) => LSC_loss 0.92, Spatial_loss 2.04, Flat_loss 0.31, Train_acc 74.72, Test_acc 37.92
2025-01-13 12:21:30,229 [podnet.py] => Task 4, Epoch 50/160 (LR 0.07778) => LSC_loss 0.96, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 73.75, Test_acc 39.24
2025-01-13 12:21:38,268 [podnet.py] => Task 4, Epoch 51/160 (LR 0.07696) => LSC_loss 0.94, Spatial_loss 2.03, Flat_loss 0.31, Train_acc 74.40, Test_acc 43.83
2025-01-13 12:21:46,187 [podnet.py] => Task 4, Epoch 52/160 (LR 0.07612) => LSC_loss 0.93, Spatial_loss 2.02, Flat_loss 0.30, Train_acc 75.16, Test_acc 37.70
2025-01-13 12:21:54,100 [podnet.py] => Task 4, Epoch 53/160 (LR 0.07528) => LSC_loss 0.94, Spatial_loss 2.04, Flat_loss 0.31, Train_acc 74.68, Test_acc 42.29
2025-01-13 12:22:02,194 [podnet.py] => Task 4, Epoch 54/160 (LR 0.07443) => LSC_loss 0.91, Spatial_loss 1.99, Flat_loss 0.30, Train_acc 75.59, Test_acc 43.61
2025-01-13 12:22:09,725 [podnet.py] => Task 4, Epoch 55/160 (LR 0.07357) => LSC_loss 0.91, Spatial_loss 2.00, Flat_loss 0.30, Train_acc 75.40, Test_acc 43.00
2025-01-13 12:22:17,743 [podnet.py] => Task 4, Epoch 56/160 (LR 0.07270) => LSC_loss 0.90, Spatial_loss 2.01, Flat_loss 0.30, Train_acc 75.61, Test_acc 42.20
2025-01-13 12:22:25,697 [podnet.py] => Task 4, Epoch 57/160 (LR 0.07182) => LSC_loss 0.90, Spatial_loss 2.01, Flat_loss 0.30, Train_acc 75.52, Test_acc 37.69
2025-01-13 12:22:33,824 [podnet.py] => Task 4, Epoch 58/160 (LR 0.07093) => LSC_loss 0.90, Spatial_loss 2.04, Flat_loss 0.31, Train_acc 75.22, Test_acc 43.49
2025-01-13 12:22:42,026 [podnet.py] => Task 4, Epoch 59/160 (LR 0.07004) => LSC_loss 0.91, Spatial_loss 2.02, Flat_loss 0.30, Train_acc 74.87, Test_acc 40.32
2025-01-13 12:22:50,117 [podnet.py] => Task 4, Epoch 60/160 (LR 0.06913) => LSC_loss 0.90, Spatial_loss 2.00, Flat_loss 0.31, Train_acc 75.49, Test_acc 39.19
2025-01-13 12:22:58,235 [podnet.py] => Task 4, Epoch 61/160 (LR 0.06822) => LSC_loss 0.88, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 75.96, Test_acc 41.46
2025-01-13 12:23:06,352 [podnet.py] => Task 4, Epoch 62/160 (LR 0.06731) => LSC_loss 0.89, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 76.51, Test_acc 42.91
2025-01-13 12:23:14,480 [podnet.py] => Task 4, Epoch 63/160 (LR 0.06638) => LSC_loss 0.87, Spatial_loss 1.99, Flat_loss 0.30, Train_acc 76.51, Test_acc 42.07
2025-01-13 12:23:22,397 [podnet.py] => Task 4, Epoch 64/160 (LR 0.06545) => LSC_loss 0.86, Spatial_loss 1.95, Flat_loss 0.30, Train_acc 76.42, Test_acc 42.55
2025-01-13 12:23:30,548 [podnet.py] => Task 4, Epoch 65/160 (LR 0.06451) => LSC_loss 0.86, Spatial_loss 1.96, Flat_loss 0.30, Train_acc 76.94, Test_acc 41.04
2025-01-13 12:23:38,599 [podnet.py] => Task 4, Epoch 66/160 (LR 0.06357) => LSC_loss 0.87, Spatial_loss 1.97, Flat_loss 0.30, Train_acc 76.53, Test_acc 40.27
2025-01-13 12:23:46,739 [podnet.py] => Task 4, Epoch 67/160 (LR 0.06262) => LSC_loss 0.86, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 76.35, Test_acc 45.70
2025-01-13 12:23:54,967 [podnet.py] => Task 4, Epoch 68/160 (LR 0.06167) => LSC_loss 0.84, Spatial_loss 1.92, Flat_loss 0.29, Train_acc 77.60, Test_acc 42.39
2025-01-13 12:24:02,979 [podnet.py] => Task 4, Epoch 69/160 (LR 0.06072) => LSC_loss 0.83, Spatial_loss 1.94, Flat_loss 0.29, Train_acc 77.47, Test_acc 44.34
2025-01-13 12:24:10,989 [podnet.py] => Task 4, Epoch 70/160 (LR 0.05975) => LSC_loss 0.84, Spatial_loss 1.92, Flat_loss 0.29, Train_acc 77.68, Test_acc 41.94
2025-01-13 12:24:18,892 [podnet.py] => Task 4, Epoch 71/160 (LR 0.05879) => LSC_loss 0.82, Spatial_loss 1.93, Flat_loss 0.29, Train_acc 78.40, Test_acc 42.30
2025-01-13 12:24:26,941 [podnet.py] => Task 4, Epoch 72/160 (LR 0.05782) => LSC_loss 0.83, Spatial_loss 1.97, Flat_loss 0.30, Train_acc 77.15, Test_acc 41.36
2025-01-13 12:24:35,182 [podnet.py] => Task 4, Epoch 73/160 (LR 0.05685) => LSC_loss 0.81, Spatial_loss 1.93, Flat_loss 0.29, Train_acc 78.26, Test_acc 42.75
2025-01-13 12:24:43,721 [podnet.py] => Task 4, Epoch 74/160 (LR 0.05588) => LSC_loss 0.79, Spatial_loss 1.91, Flat_loss 0.29, Train_acc 78.50, Test_acc 43.63
2025-01-13 12:24:51,839 [podnet.py] => Task 4, Epoch 75/160 (LR 0.05490) => LSC_loss 0.78, Spatial_loss 1.89, Flat_loss 0.29, Train_acc 78.83, Test_acc 43.08
2025-01-13 12:24:59,988 [podnet.py] => Task 4, Epoch 76/160 (LR 0.05392) => LSC_loss 0.79, Spatial_loss 1.91, Flat_loss 0.29, Train_acc 78.58, Test_acc 42.44
2025-01-13 12:25:08,092 [podnet.py] => Task 4, Epoch 77/160 (LR 0.05294) => LSC_loss 0.77, Spatial_loss 1.87, Flat_loss 0.29, Train_acc 79.32, Test_acc 44.97
2025-01-13 12:25:16,251 [podnet.py] => Task 4, Epoch 78/160 (LR 0.05196) => LSC_loss 0.79, Spatial_loss 1.90, Flat_loss 0.29, Train_acc 78.91, Test_acc 44.01
2025-01-13 12:25:24,516 [podnet.py] => Task 4, Epoch 79/160 (LR 0.05098) => LSC_loss 0.78, Spatial_loss 1.89, Flat_loss 0.29, Train_acc 78.79, Test_acc 43.56
2025-01-13 12:25:32,595 [podnet.py] => Task 4, Epoch 80/160 (LR 0.05000) => LSC_loss 0.76, Spatial_loss 1.84, Flat_loss 0.28, Train_acc 79.69, Test_acc 44.12
2025-01-13 12:25:40,816 [podnet.py] => Task 4, Epoch 81/160 (LR 0.04902) => LSC_loss 0.75, Spatial_loss 1.87, Flat_loss 0.28, Train_acc 79.73, Test_acc 40.51
2025-01-13 12:25:48,954 [podnet.py] => Task 4, Epoch 82/160 (LR 0.04804) => LSC_loss 0.76, Spatial_loss 1.85, Flat_loss 0.28, Train_acc 79.59, Test_acc 43.30
2025-01-13 12:25:56,217 [podnet.py] => Task 4, Epoch 83/160 (LR 0.04706) => LSC_loss 0.75, Spatial_loss 1.84, Flat_loss 0.28, Train_acc 79.92, Test_acc 43.10
2025-01-13 12:26:04,356 [podnet.py] => Task 4, Epoch 84/160 (LR 0.04608) => LSC_loss 0.74, Spatial_loss 1.85, Flat_loss 0.28, Train_acc 79.56, Test_acc 46.01
2025-01-13 12:26:11,522 [podnet.py] => Task 4, Epoch 85/160 (LR 0.04510) => LSC_loss 0.72, Spatial_loss 1.83, Flat_loss 0.27, Train_acc 80.92, Test_acc 45.02
2025-01-13 12:26:19,920 [podnet.py] => Task 4, Epoch 86/160 (LR 0.04412) => LSC_loss 0.71, Spatial_loss 1.82, Flat_loss 0.27, Train_acc 80.97, Test_acc 43.60
2025-01-13 12:26:27,676 [podnet.py] => Task 4, Epoch 87/160 (LR 0.04315) => LSC_loss 0.72, Spatial_loss 1.81, Flat_loss 0.27, Train_acc 80.69, Test_acc 45.68
2025-01-13 12:26:35,576 [podnet.py] => Task 4, Epoch 88/160 (LR 0.04218) => LSC_loss 0.72, Spatial_loss 1.82, Flat_loss 0.28, Train_acc 80.68, Test_acc 45.21
2025-01-13 12:26:43,820 [podnet.py] => Task 4, Epoch 89/160 (LR 0.04121) => LSC_loss 0.71, Spatial_loss 1.80, Flat_loss 0.28, Train_acc 81.12, Test_acc 45.43
2025-01-13 12:26:52,183 [podnet.py] => Task 4, Epoch 90/160 (LR 0.04025) => LSC_loss 0.68, Spatial_loss 1.78, Flat_loss 0.27, Train_acc 82.06, Test_acc 46.51
2025-01-13 12:27:00,163 [podnet.py] => Task 4, Epoch 91/160 (LR 0.03928) => LSC_loss 0.68, Spatial_loss 1.77, Flat_loss 0.27, Train_acc 81.66, Test_acc 47.05
2025-01-13 12:27:08,351 [podnet.py] => Task 4, Epoch 92/160 (LR 0.03833) => LSC_loss 0.66, Spatial_loss 1.76, Flat_loss 0.27, Train_acc 82.76, Test_acc 45.97
2025-01-13 12:27:16,606 [podnet.py] => Task 4, Epoch 93/160 (LR 0.03738) => LSC_loss 0.67, Spatial_loss 1.76, Flat_loss 0.27, Train_acc 82.04, Test_acc 44.47
2025-01-13 12:27:24,964 [podnet.py] => Task 4, Epoch 94/160 (LR 0.03643) => LSC_loss 0.66, Spatial_loss 1.74, Flat_loss 0.27, Train_acc 82.35, Test_acc 45.68
2025-01-13 12:27:33,069 [podnet.py] => Task 4, Epoch 95/160 (LR 0.03549) => LSC_loss 0.66, Spatial_loss 1.76, Flat_loss 0.27, Train_acc 83.07, Test_acc 43.41
2025-01-13 12:27:41,268 [podnet.py] => Task 4, Epoch 96/160 (LR 0.03455) => LSC_loss 0.65, Spatial_loss 1.73, Flat_loss 0.26, Train_acc 82.40, Test_acc 47.52
2025-01-13 12:27:49,449 [podnet.py] => Task 4, Epoch 97/160 (LR 0.03362) => LSC_loss 0.62, Spatial_loss 1.71, Flat_loss 0.26, Train_acc 83.44, Test_acc 45.77
2025-01-13 12:27:57,435 [podnet.py] => Task 4, Epoch 98/160 (LR 0.03269) => LSC_loss 0.63, Spatial_loss 1.71, Flat_loss 0.26, Train_acc 83.48, Test_acc 44.47
2025-01-13 12:28:05,565 [podnet.py] => Task 4, Epoch 99/160 (LR 0.03178) => LSC_loss 0.61, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 83.92, Test_acc 42.64
2025-01-13 12:28:13,837 [podnet.py] => Task 4, Epoch 100/160 (LR 0.03087) => LSC_loss 0.61, Spatial_loss 1.68, Flat_loss 0.26, Train_acc 84.02, Test_acc 44.78
2025-01-13 12:28:22,022 [podnet.py] => Task 4, Epoch 101/160 (LR 0.02996) => LSC_loss 0.61, Spatial_loss 1.69, Flat_loss 0.26, Train_acc 83.69, Test_acc 45.83
2025-01-13 12:28:29,915 [podnet.py] => Task 4, Epoch 102/160 (LR 0.02907) => LSC_loss 0.59, Spatial_loss 1.67, Flat_loss 0.25, Train_acc 84.82, Test_acc 47.03
2025-01-13 12:28:37,998 [podnet.py] => Task 4, Epoch 103/160 (LR 0.02818) => LSC_loss 0.60, Spatial_loss 1.66, Flat_loss 0.25, Train_acc 84.37, Test_acc 47.25
2025-01-13 12:28:46,093 [podnet.py] => Task 4, Epoch 104/160 (LR 0.02730) => LSC_loss 0.57, Spatial_loss 1.64, Flat_loss 0.25, Train_acc 85.28, Test_acc 47.90
2025-01-13 12:28:54,372 [podnet.py] => Task 4, Epoch 105/160 (LR 0.02643) => LSC_loss 0.58, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 85.12, Test_acc 45.40
2025-01-13 12:29:02,551 [podnet.py] => Task 4, Epoch 106/160 (LR 0.02557) => LSC_loss 0.58, Spatial_loss 1.63, Flat_loss 0.25, Train_acc 85.06, Test_acc 48.03
2025-01-13 12:29:10,676 [podnet.py] => Task 4, Epoch 107/160 (LR 0.02472) => LSC_loss 0.56, Spatial_loss 1.62, Flat_loss 0.25, Train_acc 85.58, Test_acc 45.25
2025-01-13 12:29:18,946 [podnet.py] => Task 4, Epoch 108/160 (LR 0.02388) => LSC_loss 0.56, Spatial_loss 1.60, Flat_loss 0.24, Train_acc 85.62, Test_acc 46.67
2025-01-13 12:29:27,342 [podnet.py] => Task 4, Epoch 109/160 (LR 0.02304) => LSC_loss 0.54, Spatial_loss 1.58, Flat_loss 0.24, Train_acc 86.07, Test_acc 46.93
2025-01-13 12:29:35,429 [podnet.py] => Task 4, Epoch 110/160 (LR 0.02222) => LSC_loss 0.53, Spatial_loss 1.57, Flat_loss 0.24, Train_acc 86.59, Test_acc 49.00
2025-01-13 12:29:43,581 [podnet.py] => Task 4, Epoch 111/160 (LR 0.02141) => LSC_loss 0.52, Spatial_loss 1.55, Flat_loss 0.24, Train_acc 86.99, Test_acc 48.52
2025-01-13 12:29:51,756 [podnet.py] => Task 4, Epoch 112/160 (LR 0.02061) => LSC_loss 0.53, Spatial_loss 1.56, Flat_loss 0.24, Train_acc 86.78, Test_acc 47.07
2025-01-13 12:29:59,852 [podnet.py] => Task 4, Epoch 113/160 (LR 0.01982) => LSC_loss 0.51, Spatial_loss 1.53, Flat_loss 0.23, Train_acc 86.97, Test_acc 48.66
2025-01-13 12:30:07,592 [podnet.py] => Task 4, Epoch 114/160 (LR 0.01905) => LSC_loss 0.51, Spatial_loss 1.52, Flat_loss 0.23, Train_acc 87.15, Test_acc 47.26
2025-01-13 12:30:15,392 [podnet.py] => Task 4, Epoch 115/160 (LR 0.01828) => LSC_loss 0.51, Spatial_loss 1.55, Flat_loss 0.23, Train_acc 87.45, Test_acc 48.36
2025-01-13 12:30:22,535 [podnet.py] => Task 4, Epoch 116/160 (LR 0.01753) => LSC_loss 0.50, Spatial_loss 1.52, Flat_loss 0.23, Train_acc 87.40, Test_acc 48.97
2025-01-13 12:30:30,748 [podnet.py] => Task 4, Epoch 117/160 (LR 0.01679) => LSC_loss 0.48, Spatial_loss 1.50, Flat_loss 0.23, Train_acc 87.87, Test_acc 47.80
2025-01-13 12:30:38,666 [podnet.py] => Task 4, Epoch 118/160 (LR 0.01606) => LSC_loss 0.49, Spatial_loss 1.48, Flat_loss 0.23, Train_acc 87.49, Test_acc 47.67
2025-01-13 12:30:47,002 [podnet.py] => Task 4, Epoch 119/160 (LR 0.01535) => LSC_loss 0.48, Spatial_loss 1.48, Flat_loss 0.22, Train_acc 88.22, Test_acc 46.74
2025-01-13 12:30:54,959 [podnet.py] => Task 4, Epoch 120/160 (LR 0.01464) => LSC_loss 0.47, Spatial_loss 1.48, Flat_loss 0.23, Train_acc 88.31, Test_acc 45.67
2025-01-13 12:31:02,976 [podnet.py] => Task 4, Epoch 121/160 (LR 0.01396) => LSC_loss 0.46, Spatial_loss 1.45, Flat_loss 0.22, Train_acc 89.13, Test_acc 48.15
2025-01-13 12:31:10,964 [podnet.py] => Task 4, Epoch 122/160 (LR 0.01328) => LSC_loss 0.45, Spatial_loss 1.43, Flat_loss 0.22, Train_acc 89.08, Test_acc 47.74
2025-01-13 12:31:19,118 [podnet.py] => Task 4, Epoch 123/160 (LR 0.01262) => LSC_loss 0.45, Spatial_loss 1.44, Flat_loss 0.22, Train_acc 89.15, Test_acc 48.78
2025-01-13 12:31:27,330 [podnet.py] => Task 4, Epoch 124/160 (LR 0.01198) => LSC_loss 0.45, Spatial_loss 1.43, Flat_loss 0.22, Train_acc 89.33, Test_acc 49.78
2025-01-13 12:31:35,499 [podnet.py] => Task 4, Epoch 125/160 (LR 0.01135) => LSC_loss 0.43, Spatial_loss 1.41, Flat_loss 0.21, Train_acc 89.94, Test_acc 48.36
2025-01-13 12:31:43,642 [podnet.py] => Task 4, Epoch 126/160 (LR 0.01073) => LSC_loss 0.44, Spatial_loss 1.40, Flat_loss 0.21, Train_acc 89.80, Test_acc 48.08
2025-01-13 12:31:51,783 [podnet.py] => Task 4, Epoch 127/160 (LR 0.01013) => LSC_loss 0.43, Spatial_loss 1.38, Flat_loss 0.21, Train_acc 90.01, Test_acc 50.03
2025-01-13 12:32:00,072 [podnet.py] => Task 4, Epoch 128/160 (LR 0.00955) => LSC_loss 0.43, Spatial_loss 1.38, Flat_loss 0.21, Train_acc 89.97, Test_acc 50.17
2025-01-13 12:32:08,117 [podnet.py] => Task 4, Epoch 129/160 (LR 0.00898) => LSC_loss 0.42, Spatial_loss 1.37, Flat_loss 0.21, Train_acc 90.53, Test_acc 48.59
2025-01-13 12:32:15,156 [podnet.py] => Task 4, Epoch 130/160 (LR 0.00843) => LSC_loss 0.41, Spatial_loss 1.37, Flat_loss 0.21, Train_acc 90.88, Test_acc 49.52
2025-01-13 12:32:23,206 [podnet.py] => Task 4, Epoch 131/160 (LR 0.00789) => LSC_loss 0.40, Spatial_loss 1.36, Flat_loss 0.21, Train_acc 90.62, Test_acc 47.76
2025-01-13 12:32:31,301 [podnet.py] => Task 4, Epoch 132/160 (LR 0.00737) => LSC_loss 0.40, Spatial_loss 1.34, Flat_loss 0.21, Train_acc 91.02, Test_acc 50.45
2025-01-13 12:32:39,613 [podnet.py] => Task 4, Epoch 133/160 (LR 0.00686) => LSC_loss 0.40, Spatial_loss 1.33, Flat_loss 0.20, Train_acc 91.04, Test_acc 50.29
2025-01-13 12:32:47,620 [podnet.py] => Task 4, Epoch 134/160 (LR 0.00638) => LSC_loss 0.39, Spatial_loss 1.32, Flat_loss 0.20, Train_acc 91.50, Test_acc 49.70
2025-01-13 12:32:55,811 [podnet.py] => Task 4, Epoch 135/160 (LR 0.00590) => LSC_loss 0.39, Spatial_loss 1.32, Flat_loss 0.20, Train_acc 91.14, Test_acc 50.00
2025-01-13 12:33:03,757 [podnet.py] => Task 4, Epoch 136/160 (LR 0.00545) => LSC_loss 0.38, Spatial_loss 1.31, Flat_loss 0.20, Train_acc 91.47, Test_acc 50.60
2025-01-13 12:33:12,338 [podnet.py] => Task 4, Epoch 137/160 (LR 0.00501) => LSC_loss 0.38, Spatial_loss 1.29, Flat_loss 0.20, Train_acc 91.73, Test_acc 50.28
2025-01-13 12:33:20,661 [podnet.py] => Task 4, Epoch 138/160 (LR 0.00459) => LSC_loss 0.37, Spatial_loss 1.28, Flat_loss 0.20, Train_acc 92.10, Test_acc 49.75
2025-01-13 12:33:28,750 [podnet.py] => Task 4, Epoch 139/160 (LR 0.00419) => LSC_loss 0.37, Spatial_loss 1.27, Flat_loss 0.20, Train_acc 92.26, Test_acc 50.66
2025-01-13 12:33:36,889 [podnet.py] => Task 4, Epoch 140/160 (LR 0.00381) => LSC_loss 0.37, Spatial_loss 1.28, Flat_loss 0.20, Train_acc 92.22, Test_acc 49.64
2025-01-13 12:33:44,991 [podnet.py] => Task 4, Epoch 141/160 (LR 0.00344) => LSC_loss 0.37, Spatial_loss 1.26, Flat_loss 0.20, Train_acc 92.45, Test_acc 50.49
2025-01-13 12:33:52,943 [podnet.py] => Task 4, Epoch 142/160 (LR 0.00309) => LSC_loss 0.36, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 92.57, Test_acc 50.20
2025-01-13 12:34:01,089 [podnet.py] => Task 4, Epoch 143/160 (LR 0.00276) => LSC_loss 0.36, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 92.58, Test_acc 51.05
2025-01-13 12:34:09,275 [podnet.py] => Task 4, Epoch 144/160 (LR 0.00245) => LSC_loss 0.36, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 92.72, Test_acc 50.25
2025-01-13 12:34:17,768 [podnet.py] => Task 4, Epoch 145/160 (LR 0.00215) => LSC_loss 0.36, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 92.71, Test_acc 50.19
2025-01-13 12:34:25,570 [podnet.py] => Task 4, Epoch 146/160 (LR 0.00188) => LSC_loss 0.35, Spatial_loss 1.23, Flat_loss 0.19, Train_acc 92.60, Test_acc 50.50
2025-01-13 12:34:33,635 [podnet.py] => Task 4, Epoch 147/160 (LR 0.00162) => LSC_loss 0.35, Spatial_loss 1.22, Flat_loss 0.19, Train_acc 93.04, Test_acc 50.74
2025-01-13 12:34:41,877 [podnet.py] => Task 4, Epoch 148/160 (LR 0.00138) => LSC_loss 0.35, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 92.97, Test_acc 50.87
2025-01-13 12:34:49,840 [podnet.py] => Task 4, Epoch 149/160 (LR 0.00116) => LSC_loss 0.34, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 93.26, Test_acc 50.36
2025-01-13 12:34:57,833 [podnet.py] => Task 4, Epoch 150/160 (LR 0.00096) => LSC_loss 0.34, Spatial_loss 1.22, Flat_loss 0.19, Train_acc 93.17, Test_acc 50.58
2025-01-13 12:35:06,122 [podnet.py] => Task 4, Epoch 151/160 (LR 0.00078) => LSC_loss 0.34, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 93.42, Test_acc 50.43
2025-01-13 12:35:14,410 [podnet.py] => Task 4, Epoch 152/160 (LR 0.00062) => LSC_loss 0.34, Spatial_loss 1.21, Flat_loss 0.19, Train_acc 93.43, Test_acc 50.58
2025-01-13 12:35:22,916 [podnet.py] => Task 4, Epoch 153/160 (LR 0.00047) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.19, Train_acc 93.67, Test_acc 50.76
2025-01-13 12:35:30,959 [podnet.py] => Task 4, Epoch 154/160 (LR 0.00035) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.19, Train_acc 93.43, Test_acc 50.54
2025-01-13 12:35:38,742 [podnet.py] => Task 4, Epoch 155/160 (LR 0.00024) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.19, Train_acc 93.83, Test_acc 50.49
2025-01-13 12:35:46,512 [podnet.py] => Task 4, Epoch 156/160 (LR 0.00015) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.19, Train_acc 93.66, Test_acc 50.52
2025-01-13 12:35:54,770 [podnet.py] => Task 4, Epoch 157/160 (LR 0.00009) => LSC_loss 0.33, Spatial_loss 1.20, Flat_loss 0.19, Train_acc 93.54, Test_acc 50.53
2025-01-13 12:36:02,695 [podnet.py] => Task 4, Epoch 158/160 (LR 0.00004) => LSC_loss 0.33, Spatial_loss 1.18, Flat_loss 0.19, Train_acc 93.46, Test_acc 50.68
2025-01-13 12:36:11,016 [podnet.py] => Task 4, Epoch 159/160 (LR 0.00001) => LSC_loss 0.33, Spatial_loss 1.19, Flat_loss 0.19, Train_acc 93.62, Test_acc 50.68
2025-01-13 12:36:19,012 [podnet.py] => Task 4, Epoch 160/160 (LR 0.00000) => LSC_loss 0.34, Spatial_loss 1.19, Flat_loss 0.19, Train_acc 93.33, Test_acc 50.56
2025-01-13 12:36:19,013 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-13 12:36:19,013 [base.py] => Reducing exemplars...(55 per classes)
2025-01-13 12:37:26,574 [base.py] => Constructing exemplars...(55 per classes)
2025-01-13 12:38:03,447 [podnet.py] => The size of finetune dataset: 5500
2025-01-13 12:38:08,199 [podnet.py] => Task 4, Epoch 1/20 (LR 0.00497) => LSC_loss 0.30, Spatial_loss 1.19, Flat_loss 0.14, Train_acc 94.04, Test_acc 51.63
2025-01-13 12:38:12,929 [podnet.py] => Task 4, Epoch 2/20 (LR 0.00488) => LSC_loss 0.28, Spatial_loss 1.20, Flat_loss 0.13, Train_acc 94.31, Test_acc 52.57
2025-01-13 12:38:17,466 [podnet.py] => Task 4, Epoch 3/20 (LR 0.00473) => LSC_loss 0.26, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 94.85, Test_acc 52.64
2025-01-13 12:38:21,992 [podnet.py] => Task 4, Epoch 4/20 (LR 0.00452) => LSC_loss 0.26, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 95.29, Test_acc 52.45
2025-01-13 12:38:26,844 [podnet.py] => Task 4, Epoch 5/20 (LR 0.00427) => LSC_loss 0.26, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 95.16, Test_acc 52.66
2025-01-13 12:38:31,376 [podnet.py] => Task 4, Epoch 6/20 (LR 0.00397) => LSC_loss 0.25, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 95.44, Test_acc 52.28
2025-01-13 12:38:36,113 [podnet.py] => Task 4, Epoch 7/20 (LR 0.00363) => LSC_loss 0.24, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 96.00, Test_acc 52.59
2025-01-13 12:38:41,007 [podnet.py] => Task 4, Epoch 8/20 (LR 0.00327) => LSC_loss 0.25, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 95.47, Test_acc 52.67
2025-01-13 12:38:45,695 [podnet.py] => Task 4, Epoch 9/20 (LR 0.00289) => LSC_loss 0.24, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 95.89, Test_acc 52.87
2025-01-13 12:38:50,172 [podnet.py] => Task 4, Epoch 10/20 (LR 0.00250) => LSC_loss 0.24, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 95.65, Test_acc 52.51
2025-01-13 12:38:54,474 [podnet.py] => Task 4, Epoch 11/20 (LR 0.00211) => LSC_loss 0.23, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 95.84, Test_acc 52.49
2025-01-13 12:38:59,140 [podnet.py] => Task 4, Epoch 12/20 (LR 0.00173) => LSC_loss 0.23, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 96.22, Test_acc 52.93
2025-01-13 12:39:03,761 [podnet.py] => Task 4, Epoch 13/20 (LR 0.00137) => LSC_loss 0.23, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 96.47, Test_acc 52.84
2025-01-13 12:39:08,303 [podnet.py] => Task 4, Epoch 14/20 (LR 0.00103) => LSC_loss 0.23, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 96.47, Test_acc 52.91
2025-01-13 12:39:12,987 [podnet.py] => Task 4, Epoch 15/20 (LR 0.00073) => LSC_loss 0.23, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 96.38, Test_acc 53.04
2025-01-13 12:39:17,576 [podnet.py] => Task 4, Epoch 16/20 (LR 0.00048) => LSC_loss 0.22, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 97.02, Test_acc 52.97
2025-01-13 12:39:21,944 [podnet.py] => Task 4, Epoch 17/20 (LR 0.00027) => LSC_loss 0.22, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 96.73, Test_acc 53.12
2025-01-13 12:39:26,462 [podnet.py] => Task 4, Epoch 18/20 (LR 0.00012) => LSC_loss 0.22, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 96.58, Test_acc 53.07
2025-01-13 12:39:31,090 [podnet.py] => Task 4, Epoch 19/20 (LR 0.00003) => LSC_loss 0.22, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 96.62, Test_acc 52.84
2025-01-13 12:39:35,768 [podnet.py] => Task 4, Epoch 20/20 (LR 0.00000) => LSC_loss 0.22, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 96.58, Test_acc 53.10
2025-01-13 12:39:35,769 [base.py] => Reducing exemplars...(44 per classes)
2025-01-13 12:40:40,949 [base.py] => Constructing exemplars...(44 per classes)
2025-01-13 12:41:18,824 [podnet.py] => Exemplar size: 4400
2025-01-13 12:41:18,825 [trainer.py] => CNN: {'total': np.float64(53.1), '00-09': np.float64(62.5), '10-19': np.float64(47.1), '20-29': np.float64(53.6), '30-39': np.float64(46.3), '40-49': np.float64(56.1), '50-59': np.float64(45.2), '60-69': np.float64(55.5), '70-79': np.float64(50.2), '80-89': np.float64(59.5), '90-99': np.float64(55.0), 'old': np.float64(52.06), 'new': np.float64(57.25)}
2025-01-13 12:41:18,825 [trainer.py] => NME: {'total': np.float64(51.71), '00-09': np.float64(66.4), '10-19': np.float64(52.7), '20-29': np.float64(54.0), '30-39': np.float64(46.5), '40-49': np.float64(51.9), '50-59': np.float64(40.7), '60-69': np.float64(51.7), '70-79': np.float64(47.3), '80-89': np.float64(55.4), '90-99': np.float64(50.5), 'old': np.float64(51.4), 'new': np.float64(52.95)}
2025-01-13 12:41:18,825 [trainer.py] => CNN top1 curve: [np.float64(82.45), np.float64(69.32), np.float64(62.65), np.float64(57.09), np.float64(53.1)]
2025-01-13 12:41:18,825 [trainer.py] => CNN top5 curve: [np.float64(96.4), np.float64(91.95), np.float64(88.07), np.float64(84.09), np.float64(81.03)]
2025-01-13 12:41:18,825 [trainer.py] => NME top1 curve: [np.float64(82.05), np.float64(69.08), np.float64(61.6), np.float64(55.65), np.float64(51.71)]
2025-01-13 12:41:18,825 [trainer.py] => NME top5 curve: [np.float64(96.2), np.float64(91.62), np.float64(87.72), np.float64(83.79), np.float64(80.84)]

2025-01-13 12:41:18,825 [trainer.py] => End Time:1736768478.8253455
