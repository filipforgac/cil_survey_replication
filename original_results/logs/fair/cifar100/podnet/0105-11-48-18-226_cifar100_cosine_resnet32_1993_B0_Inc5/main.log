2025-01-05 11:48:18,260 [trainer.py] => Time Str >>> 0105-11-48-18-226
2025-01-05 11:48:18,334 [trainer.py] => prefix: fair
2025-01-05 11:48:18,334 [trainer.py] => dataset: cifar100
2025-01-05 11:48:18,334 [trainer.py] => memory_size: 13475
2025-01-05 11:48:18,334 [trainer.py] => memory_per_class: 20
2025-01-05 11:48:18,334 [trainer.py] => fixed_memory: False
2025-01-05 11:48:18,334 [trainer.py] => shuffle: True
2025-01-05 11:48:18,334 [trainer.py] => init_cls: 5
2025-01-05 11:48:18,334 [trainer.py] => increment: 5
2025-01-05 11:48:18,334 [trainer.py] => model_name: podnet
2025-01-05 11:48:18,334 [trainer.py] => convnet_type: cosine_resnet32
2025-01-05 11:48:18,334 [trainer.py] => device: [device(type='cuda', index=3)]
2025-01-05 11:48:18,334 [trainer.py] => seed: 1993
2025-01-05 11:48:18,334 [trainer.py] => debug: False
2025-01-05 11:48:18,334 [trainer.py] => skip: False
2025-01-05 11:48:18,335 [trainer.py] => config: ./exps/podnet.json
2025-01-05 11:48:18,335 [trainer.py] => time_str: 0105-11-48-18-226
2025-01-05 11:48:18,335 [trainer.py] => exp_name: 0105-11-48-18-226_cifar100_cosine_resnet32_1993_B0_Inc5
2025-01-05 11:48:18,335 [trainer.py] => logfilename: logs/fair/cifar100/podnet/0105-11-48-18-226_cifar100_cosine_resnet32_1993_B0_Inc5
2025-01-05 11:48:18,335 [trainer.py] => csv_name: cifar100_1993_cosine_resnet32_B0_Inc5
2025-01-05 11:48:20,082 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-01-05 11:48:20,239 [trainer.py] => Start time:1736074100.239954
2025-01-05 11:48:20,240 [trainer.py] => All params: 466256
2025-01-05 11:48:20,240 [trainer.py] => Trainable params: 466256
2025-01-05 11:48:20,240 [podnet.py] => Learning on 0-5
2025-01-05 11:48:20,243 [podnet.py] => Adaptive factor: 0
2025-01-05 11:48:32,827 [podnet.py] => Task 0, Epoch 1/160 (LR 0.09999) => LSC_loss 1.61, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 24.08, Test_acc 39.40
2025-01-05 11:48:35,142 [podnet.py] => Task 0, Epoch 2/160 (LR 0.09996) => LSC_loss 1.58, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 41.16, Test_acc 37.00
2025-01-05 11:48:37,529 [podnet.py] => Task 0, Epoch 3/160 (LR 0.09991) => LSC_loss 1.39, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 48.72, Test_acc 43.00
2025-01-05 11:48:39,911 [podnet.py] => Task 0, Epoch 4/160 (LR 0.09985) => LSC_loss 1.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 54.44, Test_acc 47.00
2025-01-05 11:48:42,048 [podnet.py] => Task 0, Epoch 5/160 (LR 0.09976) => LSC_loss 1.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 61.16, Test_acc 54.00
2025-01-05 11:48:44,462 [podnet.py] => Task 0, Epoch 6/160 (LR 0.09965) => LSC_loss 0.92, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 65.16, Test_acc 60.40
2025-01-05 11:48:46,870 [podnet.py] => Task 0, Epoch 7/160 (LR 0.09953) => LSC_loss 0.78, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 70.64, Test_acc 71.60
2025-01-05 11:48:49,561 [podnet.py] => Task 0, Epoch 8/160 (LR 0.09938) => LSC_loss 0.69, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 74.52, Test_acc 65.60
2025-01-05 11:48:52,049 [podnet.py] => Task 0, Epoch 9/160 (LR 0.09922) => LSC_loss 0.68, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 75.88, Test_acc 73.80
2025-01-05 11:48:54,315 [podnet.py] => Task 0, Epoch 10/160 (LR 0.09904) => LSC_loss 0.58, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 79.72, Test_acc 82.20
2025-01-05 11:48:56,630 [podnet.py] => Task 0, Epoch 11/160 (LR 0.09884) => LSC_loss 0.58, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 80.36, Test_acc 77.60
2025-01-05 11:48:59,053 [podnet.py] => Task 0, Epoch 12/160 (LR 0.09862) => LSC_loss 0.52, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.12, Test_acc 78.80
2025-01-05 11:49:01,431 [podnet.py] => Task 0, Epoch 13/160 (LR 0.09838) => LSC_loss 0.51, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 83.48, Test_acc 73.00
2025-01-05 11:49:03,867 [podnet.py] => Task 0, Epoch 14/160 (LR 0.09812) => LSC_loss 0.48, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 84.28, Test_acc 84.00
2025-01-05 11:49:06,076 [podnet.py] => Task 0, Epoch 15/160 (LR 0.09785) => LSC_loss 0.42, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.92, Test_acc 84.80
2025-01-05 11:49:08,409 [podnet.py] => Task 0, Epoch 16/160 (LR 0.09755) => LSC_loss 0.44, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.32, Test_acc 83.80
2025-01-05 11:49:10,766 [podnet.py] => Task 0, Epoch 17/160 (LR 0.09724) => LSC_loss 0.42, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.88, Test_acc 76.80
2025-01-05 11:49:13,133 [podnet.py] => Task 0, Epoch 18/160 (LR 0.09691) => LSC_loss 0.41, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 86.72, Test_acc 89.60
2025-01-05 11:49:15,390 [podnet.py] => Task 0, Epoch 19/160 (LR 0.09656) => LSC_loss 0.36, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.00, Test_acc 81.80
2025-01-05 11:49:17,785 [podnet.py] => Task 0, Epoch 20/160 (LR 0.09619) => LSC_loss 0.36, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.00, Test_acc 87.80
2025-01-05 11:49:20,133 [podnet.py] => Task 0, Epoch 21/160 (LR 0.09581) => LSC_loss 0.35, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 88.84, Test_acc 91.00
2025-01-05 11:49:22,425 [podnet.py] => Task 0, Epoch 22/160 (LR 0.09541) => LSC_loss 0.32, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.64, Test_acc 89.00
2025-01-05 11:49:24,575 [podnet.py] => Task 0, Epoch 23/160 (LR 0.09499) => LSC_loss 0.33, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 89.52, Test_acc 83.80
2025-01-05 11:49:26,791 [podnet.py] => Task 0, Epoch 24/160 (LR 0.09455) => LSC_loss 0.29, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.00, Test_acc 87.80
2025-01-05 11:49:29,007 [podnet.py] => Task 0, Epoch 25/160 (LR 0.09410) => LSC_loss 0.24, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.08, Test_acc 88.80
2025-01-05 11:49:31,307 [podnet.py] => Task 0, Epoch 26/160 (LR 0.09362) => LSC_loss 0.27, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.24, Test_acc 84.40
2025-01-05 11:49:33,630 [podnet.py] => Task 0, Epoch 27/160 (LR 0.09314) => LSC_loss 0.23, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.40, Test_acc 91.40
2025-01-05 11:49:36,014 [podnet.py] => Task 0, Epoch 28/160 (LR 0.09263) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.20, Test_acc 88.40
2025-01-05 11:49:38,321 [podnet.py] => Task 0, Epoch 29/160 (LR 0.09211) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.80, Test_acc 88.20
2025-01-05 11:49:40,606 [podnet.py] => Task 0, Epoch 30/160 (LR 0.09157) => LSC_loss 0.25, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 92.68, Test_acc 87.20
2025-01-05 11:49:42,766 [podnet.py] => Task 0, Epoch 31/160 (LR 0.09102) => LSC_loss 0.28, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 91.60, Test_acc 81.00
2025-01-05 11:49:45,086 [podnet.py] => Task 0, Epoch 32/160 (LR 0.09045) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.92, Test_acc 90.80
2025-01-05 11:49:47,378 [podnet.py] => Task 0, Epoch 33/160 (LR 0.08987) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.76, Test_acc 92.20
2025-01-05 11:49:49,476 [podnet.py] => Task 0, Epoch 34/160 (LR 0.08927) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.96, Test_acc 91.60
2025-01-05 11:49:51,543 [podnet.py] => Task 0, Epoch 35/160 (LR 0.08865) => LSC_loss 0.18, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.52, Test_acc 92.40
2025-01-05 11:49:53,922 [podnet.py] => Task 0, Epoch 36/160 (LR 0.08802) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.04, Test_acc 86.60
2025-01-05 11:49:56,130 [podnet.py] => Task 0, Epoch 37/160 (LR 0.08738) => LSC_loss 0.19, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.00, Test_acc 90.80
2025-01-05 11:49:58,529 [podnet.py] => Task 0, Epoch 38/160 (LR 0.08672) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.64, Test_acc 91.00
2025-01-05 11:50:01,103 [podnet.py] => Task 0, Epoch 39/160 (LR 0.08604) => LSC_loss 0.16, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.88, Test_acc 95.20
2025-01-05 11:50:03,448 [podnet.py] => Task 0, Epoch 40/160 (LR 0.08536) => LSC_loss 0.20, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 93.56, Test_acc 90.20
2025-01-05 11:50:05,559 [podnet.py] => Task 0, Epoch 41/160 (LR 0.08465) => LSC_loss 0.17, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 94.88, Test_acc 92.20
2025-01-05 11:50:07,913 [podnet.py] => Task 0, Epoch 42/160 (LR 0.08394) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.80, Test_acc 93.60
2025-01-05 11:50:10,176 [podnet.py] => Task 0, Epoch 43/160 (LR 0.08321) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.76, Test_acc 91.80
2025-01-05 11:50:12,319 [podnet.py] => Task 0, Epoch 44/160 (LR 0.08247) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.40, Test_acc 94.40
2025-01-05 11:50:14,568 [podnet.py] => Task 0, Epoch 45/160 (LR 0.08172) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.08, Test_acc 94.40
2025-01-05 11:50:16,913 [podnet.py] => Task 0, Epoch 46/160 (LR 0.08095) => LSC_loss 0.15, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.36, Test_acc 95.40
2025-01-05 11:50:19,240 [podnet.py] => Task 0, Epoch 47/160 (LR 0.08018) => LSC_loss 0.14, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.32, Test_acc 92.40
2025-01-05 11:50:21,419 [podnet.py] => Task 0, Epoch 48/160 (LR 0.07939) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.60, Test_acc 94.20
2025-01-05 11:50:23,616 [podnet.py] => Task 0, Epoch 49/160 (LR 0.07859) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.40, Test_acc 88.20
2025-01-05 11:50:25,789 [podnet.py] => Task 0, Epoch 50/160 (LR 0.07778) => LSC_loss 0.11, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.40, Test_acc 94.20
2025-01-05 11:50:28,142 [podnet.py] => Task 0, Epoch 51/160 (LR 0.07696) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.52, Test_acc 91.60
2025-01-05 11:50:30,497 [podnet.py] => Task 0, Epoch 52/160 (LR 0.07612) => LSC_loss 0.13, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 95.92, Test_acc 92.60
2025-01-05 11:50:32,866 [podnet.py] => Task 0, Epoch 53/160 (LR 0.07528) => LSC_loss 0.12, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.44, Test_acc 92.00
2025-01-05 11:50:35,216 [podnet.py] => Task 0, Epoch 54/160 (LR 0.07443) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.72, Test_acc 91.40
2025-01-05 11:50:37,561 [podnet.py] => Task 0, Epoch 55/160 (LR 0.07357) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.16, Test_acc 95.80
2025-01-05 11:50:39,810 [podnet.py] => Task 0, Epoch 56/160 (LR 0.07270) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 94.80
2025-01-05 11:50:42,039 [podnet.py] => Task 0, Epoch 57/160 (LR 0.07182) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.04, Test_acc 89.00
2025-01-05 11:50:44,412 [podnet.py] => Task 0, Epoch 58/160 (LR 0.07093) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.40, Test_acc 95.00
2025-01-05 11:50:46,622 [podnet.py] => Task 0, Epoch 59/160 (LR 0.07004) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.20, Test_acc 91.40
2025-01-05 11:50:49,189 [podnet.py] => Task 0, Epoch 60/160 (LR 0.06913) => LSC_loss 0.10, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 96.72, Test_acc 86.80
2025-01-05 11:50:51,643 [podnet.py] => Task 0, Epoch 61/160 (LR 0.06822) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.36, Test_acc 93.40
2025-01-05 11:50:53,931 [podnet.py] => Task 0, Epoch 62/160 (LR 0.06731) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.48, Test_acc 90.40
2025-01-05 11:50:56,151 [podnet.py] => Task 0, Epoch 63/160 (LR 0.06638) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.88, Test_acc 95.40
2025-01-05 11:50:58,451 [podnet.py] => Task 0, Epoch 64/160 (LR 0.06545) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.04, Test_acc 93.80
2025-01-05 11:51:00,730 [podnet.py] => Task 0, Epoch 65/160 (LR 0.06451) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 94.40
2025-01-05 11:51:02,876 [podnet.py] => Task 0, Epoch 66/160 (LR 0.06357) => LSC_loss 0.08, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.96, Test_acc 93.00
2025-01-05 11:51:05,371 [podnet.py] => Task 0, Epoch 67/160 (LR 0.06262) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.48, Test_acc 93.80
2025-01-05 11:51:07,734 [podnet.py] => Task 0, Epoch 68/160 (LR 0.06167) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.04, Test_acc 90.80
2025-01-05 11:51:10,048 [podnet.py] => Task 0, Epoch 69/160 (LR 0.06072) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.24, Test_acc 95.80
2025-01-05 11:51:12,540 [podnet.py] => Task 0, Epoch 70/160 (LR 0.05975) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.84, Test_acc 96.20
2025-01-05 11:51:14,889 [podnet.py] => Task 0, Epoch 71/160 (LR 0.05879) => LSC_loss 0.09, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 97.24, Test_acc 92.40
2025-01-05 11:51:16,985 [podnet.py] => Task 0, Epoch 72/160 (LR 0.05782) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 94.20
2025-01-05 11:51:19,390 [podnet.py] => Task 0, Epoch 73/160 (LR 0.05685) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.08, Test_acc 94.40
2025-01-05 11:51:21,870 [podnet.py] => Task 0, Epoch 74/160 (LR 0.05588) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.64, Test_acc 95.60
2025-01-05 11:51:24,298 [podnet.py] => Task 0, Epoch 75/160 (LR 0.05490) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.88, Test_acc 94.20
2025-01-05 11:51:26,441 [podnet.py] => Task 0, Epoch 76/160 (LR 0.05392) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.44, Test_acc 95.00
2025-01-05 11:51:28,794 [podnet.py] => Task 0, Epoch 77/160 (LR 0.05294) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 94.60
2025-01-05 11:51:31,111 [podnet.py] => Task 0, Epoch 78/160 (LR 0.05196) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 94.20
2025-01-05 11:51:33,520 [podnet.py] => Task 0, Epoch 79/160 (LR 0.05098) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 96.40
2025-01-05 11:51:35,827 [podnet.py] => Task 0, Epoch 80/160 (LR 0.05000) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 94.40
2025-01-05 11:51:38,168 [podnet.py] => Task 0, Epoch 81/160 (LR 0.04902) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.28, Test_acc 91.60
2025-01-05 11:51:40,609 [podnet.py] => Task 0, Epoch 82/160 (LR 0.04804) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.72, Test_acc 94.00
2025-01-05 11:51:42,735 [podnet.py] => Task 0, Epoch 83/160 (LR 0.04706) => LSC_loss 0.05, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.60, Test_acc 95.00
2025-01-05 11:51:45,029 [podnet.py] => Task 0, Epoch 84/160 (LR 0.04608) => LSC_loss 0.07, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.00, Test_acc 93.80
2025-01-05 11:51:47,453 [podnet.py] => Task 0, Epoch 85/160 (LR 0.04510) => LSC_loss 0.06, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.32, Test_acc 95.00
2025-01-05 11:51:49,932 [podnet.py] => Task 0, Epoch 86/160 (LR 0.04412) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.92, Test_acc 95.60
2025-01-05 11:51:52,212 [podnet.py] => Task 0, Epoch 87/160 (LR 0.04315) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 98.96, Test_acc 94.00
2025-01-05 11:51:54,551 [podnet.py] => Task 0, Epoch 88/160 (LR 0.04218) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.32, Test_acc 96.00
2025-01-05 11:51:56,933 [podnet.py] => Task 0, Epoch 89/160 (LR 0.04121) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 96.20
2025-01-05 11:51:59,234 [podnet.py] => Task 0, Epoch 90/160 (LR 0.04025) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 95.60
2025-01-05 11:52:01,853 [podnet.py] => Task 0, Epoch 91/160 (LR 0.03928) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.08, Test_acc 95.00
2025-01-05 11:52:04,564 [podnet.py] => Task 0, Epoch 92/160 (LR 0.03833) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.16, Test_acc 95.60
2025-01-05 11:52:06,762 [podnet.py] => Task 0, Epoch 93/160 (LR 0.03738) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.04, Test_acc 96.20
2025-01-05 11:52:09,252 [podnet.py] => Task 0, Epoch 94/160 (LR 0.03643) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 95.60
2025-01-05 11:52:11,522 [podnet.py] => Task 0, Epoch 95/160 (LR 0.03549) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.12, Test_acc 94.80
2025-01-05 11:52:13,879 [podnet.py] => Task 0, Epoch 96/160 (LR 0.03455) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 96.00
2025-01-05 11:52:16,052 [podnet.py] => Task 0, Epoch 97/160 (LR 0.03362) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.44, Test_acc 96.20
2025-01-05 11:52:18,361 [podnet.py] => Task 0, Epoch 98/160 (LR 0.03269) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 96.00
2025-01-05 11:52:20,550 [podnet.py] => Task 0, Epoch 99/160 (LR 0.03178) => LSC_loss 0.04, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.00, Test_acc 96.40
2025-01-05 11:52:22,901 [podnet.py] => Task 0, Epoch 100/160 (LR 0.03087) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 96.00
2025-01-05 11:52:25,347 [podnet.py] => Task 0, Epoch 101/160 (LR 0.02996) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.64, Test_acc 96.60
2025-01-05 11:52:27,678 [podnet.py] => Task 0, Epoch 102/160 (LR 0.02907) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 98.00
2025-01-05 11:52:29,980 [podnet.py] => Task 0, Epoch 103/160 (LR 0.02818) => LSC_loss 0.03, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.36, Test_acc 97.20
2025-01-05 11:52:32,375 [podnet.py] => Task 0, Epoch 104/160 (LR 0.02730) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 96.00
2025-01-05 11:52:34,800 [podnet.py] => Task 0, Epoch 105/160 (LR 0.02643) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.48, Test_acc 97.20
2025-01-05 11:52:37,153 [podnet.py] => Task 0, Epoch 106/160 (LR 0.02557) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.60, Test_acc 96.80
2025-01-05 11:52:39,544 [podnet.py] => Task 0, Epoch 107/160 (LR 0.02472) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.68, Test_acc 97.60
2025-01-05 11:52:41,880 [podnet.py] => Task 0, Epoch 108/160 (LR 0.02388) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 97.80
2025-01-05 11:52:44,175 [podnet.py] => Task 0, Epoch 109/160 (LR 0.02304) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 97.00
2025-01-05 11:52:46,288 [podnet.py] => Task 0, Epoch 110/160 (LR 0.02222) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 97.00
2025-01-05 11:52:48,501 [podnet.py] => Task 0, Epoch 111/160 (LR 0.02141) => LSC_loss 0.02, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.72, Test_acc 97.20
2025-01-05 11:52:50,914 [podnet.py] => Task 0, Epoch 112/160 (LR 0.02061) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 97.00
2025-01-05 11:52:53,342 [podnet.py] => Task 0, Epoch 113/160 (LR 0.01982) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 97.40
2025-01-05 11:52:55,773 [podnet.py] => Task 0, Epoch 114/160 (LR 0.01905) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 97.60
2025-01-05 11:52:58,206 [podnet.py] => Task 0, Epoch 115/160 (LR 0.01828) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 97.60
2025-01-05 11:53:00,638 [podnet.py] => Task 0, Epoch 116/160 (LR 0.01753) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 97.80
2025-01-05 11:53:03,034 [podnet.py] => Task 0, Epoch 117/160 (LR 0.01679) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 98.00
2025-01-05 11:53:05,699 [podnet.py] => Task 0, Epoch 118/160 (LR 0.01606) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 97.60
2025-01-05 11:53:08,849 [podnet.py] => Task 0, Epoch 119/160 (LR 0.01535) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.80, Test_acc 97.40
2025-01-05 11:53:11,558 [podnet.py] => Task 0, Epoch 120/160 (LR 0.01464) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.60
2025-01-05 11:53:13,861 [podnet.py] => Task 0, Epoch 121/160 (LR 0.01396) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:53:16,324 [podnet.py] => Task 0, Epoch 122/160 (LR 0.01328) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.80
2025-01-05 11:53:18,643 [podnet.py] => Task 0, Epoch 123/160 (LR 0.01262) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 98.00
2025-01-05 11:53:20,981 [podnet.py] => Task 0, Epoch 124/160 (LR 0.01198) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:53:23,242 [podnet.py] => Task 0, Epoch 125/160 (LR 0.01135) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:53:25,499 [podnet.py] => Task 0, Epoch 126/160 (LR 0.01073) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 97.80
2025-01-05 11:53:27,878 [podnet.py] => Task 0, Epoch 127/160 (LR 0.01013) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.80
2025-01-05 11:53:30,061 [podnet.py] => Task 0, Epoch 128/160 (LR 0.00955) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:53:32,488 [podnet.py] => Task 0, Epoch 129/160 (LR 0.00898) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.76, Test_acc 98.00
2025-01-05 11:53:34,876 [podnet.py] => Task 0, Epoch 130/160 (LR 0.00843) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:53:37,130 [podnet.py] => Task 0, Epoch 131/160 (LR 0.00789) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 97.80
2025-01-05 11:53:39,484 [podnet.py] => Task 0, Epoch 132/160 (LR 0.00737) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:53:42,033 [podnet.py] => Task 0, Epoch 133/160 (LR 0.00686) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 97.60
2025-01-05 11:53:44,337 [podnet.py] => Task 0, Epoch 134/160 (LR 0.00638) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.40
2025-01-05 11:53:46,824 [podnet.py] => Task 0, Epoch 135/160 (LR 0.00590) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.84, Test_acc 97.80
2025-01-05 11:53:48,939 [podnet.py] => Task 0, Epoch 136/160 (LR 0.00545) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:53:51,277 [podnet.py] => Task 0, Epoch 137/160 (LR 0.00501) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:53:53,700 [podnet.py] => Task 0, Epoch 138/160 (LR 0.00459) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.40
2025-01-05 11:53:55,953 [podnet.py] => Task 0, Epoch 139/160 (LR 0.00419) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.40
2025-01-05 11:53:58,177 [podnet.py] => Task 0, Epoch 140/160 (LR 0.00381) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:54:00,485 [podnet.py] => Task 0, Epoch 141/160 (LR 0.00344) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:54:02,652 [podnet.py] => Task 0, Epoch 142/160 (LR 0.00309) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.40
2025-01-05 11:54:05,075 [podnet.py] => Task 0, Epoch 143/160 (LR 0.00276) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.60
2025-01-05 11:54:07,435 [podnet.py] => Task 0, Epoch 144/160 (LR 0.00245) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 97.60
2025-01-05 11:54:09,834 [podnet.py] => Task 0, Epoch 145/160 (LR 0.00215) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:54:12,134 [podnet.py] => Task 0, Epoch 146/160 (LR 0.00188) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 97.60
2025-01-05 11:54:14,437 [podnet.py] => Task 0, Epoch 147/160 (LR 0.00162) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 97.80
2025-01-05 11:54:16,895 [podnet.py] => Task 0, Epoch 148/160 (LR 0.00138) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 97.80
2025-01-05 11:54:19,216 [podnet.py] => Task 0, Epoch 149/160 (LR 0.00116) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:54:21,376 [podnet.py] => Task 0, Epoch 150/160 (LR 0.00096) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:54:23,796 [podnet.py] => Task 0, Epoch 151/160 (LR 0.00078) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:54:25,959 [podnet.py] => Task 0, Epoch 152/160 (LR 0.00062) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.60
2025-01-05 11:54:28,302 [podnet.py] => Task 0, Epoch 153/160 (LR 0.00047) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.92, Test_acc 97.80
2025-01-05 11:54:30,694 [podnet.py] => Task 0, Epoch 154/160 (LR 0.00035) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.80
2025-01-05 11:54:33,206 [podnet.py] => Task 0, Epoch 155/160 (LR 0.00024) => LSC_loss 0.01, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.88, Test_acc 97.80
2025-01-05 11:54:35,355 [podnet.py] => Task 0, Epoch 156/160 (LR 0.00015) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.60
2025-01-05 11:54:37,806 [podnet.py] => Task 0, Epoch 157/160 (LR 0.00009) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.60
2025-01-05 11:54:40,218 [podnet.py] => Task 0, Epoch 158/160 (LR 0.00004) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:54:42,503 [podnet.py] => Task 0, Epoch 159/160 (LR 0.00001) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 100.00, Test_acc 97.80
2025-01-05 11:54:44,659 [podnet.py] => Task 0, Epoch 160/160 (LR 0.00000) => LSC_loss 0.00, Spatial_loss 0.00, Flat_loss 0.00, Train_acc 99.96, Test_acc 97.80
2025-01-05 11:54:44,660 [base.py] => Reducing exemplars...(2695 per classes)
2025-01-05 11:54:44,660 [base.py] => Constructing exemplars...(2695 per classes)
2025-01-05 11:54:56,168 [podnet.py] => Exemplar size: 2500
2025-01-05 11:54:56,171 [trainer.py] => CNN: {'total': np.float64(97.8), '00-09': np.float64(97.8), 'old': 0, 'new': np.float64(97.8)}
2025-01-05 11:54:56,171 [trainer.py] => NME: {'total': np.float64(97.8), '00-09': np.float64(97.8), 'old': 0, 'new': np.float64(97.8)}
2025-01-05 11:54:56,171 [trainer.py] => CNN top1 curve: [np.float64(97.8)]
2025-01-05 11:54:56,171 [trainer.py] => CNN top5 curve: [np.float64(100.0)]
2025-01-05 11:54:56,171 [trainer.py] => NME top1 curve: [np.float64(97.8)]
2025-01-05 11:54:56,171 [trainer.py] => NME top5 curve: [np.float64(100.0)]

2025-01-05 11:54:56,172 [trainer.py] => All params: 469457
2025-01-05 11:54:56,172 [trainer.py] => Trainable params: 469457
2025-01-05 11:54:56,172 [podnet.py] => Learning on 5-10
2025-01-05 11:54:56,201 [podnet.py] => Adaptive factor: 1.4142135623730951
2025-01-05 11:55:00,306 [podnet.py] => Task 1, Epoch 1/160 (LR 0.09999) => LSC_loss 1.39, Spatial_loss 1.53, Flat_loss 0.21, Train_acc 59.56, Test_acc 55.50
2025-01-05 11:55:03,959 [podnet.py] => Task 1, Epoch 2/160 (LR 0.09996) => LSC_loss 0.99, Spatial_loss 1.57, Flat_loss 0.18, Train_acc 65.98, Test_acc 56.20
2025-01-05 11:55:07,639 [podnet.py] => Task 1, Epoch 3/160 (LR 0.09991) => LSC_loss 0.88, Spatial_loss 1.31, Flat_loss 0.16, Train_acc 69.84, Test_acc 58.40
2025-01-05 11:55:11,108 [podnet.py] => Task 1, Epoch 4/160 (LR 0.09985) => LSC_loss 0.91, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 70.06, Test_acc 64.30
2025-01-05 11:55:14,746 [podnet.py] => Task 1, Epoch 5/160 (LR 0.09976) => LSC_loss 0.84, Spatial_loss 1.56, Flat_loss 0.20, Train_acc 72.08, Test_acc 63.50
2025-01-05 11:55:18,432 [podnet.py] => Task 1, Epoch 6/160 (LR 0.09965) => LSC_loss 0.79, Spatial_loss 1.39, Flat_loss 0.18, Train_acc 74.12, Test_acc 73.10
2025-01-05 11:55:21,907 [podnet.py] => Task 1, Epoch 7/160 (LR 0.09953) => LSC_loss 0.76, Spatial_loss 1.33, Flat_loss 0.18, Train_acc 75.10, Test_acc 68.30
2025-01-05 11:55:25,495 [podnet.py] => Task 1, Epoch 8/160 (LR 0.09938) => LSC_loss 0.74, Spatial_loss 1.31, Flat_loss 0.18, Train_acc 75.36, Test_acc 61.40
2025-01-05 11:55:29,074 [podnet.py] => Task 1, Epoch 9/160 (LR 0.09922) => LSC_loss 0.76, Spatial_loss 1.39, Flat_loss 0.19, Train_acc 75.64, Test_acc 75.50
2025-01-05 11:55:32,656 [podnet.py] => Task 1, Epoch 10/160 (LR 0.09904) => LSC_loss 0.73, Spatial_loss 1.20, Flat_loss 0.18, Train_acc 76.86, Test_acc 68.80
2025-01-05 11:55:36,252 [podnet.py] => Task 1, Epoch 11/160 (LR 0.09884) => LSC_loss 0.69, Spatial_loss 1.30, Flat_loss 0.19, Train_acc 77.66, Test_acc 70.60
2025-01-05 11:55:39,842 [podnet.py] => Task 1, Epoch 12/160 (LR 0.09862) => LSC_loss 0.69, Spatial_loss 1.40, Flat_loss 0.20, Train_acc 77.16, Test_acc 72.00
2025-01-05 11:55:43,348 [podnet.py] => Task 1, Epoch 13/160 (LR 0.09838) => LSC_loss 0.67, Spatial_loss 1.25, Flat_loss 0.18, Train_acc 78.84, Test_acc 67.80
2025-01-05 11:55:46,736 [podnet.py] => Task 1, Epoch 14/160 (LR 0.09812) => LSC_loss 0.67, Spatial_loss 1.25, Flat_loss 0.19, Train_acc 78.22, Test_acc 74.90
2025-01-05 11:55:50,180 [podnet.py] => Task 1, Epoch 15/160 (LR 0.09785) => LSC_loss 0.65, Spatial_loss 1.24, Flat_loss 0.19, Train_acc 79.26, Test_acc 72.30
2025-01-05 11:55:53,841 [podnet.py] => Task 1, Epoch 16/160 (LR 0.09755) => LSC_loss 0.68, Spatial_loss 1.40, Flat_loss 0.20, Train_acc 78.06, Test_acc 77.50
2025-01-05 11:55:57,489 [podnet.py] => Task 1, Epoch 17/160 (LR 0.09724) => LSC_loss 0.64, Spatial_loss 1.18, Flat_loss 0.18, Train_acc 80.22, Test_acc 74.20
2025-01-05 11:56:01,063 [podnet.py] => Task 1, Epoch 18/160 (LR 0.09691) => LSC_loss 0.62, Spatial_loss 1.22, Flat_loss 0.19, Train_acc 79.36, Test_acc 76.70
2025-01-05 11:56:04,667 [podnet.py] => Task 1, Epoch 19/160 (LR 0.09656) => LSC_loss 0.62, Spatial_loss 1.22, Flat_loss 0.20, Train_acc 80.20, Test_acc 70.40
2025-01-05 11:56:08,285 [podnet.py] => Task 1, Epoch 20/160 (LR 0.09619) => LSC_loss 0.61, Spatial_loss 1.33, Flat_loss 0.20, Train_acc 79.94, Test_acc 69.80
2025-01-05 11:56:11,646 [podnet.py] => Task 1, Epoch 21/160 (LR 0.09581) => LSC_loss 0.63, Spatial_loss 1.31, Flat_loss 0.21, Train_acc 80.20, Test_acc 69.60
2025-01-05 11:56:15,350 [podnet.py] => Task 1, Epoch 22/160 (LR 0.09541) => LSC_loss 0.72, Spatial_loss 1.47, Flat_loss 0.22, Train_acc 77.90, Test_acc 71.40
2025-01-05 11:56:18,914 [podnet.py] => Task 1, Epoch 23/160 (LR 0.09499) => LSC_loss 0.65, Spatial_loss 1.43, Flat_loss 0.22, Train_acc 79.50, Test_acc 70.20
2025-01-05 11:56:22,322 [podnet.py] => Task 1, Epoch 24/160 (LR 0.09455) => LSC_loss 0.58, Spatial_loss 1.25, Flat_loss 0.20, Train_acc 81.78, Test_acc 76.30
2025-01-05 11:56:25,837 [podnet.py] => Task 1, Epoch 25/160 (LR 0.09410) => LSC_loss 0.60, Spatial_loss 1.30, Flat_loss 0.20, Train_acc 80.74, Test_acc 77.10
2025-01-05 11:56:29,308 [podnet.py] => Task 1, Epoch 26/160 (LR 0.09362) => LSC_loss 0.54, Spatial_loss 1.15, Flat_loss 0.19, Train_acc 82.76, Test_acc 74.70
2025-01-05 11:56:33,065 [podnet.py] => Task 1, Epoch 27/160 (LR 0.09314) => LSC_loss 0.68, Spatial_loss 1.64, Flat_loss 0.24, Train_acc 78.54, Test_acc 67.00
2025-01-05 11:56:36,670 [podnet.py] => Task 1, Epoch 28/160 (LR 0.09263) => LSC_loss 0.72, Spatial_loss 1.64, Flat_loss 0.25, Train_acc 77.74, Test_acc 65.80
2025-01-05 11:56:40,294 [podnet.py] => Task 1, Epoch 29/160 (LR 0.09211) => LSC_loss 0.62, Spatial_loss 1.39, Flat_loss 0.21, Train_acc 79.96, Test_acc 78.20
2025-01-05 11:56:43,849 [podnet.py] => Task 1, Epoch 30/160 (LR 0.09157) => LSC_loss 0.58, Spatial_loss 1.21, Flat_loss 0.20, Train_acc 82.32, Test_acc 73.70
2025-01-05 11:56:47,281 [podnet.py] => Task 1, Epoch 31/160 (LR 0.09102) => LSC_loss 0.58, Spatial_loss 1.20, Flat_loss 0.20, Train_acc 82.74, Test_acc 70.70
2025-01-05 11:56:50,953 [podnet.py] => Task 1, Epoch 32/160 (LR 0.09045) => LSC_loss 0.56, Spatial_loss 1.27, Flat_loss 0.20, Train_acc 82.50, Test_acc 74.50
2025-01-05 11:56:54,503 [podnet.py] => Task 1, Epoch 33/160 (LR 0.08987) => LSC_loss 0.62, Spatial_loss 1.45, Flat_loss 0.22, Train_acc 81.64, Test_acc 69.60
2025-01-05 11:56:58,156 [podnet.py] => Task 1, Epoch 34/160 (LR 0.08927) => LSC_loss 0.68, Spatial_loss 1.70, Flat_loss 0.26, Train_acc 78.70, Test_acc 77.50
2025-01-05 11:57:01,689 [podnet.py] => Task 1, Epoch 35/160 (LR 0.08865) => LSC_loss 0.52, Spatial_loss 1.27, Flat_loss 0.21, Train_acc 83.00, Test_acc 79.50
2025-01-05 11:57:05,182 [podnet.py] => Task 1, Epoch 36/160 (LR 0.08802) => LSC_loss 0.49, Spatial_loss 1.14, Flat_loss 0.19, Train_acc 84.66, Test_acc 79.30
2025-01-05 11:57:08,726 [podnet.py] => Task 1, Epoch 37/160 (LR 0.08738) => LSC_loss 0.51, Spatial_loss 1.16, Flat_loss 0.20, Train_acc 84.06, Test_acc 79.50
2025-01-05 11:57:12,610 [podnet.py] => Task 1, Epoch 38/160 (LR 0.08672) => LSC_loss 0.59, Spatial_loss 1.49, Flat_loss 0.24, Train_acc 81.70, Test_acc 77.50
2025-01-05 11:57:16,396 [podnet.py] => Task 1, Epoch 39/160 (LR 0.08604) => LSC_loss 0.50, Spatial_loss 1.22, Flat_loss 0.21, Train_acc 84.36, Test_acc 75.50
2025-01-05 11:57:19,954 [podnet.py] => Task 1, Epoch 40/160 (LR 0.08536) => LSC_loss 0.52, Spatial_loss 1.17, Flat_loss 0.20, Train_acc 83.90, Test_acc 82.10
2025-01-05 11:57:23,582 [podnet.py] => Task 1, Epoch 41/160 (LR 0.08465) => LSC_loss 0.50, Spatial_loss 1.14, Flat_loss 0.20, Train_acc 85.04, Test_acc 71.40
2025-01-05 11:57:26,995 [podnet.py] => Task 1, Epoch 42/160 (LR 0.08394) => LSC_loss 0.58, Spatial_loss 1.56, Flat_loss 0.24, Train_acc 81.70, Test_acc 75.90
2025-01-05 11:57:30,442 [podnet.py] => Task 1, Epoch 43/160 (LR 0.08321) => LSC_loss 0.48, Spatial_loss 1.21, Flat_loss 0.20, Train_acc 85.00, Test_acc 76.60
2025-01-05 11:57:34,010 [podnet.py] => Task 1, Epoch 44/160 (LR 0.08247) => LSC_loss 0.50, Spatial_loss 1.13, Flat_loss 0.20, Train_acc 85.34, Test_acc 70.80
2025-01-05 11:57:37,347 [podnet.py] => Task 1, Epoch 45/160 (LR 0.08172) => LSC_loss 0.67, Spatial_loss 1.61, Flat_loss 0.25, Train_acc 79.24, Test_acc 78.70
2025-01-05 11:57:40,781 [podnet.py] => Task 1, Epoch 46/160 (LR 0.08095) => LSC_loss 0.48, Spatial_loss 1.14, Flat_loss 0.20, Train_acc 85.08, Test_acc 78.50
2025-01-05 11:57:44,371 [podnet.py] => Task 1, Epoch 47/160 (LR 0.08018) => LSC_loss 0.46, Spatial_loss 1.10, Flat_loss 0.20, Train_acc 85.54, Test_acc 81.40
2025-01-05 11:57:47,946 [podnet.py] => Task 1, Epoch 48/160 (LR 0.07939) => LSC_loss 0.44, Spatial_loss 1.10, Flat_loss 0.19, Train_acc 86.60, Test_acc 79.80
2025-01-05 11:57:51,415 [podnet.py] => Task 1, Epoch 49/160 (LR 0.07859) => LSC_loss 0.46, Spatial_loss 1.11, Flat_loss 0.20, Train_acc 86.90, Test_acc 79.30
2025-01-05 11:57:55,207 [podnet.py] => Task 1, Epoch 50/160 (LR 0.07778) => LSC_loss 0.50, Spatial_loss 1.26, Flat_loss 0.21, Train_acc 85.46, Test_acc 74.50
2025-01-05 11:57:58,922 [podnet.py] => Task 1, Epoch 51/160 (LR 0.07696) => LSC_loss 0.56, Spatial_loss 1.41, Flat_loss 0.24, Train_acc 83.24, Test_acc 79.80
2025-01-05 11:58:02,311 [podnet.py] => Task 1, Epoch 52/160 (LR 0.07612) => LSC_loss 0.47, Spatial_loss 1.17, Flat_loss 0.20, Train_acc 85.54, Test_acc 78.90
2025-01-05 11:58:05,755 [podnet.py] => Task 1, Epoch 53/160 (LR 0.07528) => LSC_loss 0.49, Spatial_loss 1.20, Flat_loss 0.21, Train_acc 85.76, Test_acc 76.30
2025-01-05 11:58:09,214 [podnet.py] => Task 1, Epoch 54/160 (LR 0.07443) => LSC_loss 0.46, Spatial_loss 1.23, Flat_loss 0.21, Train_acc 85.76, Test_acc 78.40
2025-01-05 11:58:12,775 [podnet.py] => Task 1, Epoch 55/160 (LR 0.07357) => LSC_loss 0.41, Spatial_loss 1.09, Flat_loss 0.20, Train_acc 87.92, Test_acc 80.90
2025-01-05 11:58:16,491 [podnet.py] => Task 1, Epoch 56/160 (LR 0.07270) => LSC_loss 0.39, Spatial_loss 1.11, Flat_loss 0.20, Train_acc 87.66, Test_acc 77.90
2025-01-05 11:58:20,016 [podnet.py] => Task 1, Epoch 57/160 (LR 0.07182) => LSC_loss 0.43, Spatial_loss 1.10, Flat_loss 0.20, Train_acc 87.50, Test_acc 80.00
2025-01-05 11:58:23,526 [podnet.py] => Task 1, Epoch 58/160 (LR 0.07093) => LSC_loss 0.41, Spatial_loss 1.09, Flat_loss 0.20, Train_acc 86.68, Test_acc 80.80
2025-01-05 11:58:27,250 [podnet.py] => Task 1, Epoch 59/160 (LR 0.07004) => LSC_loss 0.42, Spatial_loss 1.05, Flat_loss 0.20, Train_acc 88.12, Test_acc 75.60
2025-01-05 11:58:30,734 [podnet.py] => Task 1, Epoch 60/160 (LR 0.06913) => LSC_loss 0.48, Spatial_loss 1.27, Flat_loss 0.22, Train_acc 85.74, Test_acc 77.20
2025-01-05 11:58:34,146 [podnet.py] => Task 1, Epoch 61/160 (LR 0.06822) => LSC_loss 0.41, Spatial_loss 1.09, Flat_loss 0.20, Train_acc 87.66, Test_acc 81.70
2025-01-05 11:58:37,577 [podnet.py] => Task 1, Epoch 62/160 (LR 0.06731) => LSC_loss 0.43, Spatial_loss 1.11, Flat_loss 0.20, Train_acc 87.60, Test_acc 77.90
2025-01-05 11:58:41,140 [podnet.py] => Task 1, Epoch 63/160 (LR 0.06638) => LSC_loss 0.49, Spatial_loss 1.15, Flat_loss 0.21, Train_acc 86.04, Test_acc 76.20
2025-01-05 11:58:44,781 [podnet.py] => Task 1, Epoch 64/160 (LR 0.06545) => LSC_loss 0.44, Spatial_loss 1.28, Flat_loss 0.22, Train_acc 86.72, Test_acc 76.50
2025-01-05 11:58:48,408 [podnet.py] => Task 1, Epoch 65/160 (LR 0.06451) => LSC_loss 0.45, Spatial_loss 1.15, Flat_loss 0.21, Train_acc 88.28, Test_acc 78.40
2025-01-05 11:58:51,916 [podnet.py] => Task 1, Epoch 66/160 (LR 0.06357) => LSC_loss 0.51, Spatial_loss 1.24, Flat_loss 0.22, Train_acc 85.40, Test_acc 79.20
2025-01-05 11:58:55,351 [podnet.py] => Task 1, Epoch 67/160 (LR 0.06262) => LSC_loss 0.54, Spatial_loss 1.42, Flat_loss 0.25, Train_acc 84.84, Test_acc 65.70
2025-01-05 11:58:59,065 [podnet.py] => Task 1, Epoch 68/160 (LR 0.06167) => LSC_loss 0.60, Spatial_loss 1.57, Flat_loss 0.25, Train_acc 82.44, Test_acc 74.00
2025-01-05 11:59:02,573 [podnet.py] => Task 1, Epoch 69/160 (LR 0.06072) => LSC_loss 0.61, Spatial_loss 1.50, Flat_loss 0.24, Train_acc 82.78, Test_acc 80.30
2025-01-05 11:59:05,988 [podnet.py] => Task 1, Epoch 70/160 (LR 0.05975) => LSC_loss 0.44, Spatial_loss 1.22, Flat_loss 0.21, Train_acc 87.08, Test_acc 80.10
2025-01-05 11:59:09,517 [podnet.py] => Task 1, Epoch 71/160 (LR 0.05879) => LSC_loss 0.41, Spatial_loss 1.09, Flat_loss 0.20, Train_acc 87.52, Test_acc 79.90
2025-01-05 11:59:12,887 [podnet.py] => Task 1, Epoch 72/160 (LR 0.05782) => LSC_loss 0.39, Spatial_loss 1.06, Flat_loss 0.21, Train_acc 88.72, Test_acc 83.20
2025-01-05 11:59:16,648 [podnet.py] => Task 1, Epoch 73/160 (LR 0.05685) => LSC_loss 0.43, Spatial_loss 1.13, Flat_loss 0.20, Train_acc 88.20, Test_acc 81.60
2025-01-05 11:59:20,358 [podnet.py] => Task 1, Epoch 74/160 (LR 0.05588) => LSC_loss 0.47, Spatial_loss 1.29, Flat_loss 0.23, Train_acc 85.90, Test_acc 72.40
2025-01-05 11:59:24,007 [podnet.py] => Task 1, Epoch 75/160 (LR 0.05490) => LSC_loss 0.47, Spatial_loss 1.35, Flat_loss 0.23, Train_acc 85.82, Test_acc 81.20
2025-01-05 11:59:27,675 [podnet.py] => Task 1, Epoch 76/160 (LR 0.05392) => LSC_loss 0.36, Spatial_loss 1.04, Flat_loss 0.20, Train_acc 89.36, Test_acc 81.30
2025-01-05 11:59:31,398 [podnet.py] => Task 1, Epoch 77/160 (LR 0.05294) => LSC_loss 0.37, Spatial_loss 1.03, Flat_loss 0.20, Train_acc 89.52, Test_acc 81.60
2025-01-05 11:59:34,868 [podnet.py] => Task 1, Epoch 78/160 (LR 0.05196) => LSC_loss 0.40, Spatial_loss 1.11, Flat_loss 0.20, Train_acc 89.24, Test_acc 80.10
2025-01-05 11:59:38,610 [podnet.py] => Task 1, Epoch 79/160 (LR 0.05098) => LSC_loss 0.41, Spatial_loss 1.20, Flat_loss 0.22, Train_acc 88.04, Test_acc 77.90
2025-01-05 11:59:42,207 [podnet.py] => Task 1, Epoch 80/160 (LR 0.05000) => LSC_loss 0.45, Spatial_loss 1.26, Flat_loss 0.22, Train_acc 87.32, Test_acc 79.50
2025-01-05 11:59:45,878 [podnet.py] => Task 1, Epoch 81/160 (LR 0.04902) => LSC_loss 0.44, Spatial_loss 1.12, Flat_loss 0.22, Train_acc 88.88, Test_acc 80.20
2025-01-05 11:59:49,574 [podnet.py] => Task 1, Epoch 82/160 (LR 0.04804) => LSC_loss 0.46, Spatial_loss 1.29, Flat_loss 0.23, Train_acc 86.38, Test_acc 81.60
2025-01-05 11:59:53,113 [podnet.py] => Task 1, Epoch 83/160 (LR 0.04706) => LSC_loss 0.35, Spatial_loss 1.07, Flat_loss 0.20, Train_acc 89.70, Test_acc 78.60
2025-01-05 11:59:56,568 [podnet.py] => Task 1, Epoch 84/160 (LR 0.04608) => LSC_loss 0.36, Spatial_loss 1.01, Flat_loss 0.20, Train_acc 90.22, Test_acc 80.60
2025-01-05 12:00:00,384 [podnet.py] => Task 1, Epoch 85/160 (LR 0.04510) => LSC_loss 0.35, Spatial_loss 1.02, Flat_loss 0.20, Train_acc 90.04, Test_acc 84.70
2025-01-05 12:00:03,994 [podnet.py] => Task 1, Epoch 86/160 (LR 0.04412) => LSC_loss 0.43, Spatial_loss 1.25, Flat_loss 0.21, Train_acc 87.28, Test_acc 84.00
2025-01-05 12:00:07,521 [podnet.py] => Task 1, Epoch 87/160 (LR 0.04315) => LSC_loss 0.35, Spatial_loss 1.03, Flat_loss 0.20, Train_acc 89.96, Test_acc 83.30
2025-01-05 12:00:11,087 [podnet.py] => Task 1, Epoch 88/160 (LR 0.04218) => LSC_loss 0.34, Spatial_loss 1.00, Flat_loss 0.20, Train_acc 91.06, Test_acc 83.80
2025-01-05 12:00:14,807 [podnet.py] => Task 1, Epoch 89/160 (LR 0.04121) => LSC_loss 0.34, Spatial_loss 1.04, Flat_loss 0.21, Train_acc 90.74, Test_acc 79.90
2025-01-05 12:00:18,578 [podnet.py] => Task 1, Epoch 90/160 (LR 0.04025) => LSC_loss 0.48, Spatial_loss 1.30, Flat_loss 0.22, Train_acc 86.74, Test_acc 84.00
2025-01-05 12:00:22,156 [podnet.py] => Task 1, Epoch 91/160 (LR 0.03928) => LSC_loss 0.35, Spatial_loss 1.01, Flat_loss 0.20, Train_acc 89.98, Test_acc 79.10
2025-01-05 12:00:25,898 [podnet.py] => Task 1, Epoch 92/160 (LR 0.03833) => LSC_loss 0.43, Spatial_loss 1.08, Flat_loss 0.19, Train_acc 88.12, Test_acc 82.20
2025-01-05 12:00:29,429 [podnet.py] => Task 1, Epoch 93/160 (LR 0.03738) => LSC_loss 0.31, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 91.32, Test_acc 85.10
2025-01-05 12:00:32,883 [podnet.py] => Task 1, Epoch 94/160 (LR 0.03643) => LSC_loss 0.30, Spatial_loss 0.93, Flat_loss 0.19, Train_acc 91.84, Test_acc 84.50
2025-01-05 12:00:36,496 [podnet.py] => Task 1, Epoch 95/160 (LR 0.03549) => LSC_loss 0.29, Spatial_loss 0.97, Flat_loss 0.20, Train_acc 91.22, Test_acc 83.30
2025-01-05 12:00:40,205 [podnet.py] => Task 1, Epoch 96/160 (LR 0.03455) => LSC_loss 0.29, Spatial_loss 0.92, Flat_loss 0.19, Train_acc 92.34, Test_acc 80.40
2025-01-05 12:00:43,556 [podnet.py] => Task 1, Epoch 97/160 (LR 0.03362) => LSC_loss 0.33, Spatial_loss 1.09, Flat_loss 0.21, Train_acc 90.88, Test_acc 80.40
2025-01-05 12:00:47,130 [podnet.py] => Task 1, Epoch 98/160 (LR 0.03269) => LSC_loss 0.27, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 92.16, Test_acc 83.60
2025-01-05 12:00:50,712 [podnet.py] => Task 1, Epoch 99/160 (LR 0.03178) => LSC_loss 0.27, Spatial_loss 0.92, Flat_loss 0.19, Train_acc 92.80, Test_acc 83.30
2025-01-05 12:00:54,283 [podnet.py] => Task 1, Epoch 100/160 (LR 0.03087) => LSC_loss 0.28, Spatial_loss 1.04, Flat_loss 0.20, Train_acc 91.70, Test_acc 84.50
2025-01-05 12:00:57,883 [podnet.py] => Task 1, Epoch 101/160 (LR 0.02996) => LSC_loss 0.26, Spatial_loss 0.92, Flat_loss 0.20, Train_acc 92.76, Test_acc 82.70
2025-01-05 12:01:01,522 [podnet.py] => Task 1, Epoch 102/160 (LR 0.02907) => LSC_loss 0.29, Spatial_loss 1.00, Flat_loss 0.20, Train_acc 92.00, Test_acc 81.90
2025-01-05 12:01:05,194 [podnet.py] => Task 1, Epoch 103/160 (LR 0.02818) => LSC_loss 0.29, Spatial_loss 0.91, Flat_loss 0.20, Train_acc 92.02, Test_acc 82.00
2025-01-05 12:01:08,613 [podnet.py] => Task 1, Epoch 104/160 (LR 0.02730) => LSC_loss 0.30, Spatial_loss 1.07, Flat_loss 0.21, Train_acc 91.28, Test_acc 80.90
2025-01-05 12:01:12,011 [podnet.py] => Task 1, Epoch 105/160 (LR 0.02643) => LSC_loss 0.26, Spatial_loss 0.93, Flat_loss 0.19, Train_acc 92.78, Test_acc 84.40
2025-01-05 12:01:15,677 [podnet.py] => Task 1, Epoch 106/160 (LR 0.02557) => LSC_loss 0.27, Spatial_loss 0.91, Flat_loss 0.19, Train_acc 93.20, Test_acc 84.00
2025-01-05 12:01:19,142 [podnet.py] => Task 1, Epoch 107/160 (LR 0.02472) => LSC_loss 0.30, Spatial_loss 1.02, Flat_loss 0.21, Train_acc 91.34, Test_acc 84.30
2025-01-05 12:01:22,666 [podnet.py] => Task 1, Epoch 108/160 (LR 0.02388) => LSC_loss 0.27, Spatial_loss 0.94, Flat_loss 0.20, Train_acc 92.82, Test_acc 85.60
2025-01-05 12:01:26,312 [podnet.py] => Task 1, Epoch 109/160 (LR 0.02304) => LSC_loss 0.34, Spatial_loss 0.94, Flat_loss 0.19, Train_acc 91.46, Test_acc 84.40
2025-01-05 12:01:29,802 [podnet.py] => Task 1, Epoch 110/160 (LR 0.02222) => LSC_loss 0.28, Spatial_loss 0.91, Flat_loss 0.20, Train_acc 93.22, Test_acc 82.20
2025-01-05 12:01:33,345 [podnet.py] => Task 1, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 1.00, Flat_loss 0.20, Train_acc 92.60, Test_acc 84.80
2025-01-05 12:01:36,900 [podnet.py] => Task 1, Epoch 112/160 (LR 0.02061) => LSC_loss 0.26, Spatial_loss 0.89, Flat_loss 0.19, Train_acc 94.02, Test_acc 83.70
2025-01-05 12:01:40,283 [podnet.py] => Task 1, Epoch 113/160 (LR 0.01982) => LSC_loss 0.26, Spatial_loss 0.95, Flat_loss 0.19, Train_acc 92.68, Test_acc 85.30
2025-01-05 12:01:43,892 [podnet.py] => Task 1, Epoch 114/160 (LR 0.01905) => LSC_loss 0.22, Spatial_loss 0.87, Flat_loss 0.19, Train_acc 93.80, Test_acc 84.00
2025-01-05 12:01:47,519 [podnet.py] => Task 1, Epoch 115/160 (LR 0.01828) => LSC_loss 0.21, Spatial_loss 0.84, Flat_loss 0.18, Train_acc 94.40, Test_acc 84.40
2025-01-05 12:01:51,207 [podnet.py] => Task 1, Epoch 116/160 (LR 0.01753) => LSC_loss 0.22, Spatial_loss 0.82, Flat_loss 0.19, Train_acc 95.18, Test_acc 84.60
2025-01-05 12:01:54,755 [podnet.py] => Task 1, Epoch 117/160 (LR 0.01679) => LSC_loss 0.23, Spatial_loss 0.90, Flat_loss 0.19, Train_acc 94.26, Test_acc 83.10
2025-01-05 12:01:58,446 [podnet.py] => Task 1, Epoch 118/160 (LR 0.01606) => LSC_loss 0.22, Spatial_loss 0.88, Flat_loss 0.19, Train_acc 94.08, Test_acc 84.80
2025-01-05 12:02:01,983 [podnet.py] => Task 1, Epoch 119/160 (LR 0.01535) => LSC_loss 0.23, Spatial_loss 0.83, Flat_loss 0.18, Train_acc 94.58, Test_acc 81.70
2025-01-05 12:02:05,545 [podnet.py] => Task 1, Epoch 120/160 (LR 0.01464) => LSC_loss 0.25, Spatial_loss 0.88, Flat_loss 0.19, Train_acc 93.18, Test_acc 84.50
2025-01-05 12:02:09,122 [podnet.py] => Task 1, Epoch 121/160 (LR 0.01396) => LSC_loss 0.19, Spatial_loss 0.82, Flat_loss 0.18, Train_acc 95.48, Test_acc 85.20
2025-01-05 12:02:12,806 [podnet.py] => Task 1, Epoch 122/160 (LR 0.01328) => LSC_loss 0.22, Spatial_loss 0.84, Flat_loss 0.19, Train_acc 94.62, Test_acc 82.80
2025-01-05 12:02:16,222 [podnet.py] => Task 1, Epoch 123/160 (LR 0.01262) => LSC_loss 0.29, Spatial_loss 0.86, Flat_loss 0.19, Train_acc 92.44, Test_acc 84.70
2025-01-05 12:02:19,734 [podnet.py] => Task 1, Epoch 124/160 (LR 0.01198) => LSC_loss 0.21, Spatial_loss 0.86, Flat_loss 0.19, Train_acc 94.48, Test_acc 84.80
2025-01-05 12:02:23,342 [podnet.py] => Task 1, Epoch 125/160 (LR 0.01135) => LSC_loss 0.19, Spatial_loss 0.81, Flat_loss 0.19, Train_acc 95.42, Test_acc 84.80
2025-01-05 12:02:26,796 [podnet.py] => Task 1, Epoch 126/160 (LR 0.01073) => LSC_loss 0.23, Spatial_loss 0.82, Flat_loss 0.19, Train_acc 94.34, Test_acc 85.10
2025-01-05 12:02:30,249 [podnet.py] => Task 1, Epoch 127/160 (LR 0.01013) => LSC_loss 0.25, Spatial_loss 0.87, Flat_loss 0.19, Train_acc 94.26, Test_acc 85.80
2025-01-05 12:02:33,768 [podnet.py] => Task 1, Epoch 128/160 (LR 0.00955) => LSC_loss 0.19, Spatial_loss 0.87, Flat_loss 0.20, Train_acc 95.32, Test_acc 84.50
2025-01-05 12:02:37,262 [podnet.py] => Task 1, Epoch 129/160 (LR 0.00898) => LSC_loss 0.20, Spatial_loss 0.81, Flat_loss 0.19, Train_acc 95.36, Test_acc 85.60
2025-01-05 12:02:40,769 [podnet.py] => Task 1, Epoch 130/160 (LR 0.00843) => LSC_loss 0.21, Spatial_loss 0.87, Flat_loss 0.19, Train_acc 94.38, Test_acc 85.50
2025-01-05 12:02:44,296 [podnet.py] => Task 1, Epoch 131/160 (LR 0.00789) => LSC_loss 0.21, Spatial_loss 0.80, Flat_loss 0.18, Train_acc 94.58, Test_acc 85.90
2025-01-05 12:02:47,862 [podnet.py] => Task 1, Epoch 132/160 (LR 0.00737) => LSC_loss 0.17, Spatial_loss 0.80, Flat_loss 0.18, Train_acc 95.66, Test_acc 86.30
2025-01-05 12:02:51,396 [podnet.py] => Task 1, Epoch 133/160 (LR 0.00686) => LSC_loss 0.18, Spatial_loss 0.77, Flat_loss 0.18, Train_acc 96.02, Test_acc 85.60
2025-01-05 12:02:55,314 [podnet.py] => Task 1, Epoch 134/160 (LR 0.00638) => LSC_loss 0.18, Spatial_loss 0.79, Flat_loss 0.18, Train_acc 95.64, Test_acc 85.70
2025-01-05 12:02:58,848 [podnet.py] => Task 1, Epoch 135/160 (LR 0.00590) => LSC_loss 0.19, Spatial_loss 0.81, Flat_loss 0.19, Train_acc 95.84, Test_acc 85.50
2025-01-05 12:03:02,339 [podnet.py] => Task 1, Epoch 136/160 (LR 0.00545) => LSC_loss 0.20, Spatial_loss 0.80, Flat_loss 0.18, Train_acc 95.04, Test_acc 86.40
2025-01-05 12:03:05,933 [podnet.py] => Task 1, Epoch 137/160 (LR 0.00501) => LSC_loss 0.18, Spatial_loss 0.79, Flat_loss 0.19, Train_acc 95.34, Test_acc 85.20
2025-01-05 12:03:09,183 [podnet.py] => Task 1, Epoch 138/160 (LR 0.00459) => LSC_loss 0.17, Spatial_loss 0.76, Flat_loss 0.18, Train_acc 96.18, Test_acc 84.80
2025-01-05 12:03:12,730 [podnet.py] => Task 1, Epoch 139/160 (LR 0.00419) => LSC_loss 0.19, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 96.38, Test_acc 85.70
2025-01-05 12:03:16,522 [podnet.py] => Task 1, Epoch 140/160 (LR 0.00381) => LSC_loss 0.18, Spatial_loss 0.79, Flat_loss 0.19, Train_acc 96.00, Test_acc 85.90
2025-01-05 12:03:20,236 [podnet.py] => Task 1, Epoch 141/160 (LR 0.00344) => LSC_loss 0.15, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 96.54, Test_acc 86.00
2025-01-05 12:03:23,833 [podnet.py] => Task 1, Epoch 142/160 (LR 0.00309) => LSC_loss 0.17, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.84, Test_acc 85.90
2025-01-05 12:03:27,265 [podnet.py] => Task 1, Epoch 143/160 (LR 0.00276) => LSC_loss 0.16, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 95.80, Test_acc 85.90
2025-01-05 12:03:30,824 [podnet.py] => Task 1, Epoch 144/160 (LR 0.00245) => LSC_loss 0.15, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 96.82, Test_acc 85.90
2025-01-05 12:03:34,406 [podnet.py] => Task 1, Epoch 145/160 (LR 0.00215) => LSC_loss 0.15, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 96.70, Test_acc 85.50
2025-01-05 12:03:37,696 [podnet.py] => Task 1, Epoch 146/160 (LR 0.00188) => LSC_loss 0.13, Spatial_loss 0.73, Flat_loss 0.18, Train_acc 96.94, Test_acc 86.20
2025-01-05 12:03:41,323 [podnet.py] => Task 1, Epoch 147/160 (LR 0.00162) => LSC_loss 0.15, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.78, Test_acc 86.60
2025-01-05 12:03:44,937 [podnet.py] => Task 1, Epoch 148/160 (LR 0.00138) => LSC_loss 0.17, Spatial_loss 0.76, Flat_loss 0.18, Train_acc 96.32, Test_acc 85.90
2025-01-05 12:03:48,350 [podnet.py] => Task 1, Epoch 149/160 (LR 0.00116) => LSC_loss 0.16, Spatial_loss 0.73, Flat_loss 0.19, Train_acc 97.04, Test_acc 86.50
2025-01-05 12:03:52,013 [podnet.py] => Task 1, Epoch 150/160 (LR 0.00096) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.80, Test_acc 86.10
2025-01-05 12:03:55,514 [podnet.py] => Task 1, Epoch 151/160 (LR 0.00078) => LSC_loss 0.15, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 97.04, Test_acc 85.80
2025-01-05 12:03:58,966 [podnet.py] => Task 1, Epoch 152/160 (LR 0.00062) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.54, Test_acc 85.70
2025-01-05 12:04:02,691 [podnet.py] => Task 1, Epoch 153/160 (LR 0.00047) => LSC_loss 0.17, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.58, Test_acc 86.30
2025-01-05 12:04:06,434 [podnet.py] => Task 1, Epoch 154/160 (LR 0.00035) => LSC_loss 0.14, Spatial_loss 0.74, Flat_loss 0.19, Train_acc 96.90, Test_acc 86.70
2025-01-05 12:04:09,919 [podnet.py] => Task 1, Epoch 155/160 (LR 0.00024) => LSC_loss 0.16, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 97.16, Test_acc 85.90
2025-01-05 12:04:13,570 [podnet.py] => Task 1, Epoch 156/160 (LR 0.00015) => LSC_loss 0.14, Spatial_loss 0.71, Flat_loss 0.18, Train_acc 96.86, Test_acc 86.60
2025-01-05 12:04:17,253 [podnet.py] => Task 1, Epoch 157/160 (LR 0.00009) => LSC_loss 0.14, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.68, Test_acc 86.50
2025-01-05 12:04:20,798 [podnet.py] => Task 1, Epoch 158/160 (LR 0.00004) => LSC_loss 0.14, Spatial_loss 0.71, Flat_loss 0.18, Train_acc 96.90, Test_acc 86.20
2025-01-05 12:04:24,178 [podnet.py] => Task 1, Epoch 159/160 (LR 0.00001) => LSC_loss 0.18, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.62, Test_acc 85.20
2025-01-05 12:04:27,912 [podnet.py] => Task 1, Epoch 160/160 (LR 0.00000) => LSC_loss 0.14, Spatial_loss 0.73, Flat_loss 0.18, Train_acc 96.96, Test_acc 86.50
2025-01-05 12:04:27,912 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 12:04:27,913 [base.py] => Reducing exemplars...(2695 per classes)
2025-01-05 12:04:31,982 [base.py] => Constructing exemplars...(2695 per classes)
2025-01-05 12:04:40,910 [podnet.py] => The size of finetune dataset: 5000
2025-01-05 12:04:44,580 [podnet.py] => Task 1, Epoch 1/20 (LR 0.00497) => LSC_loss 0.14, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.60, Test_acc 85.40
2025-01-05 12:04:48,296 [podnet.py] => Task 1, Epoch 2/20 (LR 0.00488) => LSC_loss 0.15, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 96.30, Test_acc 85.80
2025-01-05 12:04:51,859 [podnet.py] => Task 1, Epoch 3/20 (LR 0.00473) => LSC_loss 0.15, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.60, Test_acc 85.90
2025-01-05 12:04:55,648 [podnet.py] => Task 1, Epoch 4/20 (LR 0.00452) => LSC_loss 0.17, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 97.02, Test_acc 85.80
2025-01-05 12:04:59,333 [podnet.py] => Task 1, Epoch 5/20 (LR 0.00427) => LSC_loss 0.20, Spatial_loss 0.79, Flat_loss 0.18, Train_acc 96.50, Test_acc 84.90
2025-01-05 12:05:02,839 [podnet.py] => Task 1, Epoch 6/20 (LR 0.00397) => LSC_loss 0.17, Spatial_loss 0.77, Flat_loss 0.19, Train_acc 96.78, Test_acc 85.20
2025-01-05 12:05:06,687 [podnet.py] => Task 1, Epoch 7/20 (LR 0.00363) => LSC_loss 0.18, Spatial_loss 0.77, Flat_loss 0.19, Train_acc 96.30, Test_acc 86.80
2025-01-05 12:05:10,548 [podnet.py] => Task 1, Epoch 8/20 (LR 0.00327) => LSC_loss 0.17, Spatial_loss 0.75, Flat_loss 0.18, Train_acc 96.66, Test_acc 86.00
2025-01-05 12:05:14,060 [podnet.py] => Task 1, Epoch 9/20 (LR 0.00289) => LSC_loss 0.14, Spatial_loss 0.74, Flat_loss 0.19, Train_acc 96.88, Test_acc 86.50
2025-01-05 12:05:17,557 [podnet.py] => Task 1, Epoch 10/20 (LR 0.00250) => LSC_loss 0.14, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.80, Test_acc 85.80
2025-01-05 12:05:21,109 [podnet.py] => Task 1, Epoch 11/20 (LR 0.00211) => LSC_loss 0.15, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.62, Test_acc 85.30
2025-01-05 12:05:24,560 [podnet.py] => Task 1, Epoch 12/20 (LR 0.00173) => LSC_loss 0.15, Spatial_loss 0.76, Flat_loss 0.18, Train_acc 96.62, Test_acc 86.10
2025-01-05 12:05:27,915 [podnet.py] => Task 1, Epoch 13/20 (LR 0.00137) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.54, Test_acc 86.50
2025-01-05 12:05:31,563 [podnet.py] => Task 1, Epoch 14/20 (LR 0.00103) => LSC_loss 0.16, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.94, Test_acc 86.50
2025-01-05 12:05:35,100 [podnet.py] => Task 1, Epoch 15/20 (LR 0.00073) => LSC_loss 0.15, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.92, Test_acc 86.10
2025-01-05 12:05:38,609 [podnet.py] => Task 1, Epoch 16/20 (LR 0.00048) => LSC_loss 0.14, Spatial_loss 0.73, Flat_loss 0.18, Train_acc 96.92, Test_acc 85.80
2025-01-05 12:05:42,227 [podnet.py] => Task 1, Epoch 17/20 (LR 0.00027) => LSC_loss 0.18, Spatial_loss 0.74, Flat_loss 0.18, Train_acc 96.92, Test_acc 86.10
2025-01-05 12:05:46,053 [podnet.py] => Task 1, Epoch 18/20 (LR 0.00012) => LSC_loss 0.14, Spatial_loss 0.73, Flat_loss 0.18, Train_acc 96.88, Test_acc 85.70
2025-01-05 12:05:49,567 [podnet.py] => Task 1, Epoch 19/20 (LR 0.00003) => LSC_loss 0.14, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.86, Test_acc 85.80
2025-01-05 12:05:53,288 [podnet.py] => Task 1, Epoch 20/20 (LR 0.00000) => LSC_loss 0.17, Spatial_loss 0.72, Flat_loss 0.18, Train_acc 96.66, Test_acc 86.30
2025-01-05 12:05:53,289 [base.py] => Reducing exemplars...(1347 per classes)
2025-01-05 12:05:57,499 [base.py] => Constructing exemplars...(1347 per classes)
2025-01-05 12:06:08,244 [podnet.py] => Exemplar size: 5000
2025-01-05 12:06:08,245 [trainer.py] => CNN: {'total': np.float64(86.3), '00-09': np.float64(86.3), 'old': np.float64(94.0), 'new': np.float64(78.6)}
2025-01-05 12:06:08,245 [trainer.py] => NME: {'total': np.float64(85.7), '00-09': np.float64(85.7), 'old': np.float64(94.2), 'new': np.float64(77.2)}
2025-01-05 12:06:08,245 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3)]
2025-01-05 12:06:08,245 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9)]
2025-01-05 12:06:08,245 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7)]
2025-01-05 12:06:08,245 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7)]

2025-01-05 12:06:08,245 [trainer.py] => All params: 472657
2025-01-05 12:06:08,245 [trainer.py] => Trainable params: 472657
2025-01-05 12:06:08,246 [podnet.py] => Learning on 10-15
2025-01-05 12:06:08,273 [podnet.py] => Adaptive factor: 1.7320508075688772
2025-01-05 12:06:12,931 [podnet.py] => Task 2, Epoch 1/160 (LR 0.09999) => LSC_loss 1.34, Spatial_loss 1.44, Flat_loss 0.17, Train_acc 63.36, Test_acc 53.07
2025-01-05 12:06:17,640 [podnet.py] => Task 2, Epoch 2/160 (LR 0.09996) => LSC_loss 1.03, Spatial_loss 1.45, Flat_loss 0.11, Train_acc 68.21, Test_acc 53.27
2025-01-05 12:06:22,059 [podnet.py] => Task 2, Epoch 3/160 (LR 0.09991) => LSC_loss 0.99, Spatial_loss 1.42, Flat_loss 0.11, Train_acc 69.29, Test_acc 55.40
2025-01-05 12:06:26,448 [podnet.py] => Task 2, Epoch 4/160 (LR 0.09985) => LSC_loss 0.95, Spatial_loss 1.37, Flat_loss 0.11, Train_acc 70.63, Test_acc 62.00
2025-01-05 12:06:30,858 [podnet.py] => Task 2, Epoch 5/160 (LR 0.09976) => LSC_loss 0.93, Spatial_loss 1.33, Flat_loss 0.10, Train_acc 71.56, Test_acc 56.07
2025-01-05 12:06:35,337 [podnet.py] => Task 2, Epoch 6/160 (LR 0.09965) => LSC_loss 0.90, Spatial_loss 1.39, Flat_loss 0.11, Train_acc 72.32, Test_acc 60.80
2025-01-05 12:06:40,111 [podnet.py] => Task 2, Epoch 7/160 (LR 0.09953) => LSC_loss 0.88, Spatial_loss 1.32, Flat_loss 0.11, Train_acc 73.29, Test_acc 59.33
2025-01-05 12:06:44,566 [podnet.py] => Task 2, Epoch 8/160 (LR 0.09938) => LSC_loss 0.85, Spatial_loss 1.34, Flat_loss 0.11, Train_acc 73.99, Test_acc 60.80
2025-01-05 12:06:49,127 [podnet.py] => Task 2, Epoch 9/160 (LR 0.09922) => LSC_loss 0.85, Spatial_loss 1.32, Flat_loss 0.11, Train_acc 73.73, Test_acc 62.60
2025-01-05 12:06:53,648 [podnet.py] => Task 2, Epoch 10/160 (LR 0.09904) => LSC_loss 0.84, Spatial_loss 1.34, Flat_loss 0.11, Train_acc 74.37, Test_acc 66.53
2025-01-05 12:06:58,210 [podnet.py] => Task 2, Epoch 11/160 (LR 0.09884) => LSC_loss 0.81, Spatial_loss 1.33, Flat_loss 0.11, Train_acc 75.20, Test_acc 66.80
2025-01-05 12:07:02,754 [podnet.py] => Task 2, Epoch 12/160 (LR 0.09862) => LSC_loss 0.80, Spatial_loss 1.34, Flat_loss 0.11, Train_acc 75.79, Test_acc 60.47
2025-01-05 12:07:07,501 [podnet.py] => Task 2, Epoch 13/160 (LR 0.09838) => LSC_loss 0.78, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 76.29, Test_acc 60.87
2025-01-05 12:07:12,160 [podnet.py] => Task 2, Epoch 14/160 (LR 0.09812) => LSC_loss 0.80, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 75.52, Test_acc 64.67
2025-01-05 12:07:16,561 [podnet.py] => Task 2, Epoch 15/160 (LR 0.09785) => LSC_loss 0.77, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 76.52, Test_acc 63.00
2025-01-05 12:07:21,062 [podnet.py] => Task 2, Epoch 16/160 (LR 0.09755) => LSC_loss 0.77, Spatial_loss 1.30, Flat_loss 0.11, Train_acc 76.65, Test_acc 60.40
2025-01-05 12:07:25,627 [podnet.py] => Task 2, Epoch 17/160 (LR 0.09724) => LSC_loss 0.77, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 76.53, Test_acc 65.87
2025-01-05 12:07:30,152 [podnet.py] => Task 2, Epoch 18/160 (LR 0.09691) => LSC_loss 0.76, Spatial_loss 1.35, Flat_loss 0.12, Train_acc 77.13, Test_acc 63.93
2025-01-05 12:07:34,468 [podnet.py] => Task 2, Epoch 19/160 (LR 0.09656) => LSC_loss 0.76, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 76.48, Test_acc 63.67
2025-01-05 12:07:38,933 [podnet.py] => Task 2, Epoch 20/160 (LR 0.09619) => LSC_loss 0.73, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 77.85, Test_acc 62.20
2025-01-05 12:07:43,605 [podnet.py] => Task 2, Epoch 21/160 (LR 0.09581) => LSC_loss 0.75, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 77.07, Test_acc 60.47
2025-01-05 12:07:47,910 [podnet.py] => Task 2, Epoch 22/160 (LR 0.09541) => LSC_loss 0.71, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 78.17, Test_acc 66.73
2025-01-05 12:07:52,344 [podnet.py] => Task 2, Epoch 23/160 (LR 0.09499) => LSC_loss 0.70, Spatial_loss 1.25, Flat_loss 0.11, Train_acc 78.89, Test_acc 53.33
2025-01-05 12:07:56,770 [podnet.py] => Task 2, Epoch 24/160 (LR 0.09455) => LSC_loss 0.70, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 79.12, Test_acc 59.47
2025-01-05 12:08:01,337 [podnet.py] => Task 2, Epoch 25/160 (LR 0.09410) => LSC_loss 0.72, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 78.37, Test_acc 66.60
2025-01-05 12:08:05,781 [podnet.py] => Task 2, Epoch 26/160 (LR 0.09362) => LSC_loss 0.71, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 78.95, Test_acc 64.73
2025-01-05 12:08:10,353 [podnet.py] => Task 2, Epoch 27/160 (LR 0.09314) => LSC_loss 0.68, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 79.36, Test_acc 65.47
2025-01-05 12:08:14,995 [podnet.py] => Task 2, Epoch 28/160 (LR 0.09263) => LSC_loss 0.71, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 78.12, Test_acc 54.07
2025-01-05 12:08:19,662 [podnet.py] => Task 2, Epoch 29/160 (LR 0.09211) => LSC_loss 0.69, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 78.84, Test_acc 63.13
2025-01-05 12:08:24,340 [podnet.py] => Task 2, Epoch 30/160 (LR 0.09157) => LSC_loss 0.68, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 79.77, Test_acc 66.67
2025-01-05 12:08:28,838 [podnet.py] => Task 2, Epoch 31/160 (LR 0.09102) => LSC_loss 0.68, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 79.45, Test_acc 67.40
2025-01-05 12:08:33,393 [podnet.py] => Task 2, Epoch 32/160 (LR 0.09045) => LSC_loss 0.69, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 78.97, Test_acc 66.07
2025-01-05 12:08:37,928 [podnet.py] => Task 2, Epoch 33/160 (LR 0.08987) => LSC_loss 0.65, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 80.61, Test_acc 67.07
2025-01-05 12:08:42,317 [podnet.py] => Task 2, Epoch 34/160 (LR 0.08927) => LSC_loss 0.67, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 79.96, Test_acc 61.80
2025-01-05 12:08:46,984 [podnet.py] => Task 2, Epoch 35/160 (LR 0.08865) => LSC_loss 0.65, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 80.45, Test_acc 64.27
2025-01-05 12:08:51,422 [podnet.py] => Task 2, Epoch 36/160 (LR 0.08802) => LSC_loss 0.63, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 81.17, Test_acc 68.33
2025-01-05 12:08:56,040 [podnet.py] => Task 2, Epoch 37/160 (LR 0.08738) => LSC_loss 0.66, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 80.05, Test_acc 64.67
2025-01-05 12:09:00,620 [podnet.py] => Task 2, Epoch 38/160 (LR 0.08672) => LSC_loss 0.64, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 80.95, Test_acc 66.07
2025-01-05 12:09:05,023 [podnet.py] => Task 2, Epoch 39/160 (LR 0.08604) => LSC_loss 0.64, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 80.95, Test_acc 67.20
2025-01-05 12:09:09,347 [podnet.py] => Task 2, Epoch 40/160 (LR 0.08536) => LSC_loss 0.63, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 81.05, Test_acc 64.80
2025-01-05 12:09:13,946 [podnet.py] => Task 2, Epoch 41/160 (LR 0.08465) => LSC_loss 0.62, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 81.51, Test_acc 69.13
2025-01-05 12:09:18,535 [podnet.py] => Task 2, Epoch 42/160 (LR 0.08394) => LSC_loss 0.63, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 80.53, Test_acc 66.67
2025-01-05 12:09:22,964 [podnet.py] => Task 2, Epoch 43/160 (LR 0.08321) => LSC_loss 0.63, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 81.12, Test_acc 65.33
2025-01-05 12:09:27,555 [podnet.py] => Task 2, Epoch 44/160 (LR 0.08247) => LSC_loss 0.59, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 82.64, Test_acc 70.07
2025-01-05 12:09:32,046 [podnet.py] => Task 2, Epoch 45/160 (LR 0.08172) => LSC_loss 0.60, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 81.96, Test_acc 66.07
2025-01-05 12:09:36,693 [podnet.py] => Task 2, Epoch 46/160 (LR 0.08095) => LSC_loss 0.60, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 82.23, Test_acc 69.87
2025-01-05 12:09:41,127 [podnet.py] => Task 2, Epoch 47/160 (LR 0.08018) => LSC_loss 0.61, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 81.97, Test_acc 69.73
2025-01-05 12:09:45,827 [podnet.py] => Task 2, Epoch 48/160 (LR 0.07939) => LSC_loss 0.57, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 82.73, Test_acc 63.13
2025-01-05 12:09:50,492 [podnet.py] => Task 2, Epoch 49/160 (LR 0.07859) => LSC_loss 0.58, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 82.65, Test_acc 62.73
2025-01-05 12:09:55,062 [podnet.py] => Task 2, Epoch 50/160 (LR 0.07778) => LSC_loss 0.59, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 82.45, Test_acc 59.87
2025-01-05 12:09:59,738 [podnet.py] => Task 2, Epoch 51/160 (LR 0.07696) => LSC_loss 0.57, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 82.76, Test_acc 67.47
2025-01-05 12:10:04,274 [podnet.py] => Task 2, Epoch 52/160 (LR 0.07612) => LSC_loss 0.57, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 82.65, Test_acc 68.93
2025-01-05 12:10:08,949 [podnet.py] => Task 2, Epoch 53/160 (LR 0.07528) => LSC_loss 0.56, Spatial_loss 1.19, Flat_loss 0.12, Train_acc 83.20, Test_acc 67.87
2025-01-05 12:10:13,354 [podnet.py] => Task 2, Epoch 54/160 (LR 0.07443) => LSC_loss 0.57, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 83.20, Test_acc 64.73
2025-01-05 12:10:17,784 [podnet.py] => Task 2, Epoch 55/160 (LR 0.07357) => LSC_loss 0.59, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 82.49, Test_acc 67.20
2025-01-05 12:10:22,238 [podnet.py] => Task 2, Epoch 56/160 (LR 0.07270) => LSC_loss 0.56, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 82.92, Test_acc 68.87
2025-01-05 12:10:26,899 [podnet.py] => Task 2, Epoch 57/160 (LR 0.07182) => LSC_loss 0.56, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 83.43, Test_acc 69.40
2025-01-05 12:10:31,399 [podnet.py] => Task 2, Epoch 58/160 (LR 0.07093) => LSC_loss 0.55, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 84.09, Test_acc 66.07
2025-01-05 12:10:36,166 [podnet.py] => Task 2, Epoch 59/160 (LR 0.07004) => LSC_loss 0.56, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 83.40, Test_acc 67.67
2025-01-05 12:10:40,719 [podnet.py] => Task 2, Epoch 60/160 (LR 0.06913) => LSC_loss 0.53, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 84.11, Test_acc 66.47
2025-01-05 12:10:45,117 [podnet.py] => Task 2, Epoch 61/160 (LR 0.06822) => LSC_loss 0.55, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 83.76, Test_acc 64.53
2025-01-05 12:10:49,629 [podnet.py] => Task 2, Epoch 62/160 (LR 0.06731) => LSC_loss 0.54, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 84.09, Test_acc 69.53
2025-01-05 12:10:54,001 [podnet.py] => Task 2, Epoch 63/160 (LR 0.06638) => LSC_loss 0.52, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 84.57, Test_acc 66.73
2025-01-05 12:10:58,339 [podnet.py] => Task 2, Epoch 64/160 (LR 0.06545) => LSC_loss 0.51, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 84.96, Test_acc 69.13
2025-01-05 12:11:02,928 [podnet.py] => Task 2, Epoch 65/160 (LR 0.06451) => LSC_loss 0.50, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 85.04, Test_acc 65.53
2025-01-05 12:11:07,499 [podnet.py] => Task 2, Epoch 66/160 (LR 0.06357) => LSC_loss 0.52, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 84.55, Test_acc 68.07
2025-01-05 12:11:12,021 [podnet.py] => Task 2, Epoch 67/160 (LR 0.06262) => LSC_loss 0.49, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 85.28, Test_acc 65.00
2025-01-05 12:11:16,404 [podnet.py] => Task 2, Epoch 68/160 (LR 0.06167) => LSC_loss 0.49, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 86.40, Test_acc 72.20
2025-01-05 12:11:20,930 [podnet.py] => Task 2, Epoch 69/160 (LR 0.06072) => LSC_loss 0.50, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 85.71, Test_acc 65.53
2025-01-05 12:11:25,478 [podnet.py] => Task 2, Epoch 70/160 (LR 0.05975) => LSC_loss 0.50, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 84.91, Test_acc 67.93
2025-01-05 12:11:30,138 [podnet.py] => Task 2, Epoch 71/160 (LR 0.05879) => LSC_loss 0.50, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 85.23, Test_acc 66.53
2025-01-05 12:11:34,591 [podnet.py] => Task 2, Epoch 72/160 (LR 0.05782) => LSC_loss 0.49, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 85.33, Test_acc 73.13
2025-01-05 12:11:39,070 [podnet.py] => Task 2, Epoch 73/160 (LR 0.05685) => LSC_loss 0.48, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 86.15, Test_acc 68.33
2025-01-05 12:11:43,677 [podnet.py] => Task 2, Epoch 74/160 (LR 0.05588) => LSC_loss 0.49, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 85.84, Test_acc 61.87
2025-01-05 12:11:48,255 [podnet.py] => Task 2, Epoch 75/160 (LR 0.05490) => LSC_loss 0.49, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 85.55, Test_acc 68.93
2025-01-05 12:11:52,703 [podnet.py] => Task 2, Epoch 76/160 (LR 0.05392) => LSC_loss 0.46, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 86.31, Test_acc 68.07
2025-01-05 12:11:57,162 [podnet.py] => Task 2, Epoch 77/160 (LR 0.05294) => LSC_loss 0.44, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 87.31, Test_acc 65.60
2025-01-05 12:12:01,679 [podnet.py] => Task 2, Epoch 78/160 (LR 0.05196) => LSC_loss 0.46, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 86.55, Test_acc 70.73
2025-01-05 12:12:06,138 [podnet.py] => Task 2, Epoch 79/160 (LR 0.05098) => LSC_loss 0.42, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 87.81, Test_acc 67.73
2025-01-05 12:12:10,584 [podnet.py] => Task 2, Epoch 80/160 (LR 0.05000) => LSC_loss 0.43, Spatial_loss 1.09, Flat_loss 0.12, Train_acc 87.27, Test_acc 67.27
2025-01-05 12:12:15,193 [podnet.py] => Task 2, Epoch 81/160 (LR 0.04902) => LSC_loss 0.44, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 86.96, Test_acc 69.93
2025-01-05 12:12:19,734 [podnet.py] => Task 2, Epoch 82/160 (LR 0.04804) => LSC_loss 0.42, Spatial_loss 1.10, Flat_loss 0.12, Train_acc 88.09, Test_acc 69.20
2025-01-05 12:12:24,372 [podnet.py] => Task 2, Epoch 83/160 (LR 0.04706) => LSC_loss 0.42, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 88.09, Test_acc 71.73
2025-01-05 12:12:28,802 [podnet.py] => Task 2, Epoch 84/160 (LR 0.04608) => LSC_loss 0.41, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 88.01, Test_acc 70.13
2025-01-05 12:12:33,309 [podnet.py] => Task 2, Epoch 85/160 (LR 0.04510) => LSC_loss 0.43, Spatial_loss 1.11, Flat_loss 0.12, Train_acc 87.31, Test_acc 70.60
2025-01-05 12:12:37,749 [podnet.py] => Task 2, Epoch 86/160 (LR 0.04412) => LSC_loss 0.41, Spatial_loss 1.11, Flat_loss 0.12, Train_acc 88.59, Test_acc 68.80
2025-01-05 12:12:42,405 [podnet.py] => Task 2, Epoch 87/160 (LR 0.04315) => LSC_loss 0.40, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 88.03, Test_acc 71.60
2025-01-05 12:12:46,869 [podnet.py] => Task 2, Epoch 88/160 (LR 0.04218) => LSC_loss 0.40, Spatial_loss 1.04, Flat_loss 0.11, Train_acc 88.77, Test_acc 67.87
2025-01-05 12:12:51,318 [podnet.py] => Task 2, Epoch 89/160 (LR 0.04121) => LSC_loss 0.38, Spatial_loss 1.07, Flat_loss 0.12, Train_acc 89.00, Test_acc 72.33
2025-01-05 12:12:56,074 [podnet.py] => Task 2, Epoch 90/160 (LR 0.04025) => LSC_loss 0.38, Spatial_loss 1.06, Flat_loss 0.11, Train_acc 89.21, Test_acc 71.53
2025-01-05 12:13:00,841 [podnet.py] => Task 2, Epoch 91/160 (LR 0.03928) => LSC_loss 0.36, Spatial_loss 1.03, Flat_loss 0.12, Train_acc 89.85, Test_acc 69.53
2025-01-05 12:13:05,222 [podnet.py] => Task 2, Epoch 92/160 (LR 0.03833) => LSC_loss 0.37, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 89.28, Test_acc 70.27
2025-01-05 12:13:09,889 [podnet.py] => Task 2, Epoch 93/160 (LR 0.03738) => LSC_loss 0.37, Spatial_loss 1.06, Flat_loss 0.12, Train_acc 89.40, Test_acc 72.07
2025-01-05 12:13:14,513 [podnet.py] => Task 2, Epoch 94/160 (LR 0.03643) => LSC_loss 0.35, Spatial_loss 1.05, Flat_loss 0.11, Train_acc 90.21, Test_acc 70.67
2025-01-05 12:13:19,019 [podnet.py] => Task 2, Epoch 95/160 (LR 0.03549) => LSC_loss 0.36, Spatial_loss 1.03, Flat_loss 0.12, Train_acc 89.91, Test_acc 70.40
2025-01-05 12:13:23,481 [podnet.py] => Task 2, Epoch 96/160 (LR 0.03455) => LSC_loss 0.36, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 89.68, Test_acc 70.27
2025-01-05 12:13:27,868 [podnet.py] => Task 2, Epoch 97/160 (LR 0.03362) => LSC_loss 0.35, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 90.35, Test_acc 69.53
2025-01-05 12:13:32,385 [podnet.py] => Task 2, Epoch 98/160 (LR 0.03269) => LSC_loss 0.35, Spatial_loss 1.01, Flat_loss 0.11, Train_acc 90.52, Test_acc 70.47
2025-01-05 12:13:36,881 [podnet.py] => Task 2, Epoch 99/160 (LR 0.03178) => LSC_loss 0.33, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 91.03, Test_acc 69.87
2025-01-05 12:13:41,439 [podnet.py] => Task 2, Epoch 100/160 (LR 0.03087) => LSC_loss 0.35, Spatial_loss 1.02, Flat_loss 0.12, Train_acc 89.85, Test_acc 73.53
2025-01-05 12:13:46,089 [podnet.py] => Task 2, Epoch 101/160 (LR 0.02996) => LSC_loss 0.33, Spatial_loss 1.01, Flat_loss 0.12, Train_acc 90.53, Test_acc 71.47
2025-01-05 12:13:50,613 [podnet.py] => Task 2, Epoch 102/160 (LR 0.02907) => LSC_loss 0.31, Spatial_loss 0.95, Flat_loss 0.11, Train_acc 91.75, Test_acc 72.27
2025-01-05 12:13:55,399 [podnet.py] => Task 2, Epoch 103/160 (LR 0.02818) => LSC_loss 0.33, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 91.12, Test_acc 71.87
2025-01-05 12:13:59,947 [podnet.py] => Task 2, Epoch 104/160 (LR 0.02730) => LSC_loss 0.31, Spatial_loss 0.96, Flat_loss 0.11, Train_acc 91.44, Test_acc 70.67
2025-01-05 12:14:04,641 [podnet.py] => Task 2, Epoch 105/160 (LR 0.02643) => LSC_loss 0.30, Spatial_loss 0.94, Flat_loss 0.11, Train_acc 91.79, Test_acc 69.93
2025-01-05 12:14:09,308 [podnet.py] => Task 2, Epoch 106/160 (LR 0.02557) => LSC_loss 0.31, Spatial_loss 0.97, Flat_loss 0.11, Train_acc 91.36, Test_acc 71.13
2025-01-05 12:14:13,894 [podnet.py] => Task 2, Epoch 107/160 (LR 0.02472) => LSC_loss 0.30, Spatial_loss 0.95, Flat_loss 0.11, Train_acc 91.88, Test_acc 70.53
2025-01-05 12:14:18,598 [podnet.py] => Task 2, Epoch 108/160 (LR 0.02388) => LSC_loss 0.30, Spatial_loss 0.96, Flat_loss 0.11, Train_acc 92.01, Test_acc 70.80
2025-01-05 12:14:22,848 [podnet.py] => Task 2, Epoch 109/160 (LR 0.02304) => LSC_loss 0.28, Spatial_loss 0.92, Flat_loss 0.11, Train_acc 92.65, Test_acc 73.60
2025-01-05 12:14:27,758 [podnet.py] => Task 2, Epoch 110/160 (LR 0.02222) => LSC_loss 0.26, Spatial_loss 0.93, Flat_loss 0.11, Train_acc 93.07, Test_acc 70.67
2025-01-05 12:14:32,468 [podnet.py] => Task 2, Epoch 111/160 (LR 0.02141) => LSC_loss 0.27, Spatial_loss 0.93, Flat_loss 0.11, Train_acc 93.11, Test_acc 72.80
2025-01-05 12:14:37,180 [podnet.py] => Task 2, Epoch 112/160 (LR 0.02061) => LSC_loss 0.26, Spatial_loss 0.91, Flat_loss 0.11, Train_acc 93.09, Test_acc 72.20
2025-01-05 12:14:41,653 [podnet.py] => Task 2, Epoch 113/160 (LR 0.01982) => LSC_loss 0.25, Spatial_loss 0.89, Flat_loss 0.11, Train_acc 93.16, Test_acc 71.20
2025-01-05 12:14:46,079 [podnet.py] => Task 2, Epoch 114/160 (LR 0.01905) => LSC_loss 0.26, Spatial_loss 0.90, Flat_loss 0.11, Train_acc 93.47, Test_acc 71.27
2025-01-05 12:14:50,675 [podnet.py] => Task 2, Epoch 115/160 (LR 0.01828) => LSC_loss 0.26, Spatial_loss 0.90, Flat_loss 0.11, Train_acc 93.32, Test_acc 73.40
2025-01-05 12:14:55,443 [podnet.py] => Task 2, Epoch 116/160 (LR 0.01753) => LSC_loss 0.23, Spatial_loss 0.87, Flat_loss 0.11, Train_acc 93.81, Test_acc 72.87
2025-01-05 12:15:00,038 [podnet.py] => Task 2, Epoch 117/160 (LR 0.01679) => LSC_loss 0.24, Spatial_loss 0.86, Flat_loss 0.11, Train_acc 94.12, Test_acc 72.53
2025-01-05 12:15:04,613 [podnet.py] => Task 2, Epoch 118/160 (LR 0.01606) => LSC_loss 0.23, Spatial_loss 0.87, Flat_loss 0.11, Train_acc 94.17, Test_acc 72.07
2025-01-05 12:15:09,202 [podnet.py] => Task 2, Epoch 119/160 (LR 0.01535) => LSC_loss 0.22, Spatial_loss 0.86, Flat_loss 0.11, Train_acc 94.60, Test_acc 72.27
2025-01-05 12:15:13,910 [podnet.py] => Task 2, Epoch 120/160 (LR 0.01464) => LSC_loss 0.22, Spatial_loss 0.87, Flat_loss 0.11, Train_acc 94.32, Test_acc 69.80
2025-01-05 12:15:18,515 [podnet.py] => Task 2, Epoch 121/160 (LR 0.01396) => LSC_loss 0.21, Spatial_loss 0.82, Flat_loss 0.11, Train_acc 94.87, Test_acc 73.13
2025-01-05 12:15:23,155 [podnet.py] => Task 2, Epoch 122/160 (LR 0.01328) => LSC_loss 0.22, Spatial_loss 0.84, Flat_loss 0.11, Train_acc 94.61, Test_acc 74.73
2025-01-05 12:15:27,637 [podnet.py] => Task 2, Epoch 123/160 (LR 0.01262) => LSC_loss 0.20, Spatial_loss 0.81, Flat_loss 0.10, Train_acc 95.23, Test_acc 73.60
2025-01-05 12:15:32,176 [podnet.py] => Task 2, Epoch 124/160 (LR 0.01198) => LSC_loss 0.20, Spatial_loss 0.80, Flat_loss 0.10, Train_acc 95.47, Test_acc 72.07
2025-01-05 12:15:36,631 [podnet.py] => Task 2, Epoch 125/160 (LR 0.01135) => LSC_loss 0.20, Spatial_loss 0.82, Flat_loss 0.11, Train_acc 94.75, Test_acc 72.00
2025-01-05 12:15:40,995 [podnet.py] => Task 2, Epoch 126/160 (LR 0.01073) => LSC_loss 0.19, Spatial_loss 0.79, Flat_loss 0.10, Train_acc 95.64, Test_acc 72.53
2025-01-05 12:15:45,545 [podnet.py] => Task 2, Epoch 127/160 (LR 0.01013) => LSC_loss 0.20, Spatial_loss 0.82, Flat_loss 0.10, Train_acc 95.25, Test_acc 73.33
2025-01-05 12:15:50,023 [podnet.py] => Task 2, Epoch 128/160 (LR 0.00955) => LSC_loss 0.19, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 95.52, Test_acc 74.27
2025-01-05 12:15:54,465 [podnet.py] => Task 2, Epoch 129/160 (LR 0.00898) => LSC_loss 0.18, Spatial_loss 0.80, Flat_loss 0.10, Train_acc 96.23, Test_acc 73.27
2025-01-05 12:15:59,047 [podnet.py] => Task 2, Epoch 130/160 (LR 0.00843) => LSC_loss 0.18, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 95.89, Test_acc 73.20
2025-01-05 12:16:03,694 [podnet.py] => Task 2, Epoch 131/160 (LR 0.00789) => LSC_loss 0.19, Spatial_loss 0.77, Flat_loss 0.10, Train_acc 95.67, Test_acc 74.27
2025-01-05 12:16:08,287 [podnet.py] => Task 2, Epoch 132/160 (LR 0.00737) => LSC_loss 0.18, Spatial_loss 0.78, Flat_loss 0.10, Train_acc 95.65, Test_acc 74.47
2025-01-05 12:16:12,828 [podnet.py] => Task 2, Epoch 133/160 (LR 0.00686) => LSC_loss 0.17, Spatial_loss 0.77, Flat_loss 0.10, Train_acc 96.33, Test_acc 73.93
2025-01-05 12:16:17,127 [podnet.py] => Task 2, Epoch 134/160 (LR 0.00638) => LSC_loss 0.16, Spatial_loss 0.76, Flat_loss 0.10, Train_acc 96.39, Test_acc 74.93
2025-01-05 12:16:21,645 [podnet.py] => Task 2, Epoch 135/160 (LR 0.00590) => LSC_loss 0.16, Spatial_loss 0.74, Flat_loss 0.10, Train_acc 96.79, Test_acc 74.87
2025-01-05 12:16:26,262 [podnet.py] => Task 2, Epoch 136/160 (LR 0.00545) => LSC_loss 0.16, Spatial_loss 0.74, Flat_loss 0.10, Train_acc 96.69, Test_acc 73.80
2025-01-05 12:16:30,730 [podnet.py] => Task 2, Epoch 137/160 (LR 0.00501) => LSC_loss 0.15, Spatial_loss 0.75, Flat_loss 0.10, Train_acc 97.07, Test_acc 75.20
2025-01-05 12:16:35,275 [podnet.py] => Task 2, Epoch 138/160 (LR 0.00459) => LSC_loss 0.16, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 96.68, Test_acc 74.67
2025-01-05 12:16:39,816 [podnet.py] => Task 2, Epoch 139/160 (LR 0.00419) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 97.07, Test_acc 75.07
2025-01-05 12:16:44,432 [podnet.py] => Task 2, Epoch 140/160 (LR 0.00381) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 96.73, Test_acc 74.40
2025-01-05 12:16:48,916 [podnet.py] => Task 2, Epoch 141/160 (LR 0.00344) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 96.81, Test_acc 74.60
2025-01-05 12:16:53,376 [podnet.py] => Task 2, Epoch 142/160 (LR 0.00309) => LSC_loss 0.16, Spatial_loss 0.74, Flat_loss 0.10, Train_acc 96.67, Test_acc 74.67
2025-01-05 12:16:57,774 [podnet.py] => Task 2, Epoch 143/160 (LR 0.00276) => LSC_loss 0.15, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 97.01, Test_acc 74.33
2025-01-05 12:17:02,328 [podnet.py] => Task 2, Epoch 144/160 (LR 0.00245) => LSC_loss 0.15, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 97.20, Test_acc 74.80
2025-01-05 12:17:06,722 [podnet.py] => Task 2, Epoch 145/160 (LR 0.00215) => LSC_loss 0.13, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.49, Test_acc 73.93
2025-01-05 12:17:11,152 [podnet.py] => Task 2, Epoch 146/160 (LR 0.00188) => LSC_loss 0.14, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.23, Test_acc 73.93
2025-01-05 12:17:15,704 [podnet.py] => Task 2, Epoch 147/160 (LR 0.00162) => LSC_loss 0.14, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.48, Test_acc 74.53
2025-01-05 12:17:20,238 [podnet.py] => Task 2, Epoch 148/160 (LR 0.00138) => LSC_loss 0.14, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.33, Test_acc 76.00
2025-01-05 12:17:24,631 [podnet.py] => Task 2, Epoch 149/160 (LR 0.00116) => LSC_loss 0.14, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 97.39, Test_acc 74.47
2025-01-05 12:17:29,293 [podnet.py] => Task 2, Epoch 150/160 (LR 0.00096) => LSC_loss 0.14, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.49, Test_acc 75.20
2025-01-05 12:17:33,913 [podnet.py] => Task 2, Epoch 151/160 (LR 0.00078) => LSC_loss 0.14, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.43, Test_acc 74.73
2025-01-05 12:17:38,529 [podnet.py] => Task 2, Epoch 152/160 (LR 0.00062) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.56, Test_acc 75.27
2025-01-05 12:17:42,863 [podnet.py] => Task 2, Epoch 153/160 (LR 0.00047) => LSC_loss 0.13, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 97.64, Test_acc 74.80
2025-01-05 12:17:47,625 [podnet.py] => Task 2, Epoch 154/160 (LR 0.00035) => LSC_loss 0.14, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.31, Test_acc 75.07
2025-01-05 12:17:52,082 [podnet.py] => Task 2, Epoch 155/160 (LR 0.00024) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.47, Test_acc 75.20
2025-01-05 12:17:56,882 [podnet.py] => Task 2, Epoch 156/160 (LR 0.00015) => LSC_loss 0.13, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 97.69, Test_acc 74.93
2025-01-05 12:18:01,448 [podnet.py] => Task 2, Epoch 157/160 (LR 0.00009) => LSC_loss 0.14, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 97.49, Test_acc 75.33
2025-01-05 12:18:05,864 [podnet.py] => Task 2, Epoch 158/160 (LR 0.00004) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.59, Test_acc 75.33
2025-01-05 12:18:10,450 [podnet.py] => Task 2, Epoch 159/160 (LR 0.00001) => LSC_loss 0.13, Spatial_loss 0.67, Flat_loss 0.10, Train_acc 97.55, Test_acc 75.27
2025-01-05 12:18:14,988 [podnet.py] => Task 2, Epoch 160/160 (LR 0.00000) => LSC_loss 0.13, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 97.60, Test_acc 75.20
2025-01-05 12:18:14,988 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 12:18:14,988 [base.py] => Reducing exemplars...(1347 per classes)
2025-01-05 12:18:22,965 [base.py] => Constructing exemplars...(1347 per classes)
2025-01-05 12:18:31,838 [podnet.py] => The size of finetune dataset: 7500
2025-01-05 12:18:36,478 [podnet.py] => Task 2, Epoch 1/20 (LR 0.00497) => LSC_loss 0.13, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 97.51, Test_acc 74.13
2025-01-05 12:18:41,231 [podnet.py] => Task 2, Epoch 2/20 (LR 0.00488) => LSC_loss 0.16, Spatial_loss 0.73, Flat_loss 0.10, Train_acc 96.75, Test_acc 74.73
2025-01-05 12:18:45,511 [podnet.py] => Task 2, Epoch 3/20 (LR 0.00473) => LSC_loss 0.15, Spatial_loss 0.73, Flat_loss 0.10, Train_acc 97.09, Test_acc 73.93
2025-01-05 12:18:49,922 [podnet.py] => Task 2, Epoch 4/20 (LR 0.00452) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 96.91, Test_acc 73.80
2025-01-05 12:18:54,541 [podnet.py] => Task 2, Epoch 5/20 (LR 0.00427) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 96.77, Test_acc 73.80
2025-01-05 12:18:58,998 [podnet.py] => Task 2, Epoch 6/20 (LR 0.00397) => LSC_loss 0.14, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 97.39, Test_acc 73.93
2025-01-05 12:19:03,593 [podnet.py] => Task 2, Epoch 7/20 (LR 0.00363) => LSC_loss 0.15, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 96.88, Test_acc 74.40
2025-01-05 12:19:08,186 [podnet.py] => Task 2, Epoch 8/20 (LR 0.00327) => LSC_loss 0.14, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 97.20, Test_acc 74.27
2025-01-05 12:19:12,751 [podnet.py] => Task 2, Epoch 9/20 (LR 0.00289) => LSC_loss 0.15, Spatial_loss 0.72, Flat_loss 0.10, Train_acc 97.19, Test_acc 73.67
2025-01-05 12:19:17,273 [podnet.py] => Task 2, Epoch 10/20 (LR 0.00250) => LSC_loss 0.14, Spatial_loss 0.73, Flat_loss 0.10, Train_acc 97.37, Test_acc 74.33
2025-01-05 12:19:21,899 [podnet.py] => Task 2, Epoch 11/20 (LR 0.00211) => LSC_loss 0.14, Spatial_loss 0.70, Flat_loss 0.10, Train_acc 97.20, Test_acc 74.87
2025-01-05 12:19:26,481 [podnet.py] => Task 2, Epoch 12/20 (LR 0.00173) => LSC_loss 0.14, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.29, Test_acc 74.07
2025-01-05 12:19:31,038 [podnet.py] => Task 2, Epoch 13/20 (LR 0.00137) => LSC_loss 0.13, Spatial_loss 0.71, Flat_loss 0.10, Train_acc 97.53, Test_acc 74.73
2025-01-05 12:19:35,665 [podnet.py] => Task 2, Epoch 14/20 (LR 0.00103) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.72, Test_acc 74.13
2025-01-05 12:19:40,244 [podnet.py] => Task 2, Epoch 15/20 (LR 0.00073) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.57, Test_acc 74.87
2025-01-05 12:19:44,924 [podnet.py] => Task 2, Epoch 16/20 (LR 0.00048) => LSC_loss 0.14, Spatial_loss 0.68, Flat_loss 0.10, Train_acc 97.23, Test_acc 75.13
2025-01-05 12:19:49,532 [podnet.py] => Task 2, Epoch 17/20 (LR 0.00027) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.67, Test_acc 74.87
2025-01-05 12:19:54,367 [podnet.py] => Task 2, Epoch 18/20 (LR 0.00012) => LSC_loss 0.13, Spatial_loss 0.67, Flat_loss 0.10, Train_acc 97.79, Test_acc 75.07
2025-01-05 12:19:58,915 [podnet.py] => Task 2, Epoch 19/20 (LR 0.00003) => LSC_loss 0.13, Spatial_loss 0.67, Flat_loss 0.10, Train_acc 97.67, Test_acc 75.07
2025-01-05 12:20:03,388 [podnet.py] => Task 2, Epoch 20/20 (LR 0.00000) => LSC_loss 0.13, Spatial_loss 0.69, Flat_loss 0.10, Train_acc 97.96, Test_acc 74.67
2025-01-05 12:20:03,388 [base.py] => Reducing exemplars...(898 per classes)
2025-01-05 12:20:12,195 [base.py] => Constructing exemplars...(898 per classes)
2025-01-05 12:20:23,427 [podnet.py] => Exemplar size: 7500
2025-01-05 12:20:23,427 [trainer.py] => CNN: {'total': np.float64(74.67), '00-09': np.float64(81.9), '10-19': np.float64(60.2), 'old': np.float64(81.9), 'new': np.float64(60.2)}
2025-01-05 12:20:23,427 [trainer.py] => NME: {'total': np.float64(74.67), '00-09': np.float64(80.9), '10-19': np.float64(62.2), 'old': np.float64(80.9), 'new': np.float64(62.2)}
2025-01-05 12:20:23,427 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67)]
2025-01-05 12:20:23,427 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8)]
2025-01-05 12:20:23,427 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67)]
2025-01-05 12:20:23,427 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6)]

2025-01-05 12:20:23,427 [trainer.py] => All params: 475857
2025-01-05 12:20:23,427 [trainer.py] => Trainable params: 475857
2025-01-05 12:20:23,428 [podnet.py] => Learning on 15-20
2025-01-05 12:20:23,457 [podnet.py] => Adaptive factor: 2.0
2025-01-05 12:20:29,160 [podnet.py] => Task 3, Epoch 1/160 (LR 0.09999) => LSC_loss 1.61, Spatial_loss 2.11, Flat_loss 0.25, Train_acc 57.05, Test_acc 38.30
2025-01-05 12:20:34,742 [podnet.py] => Task 3, Epoch 2/160 (LR 0.09996) => LSC_loss 1.36, Spatial_loss 1.98, Flat_loss 0.19, Train_acc 59.62, Test_acc 48.15
2025-01-05 12:20:40,408 [podnet.py] => Task 3, Epoch 3/160 (LR 0.09991) => LSC_loss 1.20, Spatial_loss 1.70, Flat_loss 0.14, Train_acc 64.42, Test_acc 50.20
2025-01-05 12:20:45,924 [podnet.py] => Task 3, Epoch 4/160 (LR 0.09985) => LSC_loss 1.15, Spatial_loss 1.66, Flat_loss 0.14, Train_acc 65.75, Test_acc 49.95
2025-01-05 12:20:51,460 [podnet.py] => Task 3, Epoch 5/160 (LR 0.09976) => LSC_loss 1.15, Spatial_loss 1.70, Flat_loss 0.14, Train_acc 66.02, Test_acc 54.25
2025-01-05 12:20:56,966 [podnet.py] => Task 3, Epoch 6/160 (LR 0.09965) => LSC_loss 1.11, Spatial_loss 1.66, Flat_loss 0.13, Train_acc 67.02, Test_acc 54.25
2025-01-05 12:21:02,592 [podnet.py] => Task 3, Epoch 7/160 (LR 0.09953) => LSC_loss 1.07, Spatial_loss 1.56, Flat_loss 0.13, Train_acc 68.50, Test_acc 55.20
2025-01-05 12:21:08,125 [podnet.py] => Task 3, Epoch 8/160 (LR 0.09938) => LSC_loss 1.07, Spatial_loss 1.56, Flat_loss 0.12, Train_acc 68.37, Test_acc 52.35
2025-01-05 12:21:13,674 [podnet.py] => Task 3, Epoch 9/160 (LR 0.09922) => LSC_loss 1.05, Spatial_loss 1.52, Flat_loss 0.12, Train_acc 69.44, Test_acc 50.90
2025-01-05 12:21:19,337 [podnet.py] => Task 3, Epoch 10/160 (LR 0.09904) => LSC_loss 1.05, Spatial_loss 1.59, Flat_loss 0.13, Train_acc 69.27, Test_acc 51.95
2025-01-05 12:21:25,005 [podnet.py] => Task 3, Epoch 11/160 (LR 0.09884) => LSC_loss 1.03, Spatial_loss 1.48, Flat_loss 0.12, Train_acc 70.05, Test_acc 53.85
2025-01-05 12:21:30,600 [podnet.py] => Task 3, Epoch 12/160 (LR 0.09862) => LSC_loss 1.05, Spatial_loss 1.57, Flat_loss 0.13, Train_acc 69.45, Test_acc 53.10
2025-01-05 12:21:36,076 [podnet.py] => Task 3, Epoch 13/160 (LR 0.09838) => LSC_loss 1.01, Spatial_loss 1.53, Flat_loss 0.13, Train_acc 70.07, Test_acc 57.50
2025-01-05 12:21:41,852 [podnet.py] => Task 3, Epoch 14/160 (LR 0.09812) => LSC_loss 1.05, Spatial_loss 1.60, Flat_loss 0.13, Train_acc 69.59, Test_acc 54.10
2025-01-05 12:21:47,409 [podnet.py] => Task 3, Epoch 15/160 (LR 0.09785) => LSC_loss 1.00, Spatial_loss 1.54, Flat_loss 0.13, Train_acc 70.54, Test_acc 57.95
2025-01-05 12:21:53,017 [podnet.py] => Task 3, Epoch 16/160 (LR 0.09755) => LSC_loss 0.97, Spatial_loss 1.53, Flat_loss 0.12, Train_acc 70.94, Test_acc 55.00
2025-01-05 12:21:58,523 [podnet.py] => Task 3, Epoch 17/160 (LR 0.09724) => LSC_loss 0.95, Spatial_loss 1.47, Flat_loss 0.12, Train_acc 71.97, Test_acc 55.40
2025-01-05 12:22:04,073 [podnet.py] => Task 3, Epoch 18/160 (LR 0.09691) => LSC_loss 0.97, Spatial_loss 1.51, Flat_loss 0.13, Train_acc 71.43, Test_acc 59.05
2025-01-05 12:22:09,847 [podnet.py] => Task 3, Epoch 19/160 (LR 0.09656) => LSC_loss 0.96, Spatial_loss 1.54, Flat_loss 0.13, Train_acc 71.45, Test_acc 55.20
2025-01-05 12:22:15,475 [podnet.py] => Task 3, Epoch 20/160 (LR 0.09619) => LSC_loss 0.92, Spatial_loss 1.48, Flat_loss 0.12, Train_acc 73.45, Test_acc 49.60
2025-01-05 12:22:21,097 [podnet.py] => Task 3, Epoch 21/160 (LR 0.09581) => LSC_loss 0.93, Spatial_loss 1.51, Flat_loss 0.13, Train_acc 72.31, Test_acc 57.25
2025-01-05 12:22:26,798 [podnet.py] => Task 3, Epoch 22/160 (LR 0.09541) => LSC_loss 0.96, Spatial_loss 1.54, Flat_loss 0.13, Train_acc 72.36, Test_acc 54.90
2025-01-05 12:22:32,490 [podnet.py] => Task 3, Epoch 23/160 (LR 0.09499) => LSC_loss 0.95, Spatial_loss 1.53, Flat_loss 0.13, Train_acc 72.44, Test_acc 59.45
2025-01-05 12:22:38,154 [podnet.py] => Task 3, Epoch 24/160 (LR 0.09455) => LSC_loss 0.93, Spatial_loss 1.47, Flat_loss 0.12, Train_acc 72.86, Test_acc 52.50
2025-01-05 12:22:43,992 [podnet.py] => Task 3, Epoch 25/160 (LR 0.09410) => LSC_loss 0.94, Spatial_loss 1.51, Flat_loss 0.13, Train_acc 72.73, Test_acc 55.00
2025-01-05 12:22:49,687 [podnet.py] => Task 3, Epoch 26/160 (LR 0.09362) => LSC_loss 0.91, Spatial_loss 1.48, Flat_loss 0.13, Train_acc 73.95, Test_acc 54.85
2025-01-05 12:22:55,291 [podnet.py] => Task 3, Epoch 27/160 (LR 0.09314) => LSC_loss 0.91, Spatial_loss 1.52, Flat_loss 0.13, Train_acc 72.85, Test_acc 59.65
2025-01-05 12:23:00,947 [podnet.py] => Task 3, Epoch 28/160 (LR 0.09263) => LSC_loss 0.89, Spatial_loss 1.45, Flat_loss 0.12, Train_acc 74.13, Test_acc 57.50
2025-01-05 12:23:06,569 [podnet.py] => Task 3, Epoch 29/160 (LR 0.09211) => LSC_loss 0.90, Spatial_loss 1.51, Flat_loss 0.13, Train_acc 73.91, Test_acc 58.00
2025-01-05 12:23:12,227 [podnet.py] => Task 3, Epoch 30/160 (LR 0.09157) => LSC_loss 0.91, Spatial_loss 1.58, Flat_loss 0.14, Train_acc 73.69, Test_acc 55.30
2025-01-05 12:23:17,811 [podnet.py] => Task 3, Epoch 31/160 (LR 0.09102) => LSC_loss 0.96, Spatial_loss 1.56, Flat_loss 0.14, Train_acc 72.34, Test_acc 60.75
2025-01-05 12:23:23,161 [podnet.py] => Task 3, Epoch 32/160 (LR 0.09045) => LSC_loss 0.87, Spatial_loss 1.44, Flat_loss 0.12, Train_acc 75.29, Test_acc 55.40
2025-01-05 12:23:28,885 [podnet.py] => Task 3, Epoch 33/160 (LR 0.08987) => LSC_loss 0.87, Spatial_loss 1.48, Flat_loss 0.13, Train_acc 75.11, Test_acc 50.75
2025-01-05 12:23:34,498 [podnet.py] => Task 3, Epoch 34/160 (LR 0.08927) => LSC_loss 0.87, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 74.89, Test_acc 54.35
2025-01-05 12:23:40,000 [podnet.py] => Task 3, Epoch 35/160 (LR 0.08865) => LSC_loss 0.87, Spatial_loss 1.44, Flat_loss 0.12, Train_acc 74.69, Test_acc 54.85
2025-01-05 12:23:45,387 [podnet.py] => Task 3, Epoch 36/160 (LR 0.08802) => LSC_loss 0.87, Spatial_loss 1.50, Flat_loss 0.13, Train_acc 74.65, Test_acc 52.45
2025-01-05 12:23:50,927 [podnet.py] => Task 3, Epoch 37/160 (LR 0.08738) => LSC_loss 0.87, Spatial_loss 1.49, Flat_loss 0.13, Train_acc 75.21, Test_acc 58.95
2025-01-05 12:23:56,449 [podnet.py] => Task 3, Epoch 38/160 (LR 0.08672) => LSC_loss 0.83, Spatial_loss 1.44, Flat_loss 0.12, Train_acc 75.93, Test_acc 56.50
2025-01-05 12:24:02,078 [podnet.py] => Task 3, Epoch 39/160 (LR 0.08604) => LSC_loss 0.87, Spatial_loss 1.49, Flat_loss 0.13, Train_acc 75.01, Test_acc 58.85
2025-01-05 12:24:07,744 [podnet.py] => Task 3, Epoch 40/160 (LR 0.08536) => LSC_loss 0.88, Spatial_loss 1.52, Flat_loss 0.14, Train_acc 74.33, Test_acc 54.00
2025-01-05 12:24:13,328 [podnet.py] => Task 3, Epoch 41/160 (LR 0.08465) => LSC_loss 0.84, Spatial_loss 1.50, Flat_loss 0.13, Train_acc 75.22, Test_acc 56.55
2025-01-05 12:24:18,845 [podnet.py] => Task 3, Epoch 42/160 (LR 0.08394) => LSC_loss 0.83, Spatial_loss 1.47, Flat_loss 0.12, Train_acc 76.23, Test_acc 56.15
2025-01-05 12:24:24,603 [podnet.py] => Task 3, Epoch 43/160 (LR 0.08321) => LSC_loss 0.84, Spatial_loss 1.46, Flat_loss 0.13, Train_acc 75.59, Test_acc 59.15
2025-01-05 12:24:30,193 [podnet.py] => Task 3, Epoch 44/160 (LR 0.08247) => LSC_loss 0.81, Spatial_loss 1.42, Flat_loss 0.12, Train_acc 76.34, Test_acc 58.65
2025-01-05 12:24:35,591 [podnet.py] => Task 3, Epoch 45/160 (LR 0.08172) => LSC_loss 0.79, Spatial_loss 1.45, Flat_loss 0.13, Train_acc 77.07, Test_acc 57.80
2025-01-05 12:24:41,091 [podnet.py] => Task 3, Epoch 46/160 (LR 0.08095) => LSC_loss 0.82, Spatial_loss 1.47, Flat_loss 0.13, Train_acc 76.42, Test_acc 55.50
2025-01-05 12:24:46,766 [podnet.py] => Task 3, Epoch 47/160 (LR 0.08018) => LSC_loss 0.80, Spatial_loss 1.45, Flat_loss 0.13, Train_acc 76.95, Test_acc 52.70
2025-01-05 12:24:52,521 [podnet.py] => Task 3, Epoch 48/160 (LR 0.07939) => LSC_loss 0.80, Spatial_loss 1.45, Flat_loss 0.12, Train_acc 76.97, Test_acc 57.65
2025-01-05 12:24:58,195 [podnet.py] => Task 3, Epoch 49/160 (LR 0.07859) => LSC_loss 0.79, Spatial_loss 1.43, Flat_loss 0.13, Train_acc 77.16, Test_acc 59.50
2025-01-05 12:25:03,882 [podnet.py] => Task 3, Epoch 50/160 (LR 0.07778) => LSC_loss 0.81, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 76.41, Test_acc 58.40
2025-01-05 12:25:09,799 [podnet.py] => Task 3, Epoch 51/160 (LR 0.07696) => LSC_loss 0.85, Spatial_loss 1.51, Flat_loss 0.14, Train_acc 75.59, Test_acc 55.55
2025-01-05 12:25:15,787 [podnet.py] => Task 3, Epoch 52/160 (LR 0.07612) => LSC_loss 0.80, Spatial_loss 1.45, Flat_loss 0.13, Train_acc 77.11, Test_acc 56.10
2025-01-05 12:25:21,953 [podnet.py] => Task 3, Epoch 53/160 (LR 0.07528) => LSC_loss 0.76, Spatial_loss 1.41, Flat_loss 0.12, Train_acc 78.16, Test_acc 60.80
2025-01-05 12:25:27,906 [podnet.py] => Task 3, Epoch 54/160 (LR 0.07443) => LSC_loss 0.76, Spatial_loss 1.41, Flat_loss 0.12, Train_acc 78.57, Test_acc 57.95
2025-01-05 12:25:33,943 [podnet.py] => Task 3, Epoch 55/160 (LR 0.07357) => LSC_loss 0.79, Spatial_loss 1.43, Flat_loss 0.13, Train_acc 77.24, Test_acc 54.05
2025-01-05 12:25:39,931 [podnet.py] => Task 3, Epoch 56/160 (LR 0.07270) => LSC_loss 0.76, Spatial_loss 1.41, Flat_loss 0.12, Train_acc 78.28, Test_acc 59.20
2025-01-05 12:25:45,877 [podnet.py] => Task 3, Epoch 57/160 (LR 0.07182) => LSC_loss 0.73, Spatial_loss 1.37, Flat_loss 0.12, Train_acc 78.91, Test_acc 60.20
2025-01-05 12:25:51,543 [podnet.py] => Task 3, Epoch 58/160 (LR 0.07093) => LSC_loss 0.74, Spatial_loss 1.38, Flat_loss 0.12, Train_acc 78.36, Test_acc 59.35
2025-01-05 12:25:57,252 [podnet.py] => Task 3, Epoch 59/160 (LR 0.07004) => LSC_loss 0.74, Spatial_loss 1.40, Flat_loss 0.12, Train_acc 79.01, Test_acc 56.90
2025-01-05 12:26:03,015 [podnet.py] => Task 3, Epoch 60/160 (LR 0.06913) => LSC_loss 0.77, Spatial_loss 1.44, Flat_loss 0.13, Train_acc 77.87, Test_acc 61.30
2025-01-05 12:26:08,675 [podnet.py] => Task 3, Epoch 61/160 (LR 0.06822) => LSC_loss 0.71, Spatial_loss 1.39, Flat_loss 0.12, Train_acc 79.87, Test_acc 55.00
2025-01-05 12:26:14,637 [podnet.py] => Task 3, Epoch 62/160 (LR 0.06731) => LSC_loss 0.77, Spatial_loss 1.50, Flat_loss 0.14, Train_acc 78.26, Test_acc 58.45
2025-01-05 12:26:20,153 [podnet.py] => Task 3, Epoch 63/160 (LR 0.06638) => LSC_loss 0.75, Spatial_loss 1.40, Flat_loss 0.13, Train_acc 78.22, Test_acc 60.00
2025-01-05 12:26:25,677 [podnet.py] => Task 3, Epoch 64/160 (LR 0.06545) => LSC_loss 0.70, Spatial_loss 1.35, Flat_loss 0.12, Train_acc 80.34, Test_acc 58.05
2025-01-05 12:26:31,401 [podnet.py] => Task 3, Epoch 65/160 (LR 0.06451) => LSC_loss 0.71, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 79.90, Test_acc 63.65
2025-01-05 12:26:36,911 [podnet.py] => Task 3, Epoch 66/160 (LR 0.06357) => LSC_loss 0.71, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 79.35, Test_acc 59.90
2025-01-05 12:26:42,385 [podnet.py] => Task 3, Epoch 67/160 (LR 0.06262) => LSC_loss 0.75, Spatial_loss 1.42, Flat_loss 0.13, Train_acc 78.61, Test_acc 60.25
2025-01-05 12:26:47,919 [podnet.py] => Task 3, Epoch 68/160 (LR 0.06167) => LSC_loss 0.71, Spatial_loss 1.39, Flat_loss 0.12, Train_acc 79.52, Test_acc 57.60
2025-01-05 12:26:53,673 [podnet.py] => Task 3, Epoch 69/160 (LR 0.06072) => LSC_loss 0.71, Spatial_loss 1.41, Flat_loss 0.13, Train_acc 79.64, Test_acc 58.35
2025-01-05 12:26:59,363 [podnet.py] => Task 3, Epoch 70/160 (LR 0.05975) => LSC_loss 0.68, Spatial_loss 1.32, Flat_loss 0.12, Train_acc 80.61, Test_acc 63.65
2025-01-05 12:27:04,776 [podnet.py] => Task 3, Epoch 71/160 (LR 0.05879) => LSC_loss 0.65, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 81.15, Test_acc 57.45
2025-01-05 12:27:10,629 [podnet.py] => Task 3, Epoch 72/160 (LR 0.05782) => LSC_loss 0.71, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 79.78, Test_acc 59.90
2025-01-05 12:27:16,140 [podnet.py] => Task 3, Epoch 73/160 (LR 0.05685) => LSC_loss 0.67, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 80.67, Test_acc 58.85
2025-01-05 12:27:21,581 [podnet.py] => Task 3, Epoch 74/160 (LR 0.05588) => LSC_loss 0.66, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 81.16, Test_acc 60.10
2025-01-05 12:27:27,051 [podnet.py] => Task 3, Epoch 75/160 (LR 0.05490) => LSC_loss 0.69, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 80.08, Test_acc 62.85
2025-01-05 12:27:32,406 [podnet.py] => Task 3, Epoch 76/160 (LR 0.05392) => LSC_loss 0.66, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 81.21, Test_acc 58.60
2025-01-05 12:27:37,821 [podnet.py] => Task 3, Epoch 77/160 (LR 0.05294) => LSC_loss 0.65, Spatial_loss 1.33, Flat_loss 0.12, Train_acc 81.32, Test_acc 56.80
2025-01-05 12:27:43,325 [podnet.py] => Task 3, Epoch 78/160 (LR 0.05196) => LSC_loss 0.63, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 82.60, Test_acc 57.35
2025-01-05 12:27:48,693 [podnet.py] => Task 3, Epoch 79/160 (LR 0.05098) => LSC_loss 0.66, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 81.26, Test_acc 61.90
2025-01-05 12:27:54,438 [podnet.py] => Task 3, Epoch 80/160 (LR 0.05000) => LSC_loss 0.64, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 82.05, Test_acc 58.35
2025-01-05 12:27:59,877 [podnet.py] => Task 3, Epoch 81/160 (LR 0.04902) => LSC_loss 0.61, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 82.69, Test_acc 62.25
2025-01-05 12:28:05,312 [podnet.py] => Task 3, Epoch 82/160 (LR 0.04804) => LSC_loss 0.61, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 82.65, Test_acc 61.75
2025-01-05 12:28:10,662 [podnet.py] => Task 3, Epoch 83/160 (LR 0.04706) => LSC_loss 0.61, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 82.66, Test_acc 60.30
2025-01-05 12:28:16,113 [podnet.py] => Task 3, Epoch 84/160 (LR 0.04608) => LSC_loss 0.62, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 82.69, Test_acc 61.35
2025-01-05 12:28:21,745 [podnet.py] => Task 3, Epoch 85/160 (LR 0.04510) => LSC_loss 0.60, Spatial_loss 1.26, Flat_loss 0.11, Train_acc 83.26, Test_acc 62.75
2025-01-05 12:28:27,505 [podnet.py] => Task 3, Epoch 86/160 (LR 0.04412) => LSC_loss 0.63, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 82.19, Test_acc 62.05
2025-01-05 12:28:33,117 [podnet.py] => Task 3, Epoch 87/160 (LR 0.04315) => LSC_loss 0.57, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 83.91, Test_acc 62.30
2025-01-05 12:28:38,776 [podnet.py] => Task 3, Epoch 88/160 (LR 0.04218) => LSC_loss 0.57, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 84.05, Test_acc 57.20
2025-01-05 12:28:44,134 [podnet.py] => Task 3, Epoch 89/160 (LR 0.04121) => LSC_loss 0.65, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 81.65, Test_acc 62.00
2025-01-05 12:28:49,543 [podnet.py] => Task 3, Epoch 90/160 (LR 0.04025) => LSC_loss 0.59, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 83.07, Test_acc 62.15
2025-01-05 12:28:55,063 [podnet.py] => Task 3, Epoch 91/160 (LR 0.03928) => LSC_loss 0.56, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 84.77, Test_acc 63.45
2025-01-05 12:29:00,585 [podnet.py] => Task 3, Epoch 92/160 (LR 0.03833) => LSC_loss 0.58, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 83.57, Test_acc 64.20
2025-01-05 12:29:06,377 [podnet.py] => Task 3, Epoch 93/160 (LR 0.03738) => LSC_loss 0.58, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 84.25, Test_acc 60.20
2025-01-05 12:29:11,763 [podnet.py] => Task 3, Epoch 94/160 (LR 0.03643) => LSC_loss 0.60, Spatial_loss 1.30, Flat_loss 0.13, Train_acc 83.25, Test_acc 61.50
2025-01-05 12:29:17,291 [podnet.py] => Task 3, Epoch 95/160 (LR 0.03549) => LSC_loss 0.57, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 83.88, Test_acc 59.60
2025-01-05 12:29:22,889 [podnet.py] => Task 3, Epoch 96/160 (LR 0.03455) => LSC_loss 0.52, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 85.20, Test_acc 62.65
2025-01-05 12:29:28,463 [podnet.py] => Task 3, Epoch 97/160 (LR 0.03362) => LSC_loss 0.49, Spatial_loss 1.14, Flat_loss 0.10, Train_acc 86.56, Test_acc 65.40
2025-01-05 12:29:34,015 [podnet.py] => Task 3, Epoch 98/160 (LR 0.03269) => LSC_loss 0.51, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 85.94, Test_acc 62.45
2025-01-05 12:29:39,487 [podnet.py] => Task 3, Epoch 99/160 (LR 0.03178) => LSC_loss 0.49, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 86.06, Test_acc 62.95
2025-01-05 12:29:45,099 [podnet.py] => Task 3, Epoch 100/160 (LR 0.03087) => LSC_loss 0.51, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 85.63, Test_acc 64.80
2025-01-05 12:29:50,551 [podnet.py] => Task 3, Epoch 101/160 (LR 0.02996) => LSC_loss 0.47, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 87.26, Test_acc 64.90
2025-01-05 12:29:56,044 [podnet.py] => Task 3, Epoch 102/160 (LR 0.02907) => LSC_loss 0.46, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 87.64, Test_acc 65.20
2025-01-05 12:30:01,386 [podnet.py] => Task 3, Epoch 103/160 (LR 0.02818) => LSC_loss 0.48, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 86.29, Test_acc 60.35
2025-01-05 12:30:06,822 [podnet.py] => Task 3, Epoch 104/160 (LR 0.02730) => LSC_loss 0.45, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 87.84, Test_acc 63.55
2025-01-05 12:30:12,308 [podnet.py] => Task 3, Epoch 105/160 (LR 0.02643) => LSC_loss 0.46, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 87.27, Test_acc 65.00
2025-01-05 12:30:17,482 [podnet.py] => Task 3, Epoch 106/160 (LR 0.02557) => LSC_loss 0.44, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 87.88, Test_acc 64.20
2025-01-05 12:30:23,010 [podnet.py] => Task 3, Epoch 107/160 (LR 0.02472) => LSC_loss 0.41, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 89.03, Test_acc 66.10
2025-01-05 12:30:28,408 [podnet.py] => Task 3, Epoch 108/160 (LR 0.02388) => LSC_loss 0.43, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 88.18, Test_acc 64.65
2025-01-05 12:30:33,943 [podnet.py] => Task 3, Epoch 109/160 (LR 0.02304) => LSC_loss 0.43, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 88.67, Test_acc 64.00
2025-01-05 12:30:39,599 [podnet.py] => Task 3, Epoch 110/160 (LR 0.02222) => LSC_loss 0.45, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 87.58, Test_acc 64.15
2025-01-05 12:30:45,024 [podnet.py] => Task 3, Epoch 111/160 (LR 0.02141) => LSC_loss 0.41, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 88.88, Test_acc 65.75
2025-01-05 12:30:50,824 [podnet.py] => Task 3, Epoch 112/160 (LR 0.02061) => LSC_loss 0.39, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 89.44, Test_acc 65.00
2025-01-05 12:30:56,391 [podnet.py] => Task 3, Epoch 113/160 (LR 0.01982) => LSC_loss 0.40, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 89.94, Test_acc 64.15
2025-01-05 12:31:01,810 [podnet.py] => Task 3, Epoch 114/160 (LR 0.01905) => LSC_loss 0.41, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 89.23, Test_acc 64.55
2025-01-05 12:31:07,502 [podnet.py] => Task 3, Epoch 115/160 (LR 0.01828) => LSC_loss 0.39, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 89.51, Test_acc 66.10
2025-01-05 12:31:12,944 [podnet.py] => Task 3, Epoch 116/160 (LR 0.01753) => LSC_loss 0.37, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 90.58, Test_acc 64.75
2025-01-05 12:31:18,581 [podnet.py] => Task 3, Epoch 117/160 (LR 0.01679) => LSC_loss 0.37, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 90.27, Test_acc 64.00
2025-01-05 12:31:24,089 [podnet.py] => Task 3, Epoch 118/160 (LR 0.01606) => LSC_loss 0.39, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 89.78, Test_acc 65.00
2025-01-05 12:31:29,669 [podnet.py] => Task 3, Epoch 119/160 (LR 0.01535) => LSC_loss 0.36, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 90.93, Test_acc 66.35
2025-01-05 12:31:35,181 [podnet.py] => Task 3, Epoch 120/160 (LR 0.01464) => LSC_loss 0.36, Spatial_loss 0.97, Flat_loss 0.10, Train_acc 90.81, Test_acc 65.05
2025-01-05 12:31:40,743 [podnet.py] => Task 3, Epoch 121/160 (LR 0.01396) => LSC_loss 0.38, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 90.01, Test_acc 66.70
2025-01-05 12:31:46,236 [podnet.py] => Task 3, Epoch 122/160 (LR 0.01328) => LSC_loss 0.34, Spatial_loss 0.95, Flat_loss 0.09, Train_acc 91.45, Test_acc 63.95
2025-01-05 12:31:51,719 [podnet.py] => Task 3, Epoch 123/160 (LR 0.01262) => LSC_loss 0.35, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 90.94, Test_acc 67.40
2025-01-05 12:31:57,203 [podnet.py] => Task 3, Epoch 124/160 (LR 0.01198) => LSC_loss 0.32, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 91.96, Test_acc 67.40
2025-01-05 12:32:02,717 [podnet.py] => Task 3, Epoch 125/160 (LR 0.01135) => LSC_loss 0.31, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 92.20, Test_acc 67.20
2025-01-05 12:32:08,466 [podnet.py] => Task 3, Epoch 126/160 (LR 0.01073) => LSC_loss 0.31, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 92.31, Test_acc 66.10
2025-01-05 12:32:13,984 [podnet.py] => Task 3, Epoch 127/160 (LR 0.01013) => LSC_loss 0.31, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 92.85, Test_acc 66.10
2025-01-05 12:32:19,469 [podnet.py] => Task 3, Epoch 128/160 (LR 0.00955) => LSC_loss 0.31, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 92.73, Test_acc 66.00
2025-01-05 12:32:25,001 [podnet.py] => Task 3, Epoch 129/160 (LR 0.00898) => LSC_loss 0.30, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 92.37, Test_acc 66.45
2025-01-05 12:32:30,410 [podnet.py] => Task 3, Epoch 130/160 (LR 0.00843) => LSC_loss 0.30, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 92.68, Test_acc 67.90
2025-01-05 12:32:35,937 [podnet.py] => Task 3, Epoch 131/160 (LR 0.00789) => LSC_loss 0.27, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 93.42, Test_acc 66.65
2025-01-05 12:32:41,520 [podnet.py] => Task 3, Epoch 132/160 (LR 0.00737) => LSC_loss 0.28, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 93.29, Test_acc 66.65
2025-01-05 12:32:46,995 [podnet.py] => Task 3, Epoch 133/160 (LR 0.00686) => LSC_loss 0.28, Spatial_loss 0.86, Flat_loss 0.09, Train_acc 93.18, Test_acc 67.50
2025-01-05 12:32:52,640 [podnet.py] => Task 3, Epoch 134/160 (LR 0.00638) => LSC_loss 0.28, Spatial_loss 0.86, Flat_loss 0.09, Train_acc 93.45, Test_acc 67.15
2025-01-05 12:32:58,478 [podnet.py] => Task 3, Epoch 135/160 (LR 0.00590) => LSC_loss 0.27, Spatial_loss 0.86, Flat_loss 0.09, Train_acc 93.64, Test_acc 67.30
2025-01-05 12:33:04,715 [podnet.py] => Task 3, Epoch 136/160 (LR 0.00545) => LSC_loss 0.26, Spatial_loss 0.85, Flat_loss 0.09, Train_acc 93.89, Test_acc 67.30
2025-01-05 12:33:11,175 [podnet.py] => Task 3, Epoch 137/160 (LR 0.00501) => LSC_loss 0.27, Spatial_loss 0.83, Flat_loss 0.09, Train_acc 94.07, Test_acc 66.85
2025-01-05 12:33:17,043 [podnet.py] => Task 3, Epoch 138/160 (LR 0.00459) => LSC_loss 0.25, Spatial_loss 0.83, Flat_loss 0.09, Train_acc 94.39, Test_acc 69.05
2025-01-05 12:33:22,484 [podnet.py] => Task 3, Epoch 139/160 (LR 0.00419) => LSC_loss 0.25, Spatial_loss 0.84, Flat_loss 0.09, Train_acc 94.44, Test_acc 66.75
2025-01-05 12:33:28,049 [podnet.py] => Task 3, Epoch 140/160 (LR 0.00381) => LSC_loss 0.25, Spatial_loss 0.83, Flat_loss 0.09, Train_acc 94.43, Test_acc 66.95
2025-01-05 12:33:33,621 [podnet.py] => Task 3, Epoch 141/160 (LR 0.00344) => LSC_loss 0.24, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 94.62, Test_acc 67.10
2025-01-05 12:33:39,031 [podnet.py] => Task 3, Epoch 142/160 (LR 0.00309) => LSC_loss 0.23, Spatial_loss 0.82, Flat_loss 0.09, Train_acc 94.62, Test_acc 66.65
2025-01-05 12:33:44,502 [podnet.py] => Task 3, Epoch 143/160 (LR 0.00276) => LSC_loss 0.22, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 95.25, Test_acc 67.10
2025-01-05 12:33:50,132 [podnet.py] => Task 3, Epoch 144/160 (LR 0.00245) => LSC_loss 0.22, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 95.42, Test_acc 67.20
2025-01-05 12:33:55,529 [podnet.py] => Task 3, Epoch 145/160 (LR 0.00215) => LSC_loss 0.23, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 94.81, Test_acc 66.85
2025-01-05 12:34:01,017 [podnet.py] => Task 3, Epoch 146/160 (LR 0.00188) => LSC_loss 0.23, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.17, Test_acc 67.35
2025-01-05 12:34:06,593 [podnet.py] => Task 3, Epoch 147/160 (LR 0.00162) => LSC_loss 0.22, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 95.39, Test_acc 67.45
2025-01-05 12:34:12,205 [podnet.py] => Task 3, Epoch 148/160 (LR 0.00138) => LSC_loss 0.21, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.62, Test_acc 68.00
2025-01-05 12:34:17,668 [podnet.py] => Task 3, Epoch 149/160 (LR 0.00116) => LSC_loss 0.22, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.35, Test_acc 67.50
2025-01-05 12:34:23,169 [podnet.py] => Task 3, Epoch 150/160 (LR 0.00096) => LSC_loss 0.22, Spatial_loss 0.74, Flat_loss 0.08, Train_acc 95.44, Test_acc 66.95
2025-01-05 12:34:28,857 [podnet.py] => Task 3, Epoch 151/160 (LR 0.00078) => LSC_loss 0.22, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.42, Test_acc 67.90
2025-01-05 12:34:34,363 [podnet.py] => Task 3, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.31, Test_acc 67.95
2025-01-05 12:34:39,918 [podnet.py] => Task 3, Epoch 153/160 (LR 0.00047) => LSC_loss 0.21, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.68, Test_acc 67.35
2025-01-05 12:34:45,556 [podnet.py] => Task 3, Epoch 154/160 (LR 0.00035) => LSC_loss 0.22, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.28, Test_acc 68.30
2025-01-05 12:34:51,297 [podnet.py] => Task 3, Epoch 155/160 (LR 0.00024) => LSC_loss 0.21, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.68, Test_acc 67.90
2025-01-05 12:34:56,770 [podnet.py] => Task 3, Epoch 156/160 (LR 0.00015) => LSC_loss 0.22, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.40, Test_acc 67.25
2025-01-05 12:35:02,615 [podnet.py] => Task 3, Epoch 157/160 (LR 0.00009) => LSC_loss 0.21, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.66, Test_acc 67.80
2025-01-05 12:35:07,979 [podnet.py] => Task 3, Epoch 158/160 (LR 0.00004) => LSC_loss 0.21, Spatial_loss 0.75, Flat_loss 0.08, Train_acc 95.47, Test_acc 67.35
2025-01-05 12:35:13,642 [podnet.py] => Task 3, Epoch 159/160 (LR 0.00001) => LSC_loss 0.21, Spatial_loss 0.75, Flat_loss 0.08, Train_acc 95.49, Test_acc 67.65
2025-01-05 12:35:19,301 [podnet.py] => Task 3, Epoch 160/160 (LR 0.00000) => LSC_loss 0.21, Spatial_loss 0.75, Flat_loss 0.08, Train_acc 95.61, Test_acc 68.00
2025-01-05 12:35:19,301 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 12:35:19,302 [base.py] => Reducing exemplars...(898 per classes)
2025-01-05 12:35:31,709 [base.py] => Constructing exemplars...(898 per classes)
2025-01-05 12:35:40,486 [podnet.py] => The size of finetune dataset: 10000
2025-01-05 12:35:45,951 [podnet.py] => Task 3, Epoch 1/20 (LR 0.00497) => LSC_loss 0.23, Spatial_loss 0.82, Flat_loss 0.08, Train_acc 95.08, Test_acc 66.85
2025-01-05 12:35:51,597 [podnet.py] => Task 3, Epoch 2/20 (LR 0.00488) => LSC_loss 0.23, Spatial_loss 0.82, Flat_loss 0.09, Train_acc 94.95, Test_acc 66.00
2025-01-05 12:35:57,181 [podnet.py] => Task 3, Epoch 3/20 (LR 0.00473) => LSC_loss 0.23, Spatial_loss 0.81, Flat_loss 0.09, Train_acc 94.83, Test_acc 66.70
2025-01-05 12:36:02,826 [podnet.py] => Task 3, Epoch 4/20 (LR 0.00452) => LSC_loss 0.25, Spatial_loss 0.82, Flat_loss 0.09, Train_acc 94.23, Test_acc 67.45
2025-01-05 12:36:08,542 [podnet.py] => Task 3, Epoch 5/20 (LR 0.00427) => LSC_loss 0.23, Spatial_loss 0.82, Flat_loss 0.09, Train_acc 94.50, Test_acc 67.25
2025-01-05 12:36:13,783 [podnet.py] => Task 3, Epoch 6/20 (LR 0.00397) => LSC_loss 0.24, Spatial_loss 0.81, Flat_loss 0.09, Train_acc 94.57, Test_acc 67.55
2025-01-05 12:36:19,363 [podnet.py] => Task 3, Epoch 7/20 (LR 0.00363) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.09, Train_acc 94.92, Test_acc 66.75
2025-01-05 12:36:24,595 [podnet.py] => Task 3, Epoch 8/20 (LR 0.00327) => LSC_loss 0.23, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 95.03, Test_acc 66.85
2025-01-05 12:36:29,955 [podnet.py] => Task 3, Epoch 9/20 (LR 0.00289) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 95.14, Test_acc 67.80
2025-01-05 12:36:35,524 [podnet.py] => Task 3, Epoch 10/20 (LR 0.00250) => LSC_loss 0.23, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.04, Test_acc 67.10
2025-01-05 12:36:41,088 [podnet.py] => Task 3, Epoch 11/20 (LR 0.00211) => LSC_loss 0.22, Spatial_loss 0.80, Flat_loss 0.09, Train_acc 95.29, Test_acc 67.95
2025-01-05 12:36:46,570 [podnet.py] => Task 3, Epoch 12/20 (LR 0.00173) => LSC_loss 0.22, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.53, Test_acc 67.65
2025-01-05 12:36:52,046 [podnet.py] => Task 3, Epoch 13/20 (LR 0.00137) => LSC_loss 0.21, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.58, Test_acc 68.00
2025-01-05 12:36:57,504 [podnet.py] => Task 3, Epoch 14/20 (LR 0.00103) => LSC_loss 0.21, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.52, Test_acc 68.20
2025-01-05 12:37:02,779 [podnet.py] => Task 3, Epoch 15/20 (LR 0.00073) => LSC_loss 0.21, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.61, Test_acc 67.60
2025-01-05 12:37:08,218 [podnet.py] => Task 3, Epoch 16/20 (LR 0.00048) => LSC_loss 0.22, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.36, Test_acc 68.25
2025-01-05 12:37:13,633 [podnet.py] => Task 3, Epoch 17/20 (LR 0.00027) => LSC_loss 0.20, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.70, Test_acc 67.90
2025-01-05 12:37:19,354 [podnet.py] => Task 3, Epoch 18/20 (LR 0.00012) => LSC_loss 0.22, Spatial_loss 0.75, Flat_loss 0.08, Train_acc 95.86, Test_acc 67.85
2025-01-05 12:37:24,663 [podnet.py] => Task 3, Epoch 19/20 (LR 0.00003) => LSC_loss 0.21, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.74, Test_acc 68.15
2025-01-05 12:37:30,289 [podnet.py] => Task 3, Epoch 20/20 (LR 0.00000) => LSC_loss 0.20, Spatial_loss 0.75, Flat_loss 0.08, Train_acc 95.86, Test_acc 67.65
2025-01-05 12:37:30,290 [base.py] => Reducing exemplars...(673 per classes)
2025-01-05 12:37:43,423 [base.py] => Constructing exemplars...(673 per classes)
2025-01-05 12:37:55,136 [podnet.py] => Exemplar size: 10000
2025-01-05 12:37:55,136 [trainer.py] => CNN: {'total': np.float64(67.65), '00-09': np.float64(78.1), '10-19': np.float64(57.2), 'old': np.float64(71.2), 'new': np.float64(57.0)}
2025-01-05 12:37:55,136 [trainer.py] => NME: {'total': np.float64(67.15), '00-09': np.float64(77.0), '10-19': np.float64(57.3), 'old': np.float64(69.8), 'new': np.float64(59.2)}
2025-01-05 12:37:55,136 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65)]
2025-01-05 12:37:55,136 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5)]
2025-01-05 12:37:55,136 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15)]
2025-01-05 12:37:55,136 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35)]

2025-01-05 12:37:55,136 [trainer.py] => All params: 479057
2025-01-05 12:37:55,136 [trainer.py] => Trainable params: 479057
2025-01-05 12:37:55,137 [podnet.py] => Learning on 20-25
2025-01-05 12:37:55,165 [podnet.py] => Adaptive factor: 2.23606797749979
2025-01-05 12:38:01,919 [podnet.py] => Task 4, Epoch 1/160 (LR 0.09999) => LSC_loss 1.67, Spatial_loss 2.25, Flat_loss 0.29, Train_acc 55.06, Test_acc 41.80
2025-01-05 12:38:08,266 [podnet.py] => Task 4, Epoch 2/160 (LR 0.09996) => LSC_loss 1.29, Spatial_loss 1.91, Flat_loss 0.19, Train_acc 61.33, Test_acc 42.48
2025-01-05 12:38:14,719 [podnet.py] => Task 4, Epoch 3/160 (LR 0.09991) => LSC_loss 1.23, Spatial_loss 1.85, Flat_loss 0.18, Train_acc 63.30, Test_acc 50.24
2025-01-05 12:38:21,237 [podnet.py] => Task 4, Epoch 4/160 (LR 0.09985) => LSC_loss 1.18, Spatial_loss 1.83, Flat_loss 0.17, Train_acc 64.70, Test_acc 48.00
2025-01-05 12:38:27,904 [podnet.py] => Task 4, Epoch 5/160 (LR 0.09976) => LSC_loss 1.14, Spatial_loss 1.75, Flat_loss 0.16, Train_acc 66.50, Test_acc 47.80
2025-01-05 12:38:34,363 [podnet.py] => Task 4, Epoch 6/160 (LR 0.09965) => LSC_loss 1.12, Spatial_loss 1.72, Flat_loss 0.16, Train_acc 67.31, Test_acc 49.96
2025-01-05 12:38:40,864 [podnet.py] => Task 4, Epoch 7/160 (LR 0.09953) => LSC_loss 1.11, Spatial_loss 1.72, Flat_loss 0.16, Train_acc 66.96, Test_acc 56.08
2025-01-05 12:38:47,533 [podnet.py] => Task 4, Epoch 8/160 (LR 0.09938) => LSC_loss 1.07, Spatial_loss 1.67, Flat_loss 0.16, Train_acc 68.18, Test_acc 51.56
2025-01-05 12:38:54,022 [podnet.py] => Task 4, Epoch 9/160 (LR 0.09922) => LSC_loss 1.07, Spatial_loss 1.70, Flat_loss 0.16, Train_acc 68.34, Test_acc 55.56
2025-01-05 12:39:00,645 [podnet.py] => Task 4, Epoch 10/160 (LR 0.09904) => LSC_loss 1.06, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 68.62, Test_acc 58.80
2025-01-05 12:39:07,431 [podnet.py] => Task 4, Epoch 11/160 (LR 0.09884) => LSC_loss 1.06, Spatial_loss 1.68, Flat_loss 0.16, Train_acc 68.86, Test_acc 50.16
2025-01-05 12:39:14,143 [podnet.py] => Task 4, Epoch 12/160 (LR 0.09862) => LSC_loss 1.02, Spatial_loss 1.65, Flat_loss 0.15, Train_acc 70.22, Test_acc 56.28
2025-01-05 12:39:20,488 [podnet.py] => Task 4, Epoch 13/160 (LR 0.09838) => LSC_loss 1.03, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 69.53, Test_acc 52.36
2025-01-05 12:39:27,159 [podnet.py] => Task 4, Epoch 14/160 (LR 0.09812) => LSC_loss 0.99, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 70.56, Test_acc 45.40
2025-01-05 12:39:33,455 [podnet.py] => Task 4, Epoch 15/160 (LR 0.09785) => LSC_loss 1.00, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 70.98, Test_acc 54.36
2025-01-05 12:39:40,188 [podnet.py] => Task 4, Epoch 16/160 (LR 0.09755) => LSC_loss 1.02, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 69.97, Test_acc 54.56
2025-01-05 12:39:46,711 [podnet.py] => Task 4, Epoch 17/160 (LR 0.09724) => LSC_loss 0.98, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 71.38, Test_acc 57.72
2025-01-05 12:39:53,235 [podnet.py] => Task 4, Epoch 18/160 (LR 0.09691) => LSC_loss 0.98, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 70.95, Test_acc 56.16
2025-01-05 12:39:59,573 [podnet.py] => Task 4, Epoch 19/160 (LR 0.09656) => LSC_loss 0.97, Spatial_loss 1.62, Flat_loss 0.15, Train_acc 71.62, Test_acc 57.72
2025-01-05 12:40:06,133 [podnet.py] => Task 4, Epoch 20/160 (LR 0.09619) => LSC_loss 0.97, Spatial_loss 1.67, Flat_loss 0.16, Train_acc 71.29, Test_acc 54.80
2025-01-05 12:40:12,629 [podnet.py] => Task 4, Epoch 21/160 (LR 0.09581) => LSC_loss 0.95, Spatial_loss 1.63, Flat_loss 0.15, Train_acc 71.70, Test_acc 47.12
2025-01-05 12:40:18,928 [podnet.py] => Task 4, Epoch 22/160 (LR 0.09541) => LSC_loss 0.96, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 71.47, Test_acc 55.36
2025-01-05 12:40:25,569 [podnet.py] => Task 4, Epoch 23/160 (LR 0.09499) => LSC_loss 0.97, Spatial_loss 1.63, Flat_loss 0.16, Train_acc 71.68, Test_acc 57.28
2025-01-05 12:40:32,039 [podnet.py] => Task 4, Epoch 24/160 (LR 0.09455) => LSC_loss 0.96, Spatial_loss 1.60, Flat_loss 0.16, Train_acc 71.93, Test_acc 53.88
2025-01-05 12:40:38,506 [podnet.py] => Task 4, Epoch 25/160 (LR 0.09410) => LSC_loss 0.94, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 72.87, Test_acc 52.92
2025-01-05 12:40:45,145 [podnet.py] => Task 4, Epoch 26/160 (LR 0.09362) => LSC_loss 0.98, Spatial_loss 1.67, Flat_loss 0.16, Train_acc 71.41, Test_acc 55.08
2025-01-05 12:40:51,659 [podnet.py] => Task 4, Epoch 27/160 (LR 0.09314) => LSC_loss 0.95, Spatial_loss 1.63, Flat_loss 0.15, Train_acc 72.42, Test_acc 52.00
2025-01-05 12:40:58,193 [podnet.py] => Task 4, Epoch 28/160 (LR 0.09263) => LSC_loss 0.93, Spatial_loss 1.63, Flat_loss 0.15, Train_acc 73.27, Test_acc 56.96
2025-01-05 12:41:04,967 [podnet.py] => Task 4, Epoch 29/160 (LR 0.09211) => LSC_loss 0.93, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 72.42, Test_acc 53.32
2025-01-05 12:41:11,472 [podnet.py] => Task 4, Epoch 30/160 (LR 0.09157) => LSC_loss 0.93, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 73.02, Test_acc 50.68
2025-01-05 12:41:18,096 [podnet.py] => Task 4, Epoch 31/160 (LR 0.09102) => LSC_loss 0.91, Spatial_loss 1.61, Flat_loss 0.15, Train_acc 73.22, Test_acc 52.96
2025-01-05 12:41:24,847 [podnet.py] => Task 4, Epoch 32/160 (LR 0.09045) => LSC_loss 0.94, Spatial_loss 1.66, Flat_loss 0.16, Train_acc 72.58, Test_acc 53.40
2025-01-05 12:41:31,323 [podnet.py] => Task 4, Epoch 33/160 (LR 0.08987) => LSC_loss 0.93, Spatial_loss 1.62, Flat_loss 0.16, Train_acc 72.55, Test_acc 57.32
2025-01-05 12:41:37,900 [podnet.py] => Task 4, Epoch 34/160 (LR 0.08927) => LSC_loss 0.91, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 73.09, Test_acc 51.20
2025-01-05 12:41:44,396 [podnet.py] => Task 4, Epoch 35/160 (LR 0.08865) => LSC_loss 0.91, Spatial_loss 1.61, Flat_loss 0.16, Train_acc 73.26, Test_acc 55.12
2025-01-05 12:41:50,850 [podnet.py] => Task 4, Epoch 36/160 (LR 0.08802) => LSC_loss 0.91, Spatial_loss 1.64, Flat_loss 0.16, Train_acc 73.44, Test_acc 53.32
2025-01-05 12:41:57,340 [podnet.py] => Task 4, Epoch 37/160 (LR 0.08738) => LSC_loss 0.88, Spatial_loss 1.62, Flat_loss 0.16, Train_acc 74.34, Test_acc 57.28
2025-01-05 12:42:03,595 [podnet.py] => Task 4, Epoch 38/160 (LR 0.08672) => LSC_loss 0.88, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 74.67, Test_acc 58.08
2025-01-05 12:42:10,108 [podnet.py] => Task 4, Epoch 39/160 (LR 0.08604) => LSC_loss 0.88, Spatial_loss 1.61, Flat_loss 0.16, Train_acc 74.46, Test_acc 46.68
2025-01-05 12:42:16,827 [podnet.py] => Task 4, Epoch 40/160 (LR 0.08536) => LSC_loss 0.87, Spatial_loss 1.63, Flat_loss 0.16, Train_acc 74.65, Test_acc 53.72
2025-01-05 12:42:23,162 [podnet.py] => Task 4, Epoch 41/160 (LR 0.08465) => LSC_loss 0.88, Spatial_loss 1.63, Flat_loss 0.16, Train_acc 74.87, Test_acc 47.96
2025-01-05 12:42:29,809 [podnet.py] => Task 4, Epoch 42/160 (LR 0.08394) => LSC_loss 0.86, Spatial_loss 1.57, Flat_loss 0.15, Train_acc 74.74, Test_acc 57.60
2025-01-05 12:42:36,448 [podnet.py] => Task 4, Epoch 43/160 (LR 0.08321) => LSC_loss 0.87, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 75.06, Test_acc 57.24
2025-01-05 12:42:43,058 [podnet.py] => Task 4, Epoch 44/160 (LR 0.08247) => LSC_loss 0.87, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 74.91, Test_acc 52.36
2025-01-05 12:42:49,762 [podnet.py] => Task 4, Epoch 45/160 (LR 0.08172) => LSC_loss 0.85, Spatial_loss 1.60, Flat_loss 0.15, Train_acc 75.34, Test_acc 58.28
2025-01-05 12:42:56,394 [podnet.py] => Task 4, Epoch 46/160 (LR 0.08095) => LSC_loss 0.85, Spatial_loss 1.58, Flat_loss 0.15, Train_acc 75.61, Test_acc 54.00
2025-01-05 12:43:03,009 [podnet.py] => Task 4, Epoch 47/160 (LR 0.08018) => LSC_loss 0.84, Spatial_loss 1.60, Flat_loss 0.15, Train_acc 75.70, Test_acc 55.76
2025-01-05 12:43:09,575 [podnet.py] => Task 4, Epoch 48/160 (LR 0.07939) => LSC_loss 0.87, Spatial_loss 1.60, Flat_loss 0.16, Train_acc 74.70, Test_acc 55.56
2025-01-05 12:43:15,974 [podnet.py] => Task 4, Epoch 49/160 (LR 0.07859) => LSC_loss 0.84, Spatial_loss 1.60, Flat_loss 0.15, Train_acc 75.83, Test_acc 56.32
2025-01-05 12:43:22,300 [podnet.py] => Task 4, Epoch 50/160 (LR 0.07778) => LSC_loss 0.84, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 75.74, Test_acc 58.72
2025-01-05 12:43:28,852 [podnet.py] => Task 4, Epoch 51/160 (LR 0.07696) => LSC_loss 0.85, Spatial_loss 1.59, Flat_loss 0.15, Train_acc 75.23, Test_acc 57.76
2025-01-05 12:43:35,434 [podnet.py] => Task 4, Epoch 52/160 (LR 0.07612) => LSC_loss 0.82, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 76.22, Test_acc 45.12
2025-01-05 12:43:41,842 [podnet.py] => Task 4, Epoch 53/160 (LR 0.07528) => LSC_loss 0.82, Spatial_loss 1.55, Flat_loss 0.15, Train_acc 76.23, Test_acc 56.56
2025-01-05 12:43:48,064 [podnet.py] => Task 4, Epoch 54/160 (LR 0.07443) => LSC_loss 0.83, Spatial_loss 1.56, Flat_loss 0.15, Train_acc 76.24, Test_acc 53.40
2025-01-05 12:43:54,599 [podnet.py] => Task 4, Epoch 55/160 (LR 0.07357) => LSC_loss 0.81, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 76.51, Test_acc 56.96
2025-01-05 12:44:01,323 [podnet.py] => Task 4, Epoch 56/160 (LR 0.07270) => LSC_loss 0.81, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 76.62, Test_acc 57.36
2025-01-05 12:44:07,614 [podnet.py] => Task 4, Epoch 57/160 (LR 0.07182) => LSC_loss 0.80, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 76.80, Test_acc 58.64
2025-01-05 12:44:14,254 [podnet.py] => Task 4, Epoch 58/160 (LR 0.07093) => LSC_loss 0.82, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 76.54, Test_acc 58.16
2025-01-05 12:44:20,602 [podnet.py] => Task 4, Epoch 59/160 (LR 0.07004) => LSC_loss 0.79, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 77.33, Test_acc 56.20
2025-01-05 12:44:27,272 [podnet.py] => Task 4, Epoch 60/160 (LR 0.06913) => LSC_loss 0.81, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 76.66, Test_acc 58.40
2025-01-05 12:44:33,896 [podnet.py] => Task 4, Epoch 61/160 (LR 0.06822) => LSC_loss 0.76, Spatial_loss 1.52, Flat_loss 0.14, Train_acc 78.14, Test_acc 58.76
2025-01-05 12:44:40,277 [podnet.py] => Task 4, Epoch 62/160 (LR 0.06731) => LSC_loss 0.77, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 77.82, Test_acc 58.48
2025-01-05 12:44:46,841 [podnet.py] => Task 4, Epoch 63/160 (LR 0.06638) => LSC_loss 0.79, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 77.47, Test_acc 59.04
2025-01-05 12:44:53,530 [podnet.py] => Task 4, Epoch 64/160 (LR 0.06545) => LSC_loss 0.80, Spatial_loss 1.53, Flat_loss 0.15, Train_acc 77.08, Test_acc 56.84
2025-01-05 12:45:00,019 [podnet.py] => Task 4, Epoch 65/160 (LR 0.06451) => LSC_loss 0.77, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 77.50, Test_acc 57.20
2025-01-05 12:45:06,334 [podnet.py] => Task 4, Epoch 66/160 (LR 0.06357) => LSC_loss 0.75, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 78.58, Test_acc 56.88
2025-01-05 12:45:12,862 [podnet.py] => Task 4, Epoch 67/160 (LR 0.06262) => LSC_loss 0.76, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 78.54, Test_acc 57.12
2025-01-05 12:45:19,342 [podnet.py] => Task 4, Epoch 68/160 (LR 0.06167) => LSC_loss 0.73, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 78.50, Test_acc 59.20
2025-01-05 12:45:26,032 [podnet.py] => Task 4, Epoch 69/160 (LR 0.06072) => LSC_loss 0.76, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 78.11, Test_acc 59.12
2025-01-05 12:45:32,548 [podnet.py] => Task 4, Epoch 70/160 (LR 0.05975) => LSC_loss 0.71, Spatial_loss 1.48, Flat_loss 0.14, Train_acc 79.41, Test_acc 59.28
2025-01-05 12:45:39,343 [podnet.py] => Task 4, Epoch 71/160 (LR 0.05879) => LSC_loss 0.71, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 79.32, Test_acc 58.80
2025-01-05 12:45:45,756 [podnet.py] => Task 4, Epoch 72/160 (LR 0.05782) => LSC_loss 0.71, Spatial_loss 1.44, Flat_loss 0.14, Train_acc 79.59, Test_acc 59.28
2025-01-05 12:45:52,419 [podnet.py] => Task 4, Epoch 73/160 (LR 0.05685) => LSC_loss 0.71, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 79.62, Test_acc 60.44
2025-01-05 12:45:58,726 [podnet.py] => Task 4, Epoch 74/160 (LR 0.05588) => LSC_loss 0.72, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 79.04, Test_acc 58.84
2025-01-05 12:46:05,087 [podnet.py] => Task 4, Epoch 75/160 (LR 0.05490) => LSC_loss 0.68, Spatial_loss 1.42, Flat_loss 0.14, Train_acc 80.16, Test_acc 58.64
2025-01-05 12:46:11,559 [podnet.py] => Task 4, Epoch 76/160 (LR 0.05392) => LSC_loss 0.71, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 79.84, Test_acc 60.64
2025-01-05 12:46:17,824 [podnet.py] => Task 4, Epoch 77/160 (LR 0.05294) => LSC_loss 0.69, Spatial_loss 1.45, Flat_loss 0.14, Train_acc 80.40, Test_acc 57.68
2025-01-05 12:46:24,581 [podnet.py] => Task 4, Epoch 78/160 (LR 0.05196) => LSC_loss 0.67, Spatial_loss 1.43, Flat_loss 0.14, Train_acc 80.75, Test_acc 60.96
2025-01-05 12:46:31,137 [podnet.py] => Task 4, Epoch 79/160 (LR 0.05098) => LSC_loss 0.68, Spatial_loss 1.43, Flat_loss 0.14, Train_acc 80.67, Test_acc 58.88
2025-01-05 12:46:37,740 [podnet.py] => Task 4, Epoch 80/160 (LR 0.05000) => LSC_loss 0.67, Spatial_loss 1.43, Flat_loss 0.14, Train_acc 80.78, Test_acc 61.24
2025-01-05 12:46:44,431 [podnet.py] => Task 4, Epoch 81/160 (LR 0.04902) => LSC_loss 0.67, Spatial_loss 1.44, Flat_loss 0.14, Train_acc 80.77, Test_acc 60.84
2025-01-05 12:46:50,971 [podnet.py] => Task 4, Epoch 82/160 (LR 0.04804) => LSC_loss 0.66, Spatial_loss 1.41, Flat_loss 0.14, Train_acc 80.86, Test_acc 57.40
2025-01-05 12:46:57,325 [podnet.py] => Task 4, Epoch 83/160 (LR 0.04706) => LSC_loss 0.66, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 81.18, Test_acc 61.60
2025-01-05 12:47:04,036 [podnet.py] => Task 4, Epoch 84/160 (LR 0.04608) => LSC_loss 0.63, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 82.36, Test_acc 58.40
2025-01-05 12:47:10,399 [podnet.py] => Task 4, Epoch 85/160 (LR 0.04510) => LSC_loss 0.65, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 81.42, Test_acc 60.52
2025-01-05 12:47:16,910 [podnet.py] => Task 4, Epoch 86/160 (LR 0.04412) => LSC_loss 0.64, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 81.71, Test_acc 60.92
2025-01-05 12:47:23,227 [podnet.py] => Task 4, Epoch 87/160 (LR 0.04315) => LSC_loss 0.61, Spatial_loss 1.39, Flat_loss 0.13, Train_acc 82.86, Test_acc 60.56
2025-01-05 12:47:29,608 [podnet.py] => Task 4, Epoch 88/160 (LR 0.04218) => LSC_loss 0.63, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 82.01, Test_acc 57.72
2025-01-05 12:47:36,175 [podnet.py] => Task 4, Epoch 89/160 (LR 0.04121) => LSC_loss 0.61, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 82.52, Test_acc 53.40
2025-01-05 12:47:42,780 [podnet.py] => Task 4, Epoch 90/160 (LR 0.04025) => LSC_loss 0.60, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 83.13, Test_acc 57.16
2025-01-05 12:47:49,396 [podnet.py] => Task 4, Epoch 91/160 (LR 0.03928) => LSC_loss 0.59, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 83.22, Test_acc 59.92
2025-01-05 12:47:55,773 [podnet.py] => Task 4, Epoch 92/160 (LR 0.03833) => LSC_loss 0.58, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 83.76, Test_acc 61.00
2025-01-05 12:48:02,647 [podnet.py] => Task 4, Epoch 93/160 (LR 0.03738) => LSC_loss 0.58, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 83.98, Test_acc 61.08
2025-01-05 12:48:09,099 [podnet.py] => Task 4, Epoch 94/160 (LR 0.03643) => LSC_loss 0.57, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 83.68, Test_acc 61.08
2025-01-05 12:48:15,361 [podnet.py] => Task 4, Epoch 95/160 (LR 0.03549) => LSC_loss 0.56, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 84.33, Test_acc 60.76
2025-01-05 12:48:21,893 [podnet.py] => Task 4, Epoch 96/160 (LR 0.03455) => LSC_loss 0.55, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 84.10, Test_acc 60.28
2025-01-05 12:48:28,424 [podnet.py] => Task 4, Epoch 97/160 (LR 0.03362) => LSC_loss 0.54, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 84.70, Test_acc 60.24
2025-01-05 12:48:35,130 [podnet.py] => Task 4, Epoch 98/160 (LR 0.03269) => LSC_loss 0.53, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 85.13, Test_acc 61.88
2025-01-05 12:48:41,842 [podnet.py] => Task 4, Epoch 99/160 (LR 0.03178) => LSC_loss 0.52, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 85.79, Test_acc 62.04
2025-01-05 12:48:48,512 [podnet.py] => Task 4, Epoch 100/160 (LR 0.03087) => LSC_loss 0.50, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 86.25, Test_acc 59.92
2025-01-05 12:48:55,162 [podnet.py] => Task 4, Epoch 101/160 (LR 0.02996) => LSC_loss 0.51, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 85.71, Test_acc 62.08
2025-01-05 12:49:01,546 [podnet.py] => Task 4, Epoch 102/160 (LR 0.02907) => LSC_loss 0.52, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 85.32, Test_acc 60.52
2025-01-05 12:49:07,990 [podnet.py] => Task 4, Epoch 103/160 (LR 0.02818) => LSC_loss 0.51, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 85.64, Test_acc 62.76
2025-01-05 12:49:14,646 [podnet.py] => Task 4, Epoch 104/160 (LR 0.02730) => LSC_loss 0.51, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 85.68, Test_acc 62.60
2025-01-05 12:49:21,148 [podnet.py] => Task 4, Epoch 105/160 (LR 0.02643) => LSC_loss 0.47, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 86.70, Test_acc 64.12
2025-01-05 12:49:27,884 [podnet.py] => Task 4, Epoch 106/160 (LR 0.02557) => LSC_loss 0.47, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 86.76, Test_acc 64.16
2025-01-05 12:49:34,550 [podnet.py] => Task 4, Epoch 107/160 (LR 0.02472) => LSC_loss 0.46, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 87.15, Test_acc 61.72
2025-01-05 12:49:41,199 [podnet.py] => Task 4, Epoch 108/160 (LR 0.02388) => LSC_loss 0.46, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 87.36, Test_acc 62.84
2025-01-05 12:49:48,069 [podnet.py] => Task 4, Epoch 109/160 (LR 0.02304) => LSC_loss 0.46, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 87.46, Test_acc 61.68
2025-01-05 12:49:54,575 [podnet.py] => Task 4, Epoch 110/160 (LR 0.02222) => LSC_loss 0.45, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 87.60, Test_acc 64.40
2025-01-05 12:50:01,128 [podnet.py] => Task 4, Epoch 111/160 (LR 0.02141) => LSC_loss 0.44, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 88.18, Test_acc 63.24
2025-01-05 12:50:07,753 [podnet.py] => Task 4, Epoch 112/160 (LR 0.02061) => LSC_loss 0.42, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 88.33, Test_acc 63.76
2025-01-05 12:50:14,403 [podnet.py] => Task 4, Epoch 113/160 (LR 0.01982) => LSC_loss 0.39, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 89.58, Test_acc 63.12
2025-01-05 12:50:21,019 [podnet.py] => Task 4, Epoch 114/160 (LR 0.01905) => LSC_loss 0.40, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 89.38, Test_acc 63.32
2025-01-05 12:50:27,425 [podnet.py] => Task 4, Epoch 115/160 (LR 0.01828) => LSC_loss 0.39, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 89.33, Test_acc 62.76
2025-01-05 12:50:33,863 [podnet.py] => Task 4, Epoch 116/160 (LR 0.01753) => LSC_loss 0.39, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 89.80, Test_acc 64.24
2025-01-05 12:50:40,814 [podnet.py] => Task 4, Epoch 117/160 (LR 0.01679) => LSC_loss 0.38, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 89.61, Test_acc 63.56
2025-01-05 12:50:47,363 [podnet.py] => Task 4, Epoch 118/160 (LR 0.01606) => LSC_loss 0.36, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 90.06, Test_acc 63.52
2025-01-05 12:50:54,135 [podnet.py] => Task 4, Epoch 119/160 (LR 0.01535) => LSC_loss 0.38, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 89.86, Test_acc 63.00
2025-01-05 12:51:00,998 [podnet.py] => Task 4, Epoch 120/160 (LR 0.01464) => LSC_loss 0.37, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 90.65, Test_acc 63.04
2025-01-05 12:51:08,038 [podnet.py] => Task 4, Epoch 121/160 (LR 0.01396) => LSC_loss 0.36, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 90.41, Test_acc 64.60
2025-01-05 12:51:15,174 [podnet.py] => Task 4, Epoch 122/160 (LR 0.01328) => LSC_loss 0.34, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 91.04, Test_acc 63.12
2025-01-05 12:51:21,735 [podnet.py] => Task 4, Epoch 123/160 (LR 0.01262) => LSC_loss 0.33, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 91.46, Test_acc 64.12
2025-01-05 12:51:28,064 [podnet.py] => Task 4, Epoch 124/160 (LR 0.01198) => LSC_loss 0.33, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 91.45, Test_acc 64.32
2025-01-05 12:51:34,701 [podnet.py] => Task 4, Epoch 125/160 (LR 0.01135) => LSC_loss 0.32, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 91.36, Test_acc 64.48
2025-01-05 12:51:41,325 [podnet.py] => Task 4, Epoch 126/160 (LR 0.01073) => LSC_loss 0.31, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 91.91, Test_acc 63.72
2025-01-05 12:51:47,758 [podnet.py] => Task 4, Epoch 127/160 (LR 0.01013) => LSC_loss 0.30, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 92.54, Test_acc 63.96
2025-01-05 12:51:54,237 [podnet.py] => Task 4, Epoch 128/160 (LR 0.00955) => LSC_loss 0.30, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 92.63, Test_acc 63.96
2025-01-05 12:52:00,756 [podnet.py] => Task 4, Epoch 129/160 (LR 0.00898) => LSC_loss 0.30, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 92.54, Test_acc 65.16
2025-01-05 12:52:07,553 [podnet.py] => Task 4, Epoch 130/160 (LR 0.00843) => LSC_loss 0.29, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 92.91, Test_acc 64.76
2025-01-05 12:52:14,441 [podnet.py] => Task 4, Epoch 131/160 (LR 0.00789) => LSC_loss 0.28, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 93.20, Test_acc 65.44
2025-01-05 12:52:21,065 [podnet.py] => Task 4, Epoch 132/160 (LR 0.00737) => LSC_loss 0.28, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 93.15, Test_acc 63.68
2025-01-05 12:52:27,595 [podnet.py] => Task 4, Epoch 133/160 (LR 0.00686) => LSC_loss 0.27, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 93.45, Test_acc 64.16
2025-01-05 12:52:33,990 [podnet.py] => Task 4, Epoch 134/160 (LR 0.00638) => LSC_loss 0.27, Spatial_loss 0.89, Flat_loss 0.09, Train_acc 93.43, Test_acc 65.76
2025-01-05 12:52:40,301 [podnet.py] => Task 4, Epoch 135/160 (LR 0.00590) => LSC_loss 0.26, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 93.43, Test_acc 65.28
2025-01-05 12:52:47,059 [podnet.py] => Task 4, Epoch 136/160 (LR 0.00545) => LSC_loss 0.27, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 93.66, Test_acc 65.04
2025-01-05 12:52:53,893 [podnet.py] => Task 4, Epoch 137/160 (LR 0.00501) => LSC_loss 0.26, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 93.84, Test_acc 65.72
2025-01-05 12:53:00,686 [podnet.py] => Task 4, Epoch 138/160 (LR 0.00459) => LSC_loss 0.26, Spatial_loss 0.88, Flat_loss 0.09, Train_acc 93.81, Test_acc 65.80
2025-01-05 12:53:07,331 [podnet.py] => Task 4, Epoch 139/160 (LR 0.00419) => LSC_loss 0.25, Spatial_loss 0.87, Flat_loss 0.09, Train_acc 94.39, Test_acc 65.68
2025-01-05 12:53:13,718 [podnet.py] => Task 4, Epoch 140/160 (LR 0.00381) => LSC_loss 0.25, Spatial_loss 0.85, Flat_loss 0.08, Train_acc 94.15, Test_acc 65.76
2025-01-05 12:53:20,175 [podnet.py] => Task 4, Epoch 141/160 (LR 0.00344) => LSC_loss 0.24, Spatial_loss 0.83, Flat_loss 0.08, Train_acc 94.70, Test_acc 66.16
2025-01-05 12:53:26,794 [podnet.py] => Task 4, Epoch 142/160 (LR 0.00309) => LSC_loss 0.25, Spatial_loss 0.83, Flat_loss 0.08, Train_acc 94.26, Test_acc 66.40
2025-01-05 12:53:33,605 [podnet.py] => Task 4, Epoch 143/160 (LR 0.00276) => LSC_loss 0.24, Spatial_loss 0.82, Flat_loss 0.08, Train_acc 94.76, Test_acc 65.88
2025-01-05 12:53:40,209 [podnet.py] => Task 4, Epoch 144/160 (LR 0.00245) => LSC_loss 0.23, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 94.88, Test_acc 66.12
2025-01-05 12:53:46,767 [podnet.py] => Task 4, Epoch 145/160 (LR 0.00215) => LSC_loss 0.23, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 95.07, Test_acc 65.76
2025-01-05 12:53:53,445 [podnet.py] => Task 4, Epoch 146/160 (LR 0.00188) => LSC_loss 0.22, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 95.23, Test_acc 66.08
2025-01-05 12:54:00,037 [podnet.py] => Task 4, Epoch 147/160 (LR 0.00162) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 94.95, Test_acc 66.44
2025-01-05 12:54:06,626 [podnet.py] => Task 4, Epoch 148/160 (LR 0.00138) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 94.88, Test_acc 66.88
2025-01-05 12:54:13,304 [podnet.py] => Task 4, Epoch 149/160 (LR 0.00116) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 94.89, Test_acc 65.92
2025-01-05 12:54:19,997 [podnet.py] => Task 4, Epoch 150/160 (LR 0.00096) => LSC_loss 0.23, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 94.98, Test_acc 66.24
2025-01-05 12:54:26,630 [podnet.py] => Task 4, Epoch 151/160 (LR 0.00078) => LSC_loss 0.22, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 95.30, Test_acc 65.60
2025-01-05 12:54:32,926 [podnet.py] => Task 4, Epoch 152/160 (LR 0.00062) => LSC_loss 0.22, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.42, Test_acc 66.12
2025-01-05 12:54:39,532 [podnet.py] => Task 4, Epoch 153/160 (LR 0.00047) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 95.12, Test_acc 66.20
2025-01-05 12:54:45,931 [podnet.py] => Task 4, Epoch 154/160 (LR 0.00035) => LSC_loss 0.22, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.34, Test_acc 66.40
2025-01-05 12:54:52,280 [podnet.py] => Task 4, Epoch 155/160 (LR 0.00024) => LSC_loss 0.22, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.46, Test_acc 66.20
2025-01-05 12:54:58,874 [podnet.py] => Task 4, Epoch 156/160 (LR 0.00015) => LSC_loss 0.22, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.30, Test_acc 66.04
2025-01-05 12:55:05,554 [podnet.py] => Task 4, Epoch 157/160 (LR 0.00009) => LSC_loss 0.22, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.58, Test_acc 65.88
2025-01-05 12:55:12,160 [podnet.py] => Task 4, Epoch 158/160 (LR 0.00004) => LSC_loss 0.22, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.26, Test_acc 66.08
2025-01-05 12:55:18,737 [podnet.py] => Task 4, Epoch 159/160 (LR 0.00001) => LSC_loss 0.22, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.28, Test_acc 66.08
2025-01-05 12:55:25,169 [podnet.py] => Task 4, Epoch 160/160 (LR 0.00000) => LSC_loss 0.21, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.48, Test_acc 66.04
2025-01-05 12:55:25,169 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 12:55:25,169 [base.py] => Reducing exemplars...(673 per classes)
2025-01-05 12:55:42,964 [base.py] => Constructing exemplars...(673 per classes)
2025-01-05 12:55:51,391 [podnet.py] => The size of finetune dataset: 12500
2025-01-05 12:55:57,831 [podnet.py] => Task 4, Epoch 1/20 (LR 0.00497) => LSC_loss 0.24, Spatial_loss 0.84, Flat_loss 0.09, Train_acc 94.47, Test_acc 65.40
2025-01-05 12:56:04,494 [podnet.py] => Task 4, Epoch 2/20 (LR 0.00488) => LSC_loss 0.25, Spatial_loss 0.83, Flat_loss 0.09, Train_acc 94.10, Test_acc 65.76
2025-01-05 12:56:10,836 [podnet.py] => Task 4, Epoch 3/20 (LR 0.00473) => LSC_loss 0.24, Spatial_loss 0.83, Flat_loss 0.08, Train_acc 94.58, Test_acc 64.88
2025-01-05 12:56:17,488 [podnet.py] => Task 4, Epoch 4/20 (LR 0.00452) => LSC_loss 0.25, Spatial_loss 0.84, Flat_loss 0.08, Train_acc 94.04, Test_acc 65.60
2025-01-05 12:56:24,045 [podnet.py] => Task 4, Epoch 5/20 (LR 0.00427) => LSC_loss 0.24, Spatial_loss 0.82, Flat_loss 0.08, Train_acc 94.62, Test_acc 65.52
2025-01-05 12:56:30,472 [podnet.py] => Task 4, Epoch 6/20 (LR 0.00397) => LSC_loss 0.24, Spatial_loss 0.84, Flat_loss 0.08, Train_acc 94.54, Test_acc 65.00
2025-01-05 12:56:37,233 [podnet.py] => Task 4, Epoch 7/20 (LR 0.00363) => LSC_loss 0.24, Spatial_loss 0.84, Flat_loss 0.08, Train_acc 94.48, Test_acc 65.04
2025-01-05 12:56:43,667 [podnet.py] => Task 4, Epoch 8/20 (LR 0.00327) => LSC_loss 0.24, Spatial_loss 0.83, Flat_loss 0.08, Train_acc 94.70, Test_acc 65.96
2025-01-05 12:56:50,296 [podnet.py] => Task 4, Epoch 9/20 (LR 0.00289) => LSC_loss 0.23, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 95.09, Test_acc 65.28
2025-01-05 12:56:56,760 [podnet.py] => Task 4, Epoch 10/20 (LR 0.00250) => LSC_loss 0.23, Spatial_loss 0.82, Flat_loss 0.08, Train_acc 94.82, Test_acc 65.92
2025-01-05 12:57:03,346 [podnet.py] => Task 4, Epoch 11/20 (LR 0.00211) => LSC_loss 0.23, Spatial_loss 0.81, Flat_loss 0.08, Train_acc 95.17, Test_acc 65.72
2025-01-05 12:57:09,846 [podnet.py] => Task 4, Epoch 12/20 (LR 0.00173) => LSC_loss 0.23, Spatial_loss 0.80, Flat_loss 0.08, Train_acc 95.14, Test_acc 65.64
2025-01-05 12:57:16,414 [podnet.py] => Task 4, Epoch 13/20 (LR 0.00137) => LSC_loss 0.22, Spatial_loss 0.79, Flat_loss 0.08, Train_acc 95.12, Test_acc 66.24
2025-01-05 12:57:22,980 [podnet.py] => Task 4, Epoch 14/20 (LR 0.00103) => LSC_loss 0.22, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.20, Test_acc 66.40
2025-01-05 12:57:29,443 [podnet.py] => Task 4, Epoch 15/20 (LR 0.00073) => LSC_loss 0.21, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.51, Test_acc 66.36
2025-01-05 12:57:35,845 [podnet.py] => Task 4, Epoch 16/20 (LR 0.00048) => LSC_loss 0.22, Spatial_loss 0.78, Flat_loss 0.08, Train_acc 95.44, Test_acc 65.92
2025-01-05 12:57:42,496 [podnet.py] => Task 4, Epoch 17/20 (LR 0.00027) => LSC_loss 0.21, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.75, Test_acc 65.96
2025-01-05 12:57:49,236 [podnet.py] => Task 4, Epoch 18/20 (LR 0.00012) => LSC_loss 0.20, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 96.10, Test_acc 66.32
2025-01-05 12:57:55,939 [podnet.py] => Task 4, Epoch 19/20 (LR 0.00003) => LSC_loss 0.20, Spatial_loss 0.76, Flat_loss 0.08, Train_acc 95.86, Test_acc 66.48
2025-01-05 12:58:02,563 [podnet.py] => Task 4, Epoch 20/20 (LR 0.00000) => LSC_loss 0.21, Spatial_loss 0.77, Flat_loss 0.08, Train_acc 95.76, Test_acc 66.32
2025-01-05 12:58:02,566 [base.py] => Reducing exemplars...(539 per classes)
2025-01-05 12:58:19,379 [base.py] => Constructing exemplars...(539 per classes)
2025-01-05 12:58:30,103 [podnet.py] => Exemplar size: 12500
2025-01-05 12:58:30,103 [trainer.py] => CNN: {'total': np.float64(66.32), '00-09': np.float64(76.6), '10-19': np.float64(57.6), '20-29': np.float64(63.2), 'old': np.float64(67.1), 'new': np.float64(63.2)}
2025-01-05 12:58:30,104 [trainer.py] => NME: {'total': np.float64(65.4), '00-09': np.float64(75.2), '10-19': np.float64(56.1), '20-29': np.float64(64.4), 'old': np.float64(65.65), 'new': np.float64(64.4)}
2025-01-05 12:58:30,104 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32)]
2025-01-05 12:58:30,104 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44)]
2025-01-05 12:58:30,104 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4)]
2025-01-05 12:58:30,104 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24)]

2025-01-05 12:58:30,104 [trainer.py] => All params: 482257
2025-01-05 12:58:30,104 [trainer.py] => Trainable params: 482257
2025-01-05 12:58:30,105 [podnet.py] => Learning on 25-30
2025-01-05 12:58:30,143 [podnet.py] => Adaptive factor: 2.449489742783178
2025-01-05 12:58:37,554 [podnet.py] => Task 5, Epoch 1/160 (LR 0.09999) => LSC_loss 1.74, Spatial_loss 2.45, Flat_loss 0.33, Train_acc 54.10, Test_acc 43.53
2025-01-05 12:58:44,970 [podnet.py] => Task 5, Epoch 2/160 (LR 0.09996) => LSC_loss 1.42, Spatial_loss 2.22, Flat_loss 0.25, Train_acc 59.71, Test_acc 43.10
2025-01-05 12:58:52,475 [podnet.py] => Task 5, Epoch 3/160 (LR 0.09991) => LSC_loss 1.31, Spatial_loss 2.04, Flat_loss 0.22, Train_acc 63.20, Test_acc 47.73
2025-01-05 12:58:59,957 [podnet.py] => Task 5, Epoch 4/160 (LR 0.09985) => LSC_loss 1.23, Spatial_loss 1.98, Flat_loss 0.21, Train_acc 65.41, Test_acc 43.33
2025-01-05 12:59:07,638 [podnet.py] => Task 5, Epoch 5/160 (LR 0.09976) => LSC_loss 1.22, Spatial_loss 1.99, Flat_loss 0.21, Train_acc 65.67, Test_acc 53.37
2025-01-05 12:59:15,371 [podnet.py] => Task 5, Epoch 6/160 (LR 0.09965) => LSC_loss 1.20, Spatial_loss 1.92, Flat_loss 0.20, Train_acc 66.20, Test_acc 51.40
2025-01-05 12:59:22,927 [podnet.py] => Task 5, Epoch 7/160 (LR 0.09953) => LSC_loss 1.18, Spatial_loss 1.86, Flat_loss 0.20, Train_acc 66.89, Test_acc 48.97
2025-01-05 12:59:30,555 [podnet.py] => Task 5, Epoch 8/160 (LR 0.09938) => LSC_loss 1.13, Spatial_loss 1.86, Flat_loss 0.19, Train_acc 68.08, Test_acc 51.23
2025-01-05 12:59:37,882 [podnet.py] => Task 5, Epoch 9/160 (LR 0.09922) => LSC_loss 1.11, Spatial_loss 1.83, Flat_loss 0.19, Train_acc 68.56, Test_acc 53.97
2025-01-05 12:59:45,407 [podnet.py] => Task 5, Epoch 10/160 (LR 0.09904) => LSC_loss 1.13, Spatial_loss 1.88, Flat_loss 0.19, Train_acc 68.51, Test_acc 51.93
2025-01-05 12:59:52,867 [podnet.py] => Task 5, Epoch 11/160 (LR 0.09884) => LSC_loss 1.12, Spatial_loss 1.85, Flat_loss 0.19, Train_acc 68.52, Test_acc 43.13
2025-01-05 13:00:00,177 [podnet.py] => Task 5, Epoch 12/160 (LR 0.09862) => LSC_loss 1.11, Spatial_loss 1.84, Flat_loss 0.19, Train_acc 68.74, Test_acc 51.40
2025-01-05 13:00:07,599 [podnet.py] => Task 5, Epoch 13/160 (LR 0.09838) => LSC_loss 1.11, Spatial_loss 1.80, Flat_loss 0.19, Train_acc 68.83, Test_acc 55.37
2025-01-05 13:00:15,278 [podnet.py] => Task 5, Epoch 14/160 (LR 0.09812) => LSC_loss 1.09, Spatial_loss 1.84, Flat_loss 0.19, Train_acc 69.28, Test_acc 50.73
2025-01-05 13:00:23,095 [podnet.py] => Task 5, Epoch 15/160 (LR 0.09785) => LSC_loss 1.09, Spatial_loss 1.82, Flat_loss 0.19, Train_acc 69.54, Test_acc 52.90
2025-01-05 13:00:30,408 [podnet.py] => Task 5, Epoch 16/160 (LR 0.09755) => LSC_loss 1.10, Spatial_loss 1.84, Flat_loss 0.19, Train_acc 68.87, Test_acc 55.00
2025-01-05 13:00:37,901 [podnet.py] => Task 5, Epoch 17/160 (LR 0.09724) => LSC_loss 1.06, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 70.27, Test_acc 47.53
2025-01-05 13:00:45,345 [podnet.py] => Task 5, Epoch 18/160 (LR 0.09691) => LSC_loss 1.06, Spatial_loss 1.82, Flat_loss 0.19, Train_acc 70.46, Test_acc 50.93
2025-01-05 13:00:53,119 [podnet.py] => Task 5, Epoch 19/160 (LR 0.09656) => LSC_loss 1.07, Spatial_loss 1.80, Flat_loss 0.19, Train_acc 70.30, Test_acc 53.53
2025-01-05 13:01:00,736 [podnet.py] => Task 5, Epoch 20/160 (LR 0.09619) => LSC_loss 1.07, Spatial_loss 1.82, Flat_loss 0.19, Train_acc 69.90, Test_acc 57.00
2025-01-05 13:01:08,273 [podnet.py] => Task 5, Epoch 21/160 (LR 0.09581) => LSC_loss 1.06, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 70.26, Test_acc 48.83
2025-01-05 13:01:15,831 [podnet.py] => Task 5, Epoch 22/160 (LR 0.09541) => LSC_loss 1.04, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 70.99, Test_acc 54.43
2025-01-05 13:01:23,491 [podnet.py] => Task 5, Epoch 23/160 (LR 0.09499) => LSC_loss 1.05, Spatial_loss 1.83, Flat_loss 0.19, Train_acc 70.70, Test_acc 50.47
2025-01-05 13:01:30,956 [podnet.py] => Task 5, Epoch 24/160 (LR 0.09455) => LSC_loss 1.05, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 70.54, Test_acc 54.13
2025-01-05 13:01:38,560 [podnet.py] => Task 5, Epoch 25/160 (LR 0.09410) => LSC_loss 1.00, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 71.95, Test_acc 54.70
2025-01-05 13:01:45,998 [podnet.py] => Task 5, Epoch 26/160 (LR 0.09362) => LSC_loss 1.03, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 71.37, Test_acc 52.10
2025-01-05 13:01:53,538 [podnet.py] => Task 5, Epoch 27/160 (LR 0.09314) => LSC_loss 1.06, Spatial_loss 1.83, Flat_loss 0.19, Train_acc 70.50, Test_acc 52.20
2025-01-05 13:02:00,821 [podnet.py] => Task 5, Epoch 28/160 (LR 0.09263) => LSC_loss 1.03, Spatial_loss 1.78, Flat_loss 0.19, Train_acc 71.27, Test_acc 52.33
2025-01-05 13:02:08,525 [podnet.py] => Task 5, Epoch 29/160 (LR 0.09211) => LSC_loss 1.03, Spatial_loss 1.78, Flat_loss 0.19, Train_acc 71.28, Test_acc 54.30
2025-01-05 13:02:16,255 [podnet.py] => Task 5, Epoch 30/160 (LR 0.09157) => LSC_loss 1.02, Spatial_loss 1.76, Flat_loss 0.18, Train_acc 71.63, Test_acc 52.77
2025-01-05 13:02:23,698 [podnet.py] => Task 5, Epoch 31/160 (LR 0.09102) => LSC_loss 1.01, Spatial_loss 1.75, Flat_loss 0.19, Train_acc 71.97, Test_acc 52.63
2025-01-05 13:02:31,161 [podnet.py] => Task 5, Epoch 32/160 (LR 0.09045) => LSC_loss 1.03, Spatial_loss 1.80, Flat_loss 0.19, Train_acc 71.27, Test_acc 46.57
2025-01-05 13:02:38,581 [podnet.py] => Task 5, Epoch 33/160 (LR 0.08987) => LSC_loss 1.01, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 71.81, Test_acc 47.73
2025-01-05 13:02:46,166 [podnet.py] => Task 5, Epoch 34/160 (LR 0.08927) => LSC_loss 1.00, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 72.64, Test_acc 56.87
2025-01-05 13:02:53,787 [podnet.py] => Task 5, Epoch 35/160 (LR 0.08865) => LSC_loss 1.02, Spatial_loss 1.81, Flat_loss 0.19, Train_acc 71.31, Test_acc 53.40
2025-01-05 13:03:01,238 [podnet.py] => Task 5, Epoch 36/160 (LR 0.08802) => LSC_loss 1.01, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 72.03, Test_acc 52.50
2025-01-05 13:03:08,926 [podnet.py] => Task 5, Epoch 37/160 (LR 0.08738) => LSC_loss 1.00, Spatial_loss 1.76, Flat_loss 0.18, Train_acc 71.95, Test_acc 51.47
2025-01-05 13:03:16,418 [podnet.py] => Task 5, Epoch 38/160 (LR 0.08672) => LSC_loss 1.01, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 72.41, Test_acc 55.37
2025-01-05 13:03:23,831 [podnet.py] => Task 5, Epoch 39/160 (LR 0.08604) => LSC_loss 0.98, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 72.21, Test_acc 47.07
2025-01-05 13:03:31,283 [podnet.py] => Task 5, Epoch 40/160 (LR 0.08536) => LSC_loss 0.97, Spatial_loss 1.73, Flat_loss 0.18, Train_acc 73.10, Test_acc 49.60
2025-01-05 13:03:38,845 [podnet.py] => Task 5, Epoch 41/160 (LR 0.08465) => LSC_loss 0.99, Spatial_loss 1.76, Flat_loss 0.18, Train_acc 72.25, Test_acc 52.90
2025-01-05 13:03:46,327 [podnet.py] => Task 5, Epoch 42/160 (LR 0.08394) => LSC_loss 1.00, Spatial_loss 1.80, Flat_loss 0.18, Train_acc 71.97, Test_acc 52.73
2025-01-05 13:03:53,861 [podnet.py] => Task 5, Epoch 43/160 (LR 0.08321) => LSC_loss 0.97, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 72.73, Test_acc 50.50
2025-01-05 13:04:01,425 [podnet.py] => Task 5, Epoch 44/160 (LR 0.08247) => LSC_loss 0.97, Spatial_loss 1.71, Flat_loss 0.18, Train_acc 72.68, Test_acc 53.13
2025-01-05 13:04:08,946 [podnet.py] => Task 5, Epoch 45/160 (LR 0.08172) => LSC_loss 1.00, Spatial_loss 1.78, Flat_loss 0.19, Train_acc 71.64, Test_acc 54.77
2025-01-05 13:04:16,686 [podnet.py] => Task 5, Epoch 46/160 (LR 0.08095) => LSC_loss 0.95, Spatial_loss 1.70, Flat_loss 0.18, Train_acc 73.21, Test_acc 52.97
2025-01-05 13:04:24,060 [podnet.py] => Task 5, Epoch 47/160 (LR 0.08018) => LSC_loss 0.98, Spatial_loss 1.75, Flat_loss 0.18, Train_acc 73.03, Test_acc 51.30
2025-01-05 13:04:31,775 [podnet.py] => Task 5, Epoch 48/160 (LR 0.07939) => LSC_loss 0.96, Spatial_loss 1.78, Flat_loss 0.18, Train_acc 73.61, Test_acc 56.83
2025-01-05 13:04:39,180 [podnet.py] => Task 5, Epoch 49/160 (LR 0.07859) => LSC_loss 0.96, Spatial_loss 1.70, Flat_loss 0.18, Train_acc 73.63, Test_acc 54.57
2025-01-05 13:04:46,619 [podnet.py] => Task 5, Epoch 50/160 (LR 0.07778) => LSC_loss 0.95, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 73.17, Test_acc 54.87
2025-01-05 13:04:54,151 [podnet.py] => Task 5, Epoch 51/160 (LR 0.07696) => LSC_loss 0.95, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 73.52, Test_acc 55.87
2025-01-05 13:05:01,495 [podnet.py] => Task 5, Epoch 52/160 (LR 0.07612) => LSC_loss 0.95, Spatial_loss 1.74, Flat_loss 0.18, Train_acc 73.33, Test_acc 54.23
2025-01-05 13:05:08,959 [podnet.py] => Task 5, Epoch 53/160 (LR 0.07528) => LSC_loss 0.92, Spatial_loss 1.67, Flat_loss 0.17, Train_acc 74.61, Test_acc 58.33
2025-01-05 13:05:16,349 [podnet.py] => Task 5, Epoch 54/160 (LR 0.07443) => LSC_loss 0.94, Spatial_loss 1.68, Flat_loss 0.18, Train_acc 73.88, Test_acc 49.20
2025-01-05 13:05:23,941 [podnet.py] => Task 5, Epoch 55/160 (LR 0.07357) => LSC_loss 0.93, Spatial_loss 1.70, Flat_loss 0.18, Train_acc 73.69, Test_acc 54.47
2025-01-05 13:05:31,633 [podnet.py] => Task 5, Epoch 56/160 (LR 0.07270) => LSC_loss 0.92, Spatial_loss 1.69, Flat_loss 0.17, Train_acc 74.01, Test_acc 57.30
2025-01-05 13:05:39,232 [podnet.py] => Task 5, Epoch 57/160 (LR 0.07182) => LSC_loss 0.93, Spatial_loss 1.70, Flat_loss 0.18, Train_acc 73.88, Test_acc 54.77
2025-01-05 13:05:46,784 [podnet.py] => Task 5, Epoch 58/160 (LR 0.07093) => LSC_loss 0.91, Spatial_loss 1.69, Flat_loss 0.17, Train_acc 74.50, Test_acc 58.27
2025-01-05 13:05:54,239 [podnet.py] => Task 5, Epoch 59/160 (LR 0.07004) => LSC_loss 0.91, Spatial_loss 1.64, Flat_loss 0.17, Train_acc 74.47, Test_acc 56.27
2025-01-05 13:06:01,578 [podnet.py] => Task 5, Epoch 60/160 (LR 0.06913) => LSC_loss 0.89, Spatial_loss 1.67, Flat_loss 0.17, Train_acc 74.89, Test_acc 56.07
2025-01-05 13:06:08,866 [podnet.py] => Task 5, Epoch 61/160 (LR 0.06822) => LSC_loss 0.90, Spatial_loss 1.64, Flat_loss 0.17, Train_acc 75.28, Test_acc 52.13
2025-01-05 13:06:16,205 [podnet.py] => Task 5, Epoch 62/160 (LR 0.06731) => LSC_loss 0.91, Spatial_loss 1.66, Flat_loss 0.17, Train_acc 74.67, Test_acc 57.50
2025-01-05 13:06:23,854 [podnet.py] => Task 5, Epoch 63/160 (LR 0.06638) => LSC_loss 0.88, Spatial_loss 1.66, Flat_loss 0.17, Train_acc 75.45, Test_acc 57.17
2025-01-05 13:06:31,310 [podnet.py] => Task 5, Epoch 64/160 (LR 0.06545) => LSC_loss 0.88, Spatial_loss 1.62, Flat_loss 0.17, Train_acc 75.82, Test_acc 56.43
2025-01-05 13:06:38,861 [podnet.py] => Task 5, Epoch 65/160 (LR 0.06451) => LSC_loss 0.89, Spatial_loss 1.65, Flat_loss 0.17, Train_acc 75.27, Test_acc 55.23
2025-01-05 13:06:46,365 [podnet.py] => Task 5, Epoch 66/160 (LR 0.06357) => LSC_loss 0.89, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 75.23, Test_acc 59.70
2025-01-05 13:06:54,262 [podnet.py] => Task 5, Epoch 67/160 (LR 0.06262) => LSC_loss 0.87, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 75.71, Test_acc 56.13
2025-01-05 13:07:01,811 [podnet.py] => Task 5, Epoch 68/160 (LR 0.06167) => LSC_loss 0.87, Spatial_loss 1.62, Flat_loss 0.17, Train_acc 76.00, Test_acc 54.33
2025-01-05 13:07:09,366 [podnet.py] => Task 5, Epoch 69/160 (LR 0.06072) => LSC_loss 0.85, Spatial_loss 1.65, Flat_loss 0.16, Train_acc 76.22, Test_acc 60.37
2025-01-05 13:07:16,862 [podnet.py] => Task 5, Epoch 70/160 (LR 0.05975) => LSC_loss 0.86, Spatial_loss 1.63, Flat_loss 0.16, Train_acc 75.80, Test_acc 56.20
2025-01-05 13:07:24,372 [podnet.py] => Task 5, Epoch 71/160 (LR 0.05879) => LSC_loss 0.83, Spatial_loss 1.55, Flat_loss 0.16, Train_acc 76.76, Test_acc 55.00
2025-01-05 13:07:32,072 [podnet.py] => Task 5, Epoch 72/160 (LR 0.05782) => LSC_loss 0.83, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 76.75, Test_acc 56.50
2025-01-05 13:07:39,789 [podnet.py] => Task 5, Epoch 73/160 (LR 0.05685) => LSC_loss 0.83, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 76.75, Test_acc 59.57
2025-01-05 13:07:47,618 [podnet.py] => Task 5, Epoch 74/160 (LR 0.05588) => LSC_loss 0.82, Spatial_loss 1.57, Flat_loss 0.16, Train_acc 77.29, Test_acc 54.97
2025-01-05 13:07:55,075 [podnet.py] => Task 5, Epoch 75/160 (LR 0.05490) => LSC_loss 0.83, Spatial_loss 1.57, Flat_loss 0.16, Train_acc 76.66, Test_acc 56.10
2025-01-05 13:08:02,556 [podnet.py] => Task 5, Epoch 76/160 (LR 0.05392) => LSC_loss 0.81, Spatial_loss 1.55, Flat_loss 0.16, Train_acc 77.30, Test_acc 59.30
2025-01-05 13:08:10,595 [podnet.py] => Task 5, Epoch 77/160 (LR 0.05294) => LSC_loss 0.82, Spatial_loss 1.57, Flat_loss 0.16, Train_acc 76.71, Test_acc 55.80
2025-01-05 13:08:18,752 [podnet.py] => Task 5, Epoch 78/160 (LR 0.05196) => LSC_loss 0.83, Spatial_loss 1.57, Flat_loss 0.16, Train_acc 76.75, Test_acc 58.27
2025-01-05 13:08:26,654 [podnet.py] => Task 5, Epoch 79/160 (LR 0.05098) => LSC_loss 0.80, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 77.69, Test_acc 52.13
2025-01-05 13:08:34,479 [podnet.py] => Task 5, Epoch 80/160 (LR 0.05000) => LSC_loss 0.81, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 77.46, Test_acc 59.43
2025-01-05 13:08:42,002 [podnet.py] => Task 5, Epoch 81/160 (LR 0.04902) => LSC_loss 0.77, Spatial_loss 1.54, Flat_loss 0.15, Train_acc 78.76, Test_acc 60.50
2025-01-05 13:08:49,455 [podnet.py] => Task 5, Epoch 82/160 (LR 0.04804) => LSC_loss 0.79, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 77.87, Test_acc 58.57
2025-01-05 13:08:57,203 [podnet.py] => Task 5, Epoch 83/160 (LR 0.04706) => LSC_loss 0.78, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 78.37, Test_acc 54.53
2025-01-05 13:09:04,797 [podnet.py] => Task 5, Epoch 84/160 (LR 0.04608) => LSC_loss 0.74, Spatial_loss 1.48, Flat_loss 0.14, Train_acc 79.48, Test_acc 56.00
2025-01-05 13:09:12,436 [podnet.py] => Task 5, Epoch 85/160 (LR 0.04510) => LSC_loss 0.74, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 79.35, Test_acc 56.57
2025-01-05 13:09:20,111 [podnet.py] => Task 5, Epoch 86/160 (LR 0.04412) => LSC_loss 0.74, Spatial_loss 1.48, Flat_loss 0.15, Train_acc 78.94, Test_acc 57.37
2025-01-05 13:09:27,719 [podnet.py] => Task 5, Epoch 87/160 (LR 0.04315) => LSC_loss 0.73, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 79.90, Test_acc 59.53
2025-01-05 13:09:35,351 [podnet.py] => Task 5, Epoch 88/160 (LR 0.04218) => LSC_loss 0.73, Spatial_loss 1.49, Flat_loss 0.14, Train_acc 79.53, Test_acc 58.00
2025-01-05 13:09:43,198 [podnet.py] => Task 5, Epoch 89/160 (LR 0.04121) => LSC_loss 0.72, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 80.09, Test_acc 59.17
2025-01-05 13:09:50,813 [podnet.py] => Task 5, Epoch 90/160 (LR 0.04025) => LSC_loss 0.72, Spatial_loss 1.47, Flat_loss 0.14, Train_acc 79.77, Test_acc 58.17
2025-01-05 13:09:58,520 [podnet.py] => Task 5, Epoch 91/160 (LR 0.03928) => LSC_loss 0.72, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 80.20, Test_acc 58.27
2025-01-05 13:10:06,496 [podnet.py] => Task 5, Epoch 92/160 (LR 0.03833) => LSC_loss 0.69, Spatial_loss 1.46, Flat_loss 0.14, Train_acc 80.60, Test_acc 55.70
2025-01-05 13:10:13,983 [podnet.py] => Task 5, Epoch 93/160 (LR 0.03738) => LSC_loss 0.71, Spatial_loss 1.44, Flat_loss 0.14, Train_acc 80.02, Test_acc 59.77
2025-01-05 13:10:21,473 [podnet.py] => Task 5, Epoch 94/160 (LR 0.03643) => LSC_loss 0.69, Spatial_loss 1.41, Flat_loss 0.14, Train_acc 80.76, Test_acc 62.97
2025-01-05 13:10:29,200 [podnet.py] => Task 5, Epoch 95/160 (LR 0.03549) => LSC_loss 0.68, Spatial_loss 1.37, Flat_loss 0.13, Train_acc 80.87, Test_acc 56.60
2025-01-05 13:10:36,819 [podnet.py] => Task 5, Epoch 96/160 (LR 0.03455) => LSC_loss 0.66, Spatial_loss 1.38, Flat_loss 0.13, Train_acc 81.85, Test_acc 59.67
2025-01-05 13:10:44,216 [podnet.py] => Task 5, Epoch 97/160 (LR 0.03362) => LSC_loss 0.67, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 81.23, Test_acc 59.07
2025-01-05 13:10:51,856 [podnet.py] => Task 5, Epoch 98/160 (LR 0.03269) => LSC_loss 0.65, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 81.84, Test_acc 56.73
2025-01-05 13:10:59,503 [podnet.py] => Task 5, Epoch 99/160 (LR 0.03178) => LSC_loss 0.64, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 82.10, Test_acc 61.77
2025-01-05 13:11:07,242 [podnet.py] => Task 5, Epoch 100/160 (LR 0.03087) => LSC_loss 0.64, Spatial_loss 1.39, Flat_loss 0.13, Train_acc 82.43, Test_acc 60.33
2025-01-05 13:11:14,780 [podnet.py] => Task 5, Epoch 101/160 (LR 0.02996) => LSC_loss 0.63, Spatial_loss 1.32, Flat_loss 0.13, Train_acc 82.90, Test_acc 58.67
2025-01-05 13:11:22,495 [podnet.py] => Task 5, Epoch 102/160 (LR 0.02907) => LSC_loss 0.62, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 82.73, Test_acc 58.93
2025-01-05 13:11:30,051 [podnet.py] => Task 5, Epoch 103/160 (LR 0.02818) => LSC_loss 0.62, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 83.07, Test_acc 57.33
2025-01-05 13:11:37,591 [podnet.py] => Task 5, Epoch 104/160 (LR 0.02730) => LSC_loss 0.62, Spatial_loss 1.34, Flat_loss 0.12, Train_acc 83.38, Test_acc 59.90
2025-01-05 13:11:45,210 [podnet.py] => Task 5, Epoch 105/160 (LR 0.02643) => LSC_loss 0.60, Spatial_loss 1.31, Flat_loss 0.12, Train_acc 83.25, Test_acc 60.43
2025-01-05 13:11:53,002 [podnet.py] => Task 5, Epoch 106/160 (LR 0.02557) => LSC_loss 0.58, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 84.08, Test_acc 60.40
2025-01-05 13:12:00,380 [podnet.py] => Task 5, Epoch 107/160 (LR 0.02472) => LSC_loss 0.60, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 83.33, Test_acc 61.77
2025-01-05 13:12:07,889 [podnet.py] => Task 5, Epoch 108/160 (LR 0.02388) => LSC_loss 0.56, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 84.47, Test_acc 61.73
2025-01-05 13:12:15,534 [podnet.py] => Task 5, Epoch 109/160 (LR 0.02304) => LSC_loss 0.54, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 85.37, Test_acc 60.27
2025-01-05 13:12:23,168 [podnet.py] => Task 5, Epoch 110/160 (LR 0.02222) => LSC_loss 0.55, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 84.75, Test_acc 63.50
2025-01-05 13:12:30,726 [podnet.py] => Task 5, Epoch 111/160 (LR 0.02141) => LSC_loss 0.54, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 85.06, Test_acc 61.80
2025-01-05 13:12:38,313 [podnet.py] => Task 5, Epoch 112/160 (LR 0.02061) => LSC_loss 0.54, Spatial_loss 1.23, Flat_loss 0.11, Train_acc 85.08, Test_acc 60.03
2025-01-05 13:12:45,920 [podnet.py] => Task 5, Epoch 113/160 (LR 0.01982) => LSC_loss 0.54, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 85.45, Test_acc 62.10
2025-01-05 13:12:53,531 [podnet.py] => Task 5, Epoch 114/160 (LR 0.01905) => LSC_loss 0.49, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 86.89, Test_acc 61.40
2025-01-05 13:13:01,196 [podnet.py] => Task 5, Epoch 115/160 (LR 0.01828) => LSC_loss 0.50, Spatial_loss 1.16, Flat_loss 0.10, Train_acc 86.58, Test_acc 59.53
2025-01-05 13:13:08,924 [podnet.py] => Task 5, Epoch 116/160 (LR 0.01753) => LSC_loss 0.49, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 86.95, Test_acc 61.13
2025-01-05 13:13:16,742 [podnet.py] => Task 5, Epoch 117/160 (LR 0.01679) => LSC_loss 0.50, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 86.29, Test_acc 63.27
2025-01-05 13:13:24,341 [podnet.py] => Task 5, Epoch 118/160 (LR 0.01606) => LSC_loss 0.48, Spatial_loss 1.17, Flat_loss 0.10, Train_acc 87.11, Test_acc 63.17
2025-01-05 13:13:31,964 [podnet.py] => Task 5, Epoch 119/160 (LR 0.01535) => LSC_loss 0.47, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 87.46, Test_acc 63.30
2025-01-05 13:13:39,354 [podnet.py] => Task 5, Epoch 120/160 (LR 0.01464) => LSC_loss 0.45, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 87.79, Test_acc 62.67
2025-01-05 13:13:46,934 [podnet.py] => Task 5, Epoch 121/160 (LR 0.01396) => LSC_loss 0.44, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 88.47, Test_acc 63.80
2025-01-05 13:13:54,548 [podnet.py] => Task 5, Epoch 122/160 (LR 0.01328) => LSC_loss 0.42, Spatial_loss 1.06, Flat_loss 0.09, Train_acc 88.75, Test_acc 62.20
2025-01-05 13:14:01,944 [podnet.py] => Task 5, Epoch 123/160 (LR 0.01262) => LSC_loss 0.43, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 88.80, Test_acc 63.00
2025-01-05 13:14:09,946 [podnet.py] => Task 5, Epoch 124/160 (LR 0.01198) => LSC_loss 0.42, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 88.91, Test_acc 62.37
2025-01-05 13:14:17,836 [podnet.py] => Task 5, Epoch 125/160 (LR 0.01135) => LSC_loss 0.41, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 89.05, Test_acc 62.83
2025-01-05 13:14:25,561 [podnet.py] => Task 5, Epoch 126/160 (LR 0.01073) => LSC_loss 0.40, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 89.62, Test_acc 64.43
2025-01-05 13:14:33,254 [podnet.py] => Task 5, Epoch 127/160 (LR 0.01013) => LSC_loss 0.39, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 90.21, Test_acc 63.70
2025-01-05 13:14:40,928 [podnet.py] => Task 5, Epoch 128/160 (LR 0.00955) => LSC_loss 0.40, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 89.73, Test_acc 62.17
2025-01-05 13:14:48,565 [podnet.py] => Task 5, Epoch 129/160 (LR 0.00898) => LSC_loss 0.38, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 90.27, Test_acc 61.50
2025-01-05 13:14:56,395 [podnet.py] => Task 5, Epoch 130/160 (LR 0.00843) => LSC_loss 0.39, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 89.82, Test_acc 62.43
2025-01-05 13:15:03,974 [podnet.py] => Task 5, Epoch 131/160 (LR 0.00789) => LSC_loss 0.37, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 90.71, Test_acc 64.20
2025-01-05 13:15:11,594 [podnet.py] => Task 5, Epoch 132/160 (LR 0.00737) => LSC_loss 0.36, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 90.95, Test_acc 64.40
2025-01-05 13:15:19,346 [podnet.py] => Task 5, Epoch 133/160 (LR 0.00686) => LSC_loss 0.36, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 90.83, Test_acc 63.93
2025-01-05 13:15:27,056 [podnet.py] => Task 5, Epoch 134/160 (LR 0.00638) => LSC_loss 0.35, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 91.42, Test_acc 63.50
2025-01-05 13:15:34,686 [podnet.py] => Task 5, Epoch 135/160 (LR 0.00590) => LSC_loss 0.35, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 91.39, Test_acc 62.70
2025-01-05 13:15:42,223 [podnet.py] => Task 5, Epoch 136/160 (LR 0.00545) => LSC_loss 0.35, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 91.50, Test_acc 64.97
2025-01-05 13:15:49,905 [podnet.py] => Task 5, Epoch 137/160 (LR 0.00501) => LSC_loss 0.34, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 91.71, Test_acc 64.07
2025-01-05 13:15:57,569 [podnet.py] => Task 5, Epoch 138/160 (LR 0.00459) => LSC_loss 0.33, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 91.95, Test_acc 62.87
2025-01-05 13:16:05,149 [podnet.py] => Task 5, Epoch 139/160 (LR 0.00419) => LSC_loss 0.33, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 92.08, Test_acc 64.30
2025-01-05 13:16:12,666 [podnet.py] => Task 5, Epoch 140/160 (LR 0.00381) => LSC_loss 0.33, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 91.99, Test_acc 65.10
2025-01-05 13:16:20,254 [podnet.py] => Task 5, Epoch 141/160 (LR 0.00344) => LSC_loss 0.32, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 92.27, Test_acc 64.93
2025-01-05 13:16:27,928 [podnet.py] => Task 5, Epoch 142/160 (LR 0.00309) => LSC_loss 0.32, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 92.47, Test_acc 65.63
2025-01-05 13:16:35,506 [podnet.py] => Task 5, Epoch 143/160 (LR 0.00276) => LSC_loss 0.31, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 92.62, Test_acc 65.07
2025-01-05 13:16:43,154 [podnet.py] => Task 5, Epoch 144/160 (LR 0.00245) => LSC_loss 0.31, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 92.51, Test_acc 64.77
2025-01-05 13:16:50,544 [podnet.py] => Task 5, Epoch 145/160 (LR 0.00215) => LSC_loss 0.30, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 92.81, Test_acc 64.60
2025-01-05 13:16:58,148 [podnet.py] => Task 5, Epoch 146/160 (LR 0.00188) => LSC_loss 0.31, Spatial_loss 0.84, Flat_loss 0.07, Train_acc 92.95, Test_acc 64.93
2025-01-05 13:17:05,651 [podnet.py] => Task 5, Epoch 147/160 (LR 0.00162) => LSC_loss 0.30, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 93.33, Test_acc 65.10
2025-01-05 13:17:13,214 [podnet.py] => Task 5, Epoch 148/160 (LR 0.00138) => LSC_loss 0.29, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 93.08, Test_acc 64.93
2025-01-05 13:17:20,797 [podnet.py] => Task 5, Epoch 149/160 (LR 0.00116) => LSC_loss 0.30, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 92.75, Test_acc 65.13
2025-01-05 13:17:28,485 [podnet.py] => Task 5, Epoch 150/160 (LR 0.00096) => LSC_loss 0.30, Spatial_loss 0.81, Flat_loss 0.07, Train_acc 93.33, Test_acc 65.40
2025-01-05 13:17:36,094 [podnet.py] => Task 5, Epoch 151/160 (LR 0.00078) => LSC_loss 0.29, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 93.31, Test_acc 65.50
2025-01-05 13:17:43,580 [podnet.py] => Task 5, Epoch 152/160 (LR 0.00062) => LSC_loss 0.29, Spatial_loss 0.81, Flat_loss 0.07, Train_acc 93.41, Test_acc 65.27
2025-01-05 13:17:51,214 [podnet.py] => Task 5, Epoch 153/160 (LR 0.00047) => LSC_loss 0.29, Spatial_loss 0.80, Flat_loss 0.07, Train_acc 93.33, Test_acc 64.87
2025-01-05 13:17:58,556 [podnet.py] => Task 5, Epoch 154/160 (LR 0.00035) => LSC_loss 0.29, Spatial_loss 0.79, Flat_loss 0.07, Train_acc 93.67, Test_acc 65.33
2025-01-05 13:18:06,083 [podnet.py] => Task 5, Epoch 155/160 (LR 0.00024) => LSC_loss 0.29, Spatial_loss 0.80, Flat_loss 0.07, Train_acc 93.62, Test_acc 64.97
2025-01-05 13:18:13,606 [podnet.py] => Task 5, Epoch 156/160 (LR 0.00015) => LSC_loss 0.28, Spatial_loss 0.78, Flat_loss 0.07, Train_acc 93.77, Test_acc 65.30
2025-01-05 13:18:21,122 [podnet.py] => Task 5, Epoch 157/160 (LR 0.00009) => LSC_loss 0.28, Spatial_loss 0.80, Flat_loss 0.07, Train_acc 93.73, Test_acc 65.07
2025-01-05 13:18:28,441 [podnet.py] => Task 5, Epoch 158/160 (LR 0.00004) => LSC_loss 0.29, Spatial_loss 0.79, Flat_loss 0.07, Train_acc 93.58, Test_acc 64.87
2025-01-05 13:18:36,020 [podnet.py] => Task 5, Epoch 159/160 (LR 0.00001) => LSC_loss 0.29, Spatial_loss 0.80, Flat_loss 0.07, Train_acc 93.47, Test_acc 65.00
2025-01-05 13:18:43,657 [podnet.py] => Task 5, Epoch 160/160 (LR 0.00000) => LSC_loss 0.28, Spatial_loss 0.79, Flat_loss 0.07, Train_acc 93.37, Test_acc 64.90
2025-01-05 13:18:43,658 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 13:18:43,658 [base.py] => Reducing exemplars...(539 per classes)
2025-01-05 13:19:05,127 [base.py] => Constructing exemplars...(539 per classes)
2025-01-05 13:19:14,742 [podnet.py] => The size of finetune dataset: 15000
2025-01-05 13:19:22,249 [podnet.py] => Task 5, Epoch 1/20 (LR 0.00497) => LSC_loss 0.31, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 92.71, Test_acc 64.37
2025-01-05 13:19:30,015 [podnet.py] => Task 5, Epoch 2/20 (LR 0.00488) => LSC_loss 0.33, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 92.07, Test_acc 63.87
2025-01-05 13:19:37,635 [podnet.py] => Task 5, Epoch 3/20 (LR 0.00473) => LSC_loss 0.33, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 92.07, Test_acc 64.27
2025-01-05 13:19:45,153 [podnet.py] => Task 5, Epoch 4/20 (LR 0.00452) => LSC_loss 0.32, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 92.17, Test_acc 64.23
2025-01-05 13:19:52,697 [podnet.py] => Task 5, Epoch 5/20 (LR 0.00427) => LSC_loss 0.32, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 92.19, Test_acc 63.77
2025-01-05 13:20:00,363 [podnet.py] => Task 5, Epoch 6/20 (LR 0.00397) => LSC_loss 0.31, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 92.86, Test_acc 64.37
2025-01-05 13:20:07,913 [podnet.py] => Task 5, Epoch 7/20 (LR 0.00363) => LSC_loss 0.32, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 92.41, Test_acc 64.53
2025-01-05 13:20:15,280 [podnet.py] => Task 5, Epoch 8/20 (LR 0.00327) => LSC_loss 0.32, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 92.33, Test_acc 64.23
2025-01-05 13:20:22,970 [podnet.py] => Task 5, Epoch 9/20 (LR 0.00289) => LSC_loss 0.31, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 92.61, Test_acc 64.30
2025-01-05 13:20:30,753 [podnet.py] => Task 5, Epoch 10/20 (LR 0.00250) => LSC_loss 0.30, Spatial_loss 0.84, Flat_loss 0.07, Train_acc 92.64, Test_acc 64.70
2025-01-05 13:20:38,375 [podnet.py] => Task 5, Epoch 11/20 (LR 0.00211) => LSC_loss 0.30, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 92.97, Test_acc 64.37
2025-01-05 13:20:46,157 [podnet.py] => Task 5, Epoch 12/20 (LR 0.00173) => LSC_loss 0.30, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 93.36, Test_acc 64.40
2025-01-05 13:20:53,476 [podnet.py] => Task 5, Epoch 13/20 (LR 0.00137) => LSC_loss 0.29, Spatial_loss 0.81, Flat_loss 0.07, Train_acc 93.31, Test_acc 64.73
2025-01-05 13:21:01,200 [podnet.py] => Task 5, Epoch 14/20 (LR 0.00103) => LSC_loss 0.29, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 93.53, Test_acc 64.67
2025-01-05 13:21:08,882 [podnet.py] => Task 5, Epoch 15/20 (LR 0.00073) => LSC_loss 0.29, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 93.33, Test_acc 64.37
2025-01-05 13:21:16,519 [podnet.py] => Task 5, Epoch 16/20 (LR 0.00048) => LSC_loss 0.28, Spatial_loss 0.80, Flat_loss 0.07, Train_acc 93.67, Test_acc 64.67
2025-01-05 13:21:24,131 [podnet.py] => Task 5, Epoch 17/20 (LR 0.00027) => LSC_loss 0.28, Spatial_loss 0.79, Flat_loss 0.07, Train_acc 93.97, Test_acc 64.67
2025-01-05 13:21:31,875 [podnet.py] => Task 5, Epoch 18/20 (LR 0.00012) => LSC_loss 0.28, Spatial_loss 0.77, Flat_loss 0.07, Train_acc 93.82, Test_acc 64.67
2025-01-05 13:21:39,523 [podnet.py] => Task 5, Epoch 19/20 (LR 0.00003) => LSC_loss 0.27, Spatial_loss 0.78, Flat_loss 0.07, Train_acc 94.01, Test_acc 64.57
2025-01-05 13:21:47,145 [podnet.py] => Task 5, Epoch 20/20 (LR 0.00000) => LSC_loss 0.28, Spatial_loss 0.78, Flat_loss 0.07, Train_acc 93.79, Test_acc 64.47
2025-01-05 13:21:47,149 [base.py] => Reducing exemplars...(449 per classes)
2025-01-05 13:22:09,144 [base.py] => Constructing exemplars...(449 per classes)
2025-01-05 13:22:20,242 [podnet.py] => Exemplar size: 13470
2025-01-05 13:22:20,242 [trainer.py] => CNN: {'total': np.float64(64.47), '00-09': np.float64(73.6), '10-19': np.float64(54.8), '20-29': np.float64(65.0), 'old': np.float64(64.88), 'new': np.float64(62.4)}
2025-01-05 13:22:20,243 [trainer.py] => NME: {'total': np.float64(63.57), '00-09': np.float64(73.8), '10-19': np.float64(53.0), '20-29': np.float64(63.9), 'old': np.float64(63.48), 'new': np.float64(64.0)}
2025-01-05 13:22:20,243 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47)]
2025-01-05 13:22:20,243 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4)]
2025-01-05 13:22:20,243 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57)]
2025-01-05 13:22:20,243 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2)]

2025-01-05 13:22:20,243 [trainer.py] => All params: 485457
2025-01-05 13:22:20,243 [trainer.py] => Trainable params: 485457
2025-01-05 13:22:20,244 [podnet.py] => Learning on 30-35
2025-01-05 13:22:20,289 [podnet.py] => Adaptive factor: 2.6457513110645907
2025-01-05 13:22:28,308 [podnet.py] => Task 6, Epoch 1/160 (LR 0.09999) => LSC_loss 1.91, Spatial_loss 2.63, Flat_loss 0.39, Train_acc 50.23, Test_acc 42.00
2025-01-05 13:22:36,283 [podnet.py] => Task 6, Epoch 2/160 (LR 0.09996) => LSC_loss 1.50, Spatial_loss 2.27, Flat_loss 0.28, Train_acc 57.93, Test_acc 46.26
2025-01-05 13:22:44,257 [podnet.py] => Task 6, Epoch 3/160 (LR 0.09991) => LSC_loss 1.40, Spatial_loss 2.14, Flat_loss 0.25, Train_acc 60.06, Test_acc 40.43
2025-01-05 13:22:52,191 [podnet.py] => Task 6, Epoch 4/160 (LR 0.09985) => LSC_loss 1.36, Spatial_loss 2.09, Flat_loss 0.24, Train_acc 61.60, Test_acc 48.29
2025-01-05 13:23:00,206 [podnet.py] => Task 6, Epoch 5/160 (LR 0.09976) => LSC_loss 1.33, Spatial_loss 2.07, Flat_loss 0.24, Train_acc 62.07, Test_acc 47.14
2025-01-05 13:23:08,016 [podnet.py] => Task 6, Epoch 6/160 (LR 0.09965) => LSC_loss 1.32, Spatial_loss 1.98, Flat_loss 0.23, Train_acc 63.11, Test_acc 41.63
2025-01-05 13:23:16,142 [podnet.py] => Task 6, Epoch 7/160 (LR 0.09953) => LSC_loss 1.28, Spatial_loss 1.96, Flat_loss 0.22, Train_acc 63.71, Test_acc 51.43
2025-01-05 13:23:24,362 [podnet.py] => Task 6, Epoch 8/160 (LR 0.09938) => LSC_loss 1.25, Spatial_loss 2.00, Flat_loss 0.22, Train_acc 64.43, Test_acc 51.26
2025-01-05 13:23:32,128 [podnet.py] => Task 6, Epoch 9/160 (LR 0.09922) => LSC_loss 1.25, Spatial_loss 1.92, Flat_loss 0.22, Train_acc 64.56, Test_acc 49.29
2025-01-05 13:23:39,846 [podnet.py] => Task 6, Epoch 10/160 (LR 0.09904) => LSC_loss 1.23, Spatial_loss 1.91, Flat_loss 0.22, Train_acc 65.17, Test_acc 49.11
2025-01-05 13:23:48,039 [podnet.py] => Task 6, Epoch 11/160 (LR 0.09884) => LSC_loss 1.24, Spatial_loss 1.95, Flat_loss 0.22, Train_acc 64.53, Test_acc 48.71
2025-01-05 13:23:55,672 [podnet.py] => Task 6, Epoch 12/160 (LR 0.09862) => LSC_loss 1.23, Spatial_loss 1.92, Flat_loss 0.22, Train_acc 65.15, Test_acc 49.37
2025-01-05 13:24:03,734 [podnet.py] => Task 6, Epoch 13/160 (LR 0.09838) => LSC_loss 1.20, Spatial_loss 1.94, Flat_loss 0.22, Train_acc 66.11, Test_acc 50.86
2025-01-05 13:24:11,896 [podnet.py] => Task 6, Epoch 14/160 (LR 0.09812) => LSC_loss 1.21, Spatial_loss 1.92, Flat_loss 0.21, Train_acc 65.70, Test_acc 51.91
2025-01-05 13:24:19,648 [podnet.py] => Task 6, Epoch 15/160 (LR 0.09785) => LSC_loss 1.20, Spatial_loss 1.91, Flat_loss 0.21, Train_acc 65.64, Test_acc 52.11
2025-01-05 13:24:27,509 [podnet.py] => Task 6, Epoch 16/160 (LR 0.09755) => LSC_loss 1.21, Spatial_loss 1.91, Flat_loss 0.21, Train_acc 65.98, Test_acc 50.20
2025-01-05 13:24:35,442 [podnet.py] => Task 6, Epoch 17/160 (LR 0.09724) => LSC_loss 1.17, Spatial_loss 1.89, Flat_loss 0.21, Train_acc 66.76, Test_acc 46.94
2025-01-05 13:24:43,295 [podnet.py] => Task 6, Epoch 18/160 (LR 0.09691) => LSC_loss 1.18, Spatial_loss 1.87, Flat_loss 0.21, Train_acc 65.86, Test_acc 51.71
2025-01-05 13:24:51,224 [podnet.py] => Task 6, Epoch 19/160 (LR 0.09656) => LSC_loss 1.19, Spatial_loss 1.90, Flat_loss 0.21, Train_acc 66.21, Test_acc 49.49
2025-01-05 13:24:59,000 [podnet.py] => Task 6, Epoch 20/160 (LR 0.09619) => LSC_loss 1.18, Spatial_loss 1.91, Flat_loss 0.21, Train_acc 66.10, Test_acc 53.29
2025-01-05 13:25:06,963 [podnet.py] => Task 6, Epoch 21/160 (LR 0.09581) => LSC_loss 1.17, Spatial_loss 1.90, Flat_loss 0.21, Train_acc 66.88, Test_acc 50.17
2025-01-05 13:25:14,830 [podnet.py] => Task 6, Epoch 22/160 (LR 0.09541) => LSC_loss 1.18, Spatial_loss 1.93, Flat_loss 0.21, Train_acc 66.67, Test_acc 47.49
2025-01-05 13:25:22,563 [podnet.py] => Task 6, Epoch 23/160 (LR 0.09499) => LSC_loss 1.18, Spatial_loss 1.92, Flat_loss 0.21, Train_acc 66.40, Test_acc 51.03
2025-01-05 13:25:30,573 [podnet.py] => Task 6, Epoch 24/160 (LR 0.09455) => LSC_loss 1.17, Spatial_loss 1.92, Flat_loss 0.21, Train_acc 66.91, Test_acc 50.17
2025-01-05 13:25:38,482 [podnet.py] => Task 6, Epoch 25/160 (LR 0.09410) => LSC_loss 1.15, Spatial_loss 1.88, Flat_loss 0.21, Train_acc 67.66, Test_acc 49.49
2025-01-05 13:25:46,368 [podnet.py] => Task 6, Epoch 26/160 (LR 0.09362) => LSC_loss 1.16, Spatial_loss 1.88, Flat_loss 0.21, Train_acc 66.86, Test_acc 50.34
2025-01-05 13:25:54,364 [podnet.py] => Task 6, Epoch 27/160 (LR 0.09314) => LSC_loss 1.14, Spatial_loss 1.89, Flat_loss 0.21, Train_acc 67.46, Test_acc 53.00
2025-01-05 13:26:02,483 [podnet.py] => Task 6, Epoch 28/160 (LR 0.09263) => LSC_loss 1.14, Spatial_loss 1.89, Flat_loss 0.21, Train_acc 67.81, Test_acc 49.74
2025-01-05 13:26:10,377 [podnet.py] => Task 6, Epoch 29/160 (LR 0.09211) => LSC_loss 1.16, Spatial_loss 1.88, Flat_loss 0.21, Train_acc 67.08, Test_acc 49.60
2025-01-05 13:26:18,335 [podnet.py] => Task 6, Epoch 30/160 (LR 0.09157) => LSC_loss 1.14, Spatial_loss 1.87, Flat_loss 0.21, Train_acc 67.62, Test_acc 53.91
2025-01-05 13:26:26,221 [podnet.py] => Task 6, Epoch 31/160 (LR 0.09102) => LSC_loss 1.14, Spatial_loss 1.86, Flat_loss 0.21, Train_acc 67.60, Test_acc 51.66
2025-01-05 13:26:34,252 [podnet.py] => Task 6, Epoch 32/160 (LR 0.09045) => LSC_loss 1.13, Spatial_loss 1.86, Flat_loss 0.21, Train_acc 68.17, Test_acc 53.69
2025-01-05 13:26:42,187 [podnet.py] => Task 6, Epoch 33/160 (LR 0.08987) => LSC_loss 1.14, Spatial_loss 1.86, Flat_loss 0.21, Train_acc 68.32, Test_acc 50.83
2025-01-05 13:26:50,126 [podnet.py] => Task 6, Epoch 34/160 (LR 0.08927) => LSC_loss 1.13, Spatial_loss 1.86, Flat_loss 0.21, Train_acc 68.11, Test_acc 44.74
2025-01-05 13:26:58,140 [podnet.py] => Task 6, Epoch 35/160 (LR 0.08865) => LSC_loss 1.13, Spatial_loss 1.88, Flat_loss 0.21, Train_acc 68.22, Test_acc 53.06
2025-01-05 13:27:05,820 [podnet.py] => Task 6, Epoch 36/160 (LR 0.08802) => LSC_loss 1.14, Spatial_loss 1.89, Flat_loss 0.21, Train_acc 67.83, Test_acc 53.17
2025-01-05 13:27:13,827 [podnet.py] => Task 6, Epoch 37/160 (LR 0.08738) => LSC_loss 1.13, Spatial_loss 1.87, Flat_loss 0.21, Train_acc 68.07, Test_acc 53.69
2025-01-05 13:27:21,767 [podnet.py] => Task 6, Epoch 38/160 (LR 0.08672) => LSC_loss 1.13, Spatial_loss 1.81, Flat_loss 0.20, Train_acc 68.09, Test_acc 54.69
2025-01-05 13:27:29,884 [podnet.py] => Task 6, Epoch 39/160 (LR 0.08604) => LSC_loss 1.10, Spatial_loss 1.86, Flat_loss 0.20, Train_acc 68.57, Test_acc 50.49
2025-01-05 13:27:37,873 [podnet.py] => Task 6, Epoch 40/160 (LR 0.08536) => LSC_loss 1.09, Spatial_loss 1.83, Flat_loss 0.20, Train_acc 69.08, Test_acc 53.43
2025-01-05 13:27:45,989 [podnet.py] => Task 6, Epoch 41/160 (LR 0.08465) => LSC_loss 1.11, Spatial_loss 1.87, Flat_loss 0.20, Train_acc 68.36, Test_acc 53.94
2025-01-05 13:27:53,938 [podnet.py] => Task 6, Epoch 42/160 (LR 0.08394) => LSC_loss 1.10, Spatial_loss 1.85, Flat_loss 0.20, Train_acc 68.87, Test_acc 51.23
2025-01-05 13:28:02,074 [podnet.py] => Task 6, Epoch 43/160 (LR 0.08321) => LSC_loss 1.11, Spatial_loss 1.85, Flat_loss 0.20, Train_acc 68.84, Test_acc 49.49
2025-01-05 13:28:09,861 [podnet.py] => Task 6, Epoch 44/160 (LR 0.08247) => LSC_loss 1.10, Spatial_loss 1.84, Flat_loss 0.21, Train_acc 68.50, Test_acc 54.49
2025-01-05 13:28:17,904 [podnet.py] => Task 6, Epoch 45/160 (LR 0.08172) => LSC_loss 1.09, Spatial_loss 1.81, Flat_loss 0.20, Train_acc 68.97, Test_acc 51.43
2025-01-05 13:28:25,978 [podnet.py] => Task 6, Epoch 46/160 (LR 0.08095) => LSC_loss 1.08, Spatial_loss 1.80, Flat_loss 0.20, Train_acc 69.35, Test_acc 52.71
2025-01-05 13:28:33,815 [podnet.py] => Task 6, Epoch 47/160 (LR 0.08018) => LSC_loss 1.09, Spatial_loss 1.83, Flat_loss 0.20, Train_acc 68.87, Test_acc 47.89
2025-01-05 13:28:41,668 [podnet.py] => Task 6, Epoch 48/160 (LR 0.07939) => LSC_loss 1.08, Spatial_loss 1.81, Flat_loss 0.20, Train_acc 69.53, Test_acc 53.97
2025-01-05 13:28:49,662 [podnet.py] => Task 6, Epoch 49/160 (LR 0.07859) => LSC_loss 1.06, Spatial_loss 1.78, Flat_loss 0.20, Train_acc 69.92, Test_acc 53.06
2025-01-05 13:28:57,582 [podnet.py] => Task 6, Epoch 50/160 (LR 0.07778) => LSC_loss 1.07, Spatial_loss 1.79, Flat_loss 0.20, Train_acc 69.76, Test_acc 51.89
2025-01-05 13:29:05,579 [podnet.py] => Task 6, Epoch 51/160 (LR 0.07696) => LSC_loss 1.06, Spatial_loss 1.79, Flat_loss 0.20, Train_acc 70.05, Test_acc 51.06
2025-01-05 13:29:13,293 [podnet.py] => Task 6, Epoch 52/160 (LR 0.07612) => LSC_loss 1.06, Spatial_loss 1.79, Flat_loss 0.20, Train_acc 69.49, Test_acc 49.37
2025-01-05 13:29:21,202 [podnet.py] => Task 6, Epoch 53/160 (LR 0.07528) => LSC_loss 1.05, Spatial_loss 1.77, Flat_loss 0.19, Train_acc 70.41, Test_acc 52.23
2025-01-05 13:29:29,110 [podnet.py] => Task 6, Epoch 54/160 (LR 0.07443) => LSC_loss 1.06, Spatial_loss 1.78, Flat_loss 0.20, Train_acc 70.08, Test_acc 51.60
2025-01-05 13:29:36,877 [podnet.py] => Task 6, Epoch 55/160 (LR 0.07357) => LSC_loss 1.06, Spatial_loss 1.78, Flat_loss 0.20, Train_acc 69.87, Test_acc 51.37
2025-01-05 13:29:44,734 [podnet.py] => Task 6, Epoch 56/160 (LR 0.07270) => LSC_loss 1.05, Spatial_loss 1.78, Flat_loss 0.19, Train_acc 70.73, Test_acc 54.26
2025-01-05 13:29:52,584 [podnet.py] => Task 6, Epoch 57/160 (LR 0.07182) => LSC_loss 1.03, Spatial_loss 1.75, Flat_loss 0.19, Train_acc 70.75, Test_acc 53.80
2025-01-05 13:30:00,304 [podnet.py] => Task 6, Epoch 58/160 (LR 0.07093) => LSC_loss 1.03, Spatial_loss 1.78, Flat_loss 0.19, Train_acc 70.52, Test_acc 54.60
2025-01-05 13:30:08,179 [podnet.py] => Task 6, Epoch 59/160 (LR 0.07004) => LSC_loss 1.04, Spatial_loss 1.78, Flat_loss 0.19, Train_acc 70.79, Test_acc 46.23
2025-01-05 13:30:16,230 [podnet.py] => Task 6, Epoch 60/160 (LR 0.06913) => LSC_loss 1.02, Spatial_loss 1.75, Flat_loss 0.19, Train_acc 70.80, Test_acc 48.91
2025-01-05 13:30:24,389 [podnet.py] => Task 6, Epoch 61/160 (LR 0.06822) => LSC_loss 1.03, Spatial_loss 1.73, Flat_loss 0.19, Train_acc 70.93, Test_acc 52.83
2025-01-05 13:30:32,300 [podnet.py] => Task 6, Epoch 62/160 (LR 0.06731) => LSC_loss 1.01, Spatial_loss 1.74, Flat_loss 0.19, Train_acc 71.65, Test_acc 54.34
2025-01-05 13:30:40,297 [podnet.py] => Task 6, Epoch 63/160 (LR 0.06638) => LSC_loss 1.00, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 71.47, Test_acc 53.80
2025-01-05 13:30:48,439 [podnet.py] => Task 6, Epoch 64/160 (LR 0.06545) => LSC_loss 1.02, Spatial_loss 1.75, Flat_loss 0.19, Train_acc 70.56, Test_acc 51.91
2025-01-05 13:30:56,321 [podnet.py] => Task 6, Epoch 65/160 (LR 0.06451) => LSC_loss 1.00, Spatial_loss 1.73, Flat_loss 0.19, Train_acc 71.36, Test_acc 55.86
2025-01-05 13:31:03,983 [podnet.py] => Task 6, Epoch 66/160 (LR 0.06357) => LSC_loss 0.99, Spatial_loss 1.74, Flat_loss 0.19, Train_acc 71.63, Test_acc 51.23
2025-01-05 13:31:11,897 [podnet.py] => Task 6, Epoch 67/160 (LR 0.06262) => LSC_loss 1.00, Spatial_loss 1.72, Flat_loss 0.18, Train_acc 71.31, Test_acc 52.09
2025-01-05 13:31:19,605 [podnet.py] => Task 6, Epoch 68/160 (LR 0.06167) => LSC_loss 0.99, Spatial_loss 1.73, Flat_loss 0.19, Train_acc 71.75, Test_acc 52.40
2025-01-05 13:31:27,497 [podnet.py] => Task 6, Epoch 69/160 (LR 0.06072) => LSC_loss 0.98, Spatial_loss 1.68, Flat_loss 0.18, Train_acc 72.22, Test_acc 53.71
2025-01-05 13:31:35,401 [podnet.py] => Task 6, Epoch 70/160 (LR 0.05975) => LSC_loss 0.94, Spatial_loss 1.65, Flat_loss 0.18, Train_acc 73.08, Test_acc 50.20
2025-01-05 13:31:43,231 [podnet.py] => Task 6, Epoch 71/160 (LR 0.05879) => LSC_loss 0.95, Spatial_loss 1.67, Flat_loss 0.18, Train_acc 72.86, Test_acc 53.60
2025-01-05 13:31:50,971 [podnet.py] => Task 6, Epoch 72/160 (LR 0.05782) => LSC_loss 0.96, Spatial_loss 1.69, Flat_loss 0.18, Train_acc 72.81, Test_acc 53.97
2025-01-05 13:31:58,832 [podnet.py] => Task 6, Epoch 73/160 (LR 0.05685) => LSC_loss 0.95, Spatial_loss 1.67, Flat_loss 0.18, Train_acc 73.12, Test_acc 53.91
2025-01-05 13:32:07,112 [podnet.py] => Task 6, Epoch 74/160 (LR 0.05588) => LSC_loss 0.94, Spatial_loss 1.64, Flat_loss 0.17, Train_acc 73.06, Test_acc 51.89
2025-01-05 13:32:15,455 [podnet.py] => Task 6, Epoch 75/160 (LR 0.05490) => LSC_loss 0.96, Spatial_loss 1.65, Flat_loss 0.18, Train_acc 73.08, Test_acc 54.57
2025-01-05 13:32:23,556 [podnet.py] => Task 6, Epoch 76/160 (LR 0.05392) => LSC_loss 0.91, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 74.15, Test_acc 55.29
2025-01-05 13:32:31,393 [podnet.py] => Task 6, Epoch 77/160 (LR 0.05294) => LSC_loss 0.92, Spatial_loss 1.64, Flat_loss 0.17, Train_acc 73.90, Test_acc 51.77
2025-01-05 13:32:39,592 [podnet.py] => Task 6, Epoch 78/160 (LR 0.05196) => LSC_loss 0.93, Spatial_loss 1.64, Flat_loss 0.18, Train_acc 73.21, Test_acc 55.71
2025-01-05 13:32:47,237 [podnet.py] => Task 6, Epoch 79/160 (LR 0.05098) => LSC_loss 0.92, Spatial_loss 1.63, Flat_loss 0.17, Train_acc 73.85, Test_acc 50.80
2025-01-05 13:32:55,195 [podnet.py] => Task 6, Epoch 80/160 (LR 0.05000) => LSC_loss 0.92, Spatial_loss 1.61, Flat_loss 0.17, Train_acc 73.86, Test_acc 55.20
2025-01-05 13:33:03,233 [podnet.py] => Task 6, Epoch 81/160 (LR 0.04902) => LSC_loss 0.90, Spatial_loss 1.62, Flat_loss 0.17, Train_acc 74.30, Test_acc 52.40
2025-01-05 13:33:11,190 [podnet.py] => Task 6, Epoch 82/160 (LR 0.04804) => LSC_loss 0.89, Spatial_loss 1.60, Flat_loss 0.17, Train_acc 74.59, Test_acc 57.34
2025-01-05 13:33:19,280 [podnet.py] => Task 6, Epoch 83/160 (LR 0.04706) => LSC_loss 0.87, Spatial_loss 1.57, Flat_loss 0.16, Train_acc 75.63, Test_acc 54.46
2025-01-05 13:33:27,229 [podnet.py] => Task 6, Epoch 84/160 (LR 0.04608) => LSC_loss 0.87, Spatial_loss 1.58, Flat_loss 0.16, Train_acc 75.28, Test_acc 55.03
2025-01-05 13:33:35,327 [podnet.py] => Task 6, Epoch 85/160 (LR 0.04510) => LSC_loss 0.88, Spatial_loss 1.59, Flat_loss 0.16, Train_acc 75.08, Test_acc 55.00
2025-01-05 13:33:43,159 [podnet.py] => Task 6, Epoch 86/160 (LR 0.04412) => LSC_loss 0.86, Spatial_loss 1.53, Flat_loss 0.16, Train_acc 75.58, Test_acc 52.51
2025-01-05 13:33:51,104 [podnet.py] => Task 6, Epoch 87/160 (LR 0.04315) => LSC_loss 0.85, Spatial_loss 1.57, Flat_loss 0.16, Train_acc 76.19, Test_acc 51.94
2025-01-05 13:33:59,121 [podnet.py] => Task 6, Epoch 88/160 (LR 0.04218) => LSC_loss 0.85, Spatial_loss 1.53, Flat_loss 0.16, Train_acc 75.64, Test_acc 51.74
2025-01-05 13:34:07,281 [podnet.py] => Task 6, Epoch 89/160 (LR 0.04121) => LSC_loss 0.83, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 76.19, Test_acc 57.46
2025-01-05 13:34:15,430 [podnet.py] => Task 6, Epoch 90/160 (LR 0.04025) => LSC_loss 0.83, Spatial_loss 1.51, Flat_loss 0.15, Train_acc 76.27, Test_acc 57.14
2025-01-05 13:34:23,446 [podnet.py] => Task 6, Epoch 91/160 (LR 0.03928) => LSC_loss 0.82, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 76.47, Test_acc 51.60
2025-01-05 13:34:31,448 [podnet.py] => Task 6, Epoch 92/160 (LR 0.03833) => LSC_loss 0.82, Spatial_loss 1.52, Flat_loss 0.15, Train_acc 76.87, Test_acc 56.54
2025-01-05 13:34:39,408 [podnet.py] => Task 6, Epoch 93/160 (LR 0.03738) => LSC_loss 0.80, Spatial_loss 1.49, Flat_loss 0.15, Train_acc 76.81, Test_acc 57.00
2025-01-05 13:34:47,522 [podnet.py] => Task 6, Epoch 94/160 (LR 0.03643) => LSC_loss 0.79, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 77.08, Test_acc 54.86
2025-01-05 13:34:55,511 [podnet.py] => Task 6, Epoch 95/160 (LR 0.03549) => LSC_loss 0.79, Spatial_loss 1.50, Flat_loss 0.15, Train_acc 77.65, Test_acc 55.89
2025-01-05 13:35:03,353 [podnet.py] => Task 6, Epoch 96/160 (LR 0.03455) => LSC_loss 0.77, Spatial_loss 1.44, Flat_loss 0.15, Train_acc 77.74, Test_acc 55.80
2025-01-05 13:35:11,279 [podnet.py] => Task 6, Epoch 97/160 (LR 0.03362) => LSC_loss 0.77, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 78.19, Test_acc 55.60
2025-01-05 13:35:19,320 [podnet.py] => Task 6, Epoch 98/160 (LR 0.03269) => LSC_loss 0.77, Spatial_loss 1.43, Flat_loss 0.14, Train_acc 78.24, Test_acc 55.89
2025-01-05 13:35:27,161 [podnet.py] => Task 6, Epoch 99/160 (LR 0.03178) => LSC_loss 0.75, Spatial_loss 1.41, Flat_loss 0.14, Train_acc 78.68, Test_acc 54.46
2025-01-05 13:35:34,975 [podnet.py] => Task 6, Epoch 100/160 (LR 0.03087) => LSC_loss 0.74, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 78.89, Test_acc 56.60
2025-01-05 13:35:43,015 [podnet.py] => Task 6, Epoch 101/160 (LR 0.02996) => LSC_loss 0.74, Spatial_loss 1.42, Flat_loss 0.14, Train_acc 79.05, Test_acc 57.09
2025-01-05 13:35:51,117 [podnet.py] => Task 6, Epoch 102/160 (LR 0.02907) => LSC_loss 0.72, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 79.69, Test_acc 57.97
2025-01-05 13:35:59,120 [podnet.py] => Task 6, Epoch 103/160 (LR 0.02818) => LSC_loss 0.70, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 79.89, Test_acc 56.80
2025-01-05 13:36:07,207 [podnet.py] => Task 6, Epoch 104/160 (LR 0.02730) => LSC_loss 0.70, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 80.12, Test_acc 57.77
2025-01-05 13:36:15,213 [podnet.py] => Task 6, Epoch 105/160 (LR 0.02643) => LSC_loss 0.71, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 79.86, Test_acc 54.43
2025-01-05 13:36:23,020 [podnet.py] => Task 6, Epoch 106/160 (LR 0.02557) => LSC_loss 0.70, Spatial_loss 1.36, Flat_loss 0.13, Train_acc 80.18, Test_acc 58.26
2025-01-05 13:36:30,976 [podnet.py] => Task 6, Epoch 107/160 (LR 0.02472) => LSC_loss 0.68, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 80.71, Test_acc 55.69
2025-01-05 13:36:39,097 [podnet.py] => Task 6, Epoch 108/160 (LR 0.02388) => LSC_loss 0.68, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 80.90, Test_acc 57.77
2025-01-05 13:36:47,233 [podnet.py] => Task 6, Epoch 109/160 (LR 0.02304) => LSC_loss 0.68, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 80.89, Test_acc 56.17
2025-01-05 13:36:55,234 [podnet.py] => Task 6, Epoch 110/160 (LR 0.02222) => LSC_loss 0.67, Spatial_loss 1.30, Flat_loss 0.12, Train_acc 81.30, Test_acc 57.09
2025-01-05 13:37:03,111 [podnet.py] => Task 6, Epoch 111/160 (LR 0.02141) => LSC_loss 0.62, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 82.57, Test_acc 58.11
2025-01-05 13:37:11,173 [podnet.py] => Task 6, Epoch 112/160 (LR 0.02061) => LSC_loss 0.63, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 82.50, Test_acc 57.20
2025-01-05 13:37:19,194 [podnet.py] => Task 6, Epoch 113/160 (LR 0.01982) => LSC_loss 0.62, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 82.67, Test_acc 58.51
2025-01-05 13:37:27,011 [podnet.py] => Task 6, Epoch 114/160 (LR 0.01905) => LSC_loss 0.60, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 83.21, Test_acc 58.77
2025-01-05 13:37:34,849 [podnet.py] => Task 6, Epoch 115/160 (LR 0.01828) => LSC_loss 0.60, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 83.31, Test_acc 59.94
2025-01-05 13:37:42,788 [podnet.py] => Task 6, Epoch 116/160 (LR 0.01753) => LSC_loss 0.59, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 83.76, Test_acc 58.23
2025-01-05 13:37:50,642 [podnet.py] => Task 6, Epoch 117/160 (LR 0.01679) => LSC_loss 0.59, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 83.21, Test_acc 58.91
2025-01-05 13:37:58,602 [podnet.py] => Task 6, Epoch 118/160 (LR 0.01606) => LSC_loss 0.56, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 84.31, Test_acc 59.69
2025-01-05 13:38:06,745 [podnet.py] => Task 6, Epoch 119/160 (LR 0.01535) => LSC_loss 0.58, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 83.65, Test_acc 56.49
2025-01-05 13:38:15,008 [podnet.py] => Task 6, Epoch 120/160 (LR 0.01464) => LSC_loss 0.56, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 84.39, Test_acc 59.00
2025-01-05 13:38:23,056 [podnet.py] => Task 6, Epoch 121/160 (LR 0.01396) => LSC_loss 0.57, Spatial_loss 1.14, Flat_loss 0.10, Train_acc 84.60, Test_acc 58.60
2025-01-05 13:38:31,102 [podnet.py] => Task 6, Epoch 122/160 (LR 0.01328) => LSC_loss 0.53, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 85.56, Test_acc 58.94
2025-01-05 13:38:39,090 [podnet.py] => Task 6, Epoch 123/160 (LR 0.01262) => LSC_loss 0.53, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 85.07, Test_acc 58.97
2025-01-05 13:38:47,089 [podnet.py] => Task 6, Epoch 124/160 (LR 0.01198) => LSC_loss 0.51, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 85.95, Test_acc 58.66
2025-01-05 13:38:55,178 [podnet.py] => Task 6, Epoch 125/160 (LR 0.01135) => LSC_loss 0.52, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 85.56, Test_acc 59.54
2025-01-05 13:39:03,210 [podnet.py] => Task 6, Epoch 126/160 (LR 0.01073) => LSC_loss 0.51, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 85.82, Test_acc 59.49
2025-01-05 13:39:10,895 [podnet.py] => Task 6, Epoch 127/160 (LR 0.01013) => LSC_loss 0.50, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 86.49, Test_acc 59.89
2025-01-05 13:39:18,845 [podnet.py] => Task 6, Epoch 128/160 (LR 0.00955) => LSC_loss 0.49, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 86.96, Test_acc 60.51
2025-01-05 13:39:26,827 [podnet.py] => Task 6, Epoch 129/160 (LR 0.00898) => LSC_loss 0.48, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 86.88, Test_acc 60.51
2025-01-05 13:39:34,687 [podnet.py] => Task 6, Epoch 130/160 (LR 0.00843) => LSC_loss 0.47, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 87.01, Test_acc 60.06
2025-01-05 13:39:42,858 [podnet.py] => Task 6, Epoch 131/160 (LR 0.00789) => LSC_loss 0.48, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 87.01, Test_acc 60.17
2025-01-05 13:39:51,062 [podnet.py] => Task 6, Epoch 132/160 (LR 0.00737) => LSC_loss 0.46, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 87.76, Test_acc 60.37
2025-01-05 13:39:59,055 [podnet.py] => Task 6, Epoch 133/160 (LR 0.00686) => LSC_loss 0.45, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 88.03, Test_acc 59.23
2025-01-05 13:40:06,764 [podnet.py] => Task 6, Epoch 134/160 (LR 0.00638) => LSC_loss 0.45, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 87.80, Test_acc 59.60
2025-01-05 13:40:14,678 [podnet.py] => Task 6, Epoch 135/160 (LR 0.00590) => LSC_loss 0.45, Spatial_loss 0.97, Flat_loss 0.09, Train_acc 87.92, Test_acc 60.29
2025-01-05 13:40:22,576 [podnet.py] => Task 6, Epoch 136/160 (LR 0.00545) => LSC_loss 0.44, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.42, Test_acc 60.69
2025-01-05 13:40:30,556 [podnet.py] => Task 6, Epoch 137/160 (LR 0.00501) => LSC_loss 0.43, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 88.84, Test_acc 60.23
2025-01-05 13:40:38,515 [podnet.py] => Task 6, Epoch 138/160 (LR 0.00459) => LSC_loss 0.42, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.10, Test_acc 61.29
2025-01-05 13:40:46,395 [podnet.py] => Task 6, Epoch 139/160 (LR 0.00419) => LSC_loss 0.42, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.02, Test_acc 60.91
2025-01-05 13:40:54,323 [podnet.py] => Task 6, Epoch 140/160 (LR 0.00381) => LSC_loss 0.42, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.19, Test_acc 61.57
2025-01-05 13:41:02,173 [podnet.py] => Task 6, Epoch 141/160 (LR 0.00344) => LSC_loss 0.41, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 89.51, Test_acc 61.31
2025-01-05 13:41:10,230 [podnet.py] => Task 6, Epoch 142/160 (LR 0.00309) => LSC_loss 0.41, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.39, Test_acc 61.03
2025-01-05 13:41:18,119 [podnet.py] => Task 6, Epoch 143/160 (LR 0.00276) => LSC_loss 0.40, Spatial_loss 0.87, Flat_loss 0.08, Train_acc 89.86, Test_acc 60.97
2025-01-05 13:41:26,125 [podnet.py] => Task 6, Epoch 144/160 (LR 0.00245) => LSC_loss 0.40, Spatial_loss 0.87, Flat_loss 0.08, Train_acc 90.04, Test_acc 61.31
2025-01-05 13:41:34,071 [podnet.py] => Task 6, Epoch 145/160 (LR 0.00215) => LSC_loss 0.40, Spatial_loss 0.86, Flat_loss 0.08, Train_acc 89.85, Test_acc 60.94
2025-01-05 13:41:41,652 [podnet.py] => Task 6, Epoch 146/160 (LR 0.00188) => LSC_loss 0.40, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 90.06, Test_acc 61.40
2025-01-05 13:41:49,555 [podnet.py] => Task 6, Epoch 147/160 (LR 0.00162) => LSC_loss 0.38, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 90.47, Test_acc 60.83
2025-01-05 13:41:57,383 [podnet.py] => Task 6, Epoch 148/160 (LR 0.00138) => LSC_loss 0.39, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 90.31, Test_acc 61.51
2025-01-05 13:42:05,254 [podnet.py] => Task 6, Epoch 149/160 (LR 0.00116) => LSC_loss 0.39, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 90.19, Test_acc 61.49
2025-01-05 13:42:13,285 [podnet.py] => Task 6, Epoch 150/160 (LR 0.00096) => LSC_loss 0.38, Spatial_loss 0.84, Flat_loss 0.07, Train_acc 90.70, Test_acc 61.37
2025-01-05 13:42:21,329 [podnet.py] => Task 6, Epoch 151/160 (LR 0.00078) => LSC_loss 0.37, Spatial_loss 0.85, Flat_loss 0.07, Train_acc 90.81, Test_acc 61.51
2025-01-05 13:42:29,422 [podnet.py] => Task 6, Epoch 152/160 (LR 0.00062) => LSC_loss 0.38, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 90.54, Test_acc 61.37
2025-01-05 13:42:37,276 [podnet.py] => Task 6, Epoch 153/160 (LR 0.00047) => LSC_loss 0.38, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 90.54, Test_acc 60.89
2025-01-05 13:42:44,972 [podnet.py] => Task 6, Epoch 154/160 (LR 0.00035) => LSC_loss 0.38, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 90.49, Test_acc 61.06
2025-01-05 13:42:52,881 [podnet.py] => Task 6, Epoch 155/160 (LR 0.00024) => LSC_loss 0.38, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 90.64, Test_acc 61.23
2025-01-05 13:43:00,805 [podnet.py] => Task 6, Epoch 156/160 (LR 0.00015) => LSC_loss 0.37, Spatial_loss 0.81, Flat_loss 0.07, Train_acc 91.09, Test_acc 61.49
2025-01-05 13:43:08,778 [podnet.py] => Task 6, Epoch 157/160 (LR 0.00009) => LSC_loss 0.37, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 90.90, Test_acc 61.34
2025-01-05 13:43:16,631 [podnet.py] => Task 6, Epoch 158/160 (LR 0.00004) => LSC_loss 0.37, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 90.98, Test_acc 61.20
2025-01-05 13:43:24,443 [podnet.py] => Task 6, Epoch 159/160 (LR 0.00001) => LSC_loss 0.37, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 90.73, Test_acc 61.37
2025-01-05 13:43:32,057 [podnet.py] => Task 6, Epoch 160/160 (LR 0.00000) => LSC_loss 0.38, Spatial_loss 0.80, Flat_loss 0.07, Train_acc 90.66, Test_acc 61.23
2025-01-05 13:43:32,057 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 13:43:32,057 [base.py] => Reducing exemplars...(449 per classes)
2025-01-05 13:43:58,247 [base.py] => Constructing exemplars...(449 per classes)
2025-01-05 13:44:07,464 [podnet.py] => The size of finetune dataset: 15715
2025-01-05 13:44:15,089 [podnet.py] => Task 6, Epoch 1/20 (LR 0.00497) => LSC_loss 0.40, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.76, Test_acc 59.74
2025-01-05 13:44:23,084 [podnet.py] => Task 6, Epoch 2/20 (LR 0.00488) => LSC_loss 0.41, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.38, Test_acc 60.69
2025-01-05 13:44:30,855 [podnet.py] => Task 6, Epoch 3/20 (LR 0.00473) => LSC_loss 0.40, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 90.14, Test_acc 60.80
2025-01-05 13:44:38,547 [podnet.py] => Task 6, Epoch 4/20 (LR 0.00452) => LSC_loss 0.40, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.65, Test_acc 61.14
2025-01-05 13:44:46,324 [podnet.py] => Task 6, Epoch 5/20 (LR 0.00427) => LSC_loss 0.40, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.38, Test_acc 60.54
2025-01-05 13:44:54,120 [podnet.py] => Task 6, Epoch 6/20 (LR 0.00397) => LSC_loss 0.39, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 89.97, Test_acc 60.31
2025-01-05 13:45:02,063 [podnet.py] => Task 6, Epoch 7/20 (LR 0.00363) => LSC_loss 0.40, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.65, Test_acc 61.09
2025-01-05 13:45:09,976 [podnet.py] => Task 6, Epoch 8/20 (LR 0.00327) => LSC_loss 0.38, Spatial_loss 0.89, Flat_loss 0.07, Train_acc 90.35, Test_acc 60.37
2025-01-05 13:45:18,072 [podnet.py] => Task 6, Epoch 9/20 (LR 0.00289) => LSC_loss 0.39, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 90.07, Test_acc 60.20
2025-01-05 13:45:25,857 [podnet.py] => Task 6, Epoch 10/20 (LR 0.00250) => LSC_loss 0.38, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 90.19, Test_acc 60.77
2025-01-05 13:45:33,674 [podnet.py] => Task 6, Epoch 11/20 (LR 0.00211) => LSC_loss 0.38, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 90.60, Test_acc 61.14
2025-01-05 13:45:41,337 [podnet.py] => Task 6, Epoch 12/20 (LR 0.00173) => LSC_loss 0.37, Spatial_loss 0.84, Flat_loss 0.07, Train_acc 90.53, Test_acc 60.49
2025-01-05 13:45:49,251 [podnet.py] => Task 6, Epoch 13/20 (LR 0.00137) => LSC_loss 0.38, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 90.63, Test_acc 61.60
2025-01-05 13:45:57,318 [podnet.py] => Task 6, Epoch 14/20 (LR 0.00103) => LSC_loss 0.36, Spatial_loss 0.83, Flat_loss 0.07, Train_acc 90.81, Test_acc 61.03
2025-01-05 13:46:05,103 [podnet.py] => Task 6, Epoch 15/20 (LR 0.00073) => LSC_loss 0.36, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 90.99, Test_acc 61.51
2025-01-05 13:46:13,060 [podnet.py] => Task 6, Epoch 16/20 (LR 0.00048) => LSC_loss 0.36, Spatial_loss 0.81, Flat_loss 0.07, Train_acc 91.18, Test_acc 61.57
2025-01-05 13:46:20,946 [podnet.py] => Task 6, Epoch 17/20 (LR 0.00027) => LSC_loss 0.36, Spatial_loss 0.82, Flat_loss 0.07, Train_acc 91.08, Test_acc 61.34
2025-01-05 13:46:28,769 [podnet.py] => Task 6, Epoch 18/20 (LR 0.00012) => LSC_loss 0.36, Spatial_loss 0.81, Flat_loss 0.07, Train_acc 91.16, Test_acc 61.43
2025-01-05 13:46:36,718 [podnet.py] => Task 6, Epoch 19/20 (LR 0.00003) => LSC_loss 0.35, Spatial_loss 0.79, Flat_loss 0.07, Train_acc 91.32, Test_acc 61.43
2025-01-05 13:46:44,575 [podnet.py] => Task 6, Epoch 20/20 (LR 0.00000) => LSC_loss 0.36, Spatial_loss 0.79, Flat_loss 0.07, Train_acc 91.12, Test_acc 61.26
2025-01-05 13:46:44,579 [base.py] => Reducing exemplars...(385 per classes)
2025-01-05 13:47:10,669 [base.py] => Constructing exemplars...(385 per classes)
2025-01-05 13:47:21,438 [podnet.py] => Exemplar size: 13475
2025-01-05 13:47:21,438 [trainer.py] => CNN: {'total': np.float64(61.26), '00-09': np.float64(71.4), '10-19': np.float64(54.3), '20-29': np.float64(66.0), '30-39': np.float64(45.4), 'old': np.float64(63.9), 'new': np.float64(45.4)}
2025-01-05 13:47:21,438 [trainer.py] => NME: {'total': np.float64(60.23), '00-09': np.float64(71.0), '10-19': np.float64(51.1), '20-29': np.float64(64.1), '30-39': np.float64(49.2), 'old': np.float64(62.07), 'new': np.float64(49.2)}
2025-01-05 13:47:21,438 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26)]
2025-01-05 13:47:21,438 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09)]
2025-01-05 13:47:21,438 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23)]
2025-01-05 13:47:21,438 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66)]

2025-01-05 13:47:21,438 [trainer.py] => All params: 488657
2025-01-05 13:47:21,438 [trainer.py] => Trainable params: 488657
2025-01-05 13:47:21,439 [podnet.py] => Learning on 35-40
2025-01-05 13:47:21,485 [podnet.py] => Adaptive factor: 2.8284271247461903
2025-01-05 13:47:29,441 [podnet.py] => Task 7, Epoch 1/160 (LR 0.09999) => LSC_loss 2.07, Spatial_loss 2.79, Flat_loss 0.45, Train_acc 46.44, Test_acc 38.92
2025-01-05 13:47:37,331 [podnet.py] => Task 7, Epoch 2/160 (LR 0.09996) => LSC_loss 1.65, Spatial_loss 2.41, Flat_loss 0.33, Train_acc 53.98, Test_acc 45.90
2025-01-05 13:47:45,167 [podnet.py] => Task 7, Epoch 3/160 (LR 0.09991) => LSC_loss 1.53, Spatial_loss 2.26, Flat_loss 0.30, Train_acc 56.91, Test_acc 46.70
2025-01-05 13:47:53,288 [podnet.py] => Task 7, Epoch 4/160 (LR 0.09985) => LSC_loss 1.48, Spatial_loss 2.19, Flat_loss 0.28, Train_acc 58.47, Test_acc 43.20
2025-01-05 13:48:01,481 [podnet.py] => Task 7, Epoch 5/160 (LR 0.09976) => LSC_loss 1.47, Spatial_loss 2.21, Flat_loss 0.28, Train_acc 59.36, Test_acc 43.28
2025-01-05 13:48:09,217 [podnet.py] => Task 7, Epoch 6/160 (LR 0.09965) => LSC_loss 1.42, Spatial_loss 2.11, Flat_loss 0.26, Train_acc 60.60, Test_acc 48.35
2025-01-05 13:48:17,196 [podnet.py] => Task 7, Epoch 7/160 (LR 0.09953) => LSC_loss 1.40, Spatial_loss 2.11, Flat_loss 0.26, Train_acc 60.66, Test_acc 47.55
2025-01-05 13:48:25,173 [podnet.py] => Task 7, Epoch 8/160 (LR 0.09938) => LSC_loss 1.38, Spatial_loss 2.04, Flat_loss 0.25, Train_acc 61.43, Test_acc 47.18
2025-01-05 13:48:33,124 [podnet.py] => Task 7, Epoch 9/160 (LR 0.09922) => LSC_loss 1.38, Spatial_loss 2.05, Flat_loss 0.26, Train_acc 61.64, Test_acc 42.78
2025-01-05 13:48:41,197 [podnet.py] => Task 7, Epoch 10/160 (LR 0.09904) => LSC_loss 1.36, Spatial_loss 2.04, Flat_loss 0.25, Train_acc 62.03, Test_acc 50.78
2025-01-05 13:48:49,193 [podnet.py] => Task 7, Epoch 11/160 (LR 0.09884) => LSC_loss 1.37, Spatial_loss 2.05, Flat_loss 0.25, Train_acc 61.88, Test_acc 45.88
2025-01-05 13:48:57,309 [podnet.py] => Task 7, Epoch 12/160 (LR 0.09862) => LSC_loss 1.33, Spatial_loss 2.07, Flat_loss 0.25, Train_acc 62.44, Test_acc 40.58
2025-01-05 13:49:06,016 [podnet.py] => Task 7, Epoch 13/160 (LR 0.09838) => LSC_loss 1.33, Spatial_loss 2.05, Flat_loss 0.25, Train_acc 63.17, Test_acc 49.78
2025-01-05 13:49:14,063 [podnet.py] => Task 7, Epoch 14/160 (LR 0.09812) => LSC_loss 1.32, Spatial_loss 2.03, Flat_loss 0.25, Train_acc 63.79, Test_acc 40.90
2025-01-05 13:49:21,834 [podnet.py] => Task 7, Epoch 15/160 (LR 0.09785) => LSC_loss 1.31, Spatial_loss 2.02, Flat_loss 0.25, Train_acc 63.49, Test_acc 48.15
2025-01-05 13:49:29,976 [podnet.py] => Task 7, Epoch 16/160 (LR 0.09755) => LSC_loss 1.32, Spatial_loss 2.04, Flat_loss 0.25, Train_acc 63.23, Test_acc 48.45
2025-01-05 13:49:37,864 [podnet.py] => Task 7, Epoch 17/160 (LR 0.09724) => LSC_loss 1.32, Spatial_loss 2.00, Flat_loss 0.25, Train_acc 63.19, Test_acc 47.98
2025-01-05 13:49:46,131 [podnet.py] => Task 7, Epoch 18/160 (LR 0.09691) => LSC_loss 1.30, Spatial_loss 2.04, Flat_loss 0.25, Train_acc 63.45, Test_acc 49.88
2025-01-05 13:49:54,109 [podnet.py] => Task 7, Epoch 19/160 (LR 0.09656) => LSC_loss 1.32, Spatial_loss 2.04, Flat_loss 0.25, Train_acc 63.10, Test_acc 49.68
2025-01-05 13:50:02,260 [podnet.py] => Task 7, Epoch 20/160 (LR 0.09619) => LSC_loss 1.31, Spatial_loss 2.03, Flat_loss 0.25, Train_acc 63.57, Test_acc 48.92
2025-01-05 13:50:10,254 [podnet.py] => Task 7, Epoch 21/160 (LR 0.09581) => LSC_loss 1.29, Spatial_loss 2.01, Flat_loss 0.24, Train_acc 63.92, Test_acc 52.05
2025-01-05 13:50:18,380 [podnet.py] => Task 7, Epoch 22/160 (LR 0.09541) => LSC_loss 1.29, Spatial_loss 1.99, Flat_loss 0.24, Train_acc 63.61, Test_acc 45.12
2025-01-05 13:50:26,418 [podnet.py] => Task 7, Epoch 23/160 (LR 0.09499) => LSC_loss 1.28, Spatial_loss 2.03, Flat_loss 0.24, Train_acc 64.11, Test_acc 46.82
2025-01-05 13:50:34,547 [podnet.py] => Task 7, Epoch 24/160 (LR 0.09455) => LSC_loss 1.27, Spatial_loss 1.98, Flat_loss 0.24, Train_acc 64.54, Test_acc 44.48
2025-01-05 13:50:42,342 [podnet.py] => Task 7, Epoch 25/160 (LR 0.09410) => LSC_loss 1.28, Spatial_loss 2.01, Flat_loss 0.24, Train_acc 64.33, Test_acc 48.40
2025-01-05 13:50:50,434 [podnet.py] => Task 7, Epoch 26/160 (LR 0.09362) => LSC_loss 1.26, Spatial_loss 2.00, Flat_loss 0.24, Train_acc 64.93, Test_acc 46.38
2025-01-05 13:50:58,222 [podnet.py] => Task 7, Epoch 27/160 (LR 0.09314) => LSC_loss 1.27, Spatial_loss 1.95, Flat_loss 0.24, Train_acc 64.79, Test_acc 46.65
2025-01-05 13:51:06,350 [podnet.py] => Task 7, Epoch 28/160 (LR 0.09263) => LSC_loss 1.27, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 64.68, Test_acc 48.30
2025-01-05 13:51:14,280 [podnet.py] => Task 7, Epoch 29/160 (LR 0.09211) => LSC_loss 1.28, Spatial_loss 2.01, Flat_loss 0.25, Train_acc 63.90, Test_acc 47.05
2025-01-05 13:51:22,314 [podnet.py] => Task 7, Epoch 30/160 (LR 0.09157) => LSC_loss 1.26, Spatial_loss 1.97, Flat_loss 0.24, Train_acc 64.63, Test_acc 48.00
2025-01-05 13:51:30,360 [podnet.py] => Task 7, Epoch 31/160 (LR 0.09102) => LSC_loss 1.25, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 64.94, Test_acc 48.00
2025-01-05 13:51:38,347 [podnet.py] => Task 7, Epoch 32/160 (LR 0.09045) => LSC_loss 1.27, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 64.51, Test_acc 43.65
2025-01-05 13:51:46,110 [podnet.py] => Task 7, Epoch 33/160 (LR 0.08987) => LSC_loss 1.26, Spatial_loss 1.98, Flat_loss 0.24, Train_acc 65.08, Test_acc 49.18
2025-01-05 13:51:54,008 [podnet.py] => Task 7, Epoch 34/160 (LR 0.08927) => LSC_loss 1.26, Spatial_loss 1.98, Flat_loss 0.24, Train_acc 64.93, Test_acc 51.98
2025-01-05 13:52:02,284 [podnet.py] => Task 7, Epoch 35/160 (LR 0.08865) => LSC_loss 1.23, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 65.50, Test_acc 39.85
2025-01-05 13:52:10,337 [podnet.py] => Task 7, Epoch 36/160 (LR 0.08802) => LSC_loss 1.24, Spatial_loss 1.99, Flat_loss 0.24, Train_acc 65.50, Test_acc 48.92
2025-01-05 13:52:18,397 [podnet.py] => Task 7, Epoch 37/160 (LR 0.08738) => LSC_loss 1.26, Spatial_loss 1.97, Flat_loss 0.24, Train_acc 65.01, Test_acc 49.88
2025-01-05 13:52:26,333 [podnet.py] => Task 7, Epoch 38/160 (LR 0.08672) => LSC_loss 1.23, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 65.58, Test_acc 49.12
2025-01-05 13:52:34,681 [podnet.py] => Task 7, Epoch 39/160 (LR 0.08604) => LSC_loss 1.21, Spatial_loss 1.95, Flat_loss 0.24, Train_acc 66.23, Test_acc 50.70
2025-01-05 13:52:42,745 [podnet.py] => Task 7, Epoch 40/160 (LR 0.08536) => LSC_loss 1.23, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 65.67, Test_acc 46.15
2025-01-05 13:52:50,758 [podnet.py] => Task 7, Epoch 41/160 (LR 0.08465) => LSC_loss 1.21, Spatial_loss 1.93, Flat_loss 0.23, Train_acc 66.44, Test_acc 46.10
2025-01-05 13:52:58,593 [podnet.py] => Task 7, Epoch 42/160 (LR 0.08394) => LSC_loss 1.22, Spatial_loss 1.96, Flat_loss 0.24, Train_acc 65.97, Test_acc 50.95
2025-01-05 13:53:06,562 [podnet.py] => Task 7, Epoch 43/160 (LR 0.08321) => LSC_loss 1.22, Spatial_loss 1.98, Flat_loss 0.24, Train_acc 66.23, Test_acc 49.35
2025-01-05 13:53:14,625 [podnet.py] => Task 7, Epoch 44/160 (LR 0.08247) => LSC_loss 1.20, Spatial_loss 1.95, Flat_loss 0.23, Train_acc 66.65, Test_acc 48.50
2025-01-05 13:53:22,773 [podnet.py] => Task 7, Epoch 45/160 (LR 0.08172) => LSC_loss 1.21, Spatial_loss 1.94, Flat_loss 0.23, Train_acc 66.48, Test_acc 48.70
2025-01-05 13:53:30,947 [podnet.py] => Task 7, Epoch 46/160 (LR 0.08095) => LSC_loss 1.20, Spatial_loss 1.93, Flat_loss 0.23, Train_acc 66.17, Test_acc 51.00
2025-01-05 13:53:38,771 [podnet.py] => Task 7, Epoch 47/160 (LR 0.08018) => LSC_loss 1.20, Spatial_loss 1.92, Flat_loss 0.23, Train_acc 66.40, Test_acc 49.65
2025-01-05 13:53:46,649 [podnet.py] => Task 7, Epoch 48/160 (LR 0.07939) => LSC_loss 1.19, Spatial_loss 1.91, Flat_loss 0.23, Train_acc 66.72, Test_acc 52.00
2025-01-05 13:53:54,615 [podnet.py] => Task 7, Epoch 49/160 (LR 0.07859) => LSC_loss 1.20, Spatial_loss 1.93, Flat_loss 0.23, Train_acc 66.35, Test_acc 48.10
2025-01-05 13:54:02,317 [podnet.py] => Task 7, Epoch 50/160 (LR 0.07778) => LSC_loss 1.17, Spatial_loss 1.91, Flat_loss 0.23, Train_acc 67.18, Test_acc 49.30
2025-01-05 13:54:10,458 [podnet.py] => Task 7, Epoch 51/160 (LR 0.07696) => LSC_loss 1.17, Spatial_loss 1.89, Flat_loss 0.22, Train_acc 67.26, Test_acc 49.20
2025-01-05 13:54:18,249 [podnet.py] => Task 7, Epoch 52/160 (LR 0.07612) => LSC_loss 1.17, Spatial_loss 1.90, Flat_loss 0.22, Train_acc 67.19, Test_acc 50.78
2025-01-05 13:54:26,463 [podnet.py] => Task 7, Epoch 53/160 (LR 0.07528) => LSC_loss 1.16, Spatial_loss 1.91, Flat_loss 0.22, Train_acc 67.67, Test_acc 50.08
2025-01-05 13:54:34,592 [podnet.py] => Task 7, Epoch 54/160 (LR 0.07443) => LSC_loss 1.17, Spatial_loss 1.91, Flat_loss 0.23, Train_acc 67.24, Test_acc 49.68
2025-01-05 13:54:42,642 [podnet.py] => Task 7, Epoch 55/160 (LR 0.07357) => LSC_loss 1.18, Spatial_loss 1.89, Flat_loss 0.23, Train_acc 66.96, Test_acc 48.18
2025-01-05 13:54:50,518 [podnet.py] => Task 7, Epoch 56/160 (LR 0.07270) => LSC_loss 1.15, Spatial_loss 1.90, Flat_loss 0.22, Train_acc 67.90, Test_acc 52.82
2025-01-05 13:54:58,620 [podnet.py] => Task 7, Epoch 57/160 (LR 0.07182) => LSC_loss 1.14, Spatial_loss 1.87, Flat_loss 0.22, Train_acc 67.96, Test_acc 50.12
2025-01-05 13:55:06,957 [podnet.py] => Task 7, Epoch 58/160 (LR 0.07093) => LSC_loss 1.14, Spatial_loss 1.86, Flat_loss 0.22, Train_acc 68.03, Test_acc 51.95
2025-01-05 13:55:15,058 [podnet.py] => Task 7, Epoch 59/160 (LR 0.07004) => LSC_loss 1.14, Spatial_loss 1.88, Flat_loss 0.22, Train_acc 68.17, Test_acc 50.38
2025-01-05 13:55:23,178 [podnet.py] => Task 7, Epoch 60/160 (LR 0.06913) => LSC_loss 1.12, Spatial_loss 1.84, Flat_loss 0.22, Train_acc 68.81, Test_acc 50.90
2025-01-05 13:55:31,278 [podnet.py] => Task 7, Epoch 61/160 (LR 0.06822) => LSC_loss 1.13, Spatial_loss 1.85, Flat_loss 0.22, Train_acc 68.37, Test_acc 50.48
2025-01-05 13:55:39,227 [podnet.py] => Task 7, Epoch 62/160 (LR 0.06731) => LSC_loss 1.13, Spatial_loss 1.85, Flat_loss 0.22, Train_acc 68.48, Test_acc 48.70
2025-01-05 13:55:47,245 [podnet.py] => Task 7, Epoch 63/160 (LR 0.06638) => LSC_loss 1.13, Spatial_loss 1.88, Flat_loss 0.22, Train_acc 68.31, Test_acc 42.25
2025-01-05 13:55:55,012 [podnet.py] => Task 7, Epoch 64/160 (LR 0.06545) => LSC_loss 1.12, Spatial_loss 1.85, Flat_loss 0.22, Train_acc 68.78, Test_acc 50.98
2025-01-05 13:56:02,907 [podnet.py] => Task 7, Epoch 65/160 (LR 0.06451) => LSC_loss 1.14, Spatial_loss 1.85, Flat_loss 0.21, Train_acc 68.16, Test_acc 50.30
2025-01-05 13:56:10,919 [podnet.py] => Task 7, Epoch 66/160 (LR 0.06357) => LSC_loss 1.09, Spatial_loss 1.84, Flat_loss 0.21, Train_acc 69.36, Test_acc 50.58
2025-01-05 13:56:18,873 [podnet.py] => Task 7, Epoch 67/160 (LR 0.06262) => LSC_loss 1.10, Spatial_loss 1.84, Flat_loss 0.21, Train_acc 68.79, Test_acc 48.40
2025-01-05 13:56:26,835 [podnet.py] => Task 7, Epoch 68/160 (LR 0.06167) => LSC_loss 1.09, Spatial_loss 1.80, Flat_loss 0.21, Train_acc 69.32, Test_acc 50.28
2025-01-05 13:56:34,796 [podnet.py] => Task 7, Epoch 69/160 (LR 0.06072) => LSC_loss 1.07, Spatial_loss 1.79, Flat_loss 0.21, Train_acc 70.08, Test_acc 49.58
2025-01-05 13:56:42,576 [podnet.py] => Task 7, Epoch 70/160 (LR 0.05975) => LSC_loss 1.09, Spatial_loss 1.81, Flat_loss 0.21, Train_acc 69.23, Test_acc 50.85
2025-01-05 13:56:50,189 [podnet.py] => Task 7, Epoch 71/160 (LR 0.05879) => LSC_loss 1.09, Spatial_loss 1.78, Flat_loss 0.21, Train_acc 69.28, Test_acc 52.60
2025-01-05 13:56:57,986 [podnet.py] => Task 7, Epoch 72/160 (LR 0.05782) => LSC_loss 1.09, Spatial_loss 1.80, Flat_loss 0.21, Train_acc 69.53, Test_acc 52.40
2025-01-05 13:57:05,837 [podnet.py] => Task 7, Epoch 73/160 (LR 0.05685) => LSC_loss 1.03, Spatial_loss 1.74, Flat_loss 0.20, Train_acc 71.21, Test_acc 51.88
2025-01-05 13:57:13,944 [podnet.py] => Task 7, Epoch 74/160 (LR 0.05588) => LSC_loss 1.06, Spatial_loss 1.77, Flat_loss 0.20, Train_acc 70.70, Test_acc 53.20
2025-01-05 13:57:21,973 [podnet.py] => Task 7, Epoch 75/160 (LR 0.05490) => LSC_loss 1.05, Spatial_loss 1.75, Flat_loss 0.20, Train_acc 70.68, Test_acc 49.78
2025-01-05 13:57:29,773 [podnet.py] => Task 7, Epoch 76/160 (LR 0.05392) => LSC_loss 1.04, Spatial_loss 1.73, Flat_loss 0.20, Train_acc 71.12, Test_acc 52.42
2025-01-05 13:57:37,739 [podnet.py] => Task 7, Epoch 77/160 (LR 0.05294) => LSC_loss 1.03, Spatial_loss 1.75, Flat_loss 0.20, Train_acc 71.47, Test_acc 49.75
2025-01-05 13:57:45,558 [podnet.py] => Task 7, Epoch 78/160 (LR 0.05196) => LSC_loss 1.02, Spatial_loss 1.68, Flat_loss 0.19, Train_acc 71.47, Test_acc 50.52
2025-01-05 13:57:53,349 [podnet.py] => Task 7, Epoch 79/160 (LR 0.05098) => LSC_loss 1.02, Spatial_loss 1.74, Flat_loss 0.20, Train_acc 71.22, Test_acc 49.92
2025-01-05 13:58:01,530 [podnet.py] => Task 7, Epoch 80/160 (LR 0.05000) => LSC_loss 1.01, Spatial_loss 1.71, Flat_loss 0.19, Train_acc 71.46, Test_acc 52.25
2025-01-05 13:58:09,625 [podnet.py] => Task 7, Epoch 81/160 (LR 0.04902) => LSC_loss 1.00, Spatial_loss 1.72, Flat_loss 0.19, Train_acc 71.94, Test_acc 50.18
2025-01-05 13:58:17,822 [podnet.py] => Task 7, Epoch 82/160 (LR 0.04804) => LSC_loss 1.01, Spatial_loss 1.69, Flat_loss 0.19, Train_acc 72.22, Test_acc 53.02
2025-01-05 13:58:25,959 [podnet.py] => Task 7, Epoch 83/160 (LR 0.04706) => LSC_loss 0.97, Spatial_loss 1.67, Flat_loss 0.19, Train_acc 72.63, Test_acc 51.98
2025-01-05 13:58:33,723 [podnet.py] => Task 7, Epoch 84/160 (LR 0.04608) => LSC_loss 0.98, Spatial_loss 1.70, Flat_loss 0.19, Train_acc 72.30, Test_acc 51.20
2025-01-05 13:58:41,744 [podnet.py] => Task 7, Epoch 85/160 (LR 0.04510) => LSC_loss 0.96, Spatial_loss 1.66, Flat_loss 0.19, Train_acc 72.93, Test_acc 53.42
2025-01-05 13:58:49,749 [podnet.py] => Task 7, Epoch 86/160 (LR 0.04412) => LSC_loss 0.97, Spatial_loss 1.65, Flat_loss 0.19, Train_acc 72.67, Test_acc 51.78
2025-01-05 13:58:57,587 [podnet.py] => Task 7, Epoch 87/160 (LR 0.04315) => LSC_loss 0.96, Spatial_loss 1.66, Flat_loss 0.19, Train_acc 73.15, Test_acc 54.40
2025-01-05 13:59:05,376 [podnet.py] => Task 7, Epoch 88/160 (LR 0.04218) => LSC_loss 0.95, Spatial_loss 1.67, Flat_loss 0.18, Train_acc 73.11, Test_acc 53.05
2025-01-05 13:59:13,113 [podnet.py] => Task 7, Epoch 89/160 (LR 0.04121) => LSC_loss 0.95, Spatial_loss 1.64, Flat_loss 0.18, Train_acc 73.18, Test_acc 52.65
2025-01-05 13:59:21,129 [podnet.py] => Task 7, Epoch 90/160 (LR 0.04025) => LSC_loss 0.93, Spatial_loss 1.67, Flat_loss 0.18, Train_acc 73.72, Test_acc 54.18
2025-01-05 13:59:29,141 [podnet.py] => Task 7, Epoch 91/160 (LR 0.03928) => LSC_loss 0.93, Spatial_loss 1.64, Flat_loss 0.18, Train_acc 73.74, Test_acc 51.72
2025-01-05 13:59:37,291 [podnet.py] => Task 7, Epoch 92/160 (LR 0.03833) => LSC_loss 0.92, Spatial_loss 1.62, Flat_loss 0.18, Train_acc 74.23, Test_acc 52.38
2025-01-05 13:59:45,395 [podnet.py] => Task 7, Epoch 93/160 (LR 0.03738) => LSC_loss 0.92, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 74.44, Test_acc 53.98
2025-01-05 13:59:53,344 [podnet.py] => Task 7, Epoch 94/160 (LR 0.03643) => LSC_loss 0.90, Spatial_loss 1.58, Flat_loss 0.17, Train_acc 74.64, Test_acc 52.65
2025-01-05 14:00:01,289 [podnet.py] => Task 7, Epoch 95/160 (LR 0.03549) => LSC_loss 0.90, Spatial_loss 1.60, Flat_loss 0.17, Train_acc 75.08, Test_acc 54.08
2025-01-05 14:00:09,370 [podnet.py] => Task 7, Epoch 96/160 (LR 0.03455) => LSC_loss 0.89, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 75.32, Test_acc 50.85
2025-01-05 14:00:17,466 [podnet.py] => Task 7, Epoch 97/160 (LR 0.03362) => LSC_loss 0.88, Spatial_loss 1.55, Flat_loss 0.17, Train_acc 75.38, Test_acc 51.82
2025-01-05 14:00:25,639 [podnet.py] => Task 7, Epoch 98/160 (LR 0.03269) => LSC_loss 0.86, Spatial_loss 1.54, Flat_loss 0.16, Train_acc 75.93, Test_acc 53.38
2025-01-05 14:00:33,373 [podnet.py] => Task 7, Epoch 99/160 (LR 0.03178) => LSC_loss 0.84, Spatial_loss 1.49, Flat_loss 0.16, Train_acc 76.62, Test_acc 54.58
2025-01-05 14:00:41,269 [podnet.py] => Task 7, Epoch 100/160 (LR 0.03087) => LSC_loss 0.84, Spatial_loss 1.52, Flat_loss 0.16, Train_acc 76.31, Test_acc 54.02
2025-01-05 14:00:49,412 [podnet.py] => Task 7, Epoch 101/160 (LR 0.02996) => LSC_loss 0.83, Spatial_loss 1.52, Flat_loss 0.16, Train_acc 76.75, Test_acc 54.20
2025-01-05 14:00:57,693 [podnet.py] => Task 7, Epoch 102/160 (LR 0.02907) => LSC_loss 0.83, Spatial_loss 1.49, Flat_loss 0.16, Train_acc 76.86, Test_acc 53.82
2025-01-05 14:01:05,679 [podnet.py] => Task 7, Epoch 103/160 (LR 0.02818) => LSC_loss 0.83, Spatial_loss 1.53, Flat_loss 0.16, Train_acc 77.01, Test_acc 53.35
2025-01-05 14:01:13,595 [podnet.py] => Task 7, Epoch 104/160 (LR 0.02730) => LSC_loss 0.80, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 77.93, Test_acc 54.50
2025-01-05 14:01:21,501 [podnet.py] => Task 7, Epoch 105/160 (LR 0.02643) => LSC_loss 0.79, Spatial_loss 1.46, Flat_loss 0.15, Train_acc 78.06, Test_acc 53.45
2025-01-05 14:01:29,688 [podnet.py] => Task 7, Epoch 106/160 (LR 0.02557) => LSC_loss 0.78, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 78.57, Test_acc 53.58
2025-01-05 14:01:37,863 [podnet.py] => Task 7, Epoch 107/160 (LR 0.02472) => LSC_loss 0.79, Spatial_loss 1.42, Flat_loss 0.15, Train_acc 77.93, Test_acc 53.82
2025-01-05 14:01:45,859 [podnet.py] => Task 7, Epoch 108/160 (LR 0.02388) => LSC_loss 0.77, Spatial_loss 1.40, Flat_loss 0.15, Train_acc 78.28, Test_acc 54.12
2025-01-05 14:01:53,766 [podnet.py] => Task 7, Epoch 109/160 (LR 0.02304) => LSC_loss 0.75, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 79.20, Test_acc 55.22
2025-01-05 14:02:01,891 [podnet.py] => Task 7, Epoch 110/160 (LR 0.02222) => LSC_loss 0.74, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 79.36, Test_acc 54.05
2025-01-05 14:02:09,678 [podnet.py] => Task 7, Epoch 111/160 (LR 0.02141) => LSC_loss 0.74, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 79.34, Test_acc 54.00
2025-01-05 14:02:17,686 [podnet.py] => Task 7, Epoch 112/160 (LR 0.02061) => LSC_loss 0.74, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 79.70, Test_acc 53.35
2025-01-05 14:02:25,660 [podnet.py] => Task 7, Epoch 113/160 (LR 0.01982) => LSC_loss 0.72, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 80.19, Test_acc 54.90
2025-01-05 14:02:33,674 [podnet.py] => Task 7, Epoch 114/160 (LR 0.01905) => LSC_loss 0.69, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 80.77, Test_acc 55.70
2025-01-05 14:02:41,450 [podnet.py] => Task 7, Epoch 115/160 (LR 0.01828) => LSC_loss 0.70, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 80.69, Test_acc 51.85
2025-01-05 14:02:49,696 [podnet.py] => Task 7, Epoch 116/160 (LR 0.01753) => LSC_loss 0.68, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 81.25, Test_acc 55.02
2025-01-05 14:02:57,935 [podnet.py] => Task 7, Epoch 117/160 (LR 0.01679) => LSC_loss 0.67, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 81.65, Test_acc 53.75
2025-01-05 14:03:06,022 [podnet.py] => Task 7, Epoch 118/160 (LR 0.01606) => LSC_loss 0.66, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 81.63, Test_acc 55.08
2025-01-05 14:03:13,852 [podnet.py] => Task 7, Epoch 119/160 (LR 0.01535) => LSC_loss 0.66, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 81.98, Test_acc 57.20
2025-01-05 14:03:21,921 [podnet.py] => Task 7, Epoch 120/160 (LR 0.01464) => LSC_loss 0.65, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 82.04, Test_acc 56.55
2025-01-05 14:03:29,815 [podnet.py] => Task 7, Epoch 121/160 (LR 0.01396) => LSC_loss 0.64, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 82.35, Test_acc 56.50
2025-01-05 14:03:37,559 [podnet.py] => Task 7, Epoch 122/160 (LR 0.01328) => LSC_loss 0.62, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 83.80, Test_acc 56.90
2025-01-05 14:03:45,348 [podnet.py] => Task 7, Epoch 123/160 (LR 0.01262) => LSC_loss 0.60, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 83.27, Test_acc 54.28
2025-01-05 14:03:53,280 [podnet.py] => Task 7, Epoch 124/160 (LR 0.01198) => LSC_loss 0.60, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 83.91, Test_acc 56.32
2025-01-05 14:04:01,087 [podnet.py] => Task 7, Epoch 125/160 (LR 0.01135) => LSC_loss 0.60, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 83.97, Test_acc 56.50
2025-01-05 14:04:08,903 [podnet.py] => Task 7, Epoch 126/160 (LR 0.01073) => LSC_loss 0.59, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 84.48, Test_acc 56.15
2025-01-05 14:04:16,779 [podnet.py] => Task 7, Epoch 127/160 (LR 0.01013) => LSC_loss 0.58, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 84.33, Test_acc 57.95
2025-01-05 14:04:24,799 [podnet.py] => Task 7, Epoch 128/160 (LR 0.00955) => LSC_loss 0.56, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 84.79, Test_acc 57.38
2025-01-05 14:04:32,906 [podnet.py] => Task 7, Epoch 129/160 (LR 0.00898) => LSC_loss 0.55, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 85.34, Test_acc 58.18
2025-01-05 14:04:41,079 [podnet.py] => Task 7, Epoch 130/160 (LR 0.00843) => LSC_loss 0.55, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 85.13, Test_acc 57.75
2025-01-05 14:04:49,179 [podnet.py] => Task 7, Epoch 131/160 (LR 0.00789) => LSC_loss 0.54, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 85.37, Test_acc 57.68
2025-01-05 14:04:57,188 [podnet.py] => Task 7, Epoch 132/160 (LR 0.00737) => LSC_loss 0.54, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 85.93, Test_acc 57.22
2025-01-05 14:05:05,447 [podnet.py] => Task 7, Epoch 133/160 (LR 0.00686) => LSC_loss 0.54, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 85.98, Test_acc 58.52
2025-01-05 14:05:13,456 [podnet.py] => Task 7, Epoch 134/160 (LR 0.00638) => LSC_loss 0.52, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 86.45, Test_acc 58.55
2025-01-05 14:05:21,391 [podnet.py] => Task 7, Epoch 135/160 (LR 0.00590) => LSC_loss 0.51, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 86.69, Test_acc 58.12
2025-01-05 14:05:29,441 [podnet.py] => Task 7, Epoch 136/160 (LR 0.00545) => LSC_loss 0.51, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 86.92, Test_acc 58.85
2025-01-05 14:05:37,500 [podnet.py] => Task 7, Epoch 137/160 (LR 0.00501) => LSC_loss 0.49, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 87.12, Test_acc 58.15
2025-01-05 14:05:45,583 [podnet.py] => Task 7, Epoch 138/160 (LR 0.00459) => LSC_loss 0.49, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 87.32, Test_acc 58.55
2025-01-05 14:05:53,446 [podnet.py] => Task 7, Epoch 139/160 (LR 0.00419) => LSC_loss 0.48, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 87.80, Test_acc 59.58
2025-01-05 14:06:01,394 [podnet.py] => Task 7, Epoch 140/160 (LR 0.00381) => LSC_loss 0.49, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 87.69, Test_acc 58.75
2025-01-05 14:06:09,255 [podnet.py] => Task 7, Epoch 141/160 (LR 0.00344) => LSC_loss 0.48, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 87.73, Test_acc 58.72
2025-01-05 14:06:17,111 [podnet.py] => Task 7, Epoch 142/160 (LR 0.00309) => LSC_loss 0.47, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 88.03, Test_acc 59.42
2025-01-05 14:06:25,262 [podnet.py] => Task 7, Epoch 143/160 (LR 0.00276) => LSC_loss 0.46, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 88.46, Test_acc 59.18
2025-01-05 14:06:33,167 [podnet.py] => Task 7, Epoch 144/160 (LR 0.00245) => LSC_loss 0.47, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 87.91, Test_acc 58.85
2025-01-05 14:06:41,154 [podnet.py] => Task 7, Epoch 145/160 (LR 0.00215) => LSC_loss 0.46, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 88.47, Test_acc 58.80
2025-01-05 14:06:49,074 [podnet.py] => Task 7, Epoch 146/160 (LR 0.00188) => LSC_loss 0.46, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 88.45, Test_acc 59.10
2025-01-05 14:06:57,072 [podnet.py] => Task 7, Epoch 147/160 (LR 0.00162) => LSC_loss 0.46, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 88.44, Test_acc 59.55
2025-01-05 14:07:04,974 [podnet.py] => Task 7, Epoch 148/160 (LR 0.00138) => LSC_loss 0.45, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 88.72, Test_acc 59.60
2025-01-05 14:07:12,825 [podnet.py] => Task 7, Epoch 149/160 (LR 0.00116) => LSC_loss 0.45, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 88.95, Test_acc 59.20
2025-01-05 14:07:20,857 [podnet.py] => Task 7, Epoch 150/160 (LR 0.00096) => LSC_loss 0.44, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.05, Test_acc 59.20
2025-01-05 14:07:28,903 [podnet.py] => Task 7, Epoch 151/160 (LR 0.00078) => LSC_loss 0.44, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.27, Test_acc 59.20
2025-01-05 14:07:36,702 [podnet.py] => Task 7, Epoch 152/160 (LR 0.00062) => LSC_loss 0.43, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.36, Test_acc 59.05
2025-01-05 14:07:44,596 [podnet.py] => Task 7, Epoch 153/160 (LR 0.00047) => LSC_loss 0.44, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 89.28, Test_acc 59.05
2025-01-05 14:07:52,580 [podnet.py] => Task 7, Epoch 154/160 (LR 0.00035) => LSC_loss 0.44, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.21, Test_acc 59.08
2025-01-05 14:08:00,699 [podnet.py] => Task 7, Epoch 155/160 (LR 0.00024) => LSC_loss 0.44, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 88.97, Test_acc 59.15
2025-01-05 14:08:08,525 [podnet.py] => Task 7, Epoch 156/160 (LR 0.00015) => LSC_loss 0.44, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 89.31, Test_acc 59.25
2025-01-05 14:08:16,574 [podnet.py] => Task 7, Epoch 157/160 (LR 0.00009) => LSC_loss 0.43, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 89.47, Test_acc 59.05
2025-01-05 14:08:24,360 [podnet.py] => Task 7, Epoch 158/160 (LR 0.00004) => LSC_loss 0.43, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 89.31, Test_acc 59.32
2025-01-05 14:08:32,270 [podnet.py] => Task 7, Epoch 159/160 (LR 0.00001) => LSC_loss 0.44, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 89.23, Test_acc 59.40
2025-01-05 14:08:40,176 [podnet.py] => Task 7, Epoch 160/160 (LR 0.00000) => LSC_loss 0.43, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.52, Test_acc 59.22
2025-01-05 14:08:40,177 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 14:08:40,177 [base.py] => Reducing exemplars...(385 per classes)
2025-01-05 14:09:09,233 [base.py] => Constructing exemplars...(385 per classes)
2025-01-05 14:09:18,554 [podnet.py] => The size of finetune dataset: 15400
2025-01-05 14:09:26,438 [podnet.py] => Task 7, Epoch 1/20 (LR 0.00497) => LSC_loss 0.46, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 88.20, Test_acc 58.45
2025-01-05 14:09:34,161 [podnet.py] => Task 7, Epoch 2/20 (LR 0.00488) => LSC_loss 0.47, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 87.68, Test_acc 58.30
2025-01-05 14:09:42,001 [podnet.py] => Task 7, Epoch 3/20 (LR 0.00473) => LSC_loss 0.45, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 88.30, Test_acc 57.62
2025-01-05 14:09:49,627 [podnet.py] => Task 7, Epoch 4/20 (LR 0.00452) => LSC_loss 0.45, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 88.38, Test_acc 58.45
2025-01-05 14:09:57,323 [podnet.py] => Task 7, Epoch 5/20 (LR 0.00427) => LSC_loss 0.46, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 88.16, Test_acc 58.38
2025-01-05 14:10:05,231 [podnet.py] => Task 7, Epoch 6/20 (LR 0.00397) => LSC_loss 0.45, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.53, Test_acc 57.75
2025-01-05 14:10:12,803 [podnet.py] => Task 7, Epoch 7/20 (LR 0.00363) => LSC_loss 0.45, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.39, Test_acc 58.15
2025-01-05 14:10:20,831 [podnet.py] => Task 7, Epoch 8/20 (LR 0.00327) => LSC_loss 0.45, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.47, Test_acc 58.58
2025-01-05 14:10:28,648 [podnet.py] => Task 7, Epoch 9/20 (LR 0.00289) => LSC_loss 0.44, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 88.88, Test_acc 58.62
2025-01-05 14:10:36,546 [podnet.py] => Task 7, Epoch 10/20 (LR 0.00250) => LSC_loss 0.44, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.96, Test_acc 58.42
2025-01-05 14:10:44,354 [podnet.py] => Task 7, Epoch 11/20 (LR 0.00211) => LSC_loss 0.43, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 89.44, Test_acc 58.85
2025-01-05 14:10:52,091 [podnet.py] => Task 7, Epoch 12/20 (LR 0.00173) => LSC_loss 0.42, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.34, Test_acc 58.70
2025-01-05 14:11:00,067 [podnet.py] => Task 7, Epoch 13/20 (LR 0.00137) => LSC_loss 0.43, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 89.21, Test_acc 58.98
2025-01-05 14:11:07,687 [podnet.py] => Task 7, Epoch 14/20 (LR 0.00103) => LSC_loss 0.42, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 89.30, Test_acc 59.15
2025-01-05 14:11:15,446 [podnet.py] => Task 7, Epoch 15/20 (LR 0.00073) => LSC_loss 0.41, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 89.92, Test_acc 58.95
2025-01-05 14:11:23,319 [podnet.py] => Task 7, Epoch 16/20 (LR 0.00048) => LSC_loss 0.41, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 90.06, Test_acc 59.22
2025-01-05 14:11:31,110 [podnet.py] => Task 7, Epoch 17/20 (LR 0.00027) => LSC_loss 0.41, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.89, Test_acc 59.35
2025-01-05 14:11:38,780 [podnet.py] => Task 7, Epoch 18/20 (LR 0.00012) => LSC_loss 0.41, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 89.98, Test_acc 59.55
2025-01-05 14:11:46,562 [podnet.py] => Task 7, Epoch 19/20 (LR 0.00003) => LSC_loss 0.40, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 90.17, Test_acc 59.12
2025-01-05 14:11:54,215 [podnet.py] => Task 7, Epoch 20/20 (LR 0.00000) => LSC_loss 0.40, Spatial_loss 0.86, Flat_loss 0.07, Train_acc 90.14, Test_acc 59.22
2025-01-05 14:11:54,219 [base.py] => Reducing exemplars...(336 per classes)
2025-01-05 14:12:23,958 [base.py] => Constructing exemplars...(336 per classes)
2025-01-05 14:12:34,964 [podnet.py] => Exemplar size: 13440
2025-01-05 14:12:34,964 [trainer.py] => CNN: {'total': np.float64(59.22), '00-09': np.float64(71.2), '10-19': np.float64(51.2), '20-29': np.float64(64.3), '30-39': np.float64(50.2), 'old': np.float64(61.0), 'new': np.float64(46.8)}
2025-01-05 14:12:34,964 [trainer.py] => NME: {'total': np.float64(58.08), '00-09': np.float64(70.7), '10-19': np.float64(49.3), '20-29': np.float64(62.6), '30-39': np.float64(49.7), 'old': np.float64(59.54), 'new': np.float64(47.8)}
2025-01-05 14:12:34,964 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22)]
2025-01-05 14:12:34,964 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42)]
2025-01-05 14:12:34,964 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08)]
2025-01-05 14:12:34,964 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15)]

2025-01-05 14:12:34,965 [trainer.py] => All params: 491857
2025-01-05 14:12:34,965 [trainer.py] => Trainable params: 491857
2025-01-05 14:12:34,965 [podnet.py] => Learning on 40-45
2025-01-05 14:12:35,010 [podnet.py] => Adaptive factor: 3.0
2025-01-05 14:12:43,074 [podnet.py] => Task 8, Epoch 1/160 (LR 0.09999) => LSC_loss 2.11, Spatial_loss 2.95, Flat_loss 0.53, Train_acc 46.05, Test_acc 39.53
2025-01-05 14:12:51,221 [podnet.py] => Task 8, Epoch 2/160 (LR 0.09996) => LSC_loss 1.70, Spatial_loss 2.57, Flat_loss 0.39, Train_acc 53.71, Test_acc 42.20
2025-01-05 14:12:59,265 [podnet.py] => Task 8, Epoch 3/160 (LR 0.09991) => LSC_loss 1.57, Spatial_loss 2.39, Flat_loss 0.35, Train_acc 57.47, Test_acc 42.33
2025-01-05 14:13:07,036 [podnet.py] => Task 8, Epoch 4/160 (LR 0.09985) => LSC_loss 1.52, Spatial_loss 2.34, Flat_loss 0.33, Train_acc 59.23, Test_acc 46.18
2025-01-05 14:13:14,992 [podnet.py] => Task 8, Epoch 5/160 (LR 0.09976) => LSC_loss 1.48, Spatial_loss 2.28, Flat_loss 0.32, Train_acc 60.09, Test_acc 45.73
2025-01-05 14:13:22,934 [podnet.py] => Task 8, Epoch 6/160 (LR 0.09965) => LSC_loss 1.46, Spatial_loss 2.26, Flat_loss 0.31, Train_acc 60.14, Test_acc 46.78
2025-01-05 14:13:31,021 [podnet.py] => Task 8, Epoch 7/160 (LR 0.09953) => LSC_loss 1.43, Spatial_loss 2.20, Flat_loss 0.31, Train_acc 60.88, Test_acc 45.27
2025-01-05 14:13:39,216 [podnet.py] => Task 8, Epoch 8/160 (LR 0.09938) => LSC_loss 1.43, Spatial_loss 2.21, Flat_loss 0.31, Train_acc 61.12, Test_acc 43.36
2025-01-05 14:13:47,340 [podnet.py] => Task 8, Epoch 9/160 (LR 0.09922) => LSC_loss 1.42, Spatial_loss 2.22, Flat_loss 0.30, Train_acc 61.56, Test_acc 40.44
2025-01-05 14:13:55,274 [podnet.py] => Task 8, Epoch 10/160 (LR 0.09904) => LSC_loss 1.40, Spatial_loss 2.20, Flat_loss 0.30, Train_acc 61.31, Test_acc 46.33
2025-01-05 14:14:03,316 [podnet.py] => Task 8, Epoch 11/160 (LR 0.09884) => LSC_loss 1.38, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 62.16, Test_acc 44.84
2025-01-05 14:14:11,510 [podnet.py] => Task 8, Epoch 12/160 (LR 0.09862) => LSC_loss 1.38, Spatial_loss 2.14, Flat_loss 0.30, Train_acc 62.54, Test_acc 42.84
2025-01-05 14:14:19,616 [podnet.py] => Task 8, Epoch 13/160 (LR 0.09838) => LSC_loss 1.37, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 62.77, Test_acc 44.24
2025-01-05 14:14:27,649 [podnet.py] => Task 8, Epoch 14/160 (LR 0.09812) => LSC_loss 1.36, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 62.13, Test_acc 45.76
2025-01-05 14:14:36,023 [podnet.py] => Task 8, Epoch 15/160 (LR 0.09785) => LSC_loss 1.36, Spatial_loss 2.13, Flat_loss 0.29, Train_acc 62.79, Test_acc 45.49
2025-01-05 14:14:44,154 [podnet.py] => Task 8, Epoch 16/160 (LR 0.09755) => LSC_loss 1.35, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 62.94, Test_acc 47.98
2025-01-05 14:14:52,206 [podnet.py] => Task 8, Epoch 17/160 (LR 0.09724) => LSC_loss 1.35, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 62.80, Test_acc 44.29
2025-01-05 14:14:59,984 [podnet.py] => Task 8, Epoch 18/160 (LR 0.09691) => LSC_loss 1.33, Spatial_loss 2.13, Flat_loss 0.29, Train_acc 63.45, Test_acc 48.24
2025-01-05 14:15:08,233 [podnet.py] => Task 8, Epoch 19/160 (LR 0.09656) => LSC_loss 1.34, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 63.43, Test_acc 46.18
2025-01-05 14:15:16,168 [podnet.py] => Task 8, Epoch 20/160 (LR 0.09619) => LSC_loss 1.34, Spatial_loss 2.13, Flat_loss 0.29, Train_acc 63.39, Test_acc 48.76
2025-01-05 14:15:24,223 [podnet.py] => Task 8, Epoch 21/160 (LR 0.09581) => LSC_loss 1.32, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 63.62, Test_acc 44.36
2025-01-05 14:15:32,187 [podnet.py] => Task 8, Epoch 22/160 (LR 0.09541) => LSC_loss 1.33, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 63.34, Test_acc 50.02
2025-01-05 14:15:40,290 [podnet.py] => Task 8, Epoch 23/160 (LR 0.09499) => LSC_loss 1.30, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 64.15, Test_acc 51.93
2025-01-05 14:15:48,299 [podnet.py] => Task 8, Epoch 24/160 (LR 0.09455) => LSC_loss 1.30, Spatial_loss 2.10, Flat_loss 0.29, Train_acc 64.59, Test_acc 46.40
2025-01-05 14:15:56,777 [podnet.py] => Task 8, Epoch 25/160 (LR 0.09410) => LSC_loss 1.31, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 63.75, Test_acc 45.44
2025-01-05 14:16:04,846 [podnet.py] => Task 8, Epoch 26/160 (LR 0.09362) => LSC_loss 1.28, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 64.74, Test_acc 47.76
2025-01-05 14:16:13,001 [podnet.py] => Task 8, Epoch 27/160 (LR 0.09314) => LSC_loss 1.30, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 64.39, Test_acc 49.04
2025-01-05 14:16:21,152 [podnet.py] => Task 8, Epoch 28/160 (LR 0.09263) => LSC_loss 1.29, Spatial_loss 2.12, Flat_loss 0.28, Train_acc 64.37, Test_acc 46.20
2025-01-05 14:16:29,151 [podnet.py] => Task 8, Epoch 29/160 (LR 0.09211) => LSC_loss 1.31, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 64.25, Test_acc 45.51
2025-01-05 14:16:37,231 [podnet.py] => Task 8, Epoch 30/160 (LR 0.09157) => LSC_loss 1.29, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 64.87, Test_acc 49.71
2025-01-05 14:16:45,169 [podnet.py] => Task 8, Epoch 31/160 (LR 0.09102) => LSC_loss 1.27, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 64.84, Test_acc 44.02
2025-01-05 14:16:53,006 [podnet.py] => Task 8, Epoch 32/160 (LR 0.09045) => LSC_loss 1.28, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 65.03, Test_acc 49.51
2025-01-05 14:17:00,882 [podnet.py] => Task 8, Epoch 33/160 (LR 0.08987) => LSC_loss 1.28, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 64.39, Test_acc 45.80
2025-01-05 14:17:08,746 [podnet.py] => Task 8, Epoch 34/160 (LR 0.08927) => LSC_loss 1.28, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 64.55, Test_acc 46.93
2025-01-05 14:17:16,786 [podnet.py] => Task 8, Epoch 35/160 (LR 0.08865) => LSC_loss 1.28, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 65.21, Test_acc 51.02
2025-01-05 14:17:24,877 [podnet.py] => Task 8, Epoch 36/160 (LR 0.08802) => LSC_loss 1.27, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 64.92, Test_acc 50.11
2025-01-05 14:17:33,109 [podnet.py] => Task 8, Epoch 37/160 (LR 0.08738) => LSC_loss 1.26, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 65.21, Test_acc 47.67
2025-01-05 14:17:41,356 [podnet.py] => Task 8, Epoch 38/160 (LR 0.08672) => LSC_loss 1.27, Spatial_loss 2.05, Flat_loss 0.28, Train_acc 65.03, Test_acc 47.60
2025-01-05 14:17:49,372 [podnet.py] => Task 8, Epoch 39/160 (LR 0.08604) => LSC_loss 1.26, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 65.21, Test_acc 48.49
2025-01-05 14:17:57,287 [podnet.py] => Task 8, Epoch 40/160 (LR 0.08536) => LSC_loss 1.27, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 64.92, Test_acc 50.80
2025-01-05 14:18:05,154 [podnet.py] => Task 8, Epoch 41/160 (LR 0.08465) => LSC_loss 1.25, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 65.34, Test_acc 47.69
2025-01-05 14:18:13,233 [podnet.py] => Task 8, Epoch 42/160 (LR 0.08394) => LSC_loss 1.26, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 65.81, Test_acc 48.64
2025-01-05 14:18:21,079 [podnet.py] => Task 8, Epoch 43/160 (LR 0.08321) => LSC_loss 1.25, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 65.82, Test_acc 45.20
2025-01-05 14:18:29,053 [podnet.py] => Task 8, Epoch 44/160 (LR 0.08247) => LSC_loss 1.22, Spatial_loss 2.02, Flat_loss 0.27, Train_acc 66.40, Test_acc 47.24
2025-01-05 14:18:37,093 [podnet.py] => Task 8, Epoch 45/160 (LR 0.08172) => LSC_loss 1.25, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 65.53, Test_acc 46.73
2025-01-05 14:18:45,009 [podnet.py] => Task 8, Epoch 46/160 (LR 0.08095) => LSC_loss 1.22, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 66.23, Test_acc 43.51
2025-01-05 14:18:53,071 [podnet.py] => Task 8, Epoch 47/160 (LR 0.08018) => LSC_loss 1.21, Spatial_loss 2.02, Flat_loss 0.27, Train_acc 66.52, Test_acc 46.38
2025-01-05 14:19:00,825 [podnet.py] => Task 8, Epoch 48/160 (LR 0.07939) => LSC_loss 1.23, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 66.22, Test_acc 48.40
2025-01-05 14:19:08,868 [podnet.py] => Task 8, Epoch 49/160 (LR 0.07859) => LSC_loss 1.22, Spatial_loss 2.06, Flat_loss 0.27, Train_acc 66.17, Test_acc 49.29
2025-01-05 14:19:16,859 [podnet.py] => Task 8, Epoch 50/160 (LR 0.07778) => LSC_loss 1.23, Spatial_loss 2.08, Flat_loss 0.27, Train_acc 66.15, Test_acc 44.56
2025-01-05 14:19:24,941 [podnet.py] => Task 8, Epoch 51/160 (LR 0.07696) => LSC_loss 1.21, Spatial_loss 2.01, Flat_loss 0.27, Train_acc 66.64, Test_acc 43.89
2025-01-05 14:19:32,942 [podnet.py] => Task 8, Epoch 52/160 (LR 0.07612) => LSC_loss 1.20, Spatial_loss 1.98, Flat_loss 0.26, Train_acc 67.18, Test_acc 48.60
2025-01-05 14:19:40,963 [podnet.py] => Task 8, Epoch 53/160 (LR 0.07528) => LSC_loss 1.21, Spatial_loss 2.00, Flat_loss 0.26, Train_acc 66.78, Test_acc 50.36
2025-01-05 14:19:49,146 [podnet.py] => Task 8, Epoch 54/160 (LR 0.07443) => LSC_loss 1.20, Spatial_loss 2.02, Flat_loss 0.26, Train_acc 67.08, Test_acc 50.20
2025-01-05 14:19:56,955 [podnet.py] => Task 8, Epoch 55/160 (LR 0.07357) => LSC_loss 1.21, Spatial_loss 2.00, Flat_loss 0.27, Train_acc 66.90, Test_acc 41.98
2025-01-05 14:20:04,740 [podnet.py] => Task 8, Epoch 56/160 (LR 0.07270) => LSC_loss 1.19, Spatial_loss 1.98, Flat_loss 0.26, Train_acc 67.13, Test_acc 48.58
2025-01-05 14:20:12,824 [podnet.py] => Task 8, Epoch 57/160 (LR 0.07182) => LSC_loss 1.18, Spatial_loss 2.01, Flat_loss 0.26, Train_acc 67.45, Test_acc 52.60
2025-01-05 14:20:20,986 [podnet.py] => Task 8, Epoch 58/160 (LR 0.07093) => LSC_loss 1.17, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 67.71, Test_acc 49.64
2025-01-05 14:20:28,791 [podnet.py] => Task 8, Epoch 59/160 (LR 0.07004) => LSC_loss 1.16, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 67.83, Test_acc 44.31
2025-01-05 14:20:36,729 [podnet.py] => Task 8, Epoch 60/160 (LR 0.06913) => LSC_loss 1.17, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 67.79, Test_acc 48.02
2025-01-05 14:20:44,759 [podnet.py] => Task 8, Epoch 61/160 (LR 0.06822) => LSC_loss 1.14, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 68.02, Test_acc 43.91
2025-01-05 14:20:52,658 [podnet.py] => Task 8, Epoch 62/160 (LR 0.06731) => LSC_loss 1.16, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 67.92, Test_acc 48.40
2025-01-05 14:21:00,637 [podnet.py] => Task 8, Epoch 63/160 (LR 0.06638) => LSC_loss 1.16, Spatial_loss 1.93, Flat_loss 0.25, Train_acc 68.15, Test_acc 48.80
2025-01-05 14:21:08,674 [podnet.py] => Task 8, Epoch 64/160 (LR 0.06545) => LSC_loss 1.14, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 68.29, Test_acc 49.76
2025-01-05 14:21:16,599 [podnet.py] => Task 8, Epoch 65/160 (LR 0.06451) => LSC_loss 1.16, Spatial_loss 1.96, Flat_loss 0.26, Train_acc 68.02, Test_acc 48.47
2025-01-05 14:21:24,479 [podnet.py] => Task 8, Epoch 66/160 (LR 0.06357) => LSC_loss 1.15, Spatial_loss 1.94, Flat_loss 0.25, Train_acc 68.40, Test_acc 49.33
2025-01-05 14:21:32,402 [podnet.py] => Task 8, Epoch 67/160 (LR 0.06262) => LSC_loss 1.12, Spatial_loss 1.91, Flat_loss 0.24, Train_acc 69.30, Test_acc 51.33
2025-01-05 14:21:40,449 [podnet.py] => Task 8, Epoch 68/160 (LR 0.06167) => LSC_loss 1.12, Spatial_loss 1.92, Flat_loss 0.25, Train_acc 69.23, Test_acc 47.36
2025-01-05 14:21:48,604 [podnet.py] => Task 8, Epoch 69/160 (LR 0.06072) => LSC_loss 1.11, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 69.39, Test_acc 53.18
2025-01-05 14:21:56,499 [podnet.py] => Task 8, Epoch 70/160 (LR 0.05975) => LSC_loss 1.11, Spatial_loss 1.91, Flat_loss 0.25, Train_acc 69.53, Test_acc 48.62
2025-01-05 14:22:04,325 [podnet.py] => Task 8, Epoch 71/160 (LR 0.05879) => LSC_loss 1.09, Spatial_loss 1.84, Flat_loss 0.24, Train_acc 69.90, Test_acc 49.67
2025-01-05 14:22:12,406 [podnet.py] => Task 8, Epoch 72/160 (LR 0.05782) => LSC_loss 1.11, Spatial_loss 1.88, Flat_loss 0.24, Train_acc 69.29, Test_acc 45.80
2025-01-05 14:22:20,278 [podnet.py] => Task 8, Epoch 73/160 (LR 0.05685) => LSC_loss 1.09, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 70.06, Test_acc 49.89
2025-01-05 14:22:28,210 [podnet.py] => Task 8, Epoch 74/160 (LR 0.05588) => LSC_loss 1.08, Spatial_loss 1.82, Flat_loss 0.23, Train_acc 70.24, Test_acc 51.56
2025-01-05 14:22:36,220 [podnet.py] => Task 8, Epoch 75/160 (LR 0.05490) => LSC_loss 1.06, Spatial_loss 1.82, Flat_loss 0.23, Train_acc 70.80, Test_acc 49.40
2025-01-05 14:22:44,142 [podnet.py] => Task 8, Epoch 76/160 (LR 0.05392) => LSC_loss 1.06, Spatial_loss 1.83, Flat_loss 0.23, Train_acc 70.63, Test_acc 50.96
2025-01-05 14:22:52,040 [podnet.py] => Task 8, Epoch 77/160 (LR 0.05294) => LSC_loss 1.06, Spatial_loss 1.82, Flat_loss 0.23, Train_acc 70.68, Test_acc 49.82
2025-01-05 14:23:00,187 [podnet.py] => Task 8, Epoch 78/160 (LR 0.05196) => LSC_loss 1.04, Spatial_loss 1.80, Flat_loss 0.23, Train_acc 71.05, Test_acc 50.91
2025-01-05 14:23:08,133 [podnet.py] => Task 8, Epoch 79/160 (LR 0.05098) => LSC_loss 1.03, Spatial_loss 1.79, Flat_loss 0.23, Train_acc 71.53, Test_acc 52.31
2025-01-05 14:23:16,165 [podnet.py] => Task 8, Epoch 80/160 (LR 0.05000) => LSC_loss 1.04, Spatial_loss 1.82, Flat_loss 0.23, Train_acc 71.05, Test_acc 47.80
2025-01-05 14:23:24,317 [podnet.py] => Task 8, Epoch 81/160 (LR 0.04902) => LSC_loss 1.03, Spatial_loss 1.80, Flat_loss 0.23, Train_acc 72.07, Test_acc 52.27
2025-01-05 14:23:32,428 [podnet.py] => Task 8, Epoch 82/160 (LR 0.04804) => LSC_loss 1.02, Spatial_loss 1.77, Flat_loss 0.22, Train_acc 71.94, Test_acc 50.47
2025-01-05 14:23:40,256 [podnet.py] => Task 8, Epoch 83/160 (LR 0.04706) => LSC_loss 1.01, Spatial_loss 1.79, Flat_loss 0.22, Train_acc 72.62, Test_acc 51.33
2025-01-05 14:23:48,266 [podnet.py] => Task 8, Epoch 84/160 (LR 0.04608) => LSC_loss 1.00, Spatial_loss 1.76, Flat_loss 0.22, Train_acc 72.29, Test_acc 51.36
2025-01-05 14:23:56,370 [podnet.py] => Task 8, Epoch 85/160 (LR 0.04510) => LSC_loss 1.00, Spatial_loss 1.72, Flat_loss 0.22, Train_acc 72.66, Test_acc 49.96
2025-01-05 14:24:04,323 [podnet.py] => Task 8, Epoch 86/160 (LR 0.04412) => LSC_loss 1.01, Spatial_loss 1.77, Flat_loss 0.22, Train_acc 71.98, Test_acc 49.02
2025-01-05 14:24:12,367 [podnet.py] => Task 8, Epoch 87/160 (LR 0.04315) => LSC_loss 0.98, Spatial_loss 1.72, Flat_loss 0.21, Train_acc 72.86, Test_acc 46.64
2025-01-05 14:24:20,296 [podnet.py] => Task 8, Epoch 88/160 (LR 0.04218) => LSC_loss 0.97, Spatial_loss 1.72, Flat_loss 0.21, Train_acc 73.19, Test_acc 49.87
2025-01-05 14:24:28,380 [podnet.py] => Task 8, Epoch 89/160 (LR 0.04121) => LSC_loss 0.96, Spatial_loss 1.70, Flat_loss 0.21, Train_acc 73.75, Test_acc 50.84
2025-01-05 14:24:36,699 [podnet.py] => Task 8, Epoch 90/160 (LR 0.04025) => LSC_loss 0.94, Spatial_loss 1.71, Flat_loss 0.21, Train_acc 74.21, Test_acc 51.91
2025-01-05 14:24:44,751 [podnet.py] => Task 8, Epoch 91/160 (LR 0.03928) => LSC_loss 0.94, Spatial_loss 1.71, Flat_loss 0.20, Train_acc 73.91, Test_acc 49.04
2025-01-05 14:24:52,727 [podnet.py] => Task 8, Epoch 92/160 (LR 0.03833) => LSC_loss 0.94, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 73.97, Test_acc 50.64
2025-01-05 14:25:00,747 [podnet.py] => Task 8, Epoch 93/160 (LR 0.03738) => LSC_loss 0.95, Spatial_loss 1.69, Flat_loss 0.20, Train_acc 73.50, Test_acc 52.53
2025-01-05 14:25:08,532 [podnet.py] => Task 8, Epoch 94/160 (LR 0.03643) => LSC_loss 0.92, Spatial_loss 1.65, Flat_loss 0.20, Train_acc 74.31, Test_acc 52.84
2025-01-05 14:25:16,495 [podnet.py] => Task 8, Epoch 95/160 (LR 0.03549) => LSC_loss 0.90, Spatial_loss 1.68, Flat_loss 0.20, Train_acc 75.19, Test_acc 52.91
2025-01-05 14:25:24,639 [podnet.py] => Task 8, Epoch 96/160 (LR 0.03455) => LSC_loss 0.90, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 75.09, Test_acc 48.49
2025-01-05 14:25:32,672 [podnet.py] => Task 8, Epoch 97/160 (LR 0.03362) => LSC_loss 0.90, Spatial_loss 1.62, Flat_loss 0.19, Train_acc 75.28, Test_acc 49.11
2025-01-05 14:25:40,885 [podnet.py] => Task 8, Epoch 98/160 (LR 0.03269) => LSC_loss 0.90, Spatial_loss 1.63, Flat_loss 0.19, Train_acc 75.11, Test_acc 51.07
2025-01-05 14:25:48,831 [podnet.py] => Task 8, Epoch 99/160 (LR 0.03178) => LSC_loss 0.87, Spatial_loss 1.58, Flat_loss 0.19, Train_acc 75.74, Test_acc 48.64
2025-01-05 14:25:56,861 [podnet.py] => Task 8, Epoch 100/160 (LR 0.03087) => LSC_loss 0.87, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 76.32, Test_acc 54.82
2025-01-05 14:26:04,821 [podnet.py] => Task 8, Epoch 101/160 (LR 0.02996) => LSC_loss 0.85, Spatial_loss 1.55, Flat_loss 0.18, Train_acc 76.43, Test_acc 53.16
2025-01-05 14:26:12,810 [podnet.py] => Task 8, Epoch 102/160 (LR 0.02907) => LSC_loss 0.83, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 76.89, Test_acc 53.96
2025-01-05 14:26:20,653 [podnet.py] => Task 8, Epoch 103/160 (LR 0.02818) => LSC_loss 0.84, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 76.85, Test_acc 51.02
2025-01-05 14:26:28,860 [podnet.py] => Task 8, Epoch 104/160 (LR 0.02730) => LSC_loss 0.82, Spatial_loss 1.54, Flat_loss 0.18, Train_acc 77.50, Test_acc 52.09
2025-01-05 14:26:36,807 [podnet.py] => Task 8, Epoch 105/160 (LR 0.02643) => LSC_loss 0.83, Spatial_loss 1.56, Flat_loss 0.18, Train_acc 77.01, Test_acc 51.80
2025-01-05 14:26:44,776 [podnet.py] => Task 8, Epoch 106/160 (LR 0.02557) => LSC_loss 0.79, Spatial_loss 1.49, Flat_loss 0.17, Train_acc 78.27, Test_acc 52.93
2025-01-05 14:26:52,690 [podnet.py] => Task 8, Epoch 107/160 (LR 0.02472) => LSC_loss 0.79, Spatial_loss 1.48, Flat_loss 0.17, Train_acc 78.08, Test_acc 53.36
2025-01-05 14:27:00,729 [podnet.py] => Task 8, Epoch 108/160 (LR 0.02388) => LSC_loss 0.78, Spatial_loss 1.48, Flat_loss 0.17, Train_acc 78.54, Test_acc 55.11
2025-01-05 14:27:08,766 [podnet.py] => Task 8, Epoch 109/160 (LR 0.02304) => LSC_loss 0.76, Spatial_loss 1.46, Flat_loss 0.17, Train_acc 79.21, Test_acc 52.42
2025-01-05 14:27:16,849 [podnet.py] => Task 8, Epoch 110/160 (LR 0.02222) => LSC_loss 0.75, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 79.47, Test_acc 53.60
2025-01-05 14:27:24,890 [podnet.py] => Task 8, Epoch 111/160 (LR 0.02141) => LSC_loss 0.75, Spatial_loss 1.42, Flat_loss 0.16, Train_acc 79.54, Test_acc 55.78
2025-01-05 14:27:32,695 [podnet.py] => Task 8, Epoch 112/160 (LR 0.02061) => LSC_loss 0.74, Spatial_loss 1.42, Flat_loss 0.16, Train_acc 79.96, Test_acc 54.42
2025-01-05 14:27:40,857 [podnet.py] => Task 8, Epoch 113/160 (LR 0.01982) => LSC_loss 0.73, Spatial_loss 1.41, Flat_loss 0.16, Train_acc 80.10, Test_acc 54.31
2025-01-05 14:27:48,953 [podnet.py] => Task 8, Epoch 114/160 (LR 0.01905) => LSC_loss 0.71, Spatial_loss 1.38, Flat_loss 0.15, Train_acc 80.88, Test_acc 53.16
2025-01-05 14:27:56,964 [podnet.py] => Task 8, Epoch 115/160 (LR 0.01828) => LSC_loss 0.70, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 81.12, Test_acc 55.00
2025-01-05 14:28:04,969 [podnet.py] => Task 8, Epoch 116/160 (LR 0.01753) => LSC_loss 0.69, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 81.60, Test_acc 51.38
2025-01-05 14:28:12,954 [podnet.py] => Task 8, Epoch 117/160 (LR 0.01679) => LSC_loss 0.68, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 81.83, Test_acc 54.36
2025-01-05 14:28:20,760 [podnet.py] => Task 8, Epoch 118/160 (LR 0.01606) => LSC_loss 0.67, Spatial_loss 1.34, Flat_loss 0.15, Train_acc 82.14, Test_acc 55.33
2025-01-05 14:28:28,478 [podnet.py] => Task 8, Epoch 119/160 (LR 0.01535) => LSC_loss 0.66, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 82.42, Test_acc 54.76
2025-01-05 14:28:36,396 [podnet.py] => Task 8, Epoch 120/160 (LR 0.01464) => LSC_loss 0.65, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 82.40, Test_acc 57.29
2025-01-05 14:28:44,538 [podnet.py] => Task 8, Epoch 121/160 (LR 0.01396) => LSC_loss 0.64, Spatial_loss 1.30, Flat_loss 0.14, Train_acc 82.77, Test_acc 54.47
2025-01-05 14:28:52,402 [podnet.py] => Task 8, Epoch 122/160 (LR 0.01328) => LSC_loss 0.62, Spatial_loss 1.27, Flat_loss 0.14, Train_acc 83.44, Test_acc 55.64
2025-01-05 14:29:00,618 [podnet.py] => Task 8, Epoch 123/160 (LR 0.01262) => LSC_loss 0.62, Spatial_loss 1.27, Flat_loss 0.14, Train_acc 83.49, Test_acc 54.80
2025-01-05 14:29:08,616 [podnet.py] => Task 8, Epoch 124/160 (LR 0.01198) => LSC_loss 0.61, Spatial_loss 1.24, Flat_loss 0.13, Train_acc 83.68, Test_acc 55.69
2025-01-05 14:29:16,695 [podnet.py] => Task 8, Epoch 125/160 (LR 0.01135) => LSC_loss 0.60, Spatial_loss 1.23, Flat_loss 0.13, Train_acc 84.22, Test_acc 54.60
2025-01-05 14:29:24,719 [podnet.py] => Task 8, Epoch 126/160 (LR 0.01073) => LSC_loss 0.58, Spatial_loss 1.20, Flat_loss 0.13, Train_acc 84.66, Test_acc 56.13
2025-01-05 14:29:32,708 [podnet.py] => Task 8, Epoch 127/160 (LR 0.01013) => LSC_loss 0.57, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 85.06, Test_acc 56.78
2025-01-05 14:29:40,467 [podnet.py] => Task 8, Epoch 128/160 (LR 0.00955) => LSC_loss 0.57, Spatial_loss 1.19, Flat_loss 0.12, Train_acc 85.04, Test_acc 55.18
2025-01-05 14:29:48,436 [podnet.py] => Task 8, Epoch 129/160 (LR 0.00898) => LSC_loss 0.56, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 85.28, Test_acc 57.20
2025-01-05 14:29:56,502 [podnet.py] => Task 8, Epoch 130/160 (LR 0.00843) => LSC_loss 0.55, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 85.89, Test_acc 57.93
2025-01-05 14:30:04,478 [podnet.py] => Task 8, Epoch 131/160 (LR 0.00789) => LSC_loss 0.54, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 86.00, Test_acc 56.38
2025-01-05 14:30:12,501 [podnet.py] => Task 8, Epoch 132/160 (LR 0.00737) => LSC_loss 0.53, Spatial_loss 1.13, Flat_loss 0.12, Train_acc 86.30, Test_acc 57.04
2025-01-05 14:30:20,523 [podnet.py] => Task 8, Epoch 133/160 (LR 0.00686) => LSC_loss 0.53, Spatial_loss 1.11, Flat_loss 0.12, Train_acc 86.79, Test_acc 56.29
2025-01-05 14:30:28,268 [podnet.py] => Task 8, Epoch 134/160 (LR 0.00638) => LSC_loss 0.52, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 87.03, Test_acc 55.80
2025-01-05 14:30:36,215 [podnet.py] => Task 8, Epoch 135/160 (LR 0.00590) => LSC_loss 0.52, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 86.73, Test_acc 57.20
2025-01-05 14:30:44,284 [podnet.py] => Task 8, Epoch 136/160 (LR 0.00545) => LSC_loss 0.51, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 87.18, Test_acc 57.16
2025-01-05 14:30:52,196 [podnet.py] => Task 8, Epoch 137/160 (LR 0.00501) => LSC_loss 0.50, Spatial_loss 1.04, Flat_loss 0.11, Train_acc 87.35, Test_acc 57.31
2025-01-05 14:31:00,349 [podnet.py] => Task 8, Epoch 138/160 (LR 0.00459) => LSC_loss 0.50, Spatial_loss 1.04, Flat_loss 0.11, Train_acc 87.36, Test_acc 57.20
2025-01-05 14:31:08,512 [podnet.py] => Task 8, Epoch 139/160 (LR 0.00419) => LSC_loss 0.50, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 87.28, Test_acc 57.36
2025-01-05 14:31:16,577 [podnet.py] => Task 8, Epoch 140/160 (LR 0.00381) => LSC_loss 0.48, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 87.97, Test_acc 57.64
2025-01-05 14:31:24,630 [podnet.py] => Task 8, Epoch 141/160 (LR 0.00344) => LSC_loss 0.48, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 88.43, Test_acc 58.42
2025-01-05 14:31:32,558 [podnet.py] => Task 8, Epoch 142/160 (LR 0.00309) => LSC_loss 0.48, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 88.27, Test_acc 57.42
2025-01-05 14:31:40,675 [podnet.py] => Task 8, Epoch 143/160 (LR 0.00276) => LSC_loss 0.48, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 88.31, Test_acc 57.47
2025-01-05 14:31:48,497 [podnet.py] => Task 8, Epoch 144/160 (LR 0.00245) => LSC_loss 0.46, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 88.49, Test_acc 57.93
2025-01-05 14:31:56,269 [podnet.py] => Task 8, Epoch 145/160 (LR 0.00215) => LSC_loss 0.47, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 88.41, Test_acc 58.18
2025-01-05 14:32:04,291 [podnet.py] => Task 8, Epoch 146/160 (LR 0.00188) => LSC_loss 0.46, Spatial_loss 0.95, Flat_loss 0.10, Train_acc 88.81, Test_acc 58.38
2025-01-05 14:32:12,080 [podnet.py] => Task 8, Epoch 147/160 (LR 0.00162) => LSC_loss 0.45, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 88.98, Test_acc 57.98
2025-01-05 14:32:19,813 [podnet.py] => Task 8, Epoch 148/160 (LR 0.00138) => LSC_loss 0.46, Spatial_loss 0.95, Flat_loss 0.10, Train_acc 89.02, Test_acc 58.38
2025-01-05 14:32:27,859 [podnet.py] => Task 8, Epoch 149/160 (LR 0.00116) => LSC_loss 0.45, Spatial_loss 0.93, Flat_loss 0.10, Train_acc 88.90, Test_acc 58.04
2025-01-05 14:32:35,639 [podnet.py] => Task 8, Epoch 150/160 (LR 0.00096) => LSC_loss 0.44, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 89.50, Test_acc 57.82
2025-01-05 14:32:43,586 [podnet.py] => Task 8, Epoch 151/160 (LR 0.00078) => LSC_loss 0.45, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 89.22, Test_acc 58.36
2025-01-05 14:32:51,663 [podnet.py] => Task 8, Epoch 152/160 (LR 0.00062) => LSC_loss 0.44, Spatial_loss 0.94, Flat_loss 0.09, Train_acc 89.63, Test_acc 58.04
2025-01-05 14:32:59,839 [podnet.py] => Task 8, Epoch 153/160 (LR 0.00047) => LSC_loss 0.44, Spatial_loss 0.93, Flat_loss 0.09, Train_acc 89.22, Test_acc 58.47
2025-01-05 14:33:07,663 [podnet.py] => Task 8, Epoch 154/160 (LR 0.00035) => LSC_loss 0.43, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 89.79, Test_acc 58.44
2025-01-05 14:33:15,419 [podnet.py] => Task 8, Epoch 155/160 (LR 0.00024) => LSC_loss 0.43, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 89.56, Test_acc 58.07
2025-01-05 14:33:23,472 [podnet.py] => Task 8, Epoch 156/160 (LR 0.00015) => LSC_loss 0.43, Spatial_loss 0.92, Flat_loss 0.09, Train_acc 89.91, Test_acc 58.22
2025-01-05 14:33:31,445 [podnet.py] => Task 8, Epoch 157/160 (LR 0.00009) => LSC_loss 0.44, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 89.73, Test_acc 58.07
2025-01-05 14:33:39,604 [podnet.py] => Task 8, Epoch 158/160 (LR 0.00004) => LSC_loss 0.43, Spatial_loss 0.90, Flat_loss 0.09, Train_acc 89.56, Test_acc 58.09
2025-01-05 14:33:47,678 [podnet.py] => Task 8, Epoch 159/160 (LR 0.00001) => LSC_loss 0.44, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 89.46, Test_acc 58.07
2025-01-05 14:33:55,569 [podnet.py] => Task 8, Epoch 160/160 (LR 0.00000) => LSC_loss 0.44, Spatial_loss 0.91, Flat_loss 0.09, Train_acc 89.47, Test_acc 58.20
2025-01-05 14:33:55,570 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 14:33:55,570 [base.py] => Reducing exemplars...(336 per classes)
2025-01-05 14:34:30,662 [base.py] => Constructing exemplars...(336 per classes)
2025-01-05 14:34:39,504 [podnet.py] => The size of finetune dataset: 15120
2025-01-05 14:34:47,272 [podnet.py] => Task 8, Epoch 1/20 (LR 0.00497) => LSC_loss 0.46, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 88.70, Test_acc 57.31
2025-01-05 14:34:54,952 [podnet.py] => Task 8, Epoch 2/20 (LR 0.00488) => LSC_loss 0.46, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 88.44, Test_acc 56.87
2025-01-05 14:35:02,421 [podnet.py] => Task 8, Epoch 3/20 (LR 0.00473) => LSC_loss 0.46, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 88.58, Test_acc 57.20
2025-01-05 14:35:09,959 [podnet.py] => Task 8, Epoch 4/20 (LR 0.00452) => LSC_loss 0.47, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.27, Test_acc 57.64
2025-01-05 14:35:17,501 [podnet.py] => Task 8, Epoch 5/20 (LR 0.00427) => LSC_loss 0.46, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 88.91, Test_acc 56.73
2025-01-05 14:35:25,267 [podnet.py] => Task 8, Epoch 6/20 (LR 0.00397) => LSC_loss 0.45, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 89.15, Test_acc 57.73
2025-01-05 14:35:32,804 [podnet.py] => Task 8, Epoch 7/20 (LR 0.00363) => LSC_loss 0.45, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.78, Test_acc 57.49
2025-01-05 14:35:40,502 [podnet.py] => Task 8, Epoch 8/20 (LR 0.00327) => LSC_loss 0.46, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 88.71, Test_acc 57.13
2025-01-05 14:35:48,073 [podnet.py] => Task 8, Epoch 9/20 (LR 0.00289) => LSC_loss 0.43, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 89.47, Test_acc 57.98
2025-01-05 14:35:55,899 [podnet.py] => Task 8, Epoch 10/20 (LR 0.00250) => LSC_loss 0.44, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 89.33, Test_acc 57.24
2025-01-05 14:36:03,627 [podnet.py] => Task 8, Epoch 11/20 (LR 0.00211) => LSC_loss 0.43, Spatial_loss 0.96, Flat_loss 0.09, Train_acc 89.65, Test_acc 57.62
2025-01-05 14:36:11,537 [podnet.py] => Task 8, Epoch 12/20 (LR 0.00173) => LSC_loss 0.43, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 89.58, Test_acc 57.87
2025-01-05 14:36:19,335 [podnet.py] => Task 8, Epoch 13/20 (LR 0.00137) => LSC_loss 0.43, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 89.78, Test_acc 57.84
2025-01-05 14:36:26,642 [podnet.py] => Task 8, Epoch 14/20 (LR 0.00103) => LSC_loss 0.42, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 90.09, Test_acc 58.20
2025-01-05 14:36:34,301 [podnet.py] => Task 8, Epoch 15/20 (LR 0.00073) => LSC_loss 0.41, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 89.93, Test_acc 57.93
2025-01-05 14:36:41,866 [podnet.py] => Task 8, Epoch 16/20 (LR 0.00048) => LSC_loss 0.41, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 90.00, Test_acc 58.00
2025-01-05 14:36:49,469 [podnet.py] => Task 8, Epoch 17/20 (LR 0.00027) => LSC_loss 0.41, Spatial_loss 0.88, Flat_loss 0.08, Train_acc 90.19, Test_acc 58.24
2025-01-05 14:36:57,120 [podnet.py] => Task 8, Epoch 18/20 (LR 0.00012) => LSC_loss 0.40, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 90.28, Test_acc 58.31
2025-01-05 14:37:05,064 [podnet.py] => Task 8, Epoch 19/20 (LR 0.00003) => LSC_loss 0.41, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 90.53, Test_acc 57.80
2025-01-05 14:37:12,900 [podnet.py] => Task 8, Epoch 20/20 (LR 0.00000) => LSC_loss 0.41, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 90.30, Test_acc 58.40
2025-01-05 14:37:12,904 [base.py] => Reducing exemplars...(299 per classes)
2025-01-05 14:37:47,194 [base.py] => Constructing exemplars...(299 per classes)
2025-01-05 14:37:57,838 [podnet.py] => Exemplar size: 13455
2025-01-05 14:37:57,838 [trainer.py] => CNN: {'total': np.float64(58.4), '00-09': np.float64(68.1), '10-19': np.float64(49.4), '20-29': np.float64(65.1), '30-39': np.float64(53.6), '40-49': np.float64(53.2), 'old': np.float64(59.05), 'new': np.float64(53.2)}
2025-01-05 14:37:57,838 [trainer.py] => NME: {'total': np.float64(56.67), '00-09': np.float64(69.4), '10-19': np.float64(47.2), '20-29': np.float64(62.0), '30-39': np.float64(52.5), '40-49': np.float64(47.8), 'old': np.float64(57.78), 'new': np.float64(47.8)}
2025-01-05 14:37:57,838 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4)]
2025-01-05 14:37:57,838 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36)]
2025-01-05 14:37:57,838 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67)]
2025-01-05 14:37:57,838 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2)]

2025-01-05 14:37:57,839 [trainer.py] => All params: 495057
2025-01-05 14:37:57,839 [trainer.py] => Trainable params: 495057
2025-01-05 14:37:57,839 [podnet.py] => Learning on 45-50
2025-01-05 14:37:57,888 [podnet.py] => Adaptive factor: 3.1622776601683795
2025-01-05 14:38:05,917 [podnet.py] => Task 9, Epoch 1/160 (LR 0.09999) => LSC_loss 2.21, Spatial_loss 3.08, Flat_loss 0.57, Train_acc 44.14, Test_acc 38.44
2025-01-05 14:38:13,771 [podnet.py] => Task 9, Epoch 2/160 (LR 0.09996) => LSC_loss 1.70, Spatial_loss 2.63, Flat_loss 0.41, Train_acc 53.77, Test_acc 39.38
2025-01-05 14:38:21,546 [podnet.py] => Task 9, Epoch 3/160 (LR 0.09991) => LSC_loss 1.60, Spatial_loss 2.49, Flat_loss 0.37, Train_acc 56.51, Test_acc 43.64
2025-01-05 14:38:29,797 [podnet.py] => Task 9, Epoch 4/160 (LR 0.09985) => LSC_loss 1.54, Spatial_loss 2.39, Flat_loss 0.35, Train_acc 57.59, Test_acc 37.74
2025-01-05 14:38:37,607 [podnet.py] => Task 9, Epoch 5/160 (LR 0.09976) => LSC_loss 1.51, Spatial_loss 2.37, Flat_loss 0.33, Train_acc 58.31, Test_acc 43.76
2025-01-05 14:38:45,357 [podnet.py] => Task 9, Epoch 6/160 (LR 0.09965) => LSC_loss 1.48, Spatial_loss 2.34, Flat_loss 0.33, Train_acc 59.04, Test_acc 40.52
2025-01-05 14:38:53,175 [podnet.py] => Task 9, Epoch 7/160 (LR 0.09953) => LSC_loss 1.46, Spatial_loss 2.31, Flat_loss 0.32, Train_acc 60.51, Test_acc 43.80
2025-01-05 14:39:01,113 [podnet.py] => Task 9, Epoch 8/160 (LR 0.09938) => LSC_loss 1.42, Spatial_loss 2.23, Flat_loss 0.32, Train_acc 60.93, Test_acc 47.54
2025-01-05 14:39:09,607 [podnet.py] => Task 9, Epoch 9/160 (LR 0.09922) => LSC_loss 1.43, Spatial_loss 2.27, Flat_loss 0.32, Train_acc 60.88, Test_acc 49.56
2025-01-05 14:39:17,660 [podnet.py] => Task 9, Epoch 10/160 (LR 0.09904) => LSC_loss 1.41, Spatial_loss 2.24, Flat_loss 0.32, Train_acc 61.17, Test_acc 46.36
2025-01-05 14:39:25,820 [podnet.py] => Task 9, Epoch 11/160 (LR 0.09884) => LSC_loss 1.38, Spatial_loss 2.24, Flat_loss 0.31, Train_acc 62.17, Test_acc 42.66
2025-01-05 14:39:34,006 [podnet.py] => Task 9, Epoch 12/160 (LR 0.09862) => LSC_loss 1.39, Spatial_loss 2.25, Flat_loss 0.31, Train_acc 62.07, Test_acc 48.96
2025-01-05 14:39:42,113 [podnet.py] => Task 9, Epoch 13/160 (LR 0.09838) => LSC_loss 1.39, Spatial_loss 2.25, Flat_loss 0.31, Train_acc 62.09, Test_acc 45.30
2025-01-05 14:39:50,345 [podnet.py] => Task 9, Epoch 14/160 (LR 0.09812) => LSC_loss 1.39, Spatial_loss 2.23, Flat_loss 0.31, Train_acc 61.84, Test_acc 46.64
2025-01-05 14:39:58,525 [podnet.py] => Task 9, Epoch 15/160 (LR 0.09785) => LSC_loss 1.37, Spatial_loss 2.20, Flat_loss 0.30, Train_acc 62.26, Test_acc 49.88
2025-01-05 14:40:06,756 [podnet.py] => Task 9, Epoch 16/160 (LR 0.09755) => LSC_loss 1.35, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 62.83, Test_acc 44.18
2025-01-05 14:40:14,848 [podnet.py] => Task 9, Epoch 17/160 (LR 0.09724) => LSC_loss 1.37, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 61.98, Test_acc 45.44
2025-01-05 14:40:22,918 [podnet.py] => Task 9, Epoch 18/160 (LR 0.09691) => LSC_loss 1.34, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 62.62, Test_acc 47.04
2025-01-05 14:40:30,837 [podnet.py] => Task 9, Epoch 19/160 (LR 0.09656) => LSC_loss 1.34, Spatial_loss 2.21, Flat_loss 0.30, Train_acc 63.32, Test_acc 44.24
2025-01-05 14:40:39,002 [podnet.py] => Task 9, Epoch 20/160 (LR 0.09619) => LSC_loss 1.34, Spatial_loss 2.21, Flat_loss 0.30, Train_acc 63.13, Test_acc 41.94
2025-01-05 14:40:47,257 [podnet.py] => Task 9, Epoch 21/160 (LR 0.09581) => LSC_loss 1.33, Spatial_loss 2.23, Flat_loss 0.30, Train_acc 63.48, Test_acc 42.98
2025-01-05 14:40:55,492 [podnet.py] => Task 9, Epoch 22/160 (LR 0.09541) => LSC_loss 1.34, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 63.15, Test_acc 45.82
2025-01-05 14:41:03,412 [podnet.py] => Task 9, Epoch 23/160 (LR 0.09499) => LSC_loss 1.33, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 62.90, Test_acc 46.94
2025-01-05 14:41:11,334 [podnet.py] => Task 9, Epoch 24/160 (LR 0.09455) => LSC_loss 1.33, Spatial_loss 2.22, Flat_loss 0.30, Train_acc 63.37, Test_acc 46.12
2025-01-05 14:41:19,363 [podnet.py] => Task 9, Epoch 25/160 (LR 0.09410) => LSC_loss 1.33, Spatial_loss 2.21, Flat_loss 0.30, Train_acc 63.57, Test_acc 44.78
2025-01-05 14:41:27,509 [podnet.py] => Task 9, Epoch 26/160 (LR 0.09362) => LSC_loss 1.33, Spatial_loss 2.16, Flat_loss 0.30, Train_acc 63.35, Test_acc 45.62
2025-01-05 14:41:35,603 [podnet.py] => Task 9, Epoch 27/160 (LR 0.09314) => LSC_loss 1.32, Spatial_loss 2.16, Flat_loss 0.30, Train_acc 63.87, Test_acc 47.46
2025-01-05 14:41:43,805 [podnet.py] => Task 9, Epoch 28/160 (LR 0.09263) => LSC_loss 1.29, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 64.98, Test_acc 48.38
2025-01-05 14:41:51,948 [podnet.py] => Task 9, Epoch 29/160 (LR 0.09211) => LSC_loss 1.31, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 63.43, Test_acc 49.08
2025-01-05 14:42:00,100 [podnet.py] => Task 9, Epoch 30/160 (LR 0.09157) => LSC_loss 1.30, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 64.52, Test_acc 44.48
2025-01-05 14:42:08,168 [podnet.py] => Task 9, Epoch 31/160 (LR 0.09102) => LSC_loss 1.31, Spatial_loss 2.16, Flat_loss 0.30, Train_acc 63.57, Test_acc 42.40
2025-01-05 14:42:16,390 [podnet.py] => Task 9, Epoch 32/160 (LR 0.09045) => LSC_loss 1.32, Spatial_loss 2.21, Flat_loss 0.30, Train_acc 63.42, Test_acc 47.52
2025-01-05 14:42:24,519 [podnet.py] => Task 9, Epoch 33/160 (LR 0.08987) => LSC_loss 1.29, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 64.31, Test_acc 44.80
2025-01-05 14:42:32,732 [podnet.py] => Task 9, Epoch 34/160 (LR 0.08927) => LSC_loss 1.29, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 64.37, Test_acc 44.64
2025-01-05 14:42:40,737 [podnet.py] => Task 9, Epoch 35/160 (LR 0.08865) => LSC_loss 1.30, Spatial_loss 2.19, Flat_loss 0.30, Train_acc 63.93, Test_acc 44.42
2025-01-05 14:42:48,728 [podnet.py] => Task 9, Epoch 36/160 (LR 0.08802) => LSC_loss 1.30, Spatial_loss 2.17, Flat_loss 0.30, Train_acc 64.11, Test_acc 45.64
2025-01-05 14:42:57,059 [podnet.py] => Task 9, Epoch 37/160 (LR 0.08738) => LSC_loss 1.28, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 64.46, Test_acc 42.46
2025-01-05 14:43:04,999 [podnet.py] => Task 9, Epoch 38/160 (LR 0.08672) => LSC_loss 1.28, Spatial_loss 2.17, Flat_loss 0.29, Train_acc 64.69, Test_acc 46.62
2025-01-05 14:43:13,353 [podnet.py] => Task 9, Epoch 39/160 (LR 0.08604) => LSC_loss 1.28, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 64.55, Test_acc 45.38
2025-01-05 14:43:21,352 [podnet.py] => Task 9, Epoch 40/160 (LR 0.08536) => LSC_loss 1.27, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 64.74, Test_acc 49.86
2025-01-05 14:43:29,375 [podnet.py] => Task 9, Epoch 41/160 (LR 0.08465) => LSC_loss 1.27, Spatial_loss 2.11, Flat_loss 0.29, Train_acc 64.76, Test_acc 48.72
2025-01-05 14:43:37,435 [podnet.py] => Task 9, Epoch 42/160 (LR 0.08394) => LSC_loss 1.26, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 65.09, Test_acc 49.58
2025-01-05 14:43:45,619 [podnet.py] => Task 9, Epoch 43/160 (LR 0.08321) => LSC_loss 1.25, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 65.33, Test_acc 48.28
2025-01-05 14:43:53,640 [podnet.py] => Task 9, Epoch 44/160 (LR 0.08247) => LSC_loss 1.26, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 65.29, Test_acc 48.80
2025-01-05 14:44:01,995 [podnet.py] => Task 9, Epoch 45/160 (LR 0.08172) => LSC_loss 1.27, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 64.93, Test_acc 48.16
2025-01-05 14:44:10,314 [podnet.py] => Task 9, Epoch 46/160 (LR 0.08095) => LSC_loss 1.23, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 66.37, Test_acc 48.20
2025-01-05 14:44:18,557 [podnet.py] => Task 9, Epoch 47/160 (LR 0.08018) => LSC_loss 1.26, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 65.08, Test_acc 46.92
2025-01-05 14:44:26,821 [podnet.py] => Task 9, Epoch 48/160 (LR 0.07939) => LSC_loss 1.24, Spatial_loss 2.11, Flat_loss 0.28, Train_acc 65.67, Test_acc 47.04
2025-01-05 14:44:34,940 [podnet.py] => Task 9, Epoch 49/160 (LR 0.07859) => LSC_loss 1.25, Spatial_loss 2.10, Flat_loss 0.28, Train_acc 65.65, Test_acc 46.86
2025-01-05 14:44:43,344 [podnet.py] => Task 9, Epoch 50/160 (LR 0.07778) => LSC_loss 1.23, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 66.09, Test_acc 48.12
2025-01-05 14:44:51,723 [podnet.py] => Task 9, Epoch 51/160 (LR 0.07696) => LSC_loss 1.23, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 65.95, Test_acc 47.08
2025-01-05 14:44:59,940 [podnet.py] => Task 9, Epoch 52/160 (LR 0.07612) => LSC_loss 1.24, Spatial_loss 2.08, Flat_loss 0.28, Train_acc 65.46, Test_acc 46.34
2025-01-05 14:45:08,873 [podnet.py] => Task 9, Epoch 53/160 (LR 0.07528) => LSC_loss 1.21, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 66.47, Test_acc 44.58
2025-01-05 14:45:17,221 [podnet.py] => Task 9, Epoch 54/160 (LR 0.07443) => LSC_loss 1.22, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 65.75, Test_acc 48.84
2025-01-05 14:45:25,202 [podnet.py] => Task 9, Epoch 55/160 (LR 0.07357) => LSC_loss 1.20, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 66.42, Test_acc 41.86
2025-01-05 14:45:33,200 [podnet.py] => Task 9, Epoch 56/160 (LR 0.07270) => LSC_loss 1.20, Spatial_loss 2.04, Flat_loss 0.27, Train_acc 67.10, Test_acc 46.56
2025-01-05 14:45:41,382 [podnet.py] => Task 9, Epoch 57/160 (LR 0.07182) => LSC_loss 1.18, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 67.28, Test_acc 48.98
2025-01-05 14:45:49,585 [podnet.py] => Task 9, Epoch 58/160 (LR 0.07093) => LSC_loss 1.21, Spatial_loss 2.07, Flat_loss 0.27, Train_acc 66.91, Test_acc 45.48
2025-01-05 14:45:57,798 [podnet.py] => Task 9, Epoch 59/160 (LR 0.07004) => LSC_loss 1.18, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 67.62, Test_acc 43.38
2025-01-05 14:46:06,354 [podnet.py] => Task 9, Epoch 60/160 (LR 0.06913) => LSC_loss 1.17, Spatial_loss 2.02, Flat_loss 0.26, Train_acc 67.38, Test_acc 47.10
2025-01-05 14:46:15,176 [podnet.py] => Task 9, Epoch 61/160 (LR 0.06822) => LSC_loss 1.16, Spatial_loss 1.96, Flat_loss 0.26, Train_acc 67.52, Test_acc 44.16
2025-01-05 14:46:23,562 [podnet.py] => Task 9, Epoch 62/160 (LR 0.06731) => LSC_loss 1.16, Spatial_loss 1.99, Flat_loss 0.26, Train_acc 67.88, Test_acc 46.42
2025-01-05 14:46:31,749 [podnet.py] => Task 9, Epoch 63/160 (LR 0.06638) => LSC_loss 1.15, Spatial_loss 1.99, Flat_loss 0.26, Train_acc 67.51, Test_acc 49.28
2025-01-05 14:46:39,918 [podnet.py] => Task 9, Epoch 64/160 (LR 0.06545) => LSC_loss 1.15, Spatial_loss 1.99, Flat_loss 0.26, Train_acc 67.97, Test_acc 48.90
2025-01-05 14:46:47,998 [podnet.py] => Task 9, Epoch 65/160 (LR 0.06451) => LSC_loss 1.16, Spatial_loss 2.00, Flat_loss 0.26, Train_acc 67.90, Test_acc 47.60
2025-01-05 14:46:56,122 [podnet.py] => Task 9, Epoch 66/160 (LR 0.06357) => LSC_loss 1.14, Spatial_loss 1.99, Flat_loss 0.26, Train_acc 68.52, Test_acc 45.80
2025-01-05 14:47:04,293 [podnet.py] => Task 9, Epoch 67/160 (LR 0.06262) => LSC_loss 1.12, Spatial_loss 1.97, Flat_loss 0.25, Train_acc 69.07, Test_acc 46.58
2025-01-05 14:47:12,446 [podnet.py] => Task 9, Epoch 68/160 (LR 0.06167) => LSC_loss 1.11, Spatial_loss 1.94, Flat_loss 0.25, Train_acc 69.18, Test_acc 48.16
2025-01-05 14:47:20,528 [podnet.py] => Task 9, Epoch 69/160 (LR 0.06072) => LSC_loss 1.13, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 69.13, Test_acc 51.10
2025-01-05 14:47:28,667 [podnet.py] => Task 9, Epoch 70/160 (LR 0.05975) => LSC_loss 1.12, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 68.81, Test_acc 47.98
2025-01-05 14:47:36,931 [podnet.py] => Task 9, Epoch 71/160 (LR 0.05879) => LSC_loss 1.10, Spatial_loss 1.94, Flat_loss 0.25, Train_acc 69.03, Test_acc 45.02
2025-01-05 14:47:45,114 [podnet.py] => Task 9, Epoch 72/160 (LR 0.05782) => LSC_loss 1.10, Spatial_loss 1.99, Flat_loss 0.25, Train_acc 69.28, Test_acc 46.36
2025-01-05 14:47:53,150 [podnet.py] => Task 9, Epoch 73/160 (LR 0.05685) => LSC_loss 1.10, Spatial_loss 1.95, Flat_loss 0.24, Train_acc 69.76, Test_acc 48.10
2025-01-05 14:48:01,460 [podnet.py] => Task 9, Epoch 74/160 (LR 0.05588) => LSC_loss 1.08, Spatial_loss 1.88, Flat_loss 0.24, Train_acc 69.92, Test_acc 48.38
2025-01-05 14:48:10,353 [podnet.py] => Task 9, Epoch 75/160 (LR 0.05490) => LSC_loss 1.08, Spatial_loss 1.91, Flat_loss 0.24, Train_acc 70.00, Test_acc 45.28
2025-01-05 14:48:18,962 [podnet.py] => Task 9, Epoch 76/160 (LR 0.05392) => LSC_loss 1.07, Spatial_loss 1.87, Flat_loss 0.24, Train_acc 69.93, Test_acc 51.72
2025-01-05 14:48:27,177 [podnet.py] => Task 9, Epoch 77/160 (LR 0.05294) => LSC_loss 1.08, Spatial_loss 1.88, Flat_loss 0.24, Train_acc 69.87, Test_acc 49.18
2025-01-05 14:48:35,362 [podnet.py] => Task 9, Epoch 78/160 (LR 0.05196) => LSC_loss 1.07, Spatial_loss 1.90, Flat_loss 0.24, Train_acc 69.94, Test_acc 51.12
2025-01-05 14:48:43,606 [podnet.py] => Task 9, Epoch 79/160 (LR 0.05098) => LSC_loss 1.03, Spatial_loss 1.83, Flat_loss 0.23, Train_acc 71.01, Test_acc 49.24
2025-01-05 14:48:51,966 [podnet.py] => Task 9, Epoch 80/160 (LR 0.05000) => LSC_loss 1.04, Spatial_loss 1.85, Flat_loss 0.23, Train_acc 70.95, Test_acc 50.24
2025-01-05 14:49:00,010 [podnet.py] => Task 9, Epoch 81/160 (LR 0.04902) => LSC_loss 1.04, Spatial_loss 1.87, Flat_loss 0.23, Train_acc 71.05, Test_acc 50.38
2025-01-05 14:49:07,952 [podnet.py] => Task 9, Epoch 82/160 (LR 0.04804) => LSC_loss 1.03, Spatial_loss 1.84, Flat_loss 0.23, Train_acc 70.89, Test_acc 47.86
2025-01-05 14:49:16,180 [podnet.py] => Task 9, Epoch 83/160 (LR 0.04706) => LSC_loss 1.01, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 71.80, Test_acc 49.38
2025-01-05 14:49:24,452 [podnet.py] => Task 9, Epoch 84/160 (LR 0.04608) => LSC_loss 0.99, Spatial_loss 1.79, Flat_loss 0.22, Train_acc 72.54, Test_acc 50.90
2025-01-05 14:49:32,587 [podnet.py] => Task 9, Epoch 85/160 (LR 0.04510) => LSC_loss 0.99, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 72.51, Test_acc 46.10
2025-01-05 14:49:40,755 [podnet.py] => Task 9, Epoch 86/160 (LR 0.04412) => LSC_loss 0.99, Spatial_loss 1.79, Flat_loss 0.22, Train_acc 72.30, Test_acc 50.20
2025-01-05 14:49:48,954 [podnet.py] => Task 9, Epoch 87/160 (LR 0.04315) => LSC_loss 0.99, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 72.25, Test_acc 50.08
2025-01-05 14:49:57,371 [podnet.py] => Task 9, Epoch 88/160 (LR 0.04218) => LSC_loss 0.97, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 73.06, Test_acc 49.32
2025-01-05 14:50:05,638 [podnet.py] => Task 9, Epoch 89/160 (LR 0.04121) => LSC_loss 0.98, Spatial_loss 1.79, Flat_loss 0.22, Train_acc 72.66, Test_acc 51.70
2025-01-05 14:50:13,757 [podnet.py] => Task 9, Epoch 90/160 (LR 0.04025) => LSC_loss 0.97, Spatial_loss 1.79, Flat_loss 0.21, Train_acc 72.72, Test_acc 51.12
2025-01-05 14:50:21,992 [podnet.py] => Task 9, Epoch 91/160 (LR 0.03928) => LSC_loss 0.95, Spatial_loss 1.74, Flat_loss 0.21, Train_acc 73.59, Test_acc 49.36
2025-01-05 14:50:30,310 [podnet.py] => Task 9, Epoch 92/160 (LR 0.03833) => LSC_loss 0.94, Spatial_loss 1.75, Flat_loss 0.21, Train_acc 73.58, Test_acc 46.66
2025-01-05 14:50:38,366 [podnet.py] => Task 9, Epoch 93/160 (LR 0.03738) => LSC_loss 0.93, Spatial_loss 1.71, Flat_loss 0.21, Train_acc 73.90, Test_acc 49.30
2025-01-05 14:50:46,543 [podnet.py] => Task 9, Epoch 94/160 (LR 0.03643) => LSC_loss 0.93, Spatial_loss 1.73, Flat_loss 0.20, Train_acc 74.10, Test_acc 50.48
2025-01-05 14:50:54,580 [podnet.py] => Task 9, Epoch 95/160 (LR 0.03549) => LSC_loss 0.90, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 74.95, Test_acc 48.70
2025-01-05 14:51:02,894 [podnet.py] => Task 9, Epoch 96/160 (LR 0.03455) => LSC_loss 0.90, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 75.05, Test_acc 48.22
2025-01-05 14:51:11,469 [podnet.py] => Task 9, Epoch 97/160 (LR 0.03362) => LSC_loss 0.91, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 74.59, Test_acc 50.60
2025-01-05 14:51:19,592 [podnet.py] => Task 9, Epoch 98/160 (LR 0.03269) => LSC_loss 0.88, Spatial_loss 1.65, Flat_loss 0.19, Train_acc 75.59, Test_acc 51.18
2025-01-05 14:51:27,687 [podnet.py] => Task 9, Epoch 99/160 (LR 0.03178) => LSC_loss 0.86, Spatial_loss 1.64, Flat_loss 0.19, Train_acc 76.17, Test_acc 51.18
2025-01-05 14:51:35,830 [podnet.py] => Task 9, Epoch 100/160 (LR 0.03087) => LSC_loss 0.86, Spatial_loss 1.61, Flat_loss 0.18, Train_acc 75.91, Test_acc 50.62
2025-01-05 14:51:44,024 [podnet.py] => Task 9, Epoch 101/160 (LR 0.02996) => LSC_loss 0.85, Spatial_loss 1.62, Flat_loss 0.18, Train_acc 76.58, Test_acc 51.02
2025-01-05 14:51:52,036 [podnet.py] => Task 9, Epoch 102/160 (LR 0.02907) => LSC_loss 0.83, Spatial_loss 1.56, Flat_loss 0.18, Train_acc 77.25, Test_acc 47.22
2025-01-05 14:52:00,281 [podnet.py] => Task 9, Epoch 103/160 (LR 0.02818) => LSC_loss 0.85, Spatial_loss 1.60, Flat_loss 0.18, Train_acc 76.38, Test_acc 51.76
2025-01-05 14:52:08,617 [podnet.py] => Task 9, Epoch 104/160 (LR 0.02730) => LSC_loss 0.83, Spatial_loss 1.59, Flat_loss 0.18, Train_acc 77.08, Test_acc 50.10
2025-01-05 14:52:16,611 [podnet.py] => Task 9, Epoch 105/160 (LR 0.02643) => LSC_loss 0.83, Spatial_loss 1.57, Flat_loss 0.18, Train_acc 77.04, Test_acc 49.04
2025-01-05 14:52:24,774 [podnet.py] => Task 9, Epoch 106/160 (LR 0.02557) => LSC_loss 0.80, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 77.83, Test_acc 52.12
2025-01-05 14:52:32,891 [podnet.py] => Task 9, Epoch 107/160 (LR 0.02472) => LSC_loss 0.80, Spatial_loss 1.54, Flat_loss 0.17, Train_acc 77.61, Test_acc 49.90
2025-01-05 14:52:40,971 [podnet.py] => Task 9, Epoch 108/160 (LR 0.02388) => LSC_loss 0.79, Spatial_loss 1.52, Flat_loss 0.17, Train_acc 78.20, Test_acc 51.36
2025-01-05 14:52:49,017 [podnet.py] => Task 9, Epoch 109/160 (LR 0.02304) => LSC_loss 0.77, Spatial_loss 1.50, Flat_loss 0.16, Train_acc 79.18, Test_acc 51.96
2025-01-05 14:52:57,084 [podnet.py] => Task 9, Epoch 110/160 (LR 0.02222) => LSC_loss 0.76, Spatial_loss 1.48, Flat_loss 0.16, Train_acc 79.07, Test_acc 53.08
2025-01-05 14:53:05,261 [podnet.py] => Task 9, Epoch 111/160 (LR 0.02141) => LSC_loss 0.75, Spatial_loss 1.49, Flat_loss 0.16, Train_acc 79.36, Test_acc 53.74
2025-01-05 14:53:13,515 [podnet.py] => Task 9, Epoch 112/160 (LR 0.02061) => LSC_loss 0.72, Spatial_loss 1.44, Flat_loss 0.15, Train_acc 80.12, Test_acc 52.74
2025-01-05 14:53:21,486 [podnet.py] => Task 9, Epoch 113/160 (LR 0.01982) => LSC_loss 0.73, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 79.91, Test_acc 51.32
2025-01-05 14:53:29,573 [podnet.py] => Task 9, Epoch 114/160 (LR 0.01905) => LSC_loss 0.71, Spatial_loss 1.41, Flat_loss 0.15, Train_acc 80.42, Test_acc 52.86
2025-01-05 14:53:37,751 [podnet.py] => Task 9, Epoch 115/160 (LR 0.01828) => LSC_loss 0.69, Spatial_loss 1.40, Flat_loss 0.15, Train_acc 80.87, Test_acc 53.90
2025-01-05 14:53:45,696 [podnet.py] => Task 9, Epoch 116/160 (LR 0.01753) => LSC_loss 0.69, Spatial_loss 1.38, Flat_loss 0.14, Train_acc 81.18, Test_acc 53.54
2025-01-05 14:53:53,977 [podnet.py] => Task 9, Epoch 117/160 (LR 0.01679) => LSC_loss 0.67, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 81.55, Test_acc 54.02
2025-01-05 14:54:01,993 [podnet.py] => Task 9, Epoch 118/160 (LR 0.01606) => LSC_loss 0.66, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 81.97, Test_acc 54.04
2025-01-05 14:54:09,922 [podnet.py] => Task 9, Epoch 119/160 (LR 0.01535) => LSC_loss 0.66, Spatial_loss 1.37, Flat_loss 0.13, Train_acc 82.12, Test_acc 54.02
2025-01-05 14:54:17,969 [podnet.py] => Task 9, Epoch 120/160 (LR 0.01464) => LSC_loss 0.64, Spatial_loss 1.32, Flat_loss 0.13, Train_acc 82.69, Test_acc 53.94
2025-01-05 14:54:26,029 [podnet.py] => Task 9, Epoch 121/160 (LR 0.01396) => LSC_loss 0.63, Spatial_loss 1.30, Flat_loss 0.13, Train_acc 82.95, Test_acc 54.38
2025-01-05 14:54:34,109 [podnet.py] => Task 9, Epoch 122/160 (LR 0.01328) => LSC_loss 0.62, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 83.34, Test_acc 54.80
2025-01-05 14:54:42,081 [podnet.py] => Task 9, Epoch 123/160 (LR 0.01262) => LSC_loss 0.64, Spatial_loss 1.30, Flat_loss 0.13, Train_acc 82.94, Test_acc 53.86
2025-01-05 14:54:50,350 [podnet.py] => Task 9, Epoch 124/160 (LR 0.01198) => LSC_loss 0.61, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 83.45, Test_acc 54.76
2025-01-05 14:54:58,729 [podnet.py] => Task 9, Epoch 125/160 (LR 0.01135) => LSC_loss 0.60, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 83.95, Test_acc 54.94
2025-01-05 14:55:06,988 [podnet.py] => Task 9, Epoch 126/160 (LR 0.01073) => LSC_loss 0.58, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 84.78, Test_acc 54.54
2025-01-05 14:55:15,239 [podnet.py] => Task 9, Epoch 127/160 (LR 0.01013) => LSC_loss 0.57, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 84.58, Test_acc 53.30
2025-01-05 14:55:23,405 [podnet.py] => Task 9, Epoch 128/160 (LR 0.00955) => LSC_loss 0.57, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 84.95, Test_acc 54.28
2025-01-05 14:55:31,671 [podnet.py] => Task 9, Epoch 129/160 (LR 0.00898) => LSC_loss 0.57, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 85.04, Test_acc 55.12
2025-01-05 14:55:39,667 [podnet.py] => Task 9, Epoch 130/160 (LR 0.00843) => LSC_loss 0.55, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 85.78, Test_acc 54.54
2025-01-05 14:55:47,841 [podnet.py] => Task 9, Epoch 131/160 (LR 0.00789) => LSC_loss 0.55, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 85.68, Test_acc 55.64
2025-01-05 14:55:55,887 [podnet.py] => Task 9, Epoch 132/160 (LR 0.00737) => LSC_loss 0.54, Spatial_loss 1.14, Flat_loss 0.10, Train_acc 85.89, Test_acc 55.50
2025-01-05 14:56:03,796 [podnet.py] => Task 9, Epoch 133/160 (LR 0.00686) => LSC_loss 0.53, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 86.21, Test_acc 56.36
2025-01-05 14:56:11,871 [podnet.py] => Task 9, Epoch 134/160 (LR 0.00638) => LSC_loss 0.51, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 86.80, Test_acc 55.78
2025-01-05 14:56:19,902 [podnet.py] => Task 9, Epoch 135/160 (LR 0.00590) => LSC_loss 0.51, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 86.74, Test_acc 56.70
2025-01-05 14:56:28,000 [podnet.py] => Task 9, Epoch 136/160 (LR 0.00545) => LSC_loss 0.51, Spatial_loss 1.08, Flat_loss 0.09, Train_acc 86.94, Test_acc 55.58
2025-01-05 14:56:35,980 [podnet.py] => Task 9, Epoch 137/160 (LR 0.00501) => LSC_loss 0.51, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 86.98, Test_acc 56.14
2025-01-05 14:56:43,879 [podnet.py] => Task 9, Epoch 138/160 (LR 0.00459) => LSC_loss 0.50, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 87.47, Test_acc 56.26
2025-01-05 14:56:51,782 [podnet.py] => Task 9, Epoch 139/160 (LR 0.00419) => LSC_loss 0.48, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.09, Test_acc 56.00
2025-01-05 14:56:59,737 [podnet.py] => Task 9, Epoch 140/160 (LR 0.00381) => LSC_loss 0.48, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 88.03, Test_acc 56.08
2025-01-05 14:57:07,672 [podnet.py] => Task 9, Epoch 141/160 (LR 0.00344) => LSC_loss 0.48, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.23, Test_acc 56.08
2025-01-05 14:57:15,633 [podnet.py] => Task 9, Epoch 142/160 (LR 0.00309) => LSC_loss 0.47, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 88.09, Test_acc 56.58
2025-01-05 14:57:23,760 [podnet.py] => Task 9, Epoch 143/160 (LR 0.00276) => LSC_loss 0.47, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 88.53, Test_acc 56.70
2025-01-05 14:57:31,764 [podnet.py] => Task 9, Epoch 144/160 (LR 0.00245) => LSC_loss 0.46, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.51, Test_acc 56.54
2025-01-05 14:57:40,030 [podnet.py] => Task 9, Epoch 145/160 (LR 0.00215) => LSC_loss 0.47, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 88.52, Test_acc 56.64
2025-01-05 14:57:48,090 [podnet.py] => Task 9, Epoch 146/160 (LR 0.00188) => LSC_loss 0.46, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.54, Test_acc 56.92
2025-01-05 14:57:56,091 [podnet.py] => Task 9, Epoch 147/160 (LR 0.00162) => LSC_loss 0.46, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 89.06, Test_acc 57.10
2025-01-05 14:58:04,162 [podnet.py] => Task 9, Epoch 148/160 (LR 0.00138) => LSC_loss 0.46, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 88.84, Test_acc 56.68
2025-01-05 14:58:12,066 [podnet.py] => Task 9, Epoch 149/160 (LR 0.00116) => LSC_loss 0.45, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 89.06, Test_acc 57.50
2025-01-05 14:58:20,107 [podnet.py] => Task 9, Epoch 150/160 (LR 0.00096) => LSC_loss 0.45, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 89.11, Test_acc 57.16
2025-01-05 14:58:27,808 [podnet.py] => Task 9, Epoch 151/160 (LR 0.00078) => LSC_loss 0.46, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 88.83, Test_acc 56.92
2025-01-05 14:58:36,075 [podnet.py] => Task 9, Epoch 152/160 (LR 0.00062) => LSC_loss 0.44, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 89.39, Test_acc 57.12
2025-01-05 14:58:44,317 [podnet.py] => Task 9, Epoch 153/160 (LR 0.00047) => LSC_loss 0.44, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 89.13, Test_acc 57.34
2025-01-05 14:58:52,447 [podnet.py] => Task 9, Epoch 154/160 (LR 0.00035) => LSC_loss 0.45, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 89.11, Test_acc 57.16
2025-01-05 14:59:00,508 [podnet.py] => Task 9, Epoch 155/160 (LR 0.00024) => LSC_loss 0.45, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 89.13, Test_acc 57.42
2025-01-05 14:59:08,421 [podnet.py] => Task 9, Epoch 156/160 (LR 0.00015) => LSC_loss 0.45, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.25, Test_acc 57.16
2025-01-05 14:59:16,534 [podnet.py] => Task 9, Epoch 157/160 (LR 0.00009) => LSC_loss 0.44, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 89.39, Test_acc 57.08
2025-01-05 14:59:24,610 [podnet.py] => Task 9, Epoch 158/160 (LR 0.00004) => LSC_loss 0.44, Spatial_loss 0.91, Flat_loss 0.08, Train_acc 89.44, Test_acc 57.24
2025-01-05 14:59:32,776 [podnet.py] => Task 9, Epoch 159/160 (LR 0.00001) => LSC_loss 0.45, Spatial_loss 0.90, Flat_loss 0.08, Train_acc 89.24, Test_acc 57.26
2025-01-05 14:59:40,901 [podnet.py] => Task 9, Epoch 160/160 (LR 0.00000) => LSC_loss 0.43, Spatial_loss 0.89, Flat_loss 0.08, Train_acc 89.56, Test_acc 57.26
2025-01-05 14:59:40,902 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 14:59:40,902 [base.py] => Reducing exemplars...(299 per classes)
2025-01-05 15:00:18,451 [base.py] => Constructing exemplars...(299 per classes)
2025-01-05 15:00:27,436 [podnet.py] => The size of finetune dataset: 14950
2025-01-05 15:00:34,820 [podnet.py] => Task 9, Epoch 1/20 (LR 0.00497) => LSC_loss 0.45, Spatial_loss 1.02, Flat_loss 0.08, Train_acc 88.95, Test_acc 56.08
2025-01-05 15:00:42,460 [podnet.py] => Task 9, Epoch 2/20 (LR 0.00488) => LSC_loss 0.45, Spatial_loss 1.02, Flat_loss 0.08, Train_acc 88.82, Test_acc 55.48
2025-01-05 15:00:50,066 [podnet.py] => Task 9, Epoch 3/20 (LR 0.00473) => LSC_loss 0.45, Spatial_loss 1.01, Flat_loss 0.08, Train_acc 88.78, Test_acc 56.18
2025-01-05 15:00:57,543 [podnet.py] => Task 9, Epoch 4/20 (LR 0.00452) => LSC_loss 0.45, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 88.79, Test_acc 56.36
2025-01-05 15:01:05,488 [podnet.py] => Task 9, Epoch 5/20 (LR 0.00427) => LSC_loss 0.45, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 88.93, Test_acc 56.18
2025-01-05 15:01:13,143 [podnet.py] => Task 9, Epoch 6/20 (LR 0.00397) => LSC_loss 0.44, Spatial_loss 1.01, Flat_loss 0.08, Train_acc 89.31, Test_acc 56.16
2025-01-05 15:01:20,877 [podnet.py] => Task 9, Epoch 7/20 (LR 0.00363) => LSC_loss 0.45, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 88.89, Test_acc 56.66
2025-01-05 15:01:28,570 [podnet.py] => Task 9, Epoch 8/20 (LR 0.00327) => LSC_loss 0.45, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 89.04, Test_acc 55.90
2025-01-05 15:01:36,132 [podnet.py] => Task 9, Epoch 9/20 (LR 0.00289) => LSC_loss 0.43, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 89.28, Test_acc 56.94
2025-01-05 15:01:43,858 [podnet.py] => Task 9, Epoch 10/20 (LR 0.00250) => LSC_loss 0.43, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 89.56, Test_acc 56.44
2025-01-05 15:01:51,360 [podnet.py] => Task 9, Epoch 11/20 (LR 0.00211) => LSC_loss 0.42, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 90.03, Test_acc 56.32
2025-01-05 15:01:59,054 [podnet.py] => Task 9, Epoch 12/20 (LR 0.00173) => LSC_loss 0.43, Spatial_loss 0.97, Flat_loss 0.07, Train_acc 89.64, Test_acc 56.80
2025-01-05 15:02:06,663 [podnet.py] => Task 9, Epoch 13/20 (LR 0.00137) => LSC_loss 0.42, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 89.81, Test_acc 56.98
2025-01-05 15:02:14,280 [podnet.py] => Task 9, Epoch 14/20 (LR 0.00103) => LSC_loss 0.41, Spatial_loss 0.91, Flat_loss 0.07, Train_acc 90.11, Test_acc 56.72
2025-01-05 15:02:21,925 [podnet.py] => Task 9, Epoch 15/20 (LR 0.00073) => LSC_loss 0.41, Spatial_loss 0.92, Flat_loss 0.07, Train_acc 90.16, Test_acc 57.00
2025-01-05 15:02:29,697 [podnet.py] => Task 9, Epoch 16/20 (LR 0.00048) => LSC_loss 0.41, Spatial_loss 0.90, Flat_loss 0.07, Train_acc 90.26, Test_acc 57.18
2025-01-05 15:02:37,254 [podnet.py] => Task 9, Epoch 17/20 (LR 0.00027) => LSC_loss 0.40, Spatial_loss 0.91, Flat_loss 0.07, Train_acc 90.62, Test_acc 57.08
2025-01-05 15:02:44,711 [podnet.py] => Task 9, Epoch 18/20 (LR 0.00012) => LSC_loss 0.41, Spatial_loss 0.87, Flat_loss 0.07, Train_acc 90.35, Test_acc 57.04
2025-01-05 15:02:52,358 [podnet.py] => Task 9, Epoch 19/20 (LR 0.00003) => LSC_loss 0.40, Spatial_loss 0.90, Flat_loss 0.07, Train_acc 90.47, Test_acc 56.92
2025-01-05 15:03:00,025 [podnet.py] => Task 9, Epoch 20/20 (LR 0.00000) => LSC_loss 0.40, Spatial_loss 0.88, Flat_loss 0.07, Train_acc 90.41, Test_acc 57.24
2025-01-05 15:03:00,029 [base.py] => Reducing exemplars...(269 per classes)
2025-01-05 15:03:37,870 [base.py] => Constructing exemplars...(269 per classes)
2025-01-05 15:03:48,106 [podnet.py] => Exemplar size: 13450
2025-01-05 15:03:48,106 [trainer.py] => CNN: {'total': np.float64(57.24), '00-09': np.float64(67.7), '10-19': np.float64(47.9), '20-29': np.float64(61.5), '30-39': np.float64(52.1), '40-49': np.float64(57.0), 'old': np.float64(57.44), 'new': np.float64(55.4)}
2025-01-05 15:03:48,106 [trainer.py] => NME: {'total': np.float64(55.34), '00-09': np.float64(67.6), '10-19': np.float64(45.4), '20-29': np.float64(59.8), '30-39': np.float64(50.5), '40-49': np.float64(53.4), 'old': np.float64(55.31), 'new': np.float64(55.6)}
2025-01-05 15:03:48,106 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24)]
2025-01-05 15:03:48,106 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78)]
2025-01-05 15:03:48,106 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34)]
2025-01-05 15:03:48,106 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86)]

2025-01-05 15:03:48,107 [trainer.py] => All params: 498257
2025-01-05 15:03:48,107 [trainer.py] => Trainable params: 498257
2025-01-05 15:03:48,107 [podnet.py] => Learning on 50-55
2025-01-05 15:03:48,156 [podnet.py] => Adaptive factor: 3.3166247903554
2025-01-05 15:03:56,236 [podnet.py] => Task 10, Epoch 1/160 (LR 0.09999) => LSC_loss 2.20, Spatial_loss 3.19, Flat_loss 0.61, Train_acc 43.69, Test_acc 38.07
2025-01-05 15:04:04,426 [podnet.py] => Task 10, Epoch 2/160 (LR 0.09996) => LSC_loss 1.78, Spatial_loss 2.78, Flat_loss 0.46, Train_acc 51.47, Test_acc 37.58
2025-01-05 15:04:12,613 [podnet.py] => Task 10, Epoch 3/160 (LR 0.09991) => LSC_loss 1.66, Spatial_loss 2.61, Flat_loss 0.41, Train_acc 54.90, Test_acc 41.96
2025-01-05 15:04:20,838 [podnet.py] => Task 10, Epoch 4/160 (LR 0.09985) => LSC_loss 1.60, Spatial_loss 2.52, Flat_loss 0.38, Train_acc 56.18, Test_acc 42.95
2025-01-05 15:04:28,953 [podnet.py] => Task 10, Epoch 5/160 (LR 0.09976) => LSC_loss 1.54, Spatial_loss 2.44, Flat_loss 0.36, Train_acc 57.22, Test_acc 40.24
2025-01-05 15:04:37,155 [podnet.py] => Task 10, Epoch 6/160 (LR 0.09965) => LSC_loss 1.53, Spatial_loss 2.42, Flat_loss 0.36, Train_acc 58.11, Test_acc 41.42
2025-01-05 15:04:45,139 [podnet.py] => Task 10, Epoch 7/160 (LR 0.09953) => LSC_loss 1.52, Spatial_loss 2.42, Flat_loss 0.36, Train_acc 57.82, Test_acc 40.02
2025-01-05 15:04:53,128 [podnet.py] => Task 10, Epoch 8/160 (LR 0.09938) => LSC_loss 1.48, Spatial_loss 2.37, Flat_loss 0.35, Train_acc 59.22, Test_acc 44.20
2025-01-05 15:05:01,261 [podnet.py] => Task 10, Epoch 9/160 (LR 0.09922) => LSC_loss 1.48, Spatial_loss 2.37, Flat_loss 0.34, Train_acc 59.50, Test_acc 39.07
2025-01-05 15:05:09,319 [podnet.py] => Task 10, Epoch 10/160 (LR 0.09904) => LSC_loss 1.48, Spatial_loss 2.37, Flat_loss 0.34, Train_acc 59.27, Test_acc 40.45
2025-01-05 15:05:17,529 [podnet.py] => Task 10, Epoch 11/160 (LR 0.09884) => LSC_loss 1.48, Spatial_loss 2.35, Flat_loss 0.34, Train_acc 59.24, Test_acc 44.25
2025-01-05 15:05:25,557 [podnet.py] => Task 10, Epoch 12/160 (LR 0.09862) => LSC_loss 1.43, Spatial_loss 2.30, Flat_loss 0.33, Train_acc 59.93, Test_acc 46.62
2025-01-05 15:05:33,602 [podnet.py] => Task 10, Epoch 13/160 (LR 0.09838) => LSC_loss 1.43, Spatial_loss 2.33, Flat_loss 0.34, Train_acc 60.09, Test_acc 42.51
2025-01-05 15:05:41,700 [podnet.py] => Task 10, Epoch 14/160 (LR 0.09812) => LSC_loss 1.45, Spatial_loss 2.31, Flat_loss 0.33, Train_acc 59.94, Test_acc 44.95
2025-01-05 15:05:49,807 [podnet.py] => Task 10, Epoch 15/160 (LR 0.09785) => LSC_loss 1.43, Spatial_loss 2.31, Flat_loss 0.34, Train_acc 60.51, Test_acc 39.93
2025-01-05 15:05:57,611 [podnet.py] => Task 10, Epoch 16/160 (LR 0.09755) => LSC_loss 1.42, Spatial_loss 2.30, Flat_loss 0.33, Train_acc 61.01, Test_acc 38.27
2025-01-05 15:06:05,563 [podnet.py] => Task 10, Epoch 17/160 (LR 0.09724) => LSC_loss 1.44, Spatial_loss 2.34, Flat_loss 0.34, Train_acc 60.10, Test_acc 44.98
2025-01-05 15:06:13,752 [podnet.py] => Task 10, Epoch 18/160 (LR 0.09691) => LSC_loss 1.41, Spatial_loss 2.31, Flat_loss 0.33, Train_acc 60.98, Test_acc 44.22
2025-01-05 15:06:21,797 [podnet.py] => Task 10, Epoch 19/160 (LR 0.09656) => LSC_loss 1.41, Spatial_loss 2.33, Flat_loss 0.33, Train_acc 60.82, Test_acc 46.98
2025-01-05 15:06:30,020 [podnet.py] => Task 10, Epoch 20/160 (LR 0.09619) => LSC_loss 1.39, Spatial_loss 2.29, Flat_loss 0.33, Train_acc 61.32, Test_acc 46.60
2025-01-05 15:06:38,276 [podnet.py] => Task 10, Epoch 21/160 (LR 0.09581) => LSC_loss 1.40, Spatial_loss 2.31, Flat_loss 0.33, Train_acc 61.64, Test_acc 43.62
2025-01-05 15:06:46,210 [podnet.py] => Task 10, Epoch 22/160 (LR 0.09541) => LSC_loss 1.40, Spatial_loss 2.29, Flat_loss 0.33, Train_acc 61.07, Test_acc 43.29
2025-01-05 15:06:54,287 [podnet.py] => Task 10, Epoch 23/160 (LR 0.09499) => LSC_loss 1.39, Spatial_loss 2.30, Flat_loss 0.33, Train_acc 61.46, Test_acc 45.00
2025-01-05 15:07:02,499 [podnet.py] => Task 10, Epoch 24/160 (LR 0.09455) => LSC_loss 1.39, Spatial_loss 2.29, Flat_loss 0.33, Train_acc 61.63, Test_acc 48.00
2025-01-05 15:07:10,382 [podnet.py] => Task 10, Epoch 25/160 (LR 0.09410) => LSC_loss 1.38, Spatial_loss 2.28, Flat_loss 0.32, Train_acc 61.91, Test_acc 41.18
2025-01-05 15:07:18,401 [podnet.py] => Task 10, Epoch 26/160 (LR 0.09362) => LSC_loss 1.37, Spatial_loss 2.27, Flat_loss 0.32, Train_acc 62.43, Test_acc 43.31
2025-01-05 15:07:26,584 [podnet.py] => Task 10, Epoch 27/160 (LR 0.09314) => LSC_loss 1.38, Spatial_loss 2.26, Flat_loss 0.32, Train_acc 61.81, Test_acc 46.38
2025-01-05 15:07:34,620 [podnet.py] => Task 10, Epoch 28/160 (LR 0.09263) => LSC_loss 1.37, Spatial_loss 2.27, Flat_loss 0.32, Train_acc 62.40, Test_acc 43.64
2025-01-05 15:07:42,841 [podnet.py] => Task 10, Epoch 29/160 (LR 0.09211) => LSC_loss 1.39, Spatial_loss 2.31, Flat_loss 0.33, Train_acc 61.29, Test_acc 44.76
2025-01-05 15:07:51,097 [podnet.py] => Task 10, Epoch 30/160 (LR 0.09157) => LSC_loss 1.37, Spatial_loss 2.26, Flat_loss 0.32, Train_acc 61.80, Test_acc 46.27
2025-01-05 15:07:59,017 [podnet.py] => Task 10, Epoch 31/160 (LR 0.09102) => LSC_loss 1.38, Spatial_loss 2.28, Flat_loss 0.33, Train_acc 61.79, Test_acc 45.73
2025-01-05 15:08:07,050 [podnet.py] => Task 10, Epoch 32/160 (LR 0.09045) => LSC_loss 1.36, Spatial_loss 2.28, Flat_loss 0.32, Train_acc 62.19, Test_acc 45.53
2025-01-05 15:08:15,211 [podnet.py] => Task 10, Epoch 33/160 (LR 0.08987) => LSC_loss 1.35, Spatial_loss 2.23, Flat_loss 0.31, Train_acc 62.58, Test_acc 46.49
2025-01-05 15:08:23,147 [podnet.py] => Task 10, Epoch 34/160 (LR 0.08927) => LSC_loss 1.36, Spatial_loss 2.27, Flat_loss 0.32, Train_acc 62.11, Test_acc 42.15
2025-01-05 15:08:31,144 [podnet.py] => Task 10, Epoch 35/160 (LR 0.08865) => LSC_loss 1.34, Spatial_loss 2.25, Flat_loss 0.31, Train_acc 62.29, Test_acc 44.80
2025-01-05 15:08:39,568 [podnet.py] => Task 10, Epoch 36/160 (LR 0.08802) => LSC_loss 1.36, Spatial_loss 2.27, Flat_loss 0.32, Train_acc 62.66, Test_acc 44.29
2025-01-05 15:08:47,500 [podnet.py] => Task 10, Epoch 37/160 (LR 0.08738) => LSC_loss 1.35, Spatial_loss 2.25, Flat_loss 0.32, Train_acc 62.61, Test_acc 41.27
2025-01-05 15:08:55,599 [podnet.py] => Task 10, Epoch 38/160 (LR 0.08672) => LSC_loss 1.32, Spatial_loss 2.21, Flat_loss 0.31, Train_acc 62.93, Test_acc 45.22
2025-01-05 15:09:03,479 [podnet.py] => Task 10, Epoch 39/160 (LR 0.08604) => LSC_loss 1.33, Spatial_loss 2.21, Flat_loss 0.31, Train_acc 63.65, Test_acc 45.22
2025-01-05 15:09:11,606 [podnet.py] => Task 10, Epoch 40/160 (LR 0.08536) => LSC_loss 1.32, Spatial_loss 2.22, Flat_loss 0.31, Train_acc 63.13, Test_acc 45.42
2025-01-05 15:09:19,633 [podnet.py] => Task 10, Epoch 41/160 (LR 0.08465) => LSC_loss 1.32, Spatial_loss 2.22, Flat_loss 0.31, Train_acc 63.26, Test_acc 45.16
2025-01-05 15:09:27,719 [podnet.py] => Task 10, Epoch 42/160 (LR 0.08394) => LSC_loss 1.30, Spatial_loss 2.17, Flat_loss 0.31, Train_acc 64.04, Test_acc 43.27
2025-01-05 15:09:35,676 [podnet.py] => Task 10, Epoch 43/160 (LR 0.08321) => LSC_loss 1.32, Spatial_loss 2.21, Flat_loss 0.31, Train_acc 63.66, Test_acc 41.58
2025-01-05 15:09:43,909 [podnet.py] => Task 10, Epoch 44/160 (LR 0.08247) => LSC_loss 1.31, Spatial_loss 2.19, Flat_loss 0.31, Train_acc 63.77, Test_acc 46.75
2025-01-05 15:09:52,081 [podnet.py] => Task 10, Epoch 45/160 (LR 0.08172) => LSC_loss 1.32, Spatial_loss 2.23, Flat_loss 0.31, Train_acc 63.86, Test_acc 44.76
2025-01-05 15:10:00,267 [podnet.py] => Task 10, Epoch 46/160 (LR 0.08095) => LSC_loss 1.31, Spatial_loss 2.19, Flat_loss 0.31, Train_acc 63.48, Test_acc 45.05
2025-01-05 15:10:08,335 [podnet.py] => Task 10, Epoch 47/160 (LR 0.08018) => LSC_loss 1.29, Spatial_loss 2.20, Flat_loss 0.31, Train_acc 64.14, Test_acc 45.85
2025-01-05 15:10:16,297 [podnet.py] => Task 10, Epoch 48/160 (LR 0.07939) => LSC_loss 1.29, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 64.08, Test_acc 45.82
2025-01-05 15:10:24,376 [podnet.py] => Task 10, Epoch 49/160 (LR 0.07859) => LSC_loss 1.30, Spatial_loss 2.21, Flat_loss 0.30, Train_acc 63.89, Test_acc 45.69
2025-01-05 15:10:32,381 [podnet.py] => Task 10, Epoch 50/160 (LR 0.07778) => LSC_loss 1.27, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 64.47, Test_acc 44.75
2025-01-05 15:10:40,322 [podnet.py] => Task 10, Epoch 51/160 (LR 0.07696) => LSC_loss 1.29, Spatial_loss 2.21, Flat_loss 0.31, Train_acc 63.75, Test_acc 44.04
2025-01-05 15:10:48,467 [podnet.py] => Task 10, Epoch 52/160 (LR 0.07612) => LSC_loss 1.28, Spatial_loss 2.21, Flat_loss 0.30, Train_acc 64.31, Test_acc 45.18
2025-01-05 15:10:56,637 [podnet.py] => Task 10, Epoch 53/160 (LR 0.07528) => LSC_loss 1.27, Spatial_loss 2.18, Flat_loss 0.30, Train_acc 64.64, Test_acc 43.40
2025-01-05 15:11:04,848 [podnet.py] => Task 10, Epoch 54/160 (LR 0.07443) => LSC_loss 1.26, Spatial_loss 2.15, Flat_loss 0.30, Train_acc 65.35, Test_acc 47.84
2025-01-05 15:11:13,075 [podnet.py] => Task 10, Epoch 55/160 (LR 0.07357) => LSC_loss 1.26, Spatial_loss 2.16, Flat_loss 0.30, Train_acc 65.17, Test_acc 44.91
2025-01-05 15:11:21,112 [podnet.py] => Task 10, Epoch 56/160 (LR 0.07270) => LSC_loss 1.23, Spatial_loss 2.08, Flat_loss 0.29, Train_acc 65.46, Test_acc 45.62
2025-01-05 15:11:29,216 [podnet.py] => Task 10, Epoch 57/160 (LR 0.07182) => LSC_loss 1.26, Spatial_loss 2.15, Flat_loss 0.29, Train_acc 65.00, Test_acc 47.78
2025-01-05 15:11:37,422 [podnet.py] => Task 10, Epoch 58/160 (LR 0.07093) => LSC_loss 1.25, Spatial_loss 2.17, Flat_loss 0.29, Train_acc 65.29, Test_acc 47.29
2025-01-05 15:11:45,667 [podnet.py] => Task 10, Epoch 59/160 (LR 0.07004) => LSC_loss 1.23, Spatial_loss 2.14, Flat_loss 0.29, Train_acc 66.06, Test_acc 45.15
2025-01-05 15:11:53,881 [podnet.py] => Task 10, Epoch 60/160 (LR 0.06913) => LSC_loss 1.21, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 66.46, Test_acc 46.65
2025-01-05 15:12:01,896 [podnet.py] => Task 10, Epoch 61/160 (LR 0.06822) => LSC_loss 1.23, Spatial_loss 2.12, Flat_loss 0.28, Train_acc 65.99, Test_acc 45.35
2025-01-05 15:12:10,716 [podnet.py] => Task 10, Epoch 62/160 (LR 0.06731) => LSC_loss 1.21, Spatial_loss 2.16, Flat_loss 0.29, Train_acc 65.92, Test_acc 44.78
2025-01-05 15:12:19,415 [podnet.py] => Task 10, Epoch 63/160 (LR 0.06638) => LSC_loss 1.23, Spatial_loss 2.12, Flat_loss 0.29, Train_acc 65.62, Test_acc 46.67
2025-01-05 15:12:27,772 [podnet.py] => Task 10, Epoch 64/160 (LR 0.06545) => LSC_loss 1.21, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 66.01, Test_acc 48.05
2025-01-05 15:12:35,982 [podnet.py] => Task 10, Epoch 65/160 (LR 0.06451) => LSC_loss 1.18, Spatial_loss 2.09, Flat_loss 0.28, Train_acc 67.27, Test_acc 45.87
2025-01-05 15:12:44,117 [podnet.py] => Task 10, Epoch 66/160 (LR 0.06357) => LSC_loss 1.20, Spatial_loss 2.05, Flat_loss 0.28, Train_acc 66.62, Test_acc 47.31
2025-01-05 15:12:52,302 [podnet.py] => Task 10, Epoch 67/160 (LR 0.06262) => LSC_loss 1.17, Spatial_loss 2.06, Flat_loss 0.27, Train_acc 66.73, Test_acc 49.07
2025-01-05 15:13:00,329 [podnet.py] => Task 10, Epoch 68/160 (LR 0.06167) => LSC_loss 1.19, Spatial_loss 2.05, Flat_loss 0.27, Train_acc 67.10, Test_acc 44.00
2025-01-05 15:13:08,556 [podnet.py] => Task 10, Epoch 69/160 (LR 0.06072) => LSC_loss 1.18, Spatial_loss 2.07, Flat_loss 0.28, Train_acc 66.74, Test_acc 45.53
2025-01-05 15:13:16,739 [podnet.py] => Task 10, Epoch 70/160 (LR 0.05975) => LSC_loss 1.17, Spatial_loss 2.08, Flat_loss 0.27, Train_acc 67.22, Test_acc 46.80
2025-01-05 15:13:24,781 [podnet.py] => Task 10, Epoch 71/160 (LR 0.05879) => LSC_loss 1.17, Spatial_loss 2.09, Flat_loss 0.27, Train_acc 67.85, Test_acc 45.78
2025-01-05 15:13:33,180 [podnet.py] => Task 10, Epoch 72/160 (LR 0.05782) => LSC_loss 1.14, Spatial_loss 1.99, Flat_loss 0.26, Train_acc 68.23, Test_acc 48.40
2025-01-05 15:13:41,183 [podnet.py] => Task 10, Epoch 73/160 (LR 0.05685) => LSC_loss 1.15, Spatial_loss 1.98, Flat_loss 0.26, Train_acc 68.01, Test_acc 49.36
2025-01-05 15:13:49,420 [podnet.py] => Task 10, Epoch 74/160 (LR 0.05588) => LSC_loss 1.13, Spatial_loss 2.01, Flat_loss 0.26, Train_acc 68.34, Test_acc 45.84
2025-01-05 15:13:57,466 [podnet.py] => Task 10, Epoch 75/160 (LR 0.05490) => LSC_loss 1.14, Spatial_loss 2.00, Flat_loss 0.26, Train_acc 68.20, Test_acc 45.36
2025-01-05 15:14:05,702 [podnet.py] => Task 10, Epoch 76/160 (LR 0.05392) => LSC_loss 1.13, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 68.42, Test_acc 45.38
2025-01-05 15:14:14,039 [podnet.py] => Task 10, Epoch 77/160 (LR 0.05294) => LSC_loss 1.13, Spatial_loss 1.97, Flat_loss 0.26, Train_acc 68.31, Test_acc 48.49
2025-01-05 15:14:22,133 [podnet.py] => Task 10, Epoch 78/160 (LR 0.05196) => LSC_loss 1.12, Spatial_loss 1.96, Flat_loss 0.25, Train_acc 68.67, Test_acc 46.53
2025-01-05 15:14:30,407 [podnet.py] => Task 10, Epoch 79/160 (LR 0.05098) => LSC_loss 1.09, Spatial_loss 1.95, Flat_loss 0.25, Train_acc 69.18, Test_acc 49.07
2025-01-05 15:14:38,659 [podnet.py] => Task 10, Epoch 80/160 (LR 0.05000) => LSC_loss 1.08, Spatial_loss 1.91, Flat_loss 0.24, Train_acc 69.66, Test_acc 45.73
2025-01-05 15:14:47,019 [podnet.py] => Task 10, Epoch 81/160 (LR 0.04902) => LSC_loss 1.10, Spatial_loss 1.97, Flat_loss 0.25, Train_acc 69.26, Test_acc 42.02
2025-01-05 15:14:55,182 [podnet.py] => Task 10, Epoch 82/160 (LR 0.04804) => LSC_loss 1.07, Spatial_loss 1.91, Flat_loss 0.24, Train_acc 70.49, Test_acc 48.87
2025-01-05 15:15:03,216 [podnet.py] => Task 10, Epoch 83/160 (LR 0.04706) => LSC_loss 1.06, Spatial_loss 1.92, Flat_loss 0.25, Train_acc 70.38, Test_acc 44.91
2025-01-05 15:15:11,581 [podnet.py] => Task 10, Epoch 84/160 (LR 0.04608) => LSC_loss 1.06, Spatial_loss 1.89, Flat_loss 0.24, Train_acc 70.28, Test_acc 47.27
2025-01-05 15:15:19,834 [podnet.py] => Task 10, Epoch 85/160 (LR 0.04510) => LSC_loss 1.06, Spatial_loss 1.94, Flat_loss 0.24, Train_acc 70.26, Test_acc 47.71
2025-01-05 15:15:28,251 [podnet.py] => Task 10, Epoch 86/160 (LR 0.04412) => LSC_loss 1.04, Spatial_loss 1.86, Flat_loss 0.24, Train_acc 70.79, Test_acc 48.80
2025-01-05 15:15:36,636 [podnet.py] => Task 10, Epoch 87/160 (LR 0.04315) => LSC_loss 1.03, Spatial_loss 1.88, Flat_loss 0.23, Train_acc 71.36, Test_acc 45.45
2025-01-05 15:15:44,710 [podnet.py] => Task 10, Epoch 88/160 (LR 0.04218) => LSC_loss 1.01, Spatial_loss 1.86, Flat_loss 0.23, Train_acc 71.81, Test_acc 47.95
2025-01-05 15:15:53,150 [podnet.py] => Task 10, Epoch 89/160 (LR 0.04121) => LSC_loss 1.01, Spatial_loss 1.84, Flat_loss 0.23, Train_acc 71.82, Test_acc 43.67
2025-01-05 15:16:01,381 [podnet.py] => Task 10, Epoch 90/160 (LR 0.04025) => LSC_loss 0.98, Spatial_loss 1.81, Flat_loss 0.22, Train_acc 72.38, Test_acc 48.98
2025-01-05 15:16:09,544 [podnet.py] => Task 10, Epoch 91/160 (LR 0.03928) => LSC_loss 0.99, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 72.44, Test_acc 47.84
2025-01-05 15:16:17,807 [podnet.py] => Task 10, Epoch 92/160 (LR 0.03833) => LSC_loss 0.99, Spatial_loss 1.82, Flat_loss 0.22, Train_acc 72.26, Test_acc 50.44
2025-01-05 15:16:25,832 [podnet.py] => Task 10, Epoch 93/160 (LR 0.03738) => LSC_loss 0.97, Spatial_loss 1.78, Flat_loss 0.22, Train_acc 72.76, Test_acc 48.84
2025-01-05 15:16:33,939 [podnet.py] => Task 10, Epoch 94/160 (LR 0.03643) => LSC_loss 0.97, Spatial_loss 1.80, Flat_loss 0.22, Train_acc 72.72, Test_acc 50.55
2025-01-05 15:16:41,897 [podnet.py] => Task 10, Epoch 95/160 (LR 0.03549) => LSC_loss 0.95, Spatial_loss 1.74, Flat_loss 0.21, Train_acc 73.32, Test_acc 47.95
2025-01-05 15:16:50,068 [podnet.py] => Task 10, Epoch 96/160 (LR 0.03455) => LSC_loss 0.96, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 72.66, Test_acc 47.51
2025-01-05 15:16:58,161 [podnet.py] => Task 10, Epoch 97/160 (LR 0.03362) => LSC_loss 0.94, Spatial_loss 1.77, Flat_loss 0.21, Train_acc 73.72, Test_acc 49.73
2025-01-05 15:17:06,061 [podnet.py] => Task 10, Epoch 98/160 (LR 0.03269) => LSC_loss 0.94, Spatial_loss 1.76, Flat_loss 0.21, Train_acc 73.43, Test_acc 49.40
2025-01-05 15:17:14,355 [podnet.py] => Task 10, Epoch 99/160 (LR 0.03178) => LSC_loss 0.93, Spatial_loss 1.73, Flat_loss 0.20, Train_acc 74.11, Test_acc 49.51
2025-01-05 15:17:22,539 [podnet.py] => Task 10, Epoch 100/160 (LR 0.03087) => LSC_loss 0.91, Spatial_loss 1.70, Flat_loss 0.20, Train_acc 74.60, Test_acc 48.56
2025-01-05 15:17:31,006 [podnet.py] => Task 10, Epoch 101/160 (LR 0.02996) => LSC_loss 0.88, Spatial_loss 1.69, Flat_loss 0.19, Train_acc 75.39, Test_acc 50.09
2025-01-05 15:17:38,971 [podnet.py] => Task 10, Epoch 102/160 (LR 0.02907) => LSC_loss 0.90, Spatial_loss 1.67, Flat_loss 0.19, Train_acc 74.61, Test_acc 50.24
2025-01-05 15:17:47,096 [podnet.py] => Task 10, Epoch 103/160 (LR 0.02818) => LSC_loss 0.89, Spatial_loss 1.65, Flat_loss 0.19, Train_acc 75.03, Test_acc 49.51
2025-01-05 15:17:55,087 [podnet.py] => Task 10, Epoch 104/160 (LR 0.02730) => LSC_loss 0.87, Spatial_loss 1.64, Flat_loss 0.19, Train_acc 75.83, Test_acc 50.04
2025-01-05 15:18:03,076 [podnet.py] => Task 10, Epoch 105/160 (LR 0.02643) => LSC_loss 0.84, Spatial_loss 1.62, Flat_loss 0.18, Train_acc 76.43, Test_acc 51.25
2025-01-05 15:18:11,259 [podnet.py] => Task 10, Epoch 106/160 (LR 0.02557) => LSC_loss 0.84, Spatial_loss 1.60, Flat_loss 0.18, Train_acc 76.34, Test_acc 46.38
2025-01-05 15:18:19,300 [podnet.py] => Task 10, Epoch 107/160 (LR 0.02472) => LSC_loss 0.82, Spatial_loss 1.59, Flat_loss 0.18, Train_acc 77.49, Test_acc 49.00
2025-01-05 15:18:27,564 [podnet.py] => Task 10, Epoch 108/160 (LR 0.02388) => LSC_loss 0.83, Spatial_loss 1.58, Flat_loss 0.18, Train_acc 76.78, Test_acc 50.40
2025-01-05 15:18:35,708 [podnet.py] => Task 10, Epoch 109/160 (LR 0.02304) => LSC_loss 0.81, Spatial_loss 1.59, Flat_loss 0.17, Train_acc 77.39, Test_acc 51.04
2025-01-05 15:18:43,997 [podnet.py] => Task 10, Epoch 110/160 (LR 0.02222) => LSC_loss 0.79, Spatial_loss 1.56, Flat_loss 0.17, Train_acc 77.69, Test_acc 51.53
2025-01-05 15:18:52,173 [podnet.py] => Task 10, Epoch 111/160 (LR 0.02141) => LSC_loss 0.77, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 78.61, Test_acc 52.93
2025-01-05 15:19:00,221 [podnet.py] => Task 10, Epoch 112/160 (LR 0.02061) => LSC_loss 0.76, Spatial_loss 1.48, Flat_loss 0.16, Train_acc 78.90, Test_acc 50.87
2025-01-05 15:19:08,583 [podnet.py] => Task 10, Epoch 113/160 (LR 0.01982) => LSC_loss 0.77, Spatial_loss 1.50, Flat_loss 0.16, Train_acc 78.81, Test_acc 50.44
2025-01-05 15:19:16,510 [podnet.py] => Task 10, Epoch 114/160 (LR 0.01905) => LSC_loss 0.75, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 79.59, Test_acc 52.82
2025-01-05 15:19:24,732 [podnet.py] => Task 10, Epoch 115/160 (LR 0.01828) => LSC_loss 0.75, Spatial_loss 1.49, Flat_loss 0.16, Train_acc 78.93, Test_acc 51.82
2025-01-05 15:19:32,885 [podnet.py] => Task 10, Epoch 116/160 (LR 0.01753) => LSC_loss 0.73, Spatial_loss 1.44, Flat_loss 0.15, Train_acc 80.13, Test_acc 51.04
2025-01-05 15:19:40,971 [podnet.py] => Task 10, Epoch 117/160 (LR 0.01679) => LSC_loss 0.72, Spatial_loss 1.44, Flat_loss 0.15, Train_acc 80.40, Test_acc 52.27
2025-01-05 15:19:49,022 [podnet.py] => Task 10, Epoch 118/160 (LR 0.01606) => LSC_loss 0.71, Spatial_loss 1.45, Flat_loss 0.15, Train_acc 80.63, Test_acc 52.53
2025-01-05 15:19:57,224 [podnet.py] => Task 10, Epoch 119/160 (LR 0.01535) => LSC_loss 0.70, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 80.69, Test_acc 52.07
2025-01-05 15:20:05,410 [podnet.py] => Task 10, Epoch 120/160 (LR 0.01464) => LSC_loss 0.67, Spatial_loss 1.39, Flat_loss 0.14, Train_acc 81.92, Test_acc 51.00
2025-01-05 15:20:13,788 [podnet.py] => Task 10, Epoch 121/160 (LR 0.01396) => LSC_loss 0.67, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 82.06, Test_acc 52.11
2025-01-05 15:20:21,991 [podnet.py] => Task 10, Epoch 122/160 (LR 0.01328) => LSC_loss 0.65, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 82.48, Test_acc 52.04
2025-01-05 15:20:30,137 [podnet.py] => Task 10, Epoch 123/160 (LR 0.01262) => LSC_loss 0.64, Spatial_loss 1.34, Flat_loss 0.13, Train_acc 82.64, Test_acc 52.53
2025-01-05 15:20:38,088 [podnet.py] => Task 10, Epoch 124/160 (LR 0.01198) => LSC_loss 0.64, Spatial_loss 1.29, Flat_loss 0.13, Train_acc 82.74, Test_acc 52.09
2025-01-05 15:20:46,139 [podnet.py] => Task 10, Epoch 125/160 (LR 0.01135) => LSC_loss 0.63, Spatial_loss 1.28, Flat_loss 0.12, Train_acc 82.84, Test_acc 53.91
2025-01-05 15:20:54,297 [podnet.py] => Task 10, Epoch 126/160 (LR 0.01073) => LSC_loss 0.62, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 83.24, Test_acc 52.93
2025-01-05 15:21:02,480 [podnet.py] => Task 10, Epoch 127/160 (LR 0.01013) => LSC_loss 0.63, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 83.18, Test_acc 52.87
2025-01-05 15:21:10,830 [podnet.py] => Task 10, Epoch 128/160 (LR 0.00955) => LSC_loss 0.61, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 83.77, Test_acc 52.98
2025-01-05 15:21:19,163 [podnet.py] => Task 10, Epoch 129/160 (LR 0.00898) => LSC_loss 0.61, Spatial_loss 1.23, Flat_loss 0.11, Train_acc 83.91, Test_acc 54.38
2025-01-05 15:21:27,213 [podnet.py] => Task 10, Epoch 130/160 (LR 0.00843) => LSC_loss 0.59, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 84.36, Test_acc 53.60
2025-01-05 15:21:35,448 [podnet.py] => Task 10, Epoch 131/160 (LR 0.00789) => LSC_loss 0.58, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 84.67, Test_acc 53.45
2025-01-05 15:21:43,502 [podnet.py] => Task 10, Epoch 132/160 (LR 0.00737) => LSC_loss 0.57, Spatial_loss 1.16, Flat_loss 0.10, Train_acc 85.18, Test_acc 54.05
2025-01-05 15:21:51,594 [podnet.py] => Task 10, Epoch 133/160 (LR 0.00686) => LSC_loss 0.56, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 85.38, Test_acc 53.75
2025-01-05 15:21:59,818 [podnet.py] => Task 10, Epoch 134/160 (LR 0.00638) => LSC_loss 0.56, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 85.63, Test_acc 54.69
2025-01-05 15:22:07,878 [podnet.py] => Task 10, Epoch 135/160 (LR 0.00590) => LSC_loss 0.55, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 86.07, Test_acc 54.64
2025-01-05 15:22:16,065 [podnet.py] => Task 10, Epoch 136/160 (LR 0.00545) => LSC_loss 0.55, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 86.06, Test_acc 54.55
2025-01-05 15:22:24,273 [podnet.py] => Task 10, Epoch 137/160 (LR 0.00501) => LSC_loss 0.54, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 86.14, Test_acc 53.69
2025-01-05 15:22:32,496 [podnet.py] => Task 10, Epoch 138/160 (LR 0.00459) => LSC_loss 0.53, Spatial_loss 1.08, Flat_loss 0.09, Train_acc 86.45, Test_acc 54.27
2025-01-05 15:22:40,683 [podnet.py] => Task 10, Epoch 139/160 (LR 0.00419) => LSC_loss 0.53, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 86.51, Test_acc 54.20
2025-01-05 15:22:48,671 [podnet.py] => Task 10, Epoch 140/160 (LR 0.00381) => LSC_loss 0.52, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 86.93, Test_acc 55.05
2025-01-05 15:22:56,909 [podnet.py] => Task 10, Epoch 141/160 (LR 0.00344) => LSC_loss 0.53, Spatial_loss 1.08, Flat_loss 0.09, Train_acc 86.56, Test_acc 54.96
2025-01-05 15:23:05,136 [podnet.py] => Task 10, Epoch 142/160 (LR 0.00309) => LSC_loss 0.51, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 86.97, Test_acc 55.27
2025-01-05 15:23:13,368 [podnet.py] => Task 10, Epoch 143/160 (LR 0.00276) => LSC_loss 0.50, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 87.47, Test_acc 54.76
2025-01-05 15:23:21,498 [podnet.py] => Task 10, Epoch 144/160 (LR 0.00245) => LSC_loss 0.50, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 87.41, Test_acc 55.69
2025-01-05 15:23:29,697 [podnet.py] => Task 10, Epoch 145/160 (LR 0.00215) => LSC_loss 0.50, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 87.80, Test_acc 55.51
2025-01-05 15:23:37,903 [podnet.py] => Task 10, Epoch 146/160 (LR 0.00188) => LSC_loss 0.49, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 87.58, Test_acc 55.00
2025-01-05 15:23:46,186 [podnet.py] => Task 10, Epoch 147/160 (LR 0.00162) => LSC_loss 0.49, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 87.96, Test_acc 55.27
2025-01-05 15:23:54,243 [podnet.py] => Task 10, Epoch 148/160 (LR 0.00138) => LSC_loss 0.49, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 88.00, Test_acc 55.07
2025-01-05 15:24:02,456 [podnet.py] => Task 10, Epoch 149/160 (LR 0.00116) => LSC_loss 0.48, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 87.99, Test_acc 55.76
2025-01-05 15:24:10,764 [podnet.py] => Task 10, Epoch 150/160 (LR 0.00096) => LSC_loss 0.48, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.22, Test_acc 55.89
2025-01-05 15:24:19,004 [podnet.py] => Task 10, Epoch 151/160 (LR 0.00078) => LSC_loss 0.48, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 88.19, Test_acc 55.85
2025-01-05 15:24:27,273 [podnet.py] => Task 10, Epoch 152/160 (LR 0.00062) => LSC_loss 0.48, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.46, Test_acc 55.56
2025-01-05 15:24:35,570 [podnet.py] => Task 10, Epoch 153/160 (LR 0.00047) => LSC_loss 0.48, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 88.09, Test_acc 55.87
2025-01-05 15:24:43,676 [podnet.py] => Task 10, Epoch 154/160 (LR 0.00035) => LSC_loss 0.47, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.51, Test_acc 55.84
2025-01-05 15:24:51,859 [podnet.py] => Task 10, Epoch 155/160 (LR 0.00024) => LSC_loss 0.48, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.08, Test_acc 55.58
2025-01-05 15:25:00,177 [podnet.py] => Task 10, Epoch 156/160 (LR 0.00015) => LSC_loss 0.47, Spatial_loss 0.94, Flat_loss 0.08, Train_acc 88.53, Test_acc 55.84
2025-01-05 15:25:08,258 [podnet.py] => Task 10, Epoch 157/160 (LR 0.00009) => LSC_loss 0.48, Spatial_loss 0.93, Flat_loss 0.08, Train_acc 88.53, Test_acc 56.02
2025-01-05 15:25:16,312 [podnet.py] => Task 10, Epoch 158/160 (LR 0.00004) => LSC_loss 0.47, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 88.69, Test_acc 55.91
2025-01-05 15:25:24,611 [podnet.py] => Task 10, Epoch 159/160 (LR 0.00001) => LSC_loss 0.48, Spatial_loss 0.95, Flat_loss 0.08, Train_acc 88.30, Test_acc 55.95
2025-01-05 15:25:32,799 [podnet.py] => Task 10, Epoch 160/160 (LR 0.00000) => LSC_loss 0.47, Spatial_loss 0.92, Flat_loss 0.08, Train_acc 88.65, Test_acc 55.84
2025-01-05 15:25:32,799 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 15:25:32,799 [base.py] => Reducing exemplars...(269 per classes)
2025-01-05 15:26:16,258 [base.py] => Constructing exemplars...(269 per classes)
2025-01-05 15:26:25,830 [podnet.py] => The size of finetune dataset: 14795
2025-01-05 15:26:33,701 [podnet.py] => Task 10, Epoch 1/20 (LR 0.00497) => LSC_loss 0.47, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 88.44, Test_acc 54.02
2025-01-05 15:26:41,624 [podnet.py] => Task 10, Epoch 2/20 (LR 0.00488) => LSC_loss 0.48, Spatial_loss 1.08, Flat_loss 0.08, Train_acc 87.74, Test_acc 54.20
2025-01-05 15:26:49,322 [podnet.py] => Task 10, Epoch 3/20 (LR 0.00473) => LSC_loss 0.48, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 88.14, Test_acc 54.42
2025-01-05 15:26:57,168 [podnet.py] => Task 10, Epoch 4/20 (LR 0.00452) => LSC_loss 0.48, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 88.00, Test_acc 54.98
2025-01-05 15:27:04,824 [podnet.py] => Task 10, Epoch 5/20 (LR 0.00427) => LSC_loss 0.48, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 88.15, Test_acc 54.05
2025-01-05 15:27:12,577 [podnet.py] => Task 10, Epoch 6/20 (LR 0.00397) => LSC_loss 0.48, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 88.15, Test_acc 55.07
2025-01-05 15:27:20,345 [podnet.py] => Task 10, Epoch 7/20 (LR 0.00363) => LSC_loss 0.46, Spatial_loss 1.06, Flat_loss 0.08, Train_acc 88.92, Test_acc 54.62
2025-01-05 15:27:27,997 [podnet.py] => Task 10, Epoch 8/20 (LR 0.00327) => LSC_loss 0.46, Spatial_loss 1.03, Flat_loss 0.08, Train_acc 88.57, Test_acc 55.22
2025-01-05 15:27:35,461 [podnet.py] => Task 10, Epoch 9/20 (LR 0.00289) => LSC_loss 0.46, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 88.60, Test_acc 54.69
2025-01-05 15:27:43,114 [podnet.py] => Task 10, Epoch 10/20 (LR 0.00250) => LSC_loss 0.45, Spatial_loss 1.00, Flat_loss 0.07, Train_acc 88.89, Test_acc 54.45
2025-01-05 15:27:50,934 [podnet.py] => Task 10, Epoch 11/20 (LR 0.00211) => LSC_loss 0.45, Spatial_loss 1.01, Flat_loss 0.07, Train_acc 88.99, Test_acc 55.04
2025-01-05 15:27:58,696 [podnet.py] => Task 10, Epoch 12/20 (LR 0.00173) => LSC_loss 0.44, Spatial_loss 0.97, Flat_loss 0.07, Train_acc 89.44, Test_acc 54.67
2025-01-05 15:28:06,459 [podnet.py] => Task 10, Epoch 13/20 (LR 0.00137) => LSC_loss 0.44, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 89.18, Test_acc 54.91
2025-01-05 15:28:14,173 [podnet.py] => Task 10, Epoch 14/20 (LR 0.00103) => LSC_loss 0.44, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 89.39, Test_acc 55.16
2025-01-05 15:28:21,732 [podnet.py] => Task 10, Epoch 15/20 (LR 0.00073) => LSC_loss 0.44, Spatial_loss 0.93, Flat_loss 0.07, Train_acc 89.65, Test_acc 55.55
2025-01-05 15:28:29,394 [podnet.py] => Task 10, Epoch 16/20 (LR 0.00048) => LSC_loss 0.43, Spatial_loss 0.94, Flat_loss 0.07, Train_acc 89.89, Test_acc 55.18
2025-01-05 15:28:37,149 [podnet.py] => Task 10, Epoch 17/20 (LR 0.00027) => LSC_loss 0.43, Spatial_loss 0.92, Flat_loss 0.07, Train_acc 89.89, Test_acc 55.58
2025-01-05 15:28:44,898 [podnet.py] => Task 10, Epoch 18/20 (LR 0.00012) => LSC_loss 0.43, Spatial_loss 0.91, Flat_loss 0.07, Train_acc 90.15, Test_acc 55.85
2025-01-05 15:28:52,662 [podnet.py] => Task 10, Epoch 19/20 (LR 0.00003) => LSC_loss 0.42, Spatial_loss 0.92, Flat_loss 0.07, Train_acc 90.31, Test_acc 55.76
2025-01-05 15:29:00,118 [podnet.py] => Task 10, Epoch 20/20 (LR 0.00000) => LSC_loss 0.43, Spatial_loss 0.91, Flat_loss 0.06, Train_acc 89.86, Test_acc 55.56
2025-01-05 15:29:00,123 [base.py] => Reducing exemplars...(245 per classes)
2025-01-05 15:29:43,263 [base.py] => Constructing exemplars...(245 per classes)
2025-01-05 15:29:54,653 [podnet.py] => Exemplar size: 13475
2025-01-05 15:29:54,653 [trainer.py] => CNN: {'total': np.float64(55.56), '00-09': np.float64(65.9), '10-19': np.float64(45.3), '20-29': np.float64(60.2), '30-39': np.float64(51.0), '40-49': np.float64(58.1), '50-59': np.float64(50.2), 'old': np.float64(56.1), 'new': np.float64(50.2)}
2025-01-05 15:29:54,653 [trainer.py] => NME: {'total': np.float64(53.89), '00-09': np.float64(66.3), '10-19': np.float64(44.0), '20-29': np.float64(58.8), '30-39': np.float64(49.7), '40-49': np.float64(54.5), '50-59': np.float64(46.2), 'old': np.float64(54.66), 'new': np.float64(46.2)}
2025-01-05 15:29:54,653 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56)]
2025-01-05 15:29:54,653 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82)]
2025-01-05 15:29:54,654 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89)]
2025-01-05 15:29:54,654 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91)]

2025-01-05 15:29:54,654 [trainer.py] => All params: 501457
2025-01-05 15:29:54,654 [trainer.py] => Trainable params: 501457
2025-01-05 15:29:54,655 [podnet.py] => Learning on 55-60
2025-01-05 15:29:54,709 [podnet.py] => Adaptive factor: 3.4641016151377544
2025-01-05 15:30:02,658 [podnet.py] => Task 11, Epoch 1/160 (LR 0.09999) => LSC_loss 2.39, Spatial_loss 3.25, Flat_loss 0.71, Train_acc 39.94, Test_acc 37.92
2025-01-05 15:30:10,588 [podnet.py] => Task 11, Epoch 2/160 (LR 0.09996) => LSC_loss 1.92, Spatial_loss 2.77, Flat_loss 0.50, Train_acc 48.03, Test_acc 36.62
2025-01-05 15:30:18,868 [podnet.py] => Task 11, Epoch 3/160 (LR 0.09991) => LSC_loss 1.81, Spatial_loss 2.68, Flat_loss 0.46, Train_acc 50.46, Test_acc 36.38
2025-01-05 15:30:26,958 [podnet.py] => Task 11, Epoch 4/160 (LR 0.09985) => LSC_loss 1.76, Spatial_loss 2.61, Flat_loss 0.43, Train_acc 52.35, Test_acc 42.63
2025-01-05 15:30:34,938 [podnet.py] => Task 11, Epoch 5/160 (LR 0.09976) => LSC_loss 1.72, Spatial_loss 2.50, Flat_loss 0.42, Train_acc 52.93, Test_acc 38.40
2025-01-05 15:30:43,032 [podnet.py] => Task 11, Epoch 6/160 (LR 0.09965) => LSC_loss 1.69, Spatial_loss 2.49, Flat_loss 0.41, Train_acc 53.48, Test_acc 37.10
2025-01-05 15:30:51,248 [podnet.py] => Task 11, Epoch 7/160 (LR 0.09953) => LSC_loss 1.67, Spatial_loss 2.50, Flat_loss 0.41, Train_acc 54.45, Test_acc 39.45
2025-01-05 15:30:59,645 [podnet.py] => Task 11, Epoch 8/160 (LR 0.09938) => LSC_loss 1.63, Spatial_loss 2.42, Flat_loss 0.39, Train_acc 55.47, Test_acc 43.17
2025-01-05 15:31:07,863 [podnet.py] => Task 11, Epoch 9/160 (LR 0.09922) => LSC_loss 1.63, Spatial_loss 2.39, Flat_loss 0.39, Train_acc 55.16, Test_acc 40.57
2025-01-05 15:31:16,099 [podnet.py] => Task 11, Epoch 10/160 (LR 0.09904) => LSC_loss 1.62, Spatial_loss 2.46, Flat_loss 0.39, Train_acc 54.97, Test_acc 41.42
2025-01-05 15:31:24,367 [podnet.py] => Task 11, Epoch 11/160 (LR 0.09884) => LSC_loss 1.62, Spatial_loss 2.48, Flat_loss 0.40, Train_acc 55.32, Test_acc 39.97
2025-01-05 15:31:32,733 [podnet.py] => Task 11, Epoch 12/160 (LR 0.09862) => LSC_loss 1.61, Spatial_loss 2.41, Flat_loss 0.39, Train_acc 55.66, Test_acc 43.50
2025-01-05 15:31:40,771 [podnet.py] => Task 11, Epoch 13/160 (LR 0.09838) => LSC_loss 1.59, Spatial_loss 2.38, Flat_loss 0.38, Train_acc 56.90, Test_acc 41.82
2025-01-05 15:31:49,043 [podnet.py] => Task 11, Epoch 14/160 (LR 0.09812) => LSC_loss 1.59, Spatial_loss 2.39, Flat_loss 0.39, Train_acc 56.29, Test_acc 40.13
2025-01-05 15:31:57,219 [podnet.py] => Task 11, Epoch 15/160 (LR 0.09785) => LSC_loss 1.57, Spatial_loss 2.38, Flat_loss 0.38, Train_acc 56.74, Test_acc 43.32
2025-01-05 15:32:05,690 [podnet.py] => Task 11, Epoch 16/160 (LR 0.09755) => LSC_loss 1.54, Spatial_loss 2.37, Flat_loss 0.37, Train_acc 57.82, Test_acc 40.07
2025-01-05 15:32:14,079 [podnet.py] => Task 11, Epoch 17/160 (LR 0.09724) => LSC_loss 1.56, Spatial_loss 2.39, Flat_loss 0.38, Train_acc 56.97, Test_acc 43.00
2025-01-05 15:32:22,237 [podnet.py] => Task 11, Epoch 18/160 (LR 0.09691) => LSC_loss 1.55, Spatial_loss 2.35, Flat_loss 0.38, Train_acc 57.56, Test_acc 40.28
2025-01-05 15:32:30,464 [podnet.py] => Task 11, Epoch 19/160 (LR 0.09656) => LSC_loss 1.58, Spatial_loss 2.41, Flat_loss 0.39, Train_acc 56.88, Test_acc 44.13
2025-01-05 15:32:38,615 [podnet.py] => Task 11, Epoch 20/160 (LR 0.09619) => LSC_loss 1.56, Spatial_loss 2.40, Flat_loss 0.39, Train_acc 57.13, Test_acc 37.68
2025-01-05 15:32:46,951 [podnet.py] => Task 11, Epoch 21/160 (LR 0.09581) => LSC_loss 1.55, Spatial_loss 2.35, Flat_loss 0.38, Train_acc 57.04, Test_acc 45.38
2025-01-05 15:32:55,139 [podnet.py] => Task 11, Epoch 22/160 (LR 0.09541) => LSC_loss 1.55, Spatial_loss 2.35, Flat_loss 0.38, Train_acc 56.82, Test_acc 41.27
2025-01-05 15:33:03,215 [podnet.py] => Task 11, Epoch 23/160 (LR 0.09499) => LSC_loss 1.54, Spatial_loss 2.33, Flat_loss 0.38, Train_acc 57.46, Test_acc 39.72
2025-01-05 15:33:11,396 [podnet.py] => Task 11, Epoch 24/160 (LR 0.09455) => LSC_loss 1.53, Spatial_loss 2.35, Flat_loss 0.37, Train_acc 57.62, Test_acc 46.65
2025-01-05 15:33:19,552 [podnet.py] => Task 11, Epoch 25/160 (LR 0.09410) => LSC_loss 1.54, Spatial_loss 2.32, Flat_loss 0.37, Train_acc 57.41, Test_acc 44.20
2025-01-05 15:33:27,707 [podnet.py] => Task 11, Epoch 26/160 (LR 0.09362) => LSC_loss 1.52, Spatial_loss 2.35, Flat_loss 0.37, Train_acc 58.53, Test_acc 43.95
2025-01-05 15:33:35,864 [podnet.py] => Task 11, Epoch 27/160 (LR 0.09314) => LSC_loss 1.53, Spatial_loss 2.37, Flat_loss 0.38, Train_acc 57.83, Test_acc 43.88
2025-01-05 15:33:44,285 [podnet.py] => Task 11, Epoch 28/160 (LR 0.09263) => LSC_loss 1.52, Spatial_loss 2.33, Flat_loss 0.37, Train_acc 57.98, Test_acc 40.47
2025-01-05 15:33:52,652 [podnet.py] => Task 11, Epoch 29/160 (LR 0.09211) => LSC_loss 1.54, Spatial_loss 2.38, Flat_loss 0.38, Train_acc 57.71, Test_acc 39.67
2025-01-05 15:34:00,769 [podnet.py] => Task 11, Epoch 30/160 (LR 0.09157) => LSC_loss 1.50, Spatial_loss 2.34, Flat_loss 0.36, Train_acc 58.31, Test_acc 41.92
2025-01-05 15:34:09,085 [podnet.py] => Task 11, Epoch 31/160 (LR 0.09102) => LSC_loss 1.51, Spatial_loss 2.33, Flat_loss 0.37, Train_acc 58.92, Test_acc 42.33
2025-01-05 15:34:17,204 [podnet.py] => Task 11, Epoch 32/160 (LR 0.09045) => LSC_loss 1.50, Spatial_loss 2.35, Flat_loss 0.37, Train_acc 58.33, Test_acc 41.93
2025-01-05 15:34:25,340 [podnet.py] => Task 11, Epoch 33/160 (LR 0.08987) => LSC_loss 1.50, Spatial_loss 2.32, Flat_loss 0.37, Train_acc 58.49, Test_acc 42.58
2025-01-05 15:34:33,565 [podnet.py] => Task 11, Epoch 34/160 (LR 0.08927) => LSC_loss 1.48, Spatial_loss 2.32, Flat_loss 0.37, Train_acc 58.75, Test_acc 44.10
2025-01-05 15:34:41,623 [podnet.py] => Task 11, Epoch 35/160 (LR 0.08865) => LSC_loss 1.48, Spatial_loss 2.35, Flat_loss 0.37, Train_acc 58.67, Test_acc 42.97
2025-01-05 15:34:49,938 [podnet.py] => Task 11, Epoch 36/160 (LR 0.08802) => LSC_loss 1.48, Spatial_loss 2.30, Flat_loss 0.36, Train_acc 59.22, Test_acc 43.35
2025-01-05 15:34:57,994 [podnet.py] => Task 11, Epoch 37/160 (LR 0.08738) => LSC_loss 1.47, Spatial_loss 2.30, Flat_loss 0.36, Train_acc 59.10, Test_acc 39.48
2025-01-05 15:35:06,073 [podnet.py] => Task 11, Epoch 38/160 (LR 0.08672) => LSC_loss 1.49, Spatial_loss 2.31, Flat_loss 0.37, Train_acc 59.19, Test_acc 41.05
2025-01-05 15:35:14,430 [podnet.py] => Task 11, Epoch 39/160 (LR 0.08604) => LSC_loss 1.47, Spatial_loss 2.29, Flat_loss 0.36, Train_acc 59.44, Test_acc 41.00
2025-01-05 15:35:22,570 [podnet.py] => Task 11, Epoch 40/160 (LR 0.08536) => LSC_loss 1.46, Spatial_loss 2.27, Flat_loss 0.36, Train_acc 59.70, Test_acc 41.88
2025-01-05 15:35:30,667 [podnet.py] => Task 11, Epoch 41/160 (LR 0.08465) => LSC_loss 1.49, Spatial_loss 2.33, Flat_loss 0.37, Train_acc 58.91, Test_acc 38.92
2025-01-05 15:35:38,836 [podnet.py] => Task 11, Epoch 42/160 (LR 0.08394) => LSC_loss 1.47, Spatial_loss 2.29, Flat_loss 0.36, Train_acc 59.31, Test_acc 43.70
2025-01-05 15:35:46,857 [podnet.py] => Task 11, Epoch 43/160 (LR 0.08321) => LSC_loss 1.44, Spatial_loss 2.27, Flat_loss 0.36, Train_acc 60.33, Test_acc 43.95
2025-01-05 15:35:55,090 [podnet.py] => Task 11, Epoch 44/160 (LR 0.08247) => LSC_loss 1.45, Spatial_loss 2.26, Flat_loss 0.36, Train_acc 60.20, Test_acc 41.12
2025-01-05 15:36:03,183 [podnet.py] => Task 11, Epoch 45/160 (LR 0.08172) => LSC_loss 1.45, Spatial_loss 2.30, Flat_loss 0.36, Train_acc 60.17, Test_acc 42.80
2025-01-05 15:36:11,407 [podnet.py] => Task 11, Epoch 46/160 (LR 0.08095) => LSC_loss 1.45, Spatial_loss 2.27, Flat_loss 0.35, Train_acc 59.53, Test_acc 40.48
2025-01-05 15:36:19,366 [podnet.py] => Task 11, Epoch 47/160 (LR 0.08018) => LSC_loss 1.43, Spatial_loss 2.30, Flat_loss 0.35, Train_acc 60.39, Test_acc 42.30
2025-01-05 15:36:27,528 [podnet.py] => Task 11, Epoch 48/160 (LR 0.07939) => LSC_loss 1.43, Spatial_loss 2.26, Flat_loss 0.35, Train_acc 60.37, Test_acc 44.22
2025-01-05 15:36:35,436 [podnet.py] => Task 11, Epoch 49/160 (LR 0.07859) => LSC_loss 1.42, Spatial_loss 2.28, Flat_loss 0.35, Train_acc 61.01, Test_acc 37.25
2025-01-05 15:36:43,723 [podnet.py] => Task 11, Epoch 50/160 (LR 0.07778) => LSC_loss 1.42, Spatial_loss 2.26, Flat_loss 0.36, Train_acc 60.63, Test_acc 42.63
2025-01-05 15:36:51,979 [podnet.py] => Task 11, Epoch 51/160 (LR 0.07696) => LSC_loss 1.43, Spatial_loss 2.23, Flat_loss 0.35, Train_acc 61.01, Test_acc 45.28
2025-01-05 15:37:00,179 [podnet.py] => Task 11, Epoch 52/160 (LR 0.07612) => LSC_loss 1.42, Spatial_loss 2.24, Flat_loss 0.35, Train_acc 61.03, Test_acc 42.67
2025-01-05 15:37:09,023 [podnet.py] => Task 11, Epoch 53/160 (LR 0.07528) => LSC_loss 1.40, Spatial_loss 2.23, Flat_loss 0.34, Train_acc 60.91, Test_acc 43.28
2025-01-05 15:37:17,442 [podnet.py] => Task 11, Epoch 54/160 (LR 0.07443) => LSC_loss 1.39, Spatial_loss 2.18, Flat_loss 0.34, Train_acc 61.75, Test_acc 43.78
2025-01-05 15:37:25,424 [podnet.py] => Task 11, Epoch 55/160 (LR 0.07357) => LSC_loss 1.42, Spatial_loss 2.22, Flat_loss 0.35, Train_acc 60.48, Test_acc 44.42
2025-01-05 15:37:33,384 [podnet.py] => Task 11, Epoch 56/160 (LR 0.07270) => LSC_loss 1.41, Spatial_loss 2.25, Flat_loss 0.35, Train_acc 60.88, Test_acc 44.55
2025-01-05 15:37:41,396 [podnet.py] => Task 11, Epoch 57/160 (LR 0.07182) => LSC_loss 1.36, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 62.25, Test_acc 44.30
2025-01-05 15:37:49,457 [podnet.py] => Task 11, Epoch 58/160 (LR 0.07093) => LSC_loss 1.38, Spatial_loss 2.21, Flat_loss 0.34, Train_acc 62.07, Test_acc 41.65
2025-01-05 15:37:57,516 [podnet.py] => Task 11, Epoch 59/160 (LR 0.07004) => LSC_loss 1.38, Spatial_loss 2.18, Flat_loss 0.33, Train_acc 61.41, Test_acc 45.20
2025-01-05 15:38:06,171 [podnet.py] => Task 11, Epoch 60/160 (LR 0.06913) => LSC_loss 1.35, Spatial_loss 2.19, Flat_loss 0.33, Train_acc 62.83, Test_acc 42.85
2025-01-05 15:38:14,246 [podnet.py] => Task 11, Epoch 61/160 (LR 0.06822) => LSC_loss 1.35, Spatial_loss 2.14, Flat_loss 0.33, Train_acc 62.47, Test_acc 44.70
2025-01-05 15:38:22,059 [podnet.py] => Task 11, Epoch 62/160 (LR 0.06731) => LSC_loss 1.37, Spatial_loss 2.13, Flat_loss 0.33, Train_acc 62.00, Test_acc 45.10
2025-01-05 15:38:30,286 [podnet.py] => Task 11, Epoch 63/160 (LR 0.06638) => LSC_loss 1.36, Spatial_loss 2.16, Flat_loss 0.33, Train_acc 62.19, Test_acc 45.02
2025-01-05 15:38:38,389 [podnet.py] => Task 11, Epoch 64/160 (LR 0.06545) => LSC_loss 1.34, Spatial_loss 2.18, Flat_loss 0.33, Train_acc 62.96, Test_acc 41.30
2025-01-05 15:38:46,719 [podnet.py] => Task 11, Epoch 65/160 (LR 0.06451) => LSC_loss 1.35, Spatial_loss 2.18, Flat_loss 0.33, Train_acc 63.02, Test_acc 47.08
2025-01-05 15:38:54,731 [podnet.py] => Task 11, Epoch 66/160 (LR 0.06357) => LSC_loss 1.31, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 63.71, Test_acc 45.23
2025-01-05 15:39:02,826 [podnet.py] => Task 11, Epoch 67/160 (LR 0.06262) => LSC_loss 1.33, Spatial_loss 2.16, Flat_loss 0.32, Train_acc 63.11, Test_acc 43.45
2025-01-05 15:39:10,798 [podnet.py] => Task 11, Epoch 68/160 (LR 0.06167) => LSC_loss 1.30, Spatial_loss 2.09, Flat_loss 0.32, Train_acc 64.18, Test_acc 39.42
2025-01-05 15:39:18,877 [podnet.py] => Task 11, Epoch 69/160 (LR 0.06072) => LSC_loss 1.33, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 63.07, Test_acc 44.50
2025-01-05 15:39:27,097 [podnet.py] => Task 11, Epoch 70/160 (LR 0.05975) => LSC_loss 1.27, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 64.35, Test_acc 44.65
2025-01-05 15:39:35,246 [podnet.py] => Task 11, Epoch 71/160 (LR 0.05879) => LSC_loss 1.29, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 64.09, Test_acc 44.35
2025-01-05 15:39:43,436 [podnet.py] => Task 11, Epoch 72/160 (LR 0.05782) => LSC_loss 1.28, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 64.47, Test_acc 45.40
2025-01-05 15:39:51,355 [podnet.py] => Task 11, Epoch 73/160 (LR 0.05685) => LSC_loss 1.26, Spatial_loss 2.04, Flat_loss 0.30, Train_acc 65.08, Test_acc 44.35
2025-01-05 15:39:59,223 [podnet.py] => Task 11, Epoch 74/160 (LR 0.05588) => LSC_loss 1.28, Spatial_loss 2.05, Flat_loss 0.31, Train_acc 64.46, Test_acc 46.18
2025-01-05 15:40:07,270 [podnet.py] => Task 11, Epoch 75/160 (LR 0.05490) => LSC_loss 1.28, Spatial_loss 2.04, Flat_loss 0.30, Train_acc 64.86, Test_acc 47.13
2025-01-05 15:40:15,228 [podnet.py] => Task 11, Epoch 76/160 (LR 0.05392) => LSC_loss 1.26, Spatial_loss 2.04, Flat_loss 0.30, Train_acc 64.76, Test_acc 45.60
2025-01-05 15:40:23,414 [podnet.py] => Task 11, Epoch 77/160 (LR 0.05294) => LSC_loss 1.24, Spatial_loss 2.04, Flat_loss 0.30, Train_acc 65.82, Test_acc 46.97
2025-01-05 15:40:31,568 [podnet.py] => Task 11, Epoch 78/160 (LR 0.05196) => LSC_loss 1.24, Spatial_loss 2.00, Flat_loss 0.30, Train_acc 65.64, Test_acc 46.17
2025-01-05 15:40:39,398 [podnet.py] => Task 11, Epoch 79/160 (LR 0.05098) => LSC_loss 1.23, Spatial_loss 2.02, Flat_loss 0.29, Train_acc 65.77, Test_acc 41.73
2025-01-05 15:40:47,522 [podnet.py] => Task 11, Epoch 80/160 (LR 0.05000) => LSC_loss 1.22, Spatial_loss 2.05, Flat_loss 0.29, Train_acc 65.72, Test_acc 45.12
2025-01-05 15:40:55,774 [podnet.py] => Task 11, Epoch 81/160 (LR 0.04902) => LSC_loss 1.23, Spatial_loss 2.00, Flat_loss 0.29, Train_acc 65.74, Test_acc 43.40
2025-01-05 15:41:04,003 [podnet.py] => Task 11, Epoch 82/160 (LR 0.04804) => LSC_loss 1.20, Spatial_loss 1.98, Flat_loss 0.29, Train_acc 66.89, Test_acc 47.42
2025-01-05 15:41:11,812 [podnet.py] => Task 11, Epoch 83/160 (LR 0.04706) => LSC_loss 1.19, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 66.90, Test_acc 46.90
2025-01-05 15:41:19,731 [podnet.py] => Task 11, Epoch 84/160 (LR 0.04608) => LSC_loss 1.19, Spatial_loss 1.96, Flat_loss 0.29, Train_acc 67.03, Test_acc 44.18
2025-01-05 15:41:27,584 [podnet.py] => Task 11, Epoch 85/160 (LR 0.04510) => LSC_loss 1.19, Spatial_loss 1.91, Flat_loss 0.28, Train_acc 67.06, Test_acc 41.93
2025-01-05 15:41:35,778 [podnet.py] => Task 11, Epoch 86/160 (LR 0.04412) => LSC_loss 1.17, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 67.88, Test_acc 44.05
2025-01-05 15:41:44,021 [podnet.py] => Task 11, Epoch 87/160 (LR 0.04315) => LSC_loss 1.15, Spatial_loss 1.92, Flat_loss 0.27, Train_acc 67.66, Test_acc 45.33
2025-01-05 15:41:52,167 [podnet.py] => Task 11, Epoch 88/160 (LR 0.04218) => LSC_loss 1.15, Spatial_loss 1.92, Flat_loss 0.28, Train_acc 67.92, Test_acc 45.82
2025-01-05 15:42:00,188 [podnet.py] => Task 11, Epoch 89/160 (LR 0.04121) => LSC_loss 1.12, Spatial_loss 1.87, Flat_loss 0.27, Train_acc 69.02, Test_acc 48.95
2025-01-05 15:42:08,242 [podnet.py] => Task 11, Epoch 90/160 (LR 0.04025) => LSC_loss 1.12, Spatial_loss 1.87, Flat_loss 0.27, Train_acc 68.69, Test_acc 46.75
2025-01-05 15:42:16,211 [podnet.py] => Task 11, Epoch 91/160 (LR 0.03928) => LSC_loss 1.13, Spatial_loss 1.90, Flat_loss 0.27, Train_acc 68.38, Test_acc 44.70
2025-01-05 15:42:24,141 [podnet.py] => Task 11, Epoch 92/160 (LR 0.03833) => LSC_loss 1.12, Spatial_loss 1.88, Flat_loss 0.26, Train_acc 68.50, Test_acc 44.97
2025-01-05 15:42:32,451 [podnet.py] => Task 11, Epoch 93/160 (LR 0.03738) => LSC_loss 1.11, Spatial_loss 1.87, Flat_loss 0.26, Train_acc 69.34, Test_acc 47.25
2025-01-05 15:42:40,648 [podnet.py] => Task 11, Epoch 94/160 (LR 0.03643) => LSC_loss 1.09, Spatial_loss 1.83, Flat_loss 0.25, Train_acc 69.92, Test_acc 45.87
2025-01-05 15:42:48,693 [podnet.py] => Task 11, Epoch 95/160 (LR 0.03549) => LSC_loss 1.08, Spatial_loss 1.81, Flat_loss 0.25, Train_acc 70.22, Test_acc 47.13
2025-01-05 15:42:56,855 [podnet.py] => Task 11, Epoch 96/160 (LR 0.03455) => LSC_loss 1.06, Spatial_loss 1.78, Flat_loss 0.24, Train_acc 70.02, Test_acc 47.25
2025-01-05 15:43:04,897 [podnet.py] => Task 11, Epoch 97/160 (LR 0.03362) => LSC_loss 1.04, Spatial_loss 1.77, Flat_loss 0.24, Train_acc 71.26, Test_acc 48.23
2025-01-05 15:43:12,933 [podnet.py] => Task 11, Epoch 98/160 (LR 0.03269) => LSC_loss 1.04, Spatial_loss 1.79, Flat_loss 0.25, Train_acc 70.91, Test_acc 47.62
2025-01-05 15:43:21,193 [podnet.py] => Task 11, Epoch 99/160 (LR 0.03178) => LSC_loss 1.04, Spatial_loss 1.77, Flat_loss 0.24, Train_acc 70.91, Test_acc 48.50
2025-01-05 15:43:29,417 [podnet.py] => Task 11, Epoch 100/160 (LR 0.03087) => LSC_loss 1.04, Spatial_loss 1.74, Flat_loss 0.24, Train_acc 71.23, Test_acc 49.42
2025-01-05 15:43:37,532 [podnet.py] => Task 11, Epoch 101/160 (LR 0.02996) => LSC_loss 1.00, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 72.33, Test_acc 46.48
2025-01-05 15:43:45,622 [podnet.py] => Task 11, Epoch 102/160 (LR 0.02907) => LSC_loss 1.01, Spatial_loss 1.77, Flat_loss 0.24, Train_acc 71.62, Test_acc 46.67
2025-01-05 15:43:53,681 [podnet.py] => Task 11, Epoch 103/160 (LR 0.02818) => LSC_loss 1.00, Spatial_loss 1.71, Flat_loss 0.23, Train_acc 71.75, Test_acc 45.77
2025-01-05 15:44:01,766 [podnet.py] => Task 11, Epoch 104/160 (LR 0.02730) => LSC_loss 1.00, Spatial_loss 1.69, Flat_loss 0.23, Train_acc 72.02, Test_acc 48.03
2025-01-05 15:44:10,031 [podnet.py] => Task 11, Epoch 105/160 (LR 0.02643) => LSC_loss 0.96, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 73.56, Test_acc 48.23
2025-01-05 15:44:18,190 [podnet.py] => Task 11, Epoch 106/160 (LR 0.02557) => LSC_loss 0.96, Spatial_loss 1.69, Flat_loss 0.22, Train_acc 73.62, Test_acc 48.13
2025-01-05 15:44:26,485 [podnet.py] => Task 11, Epoch 107/160 (LR 0.02472) => LSC_loss 0.94, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 74.25, Test_acc 47.47
2025-01-05 15:44:34,811 [podnet.py] => Task 11, Epoch 108/160 (LR 0.02388) => LSC_loss 0.92, Spatial_loss 1.62, Flat_loss 0.21, Train_acc 74.28, Test_acc 48.73
2025-01-05 15:44:42,921 [podnet.py] => Task 11, Epoch 109/160 (LR 0.02304) => LSC_loss 0.90, Spatial_loss 1.60, Flat_loss 0.21, Train_acc 75.04, Test_acc 48.07
2025-01-05 15:44:51,064 [podnet.py] => Task 11, Epoch 110/160 (LR 0.02222) => LSC_loss 0.90, Spatial_loss 1.60, Flat_loss 0.21, Train_acc 75.19, Test_acc 49.38
2025-01-05 15:44:59,294 [podnet.py] => Task 11, Epoch 111/160 (LR 0.02141) => LSC_loss 0.90, Spatial_loss 1.56, Flat_loss 0.20, Train_acc 74.90, Test_acc 49.00
2025-01-05 15:45:07,343 [podnet.py] => Task 11, Epoch 112/160 (LR 0.02061) => LSC_loss 0.88, Spatial_loss 1.54, Flat_loss 0.20, Train_acc 75.86, Test_acc 47.52
2025-01-05 15:45:15,358 [podnet.py] => Task 11, Epoch 113/160 (LR 0.01982) => LSC_loss 0.87, Spatial_loss 1.58, Flat_loss 0.20, Train_acc 75.99, Test_acc 46.48
2025-01-05 15:45:23,545 [podnet.py] => Task 11, Epoch 114/160 (LR 0.01905) => LSC_loss 0.85, Spatial_loss 1.51, Flat_loss 0.19, Train_acc 76.78, Test_acc 48.85
2025-01-05 15:45:31,686 [podnet.py] => Task 11, Epoch 115/160 (LR 0.01828) => LSC_loss 0.84, Spatial_loss 1.51, Flat_loss 0.19, Train_acc 77.07, Test_acc 48.95
2025-01-05 15:45:39,723 [podnet.py] => Task 11, Epoch 116/160 (LR 0.01753) => LSC_loss 0.83, Spatial_loss 1.46, Flat_loss 0.18, Train_acc 77.14, Test_acc 47.98
2025-01-05 15:45:47,761 [podnet.py] => Task 11, Epoch 117/160 (LR 0.01679) => LSC_loss 0.83, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 77.36, Test_acc 48.62
2025-01-05 15:45:56,081 [podnet.py] => Task 11, Epoch 118/160 (LR 0.01606) => LSC_loss 0.80, Spatial_loss 1.45, Flat_loss 0.18, Train_acc 78.23, Test_acc 50.33
2025-01-05 15:46:04,491 [podnet.py] => Task 11, Epoch 119/160 (LR 0.01535) => LSC_loss 0.81, Spatial_loss 1.44, Flat_loss 0.18, Train_acc 77.92, Test_acc 49.87
2025-01-05 15:46:12,651 [podnet.py] => Task 11, Epoch 120/160 (LR 0.01464) => LSC_loss 0.79, Spatial_loss 1.42, Flat_loss 0.17, Train_acc 78.69, Test_acc 48.65
2025-01-05 15:46:20,790 [podnet.py] => Task 11, Epoch 121/160 (LR 0.01396) => LSC_loss 0.78, Spatial_loss 1.44, Flat_loss 0.17, Train_acc 78.71, Test_acc 49.72
2025-01-05 15:46:29,083 [podnet.py] => Task 11, Epoch 122/160 (LR 0.01328) => LSC_loss 0.79, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 78.42, Test_acc 50.52
2025-01-05 15:46:37,097 [podnet.py] => Task 11, Epoch 123/160 (LR 0.01262) => LSC_loss 0.75, Spatial_loss 1.38, Flat_loss 0.16, Train_acc 79.99, Test_acc 50.07
2025-01-05 15:46:45,266 [podnet.py] => Task 11, Epoch 124/160 (LR 0.01198) => LSC_loss 0.74, Spatial_loss 1.36, Flat_loss 0.16, Train_acc 79.92, Test_acc 50.03
2025-01-05 15:46:53,448 [podnet.py] => Task 11, Epoch 125/160 (LR 0.01135) => LSC_loss 0.73, Spatial_loss 1.36, Flat_loss 0.16, Train_acc 80.07, Test_acc 51.07
2025-01-05 15:47:01,575 [podnet.py] => Task 11, Epoch 126/160 (LR 0.01073) => LSC_loss 0.73, Spatial_loss 1.34, Flat_loss 0.16, Train_acc 80.62, Test_acc 50.90
2025-01-05 15:47:09,786 [podnet.py] => Task 11, Epoch 127/160 (LR 0.01013) => LSC_loss 0.71, Spatial_loss 1.32, Flat_loss 0.15, Train_acc 80.91, Test_acc 50.87
2025-01-05 15:47:17,939 [podnet.py] => Task 11, Epoch 128/160 (LR 0.00955) => LSC_loss 0.70, Spatial_loss 1.29, Flat_loss 0.15, Train_acc 81.61, Test_acc 50.18
2025-01-05 15:47:26,100 [podnet.py] => Task 11, Epoch 129/160 (LR 0.00898) => LSC_loss 0.70, Spatial_loss 1.28, Flat_loss 0.15, Train_acc 81.11, Test_acc 51.48
2025-01-05 15:47:34,407 [podnet.py] => Task 11, Epoch 130/160 (LR 0.00843) => LSC_loss 0.69, Spatial_loss 1.25, Flat_loss 0.14, Train_acc 81.73, Test_acc 51.48
2025-01-05 15:47:42,705 [podnet.py] => Task 11, Epoch 131/160 (LR 0.00789) => LSC_loss 0.68, Spatial_loss 1.25, Flat_loss 0.14, Train_acc 81.92, Test_acc 51.38
2025-01-05 15:47:50,977 [podnet.py] => Task 11, Epoch 132/160 (LR 0.00737) => LSC_loss 0.67, Spatial_loss 1.24, Flat_loss 0.14, Train_acc 82.40, Test_acc 51.22
2025-01-05 15:47:59,316 [podnet.py] => Task 11, Epoch 133/160 (LR 0.00686) => LSC_loss 0.66, Spatial_loss 1.21, Flat_loss 0.13, Train_acc 82.80, Test_acc 51.37
2025-01-05 15:48:07,549 [podnet.py] => Task 11, Epoch 134/160 (LR 0.00638) => LSC_loss 0.66, Spatial_loss 1.20, Flat_loss 0.13, Train_acc 82.82, Test_acc 51.85
2025-01-05 15:48:15,658 [podnet.py] => Task 11, Epoch 135/160 (LR 0.00590) => LSC_loss 0.64, Spatial_loss 1.19, Flat_loss 0.13, Train_acc 83.53, Test_acc 51.42
2025-01-05 15:48:23,810 [podnet.py] => Task 11, Epoch 136/160 (LR 0.00545) => LSC_loss 0.65, Spatial_loss 1.19, Flat_loss 0.13, Train_acc 82.88, Test_acc 52.22
2025-01-05 15:48:31,936 [podnet.py] => Task 11, Epoch 137/160 (LR 0.00501) => LSC_loss 0.64, Spatial_loss 1.16, Flat_loss 0.13, Train_acc 84.14, Test_acc 52.33
2025-01-05 15:48:40,150 [podnet.py] => Task 11, Epoch 138/160 (LR 0.00459) => LSC_loss 0.63, Spatial_loss 1.15, Flat_loss 0.13, Train_acc 83.90, Test_acc 52.25
2025-01-05 15:48:48,271 [podnet.py] => Task 11, Epoch 139/160 (LR 0.00419) => LSC_loss 0.62, Spatial_loss 1.13, Flat_loss 0.12, Train_acc 83.77, Test_acc 51.72
2025-01-05 15:48:56,357 [podnet.py] => Task 11, Epoch 140/160 (LR 0.00381) => LSC_loss 0.61, Spatial_loss 1.13, Flat_loss 0.12, Train_acc 84.66, Test_acc 52.53
2025-01-05 15:49:04,447 [podnet.py] => Task 11, Epoch 141/160 (LR 0.00344) => LSC_loss 0.60, Spatial_loss 1.11, Flat_loss 0.12, Train_acc 85.04, Test_acc 52.45
2025-01-05 15:49:12,557 [podnet.py] => Task 11, Epoch 142/160 (LR 0.00309) => LSC_loss 0.60, Spatial_loss 1.09, Flat_loss 0.12, Train_acc 84.61, Test_acc 52.13
2025-01-05 15:49:20,792 [podnet.py] => Task 11, Epoch 143/160 (LR 0.00276) => LSC_loss 0.59, Spatial_loss 1.10, Flat_loss 0.12, Train_acc 84.99, Test_acc 52.43
2025-01-05 15:49:28,980 [podnet.py] => Task 11, Epoch 144/160 (LR 0.00245) => LSC_loss 0.59, Spatial_loss 1.07, Flat_loss 0.12, Train_acc 85.14, Test_acc 52.38
2025-01-05 15:49:37,099 [podnet.py] => Task 11, Epoch 145/160 (LR 0.00215) => LSC_loss 0.59, Spatial_loss 1.05, Flat_loss 0.11, Train_acc 85.13, Test_acc 52.82
2025-01-05 15:49:45,104 [podnet.py] => Task 11, Epoch 146/160 (LR 0.00188) => LSC_loss 0.59, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 85.32, Test_acc 52.68
2025-01-05 15:49:53,231 [podnet.py] => Task 11, Epoch 147/160 (LR 0.00162) => LSC_loss 0.58, Spatial_loss 1.05, Flat_loss 0.11, Train_acc 85.56, Test_acc 53.12
2025-01-05 15:50:01,273 [podnet.py] => Task 11, Epoch 148/160 (LR 0.00138) => LSC_loss 0.57, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 86.12, Test_acc 53.12
2025-01-05 15:50:09,478 [podnet.py] => Task 11, Epoch 149/160 (LR 0.00116) => LSC_loss 0.58, Spatial_loss 1.04, Flat_loss 0.11, Train_acc 85.60, Test_acc 52.60
2025-01-05 15:50:17,671 [podnet.py] => Task 11, Epoch 150/160 (LR 0.00096) => LSC_loss 0.58, Spatial_loss 1.03, Flat_loss 0.11, Train_acc 85.15, Test_acc 52.88
2025-01-05 15:50:25,552 [podnet.py] => Task 11, Epoch 151/160 (LR 0.00078) => LSC_loss 0.57, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 85.86, Test_acc 52.83
2025-01-05 15:50:33,798 [podnet.py] => Task 11, Epoch 152/160 (LR 0.00062) => LSC_loss 0.57, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 85.82, Test_acc 52.85
2025-01-05 15:50:42,038 [podnet.py] => Task 11, Epoch 153/160 (LR 0.00047) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.11, Train_acc 85.85, Test_acc 52.98
2025-01-05 15:50:50,200 [podnet.py] => Task 11, Epoch 154/160 (LR 0.00035) => LSC_loss 0.57, Spatial_loss 1.03, Flat_loss 0.11, Train_acc 86.15, Test_acc 53.48
2025-01-05 15:50:58,625 [podnet.py] => Task 11, Epoch 155/160 (LR 0.00024) => LSC_loss 0.56, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 86.13, Test_acc 52.97
2025-01-05 15:51:06,748 [podnet.py] => Task 11, Epoch 156/160 (LR 0.00015) => LSC_loss 0.56, Spatial_loss 0.98, Flat_loss 0.11, Train_acc 85.85, Test_acc 53.17
2025-01-05 15:51:15,131 [podnet.py] => Task 11, Epoch 157/160 (LR 0.00009) => LSC_loss 0.55, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 86.38, Test_acc 53.08
2025-01-05 15:51:23,547 [podnet.py] => Task 11, Epoch 158/160 (LR 0.00004) => LSC_loss 0.56, Spatial_loss 0.97, Flat_loss 0.11, Train_acc 86.24, Test_acc 53.13
2025-01-05 15:51:31,590 [podnet.py] => Task 11, Epoch 159/160 (LR 0.00001) => LSC_loss 0.56, Spatial_loss 1.02, Flat_loss 0.11, Train_acc 86.21, Test_acc 53.03
2025-01-05 15:51:39,724 [podnet.py] => Task 11, Epoch 160/160 (LR 0.00000) => LSC_loss 0.55, Spatial_loss 0.99, Flat_loss 0.11, Train_acc 86.53, Test_acc 53.00
2025-01-05 15:51:39,725 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 15:51:39,725 [base.py] => Reducing exemplars...(245 per classes)
2025-01-05 15:52:25,971 [base.py] => Constructing exemplars...(245 per classes)
2025-01-05 15:52:34,777 [podnet.py] => The size of finetune dataset: 14700
2025-01-05 15:52:42,488 [podnet.py] => Task 11, Epoch 1/20 (LR 0.00497) => LSC_loss 0.53, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 87.26, Test_acc 52.17
2025-01-05 15:52:50,372 [podnet.py] => Task 11, Epoch 2/20 (LR 0.00488) => LSC_loss 0.53, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 86.77, Test_acc 52.12
2025-01-05 15:52:58,135 [podnet.py] => Task 11, Epoch 3/20 (LR 0.00473) => LSC_loss 0.53, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 87.01, Test_acc 51.63
2025-01-05 15:53:05,564 [podnet.py] => Task 11, Epoch 4/20 (LR 0.00452) => LSC_loss 0.54, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 86.26, Test_acc 52.93
2025-01-05 15:53:13,182 [podnet.py] => Task 11, Epoch 5/20 (LR 0.00427) => LSC_loss 0.52, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 87.37, Test_acc 53.18
2025-01-05 15:53:20,807 [podnet.py] => Task 11, Epoch 6/20 (LR 0.00397) => LSC_loss 0.53, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 86.77, Test_acc 52.13
2025-01-05 15:53:28,498 [podnet.py] => Task 11, Epoch 7/20 (LR 0.00363) => LSC_loss 0.52, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 87.34, Test_acc 52.90
2025-01-05 15:53:36,077 [podnet.py] => Task 11, Epoch 8/20 (LR 0.00327) => LSC_loss 0.51, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 87.54, Test_acc 52.28
2025-01-05 15:53:43,723 [podnet.py] => Task 11, Epoch 9/20 (LR 0.00289) => LSC_loss 0.51, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 87.35, Test_acc 52.78
2025-01-05 15:53:51,391 [podnet.py] => Task 11, Epoch 10/20 (LR 0.00250) => LSC_loss 0.51, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 87.20, Test_acc 52.20
2025-01-05 15:53:58,840 [podnet.py] => Task 11, Epoch 11/20 (LR 0.00211) => LSC_loss 0.49, Spatial_loss 1.06, Flat_loss 0.09, Train_acc 88.20, Test_acc 52.18
2025-01-05 15:54:06,606 [podnet.py] => Task 11, Epoch 12/20 (LR 0.00173) => LSC_loss 0.50, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 88.12, Test_acc 52.85
2025-01-05 15:54:14,162 [podnet.py] => Task 11, Epoch 13/20 (LR 0.00137) => LSC_loss 0.49, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.33, Test_acc 52.78
2025-01-05 15:54:21,686 [podnet.py] => Task 11, Epoch 14/20 (LR 0.00103) => LSC_loss 0.49, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 88.14, Test_acc 52.78
2025-01-05 15:54:29,122 [podnet.py] => Task 11, Epoch 15/20 (LR 0.00073) => LSC_loss 0.48, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 88.38, Test_acc 52.65
2025-01-05 15:54:36,801 [podnet.py] => Task 11, Epoch 16/20 (LR 0.00048) => LSC_loss 0.49, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 88.52, Test_acc 53.27
2025-01-05 15:54:44,553 [podnet.py] => Task 11, Epoch 17/20 (LR 0.00027) => LSC_loss 0.48, Spatial_loss 0.98, Flat_loss 0.09, Train_acc 88.86, Test_acc 53.00
2025-01-05 15:54:52,372 [podnet.py] => Task 11, Epoch 18/20 (LR 0.00012) => LSC_loss 0.47, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 89.12, Test_acc 53.02
2025-01-05 15:55:00,268 [podnet.py] => Task 11, Epoch 19/20 (LR 0.00003) => LSC_loss 0.48, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 88.58, Test_acc 52.83
2025-01-05 15:55:08,117 [podnet.py] => Task 11, Epoch 20/20 (LR 0.00000) => LSC_loss 0.48, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.40, Test_acc 53.10
2025-01-05 15:55:08,121 [base.py] => Reducing exemplars...(224 per classes)
2025-01-05 15:55:55,142 [base.py] => Constructing exemplars...(224 per classes)
2025-01-05 15:56:07,034 [podnet.py] => Exemplar size: 13440
2025-01-05 15:56:07,034 [trainer.py] => CNN: {'total': np.float64(53.1), '00-09': np.float64(64.1), '10-19': np.float64(44.1), '20-29': np.float64(59.2), '30-39': np.float64(49.5), '40-49': np.float64(59.1), '50-59': np.float64(42.6), 'old': np.float64(54.98), 'new': np.float64(32.4)}
2025-01-05 15:56:07,034 [trainer.py] => NME: {'total': np.float64(51.65), '00-09': np.float64(65.4), '10-19': np.float64(41.1), '20-29': np.float64(57.5), '30-39': np.float64(49.0), '40-49': np.float64(56.4), '50-59': np.float64(40.5), 'old': np.float64(53.38), 'new': np.float64(32.6)}
2025-01-05 15:56:07,035 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1)]
2025-01-05 15:56:07,035 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48)]
2025-01-05 15:56:07,035 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65)]
2025-01-05 15:56:07,035 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22)]

2025-01-05 15:56:07,035 [trainer.py] => All params: 504657
2025-01-05 15:56:07,035 [trainer.py] => Trainable params: 504657
2025-01-05 15:56:07,036 [podnet.py] => Learning on 60-65
2025-01-05 15:56:07,087 [podnet.py] => Adaptive factor: 3.605551275463989
2025-01-05 15:56:15,335 [podnet.py] => Task 12, Epoch 1/160 (LR 0.09999) => LSC_loss 2.54, Spatial_loss 3.44, Flat_loss 0.80, Train_acc 38.12, Test_acc 31.05
2025-01-05 15:56:23,551 [podnet.py] => Task 12, Epoch 2/160 (LR 0.09996) => LSC_loss 1.96, Spatial_loss 2.92, Flat_loss 0.54, Train_acc 47.58, Test_acc 36.42
2025-01-05 15:56:31,463 [podnet.py] => Task 12, Epoch 3/160 (LR 0.09991) => LSC_loss 1.85, Spatial_loss 2.79, Flat_loss 0.50, Train_acc 50.64, Test_acc 37.05
2025-01-05 15:56:39,593 [podnet.py] => Task 12, Epoch 4/160 (LR 0.09985) => LSC_loss 1.81, Spatial_loss 2.71, Flat_loss 0.47, Train_acc 51.64, Test_acc 38.66
2025-01-05 15:56:47,901 [podnet.py] => Task 12, Epoch 5/160 (LR 0.09976) => LSC_loss 1.75, Spatial_loss 2.64, Flat_loss 0.45, Train_acc 53.52, Test_acc 39.57
2025-01-05 15:56:55,921 [podnet.py] => Task 12, Epoch 6/160 (LR 0.09965) => LSC_loss 1.71, Spatial_loss 2.56, Flat_loss 0.44, Train_acc 54.33, Test_acc 41.65
2025-01-05 15:57:04,355 [podnet.py] => Task 12, Epoch 7/160 (LR 0.09953) => LSC_loss 1.70, Spatial_loss 2.57, Flat_loss 0.43, Train_acc 54.60, Test_acc 40.85
2025-01-05 15:57:12,587 [podnet.py] => Task 12, Epoch 8/160 (LR 0.09938) => LSC_loss 1.67, Spatial_loss 2.56, Flat_loss 0.43, Train_acc 55.15, Test_acc 41.62
2025-01-05 15:57:20,888 [podnet.py] => Task 12, Epoch 9/160 (LR 0.09922) => LSC_loss 1.65, Spatial_loss 2.54, Flat_loss 0.42, Train_acc 55.85, Test_acc 39.94
2025-01-05 15:57:29,221 [podnet.py] => Task 12, Epoch 10/160 (LR 0.09904) => LSC_loss 1.64, Spatial_loss 2.50, Flat_loss 0.42, Train_acc 55.82, Test_acc 38.91
2025-01-05 15:57:37,456 [podnet.py] => Task 12, Epoch 11/160 (LR 0.09884) => LSC_loss 1.63, Spatial_loss 2.51, Flat_loss 0.41, Train_acc 56.50, Test_acc 39.75
2025-01-05 15:57:45,836 [podnet.py] => Task 12, Epoch 12/160 (LR 0.09862) => LSC_loss 1.61, Spatial_loss 2.47, Flat_loss 0.41, Train_acc 56.58, Test_acc 39.51
2025-01-05 15:57:53,835 [podnet.py] => Task 12, Epoch 13/160 (LR 0.09838) => LSC_loss 1.63, Spatial_loss 2.51, Flat_loss 0.42, Train_acc 56.16, Test_acc 37.20
2025-01-05 15:58:02,132 [podnet.py] => Task 12, Epoch 14/160 (LR 0.09812) => LSC_loss 1.61, Spatial_loss 2.47, Flat_loss 0.41, Train_acc 56.94, Test_acc 41.49
2025-01-05 15:58:10,461 [podnet.py] => Task 12, Epoch 15/160 (LR 0.09785) => LSC_loss 1.59, Spatial_loss 2.46, Flat_loss 0.40, Train_acc 56.71, Test_acc 43.58
2025-01-05 15:58:18,785 [podnet.py] => Task 12, Epoch 16/160 (LR 0.09755) => LSC_loss 1.58, Spatial_loss 2.42, Flat_loss 0.40, Train_acc 57.79, Test_acc 40.91
2025-01-05 15:58:26,929 [podnet.py] => Task 12, Epoch 17/160 (LR 0.09724) => LSC_loss 1.59, Spatial_loss 2.48, Flat_loss 0.41, Train_acc 57.15, Test_acc 39.42
2025-01-05 15:58:34,992 [podnet.py] => Task 12, Epoch 18/160 (LR 0.09691) => LSC_loss 1.57, Spatial_loss 2.45, Flat_loss 0.40, Train_acc 57.49, Test_acc 40.45
2025-01-05 15:58:43,369 [podnet.py] => Task 12, Epoch 19/160 (LR 0.09656) => LSC_loss 1.56, Spatial_loss 2.43, Flat_loss 0.40, Train_acc 57.42, Test_acc 41.25
2025-01-05 15:58:51,529 [podnet.py] => Task 12, Epoch 20/160 (LR 0.09619) => LSC_loss 1.57, Spatial_loss 2.48, Flat_loss 0.41, Train_acc 57.42, Test_acc 40.74
2025-01-05 15:58:59,568 [podnet.py] => Task 12, Epoch 21/160 (LR 0.09581) => LSC_loss 1.57, Spatial_loss 2.47, Flat_loss 0.41, Train_acc 57.67, Test_acc 43.65
2025-01-05 15:59:07,654 [podnet.py] => Task 12, Epoch 22/160 (LR 0.09541) => LSC_loss 1.57, Spatial_loss 2.50, Flat_loss 0.41, Train_acc 57.37, Test_acc 40.92
2025-01-05 15:59:15,723 [podnet.py] => Task 12, Epoch 23/160 (LR 0.09499) => LSC_loss 1.55, Spatial_loss 2.43, Flat_loss 0.40, Train_acc 57.94, Test_acc 36.23
2025-01-05 15:59:23,925 [podnet.py] => Task 12, Epoch 24/160 (LR 0.09455) => LSC_loss 1.57, Spatial_loss 2.45, Flat_loss 0.41, Train_acc 57.17, Test_acc 40.68
2025-01-05 15:59:31,798 [podnet.py] => Task 12, Epoch 25/160 (LR 0.09410) => LSC_loss 1.55, Spatial_loss 2.45, Flat_loss 0.40, Train_acc 58.12, Test_acc 35.35
2025-01-05 15:59:39,564 [podnet.py] => Task 12, Epoch 26/160 (LR 0.09362) => LSC_loss 1.54, Spatial_loss 2.46, Flat_loss 0.40, Train_acc 58.55, Test_acc 37.00
2025-01-05 15:59:47,601 [podnet.py] => Task 12, Epoch 27/160 (LR 0.09314) => LSC_loss 1.52, Spatial_loss 2.47, Flat_loss 0.40, Train_acc 58.80, Test_acc 39.58
2025-01-05 15:59:56,027 [podnet.py] => Task 12, Epoch 28/160 (LR 0.09263) => LSC_loss 1.53, Spatial_loss 2.42, Flat_loss 0.40, Train_acc 58.51, Test_acc 37.37
2025-01-05 16:00:04,181 [podnet.py] => Task 12, Epoch 29/160 (LR 0.09211) => LSC_loss 1.54, Spatial_loss 2.43, Flat_loss 0.40, Train_acc 58.27, Test_acc 39.37
2025-01-05 16:00:12,216 [podnet.py] => Task 12, Epoch 30/160 (LR 0.09157) => LSC_loss 1.52, Spatial_loss 2.40, Flat_loss 0.39, Train_acc 58.69, Test_acc 42.82
2025-01-05 16:00:20,462 [podnet.py] => Task 12, Epoch 31/160 (LR 0.09102) => LSC_loss 1.53, Spatial_loss 2.39, Flat_loss 0.40, Train_acc 57.99, Test_acc 38.12
2025-01-05 16:00:28,633 [podnet.py] => Task 12, Epoch 32/160 (LR 0.09045) => LSC_loss 1.51, Spatial_loss 2.38, Flat_loss 0.39, Train_acc 59.30, Test_acc 38.63
2025-01-05 16:00:36,625 [podnet.py] => Task 12, Epoch 33/160 (LR 0.08987) => LSC_loss 1.52, Spatial_loss 2.40, Flat_loss 0.39, Train_acc 59.06, Test_acc 40.29
2025-01-05 16:00:44,963 [podnet.py] => Task 12, Epoch 34/160 (LR 0.08927) => LSC_loss 1.51, Spatial_loss 2.39, Flat_loss 0.39, Train_acc 59.45, Test_acc 42.52
2025-01-05 16:00:53,180 [podnet.py] => Task 12, Epoch 35/160 (LR 0.08865) => LSC_loss 1.50, Spatial_loss 2.42, Flat_loss 0.39, Train_acc 58.88, Test_acc 38.54
2025-01-05 16:01:01,210 [podnet.py] => Task 12, Epoch 36/160 (LR 0.08802) => LSC_loss 1.51, Spatial_loss 2.41, Flat_loss 0.39, Train_acc 58.85, Test_acc 42.51
2025-01-05 16:01:09,547 [podnet.py] => Task 12, Epoch 37/160 (LR 0.08738) => LSC_loss 1.50, Spatial_loss 2.36, Flat_loss 0.39, Train_acc 59.45, Test_acc 43.03
2025-01-05 16:01:17,914 [podnet.py] => Task 12, Epoch 38/160 (LR 0.08672) => LSC_loss 1.50, Spatial_loss 2.42, Flat_loss 0.39, Train_acc 58.90, Test_acc 40.51
2025-01-05 16:01:26,157 [podnet.py] => Task 12, Epoch 39/160 (LR 0.08604) => LSC_loss 1.50, Spatial_loss 2.36, Flat_loss 0.38, Train_acc 59.22, Test_acc 42.26
2025-01-05 16:01:34,543 [podnet.py] => Task 12, Epoch 40/160 (LR 0.08536) => LSC_loss 1.48, Spatial_loss 2.37, Flat_loss 0.38, Train_acc 59.82, Test_acc 41.91
2025-01-05 16:01:42,688 [podnet.py] => Task 12, Epoch 41/160 (LR 0.08465) => LSC_loss 1.49, Spatial_loss 2.37, Flat_loss 0.38, Train_acc 59.77, Test_acc 38.92
2025-01-05 16:01:51,036 [podnet.py] => Task 12, Epoch 42/160 (LR 0.08394) => LSC_loss 1.49, Spatial_loss 2.38, Flat_loss 0.38, Train_acc 59.79, Test_acc 41.51
2025-01-05 16:01:59,154 [podnet.py] => Task 12, Epoch 43/160 (LR 0.08321) => LSC_loss 1.47, Spatial_loss 2.33, Flat_loss 0.38, Train_acc 59.99, Test_acc 42.00
2025-01-05 16:02:07,370 [podnet.py] => Task 12, Epoch 44/160 (LR 0.08247) => LSC_loss 1.47, Spatial_loss 2.37, Flat_loss 0.38, Train_acc 60.02, Test_acc 39.60
2025-01-05 16:02:15,728 [podnet.py] => Task 12, Epoch 45/160 (LR 0.08172) => LSC_loss 1.47, Spatial_loss 2.34, Flat_loss 0.38, Train_acc 59.89, Test_acc 43.60
2025-01-05 16:02:23,977 [podnet.py] => Task 12, Epoch 46/160 (LR 0.08095) => LSC_loss 1.46, Spatial_loss 2.36, Flat_loss 0.38, Train_acc 60.33, Test_acc 39.86
2025-01-05 16:02:32,468 [podnet.py] => Task 12, Epoch 47/160 (LR 0.08018) => LSC_loss 1.45, Spatial_loss 2.34, Flat_loss 0.37, Train_acc 60.75, Test_acc 34.51
2025-01-05 16:02:40,663 [podnet.py] => Task 12, Epoch 48/160 (LR 0.07939) => LSC_loss 1.47, Spatial_loss 2.34, Flat_loss 0.37, Train_acc 60.04, Test_acc 39.55
2025-01-05 16:02:48,939 [podnet.py] => Task 12, Epoch 49/160 (LR 0.07859) => LSC_loss 1.45, Spatial_loss 2.35, Flat_loss 0.38, Train_acc 60.87, Test_acc 42.74
2025-01-05 16:02:57,042 [podnet.py] => Task 12, Epoch 50/160 (LR 0.07778) => LSC_loss 1.43, Spatial_loss 2.31, Flat_loss 0.36, Train_acc 61.13, Test_acc 42.26
2025-01-05 16:03:05,390 [podnet.py] => Task 12, Epoch 51/160 (LR 0.07696) => LSC_loss 1.45, Spatial_loss 2.38, Flat_loss 0.37, Train_acc 61.02, Test_acc 44.91
2025-01-05 16:03:13,382 [podnet.py] => Task 12, Epoch 52/160 (LR 0.07612) => LSC_loss 1.44, Spatial_loss 2.29, Flat_loss 0.37, Train_acc 60.80, Test_acc 43.11
2025-01-05 16:03:21,691 [podnet.py] => Task 12, Epoch 53/160 (LR 0.07528) => LSC_loss 1.42, Spatial_loss 2.26, Flat_loss 0.36, Train_acc 61.32, Test_acc 42.63
2025-01-05 16:03:30,021 [podnet.py] => Task 12, Epoch 54/160 (LR 0.07443) => LSC_loss 1.41, Spatial_loss 2.32, Flat_loss 0.37, Train_acc 61.27, Test_acc 37.17
2025-01-05 16:03:38,163 [podnet.py] => Task 12, Epoch 55/160 (LR 0.07357) => LSC_loss 1.42, Spatial_loss 2.34, Flat_loss 0.37, Train_acc 61.36, Test_acc 40.43
2025-01-05 16:03:46,430 [podnet.py] => Task 12, Epoch 56/160 (LR 0.07270) => LSC_loss 1.42, Spatial_loss 2.31, Flat_loss 0.36, Train_acc 61.22, Test_acc 42.71
2025-01-05 16:03:54,407 [podnet.py] => Task 12, Epoch 57/160 (LR 0.07182) => LSC_loss 1.40, Spatial_loss 2.27, Flat_loss 0.36, Train_acc 61.79, Test_acc 39.78
2025-01-05 16:04:02,724 [podnet.py] => Task 12, Epoch 58/160 (LR 0.07093) => LSC_loss 1.42, Spatial_loss 2.30, Flat_loss 0.36, Train_acc 61.19, Test_acc 43.29
2025-01-05 16:04:11,121 [podnet.py] => Task 12, Epoch 59/160 (LR 0.07004) => LSC_loss 1.37, Spatial_loss 2.24, Flat_loss 0.35, Train_acc 62.81, Test_acc 41.68
2025-01-05 16:04:19,442 [podnet.py] => Task 12, Epoch 60/160 (LR 0.06913) => LSC_loss 1.37, Spatial_loss 2.20, Flat_loss 0.35, Train_acc 62.63, Test_acc 41.40
2025-01-05 16:04:27,619 [podnet.py] => Task 12, Epoch 61/160 (LR 0.06822) => LSC_loss 1.36, Spatial_loss 2.24, Flat_loss 0.35, Train_acc 62.71, Test_acc 41.14
2025-01-05 16:04:35,842 [podnet.py] => Task 12, Epoch 62/160 (LR 0.06731) => LSC_loss 1.37, Spatial_loss 2.20, Flat_loss 0.35, Train_acc 62.63, Test_acc 40.49
2025-01-05 16:04:43,908 [podnet.py] => Task 12, Epoch 63/160 (LR 0.06638) => LSC_loss 1.37, Spatial_loss 2.23, Flat_loss 0.35, Train_acc 62.79, Test_acc 42.92
2025-01-05 16:04:52,172 [podnet.py] => Task 12, Epoch 64/160 (LR 0.06545) => LSC_loss 1.37, Spatial_loss 2.22, Flat_loss 0.35, Train_acc 62.44, Test_acc 44.55
2025-01-05 16:05:00,462 [podnet.py] => Task 12, Epoch 65/160 (LR 0.06451) => LSC_loss 1.36, Spatial_loss 2.23, Flat_loss 0.34, Train_acc 62.38, Test_acc 44.32
2025-01-05 16:05:08,503 [podnet.py] => Task 12, Epoch 66/160 (LR 0.06357) => LSC_loss 1.36, Spatial_loss 2.19, Flat_loss 0.34, Train_acc 62.40, Test_acc 41.46
2025-01-05 16:05:16,481 [podnet.py] => Task 12, Epoch 67/160 (LR 0.06262) => LSC_loss 1.34, Spatial_loss 2.18, Flat_loss 0.34, Train_acc 63.31, Test_acc 42.62
2025-01-05 16:05:24,834 [podnet.py] => Task 12, Epoch 68/160 (LR 0.06167) => LSC_loss 1.32, Spatial_loss 2.19, Flat_loss 0.33, Train_acc 63.86, Test_acc 42.28
2025-01-05 16:05:32,937 [podnet.py] => Task 12, Epoch 69/160 (LR 0.06072) => LSC_loss 1.31, Spatial_loss 2.16, Flat_loss 0.33, Train_acc 63.91, Test_acc 44.66
2025-01-05 16:05:41,179 [podnet.py] => Task 12, Epoch 70/160 (LR 0.05975) => LSC_loss 1.33, Spatial_loss 2.20, Flat_loss 0.34, Train_acc 64.07, Test_acc 39.38
2025-01-05 16:05:49,070 [podnet.py] => Task 12, Epoch 71/160 (LR 0.05879) => LSC_loss 1.32, Spatial_loss 2.22, Flat_loss 0.33, Train_acc 63.91, Test_acc 42.89
2025-01-05 16:05:57,454 [podnet.py] => Task 12, Epoch 72/160 (LR 0.05782) => LSC_loss 1.29, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 64.79, Test_acc 42.12
2025-01-05 16:06:05,821 [podnet.py] => Task 12, Epoch 73/160 (LR 0.05685) => LSC_loss 1.28, Spatial_loss 2.15, Flat_loss 0.32, Train_acc 64.64, Test_acc 42.17
2025-01-05 16:06:13,959 [podnet.py] => Task 12, Epoch 74/160 (LR 0.05588) => LSC_loss 1.29, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 64.93, Test_acc 44.38
2025-01-05 16:06:22,052 [podnet.py] => Task 12, Epoch 75/160 (LR 0.05490) => LSC_loss 1.28, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 65.15, Test_acc 42.83
2025-01-05 16:06:30,117 [podnet.py] => Task 12, Epoch 76/160 (LR 0.05392) => LSC_loss 1.26, Spatial_loss 2.09, Flat_loss 0.32, Train_acc 65.31, Test_acc 40.86
2025-01-05 16:06:38,298 [podnet.py] => Task 12, Epoch 77/160 (LR 0.05294) => LSC_loss 1.28, Spatial_loss 2.07, Flat_loss 0.32, Train_acc 64.60, Test_acc 40.92
2025-01-05 16:06:46,418 [podnet.py] => Task 12, Epoch 78/160 (LR 0.05196) => LSC_loss 1.26, Spatial_loss 2.10, Flat_loss 0.31, Train_acc 65.60, Test_acc 41.68
2025-01-05 16:06:54,668 [podnet.py] => Task 12, Epoch 79/160 (LR 0.05098) => LSC_loss 1.23, Spatial_loss 2.07, Flat_loss 0.30, Train_acc 66.08, Test_acc 46.18
2025-01-05 16:07:02,806 [podnet.py] => Task 12, Epoch 80/160 (LR 0.05000) => LSC_loss 1.21, Spatial_loss 2.04, Flat_loss 0.30, Train_acc 66.96, Test_acc 43.57
2025-01-05 16:07:11,105 [podnet.py] => Task 12, Epoch 81/160 (LR 0.04902) => LSC_loss 1.23, Spatial_loss 2.08, Flat_loss 0.31, Train_acc 66.18, Test_acc 41.91
2025-01-05 16:07:19,383 [podnet.py] => Task 12, Epoch 82/160 (LR 0.04804) => LSC_loss 1.23, Spatial_loss 2.05, Flat_loss 0.31, Train_acc 66.47, Test_acc 43.83
2025-01-05 16:07:27,827 [podnet.py] => Task 12, Epoch 83/160 (LR 0.04706) => LSC_loss 1.23, Spatial_loss 2.05, Flat_loss 0.30, Train_acc 66.14, Test_acc 44.48
2025-01-05 16:07:36,083 [podnet.py] => Task 12, Epoch 84/160 (LR 0.04608) => LSC_loss 1.20, Spatial_loss 1.98, Flat_loss 0.29, Train_acc 66.79, Test_acc 44.45
2025-01-05 16:07:44,177 [podnet.py] => Task 12, Epoch 85/160 (LR 0.04510) => LSC_loss 1.20, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 67.26, Test_acc 43.18
2025-01-05 16:07:52,479 [podnet.py] => Task 12, Epoch 86/160 (LR 0.04412) => LSC_loss 1.19, Spatial_loss 2.03, Flat_loss 0.30, Train_acc 67.12, Test_acc 40.45
2025-01-05 16:08:00,719 [podnet.py] => Task 12, Epoch 87/160 (LR 0.04315) => LSC_loss 1.18, Spatial_loss 1.96, Flat_loss 0.29, Train_acc 67.11, Test_acc 45.77
2025-01-05 16:08:08,891 [podnet.py] => Task 12, Epoch 88/160 (LR 0.04218) => LSC_loss 1.15, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 68.39, Test_acc 45.58
2025-01-05 16:08:17,046 [podnet.py] => Task 12, Epoch 89/160 (LR 0.04121) => LSC_loss 1.16, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 67.97, Test_acc 45.57
2025-01-05 16:08:25,250 [podnet.py] => Task 12, Epoch 90/160 (LR 0.04025) => LSC_loss 1.12, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 69.22, Test_acc 45.43
2025-01-05 16:08:33,393 [podnet.py] => Task 12, Epoch 91/160 (LR 0.03928) => LSC_loss 1.15, Spatial_loss 1.97, Flat_loss 0.28, Train_acc 68.66, Test_acc 43.51
2025-01-05 16:08:41,664 [podnet.py] => Task 12, Epoch 92/160 (LR 0.03833) => LSC_loss 1.10, Spatial_loss 1.90, Flat_loss 0.27, Train_acc 69.58, Test_acc 45.94
2025-01-05 16:08:50,007 [podnet.py] => Task 12, Epoch 93/160 (LR 0.03738) => LSC_loss 1.10, Spatial_loss 1.91, Flat_loss 0.27, Train_acc 69.49, Test_acc 43.58
2025-01-05 16:08:58,145 [podnet.py] => Task 12, Epoch 94/160 (LR 0.03643) => LSC_loss 1.08, Spatial_loss 1.86, Flat_loss 0.26, Train_acc 70.65, Test_acc 45.23
2025-01-05 16:09:06,459 [podnet.py] => Task 12, Epoch 95/160 (LR 0.03549) => LSC_loss 1.08, Spatial_loss 1.84, Flat_loss 0.26, Train_acc 70.48, Test_acc 47.55
2025-01-05 16:09:14,771 [podnet.py] => Task 12, Epoch 96/160 (LR 0.03455) => LSC_loss 1.07, Spatial_loss 1.84, Flat_loss 0.25, Train_acc 70.46, Test_acc 45.75
2025-01-05 16:09:22,972 [podnet.py] => Task 12, Epoch 97/160 (LR 0.03362) => LSC_loss 1.07, Spatial_loss 1.86, Flat_loss 0.26, Train_acc 70.56, Test_acc 46.46
2025-01-05 16:09:31,515 [podnet.py] => Task 12, Epoch 98/160 (LR 0.03269) => LSC_loss 1.07, Spatial_loss 1.85, Flat_loss 0.25, Train_acc 70.46, Test_acc 40.28
2025-01-05 16:09:39,981 [podnet.py] => Task 12, Epoch 99/160 (LR 0.03178) => LSC_loss 1.03, Spatial_loss 1.81, Flat_loss 0.25, Train_acc 71.52, Test_acc 44.86
2025-01-05 16:09:48,286 [podnet.py] => Task 12, Epoch 100/160 (LR 0.03087) => LSC_loss 1.03, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 71.49, Test_acc 46.14
2025-01-05 16:09:56,347 [podnet.py] => Task 12, Epoch 101/160 (LR 0.02996) => LSC_loss 1.02, Spatial_loss 1.78, Flat_loss 0.24, Train_acc 71.69, Test_acc 43.29
2025-01-05 16:10:04,386 [podnet.py] => Task 12, Epoch 102/160 (LR 0.02907) => LSC_loss 1.01, Spatial_loss 1.77, Flat_loss 0.24, Train_acc 72.11, Test_acc 46.08
2025-01-05 16:10:12,491 [podnet.py] => Task 12, Epoch 103/160 (LR 0.02818) => LSC_loss 1.01, Spatial_loss 1.72, Flat_loss 0.24, Train_acc 72.28, Test_acc 47.26
2025-01-05 16:10:20,719 [podnet.py] => Task 12, Epoch 104/160 (LR 0.02730) => LSC_loss 0.99, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 72.47, Test_acc 47.55
2025-01-05 16:10:28,805 [podnet.py] => Task 12, Epoch 105/160 (LR 0.02643) => LSC_loss 0.97, Spatial_loss 1.73, Flat_loss 0.23, Train_acc 73.32, Test_acc 47.11
2025-01-05 16:10:37,113 [podnet.py] => Task 12, Epoch 106/160 (LR 0.02557) => LSC_loss 0.97, Spatial_loss 1.70, Flat_loss 0.23, Train_acc 72.90, Test_acc 42.22
2025-01-05 16:10:45,334 [podnet.py] => Task 12, Epoch 107/160 (LR 0.02472) => LSC_loss 0.95, Spatial_loss 1.69, Flat_loss 0.22, Train_acc 73.44, Test_acc 45.98
2025-01-05 16:10:53,607 [podnet.py] => Task 12, Epoch 108/160 (LR 0.02388) => LSC_loss 0.93, Spatial_loss 1.67, Flat_loss 0.21, Train_acc 74.40, Test_acc 48.31
2025-01-05 16:11:01,909 [podnet.py] => Task 12, Epoch 109/160 (LR 0.02304) => LSC_loss 0.93, Spatial_loss 1.68, Flat_loss 0.21, Train_acc 74.01, Test_acc 46.15
2025-01-05 16:11:10,146 [podnet.py] => Task 12, Epoch 110/160 (LR 0.02222) => LSC_loss 0.91, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 74.77, Test_acc 46.54
2025-01-05 16:11:18,239 [podnet.py] => Task 12, Epoch 111/160 (LR 0.02141) => LSC_loss 0.90, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 75.21, Test_acc 47.09
2025-01-05 16:11:26,434 [podnet.py] => Task 12, Epoch 112/160 (LR 0.02061) => LSC_loss 0.89, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 75.95, Test_acc 48.65
2025-01-05 16:11:34,720 [podnet.py] => Task 12, Epoch 113/160 (LR 0.01982) => LSC_loss 0.89, Spatial_loss 1.61, Flat_loss 0.20, Train_acc 75.63, Test_acc 47.35
2025-01-05 16:11:42,955 [podnet.py] => Task 12, Epoch 114/160 (LR 0.01905) => LSC_loss 0.85, Spatial_loss 1.58, Flat_loss 0.19, Train_acc 76.76, Test_acc 47.06
2025-01-05 16:11:51,030 [podnet.py] => Task 12, Epoch 115/160 (LR 0.01828) => LSC_loss 0.85, Spatial_loss 1.53, Flat_loss 0.19, Train_acc 76.84, Test_acc 47.63
2025-01-05 16:11:59,213 [podnet.py] => Task 12, Epoch 116/160 (LR 0.01753) => LSC_loss 0.84, Spatial_loss 1.54, Flat_loss 0.19, Train_acc 77.46, Test_acc 46.55
2025-01-05 16:12:07,244 [podnet.py] => Task 12, Epoch 117/160 (LR 0.01679) => LSC_loss 0.82, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 77.57, Test_acc 47.88
2025-01-05 16:12:15,380 [podnet.py] => Task 12, Epoch 118/160 (LR 0.01606) => LSC_loss 0.81, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 77.94, Test_acc 47.66
2025-01-05 16:12:23,627 [podnet.py] => Task 12, Epoch 119/160 (LR 0.01535) => LSC_loss 0.80, Spatial_loss 1.48, Flat_loss 0.18, Train_acc 78.46, Test_acc 49.54
2025-01-05 16:12:31,913 [podnet.py] => Task 12, Epoch 120/160 (LR 0.01464) => LSC_loss 0.78, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 78.91, Test_acc 48.65
2025-01-05 16:12:40,205 [podnet.py] => Task 12, Epoch 121/160 (LR 0.01396) => LSC_loss 0.78, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 79.33, Test_acc 48.92
2025-01-05 16:12:48,133 [podnet.py] => Task 12, Epoch 122/160 (LR 0.01328) => LSC_loss 0.76, Spatial_loss 1.40, Flat_loss 0.16, Train_acc 79.53, Test_acc 49.22
2025-01-05 16:12:56,364 [podnet.py] => Task 12, Epoch 123/160 (LR 0.01262) => LSC_loss 0.77, Spatial_loss 1.40, Flat_loss 0.16, Train_acc 79.30, Test_acc 49.28
2025-01-05 16:13:04,819 [podnet.py] => Task 12, Epoch 124/160 (LR 0.01198) => LSC_loss 0.74, Spatial_loss 1.38, Flat_loss 0.16, Train_acc 80.44, Test_acc 49.89
2025-01-05 16:13:12,948 [podnet.py] => Task 12, Epoch 125/160 (LR 0.01135) => LSC_loss 0.74, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 80.36, Test_acc 48.22
2025-01-05 16:13:21,305 [podnet.py] => Task 12, Epoch 126/160 (LR 0.01073) => LSC_loss 0.73, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 80.67, Test_acc 49.28
2025-01-05 16:13:29,523 [podnet.py] => Task 12, Epoch 127/160 (LR 0.01013) => LSC_loss 0.72, Spatial_loss 1.31, Flat_loss 0.15, Train_acc 80.65, Test_acc 50.08
2025-01-05 16:13:37,753 [podnet.py] => Task 12, Epoch 128/160 (LR 0.00955) => LSC_loss 0.70, Spatial_loss 1.30, Flat_loss 0.14, Train_acc 81.48, Test_acc 50.00
2025-01-05 16:13:45,959 [podnet.py] => Task 12, Epoch 129/160 (LR 0.00898) => LSC_loss 0.71, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 81.17, Test_acc 50.17
2025-01-05 16:13:54,259 [podnet.py] => Task 12, Epoch 130/160 (LR 0.00843) => LSC_loss 0.69, Spatial_loss 1.28, Flat_loss 0.14, Train_acc 81.52, Test_acc 48.82
2025-01-05 16:14:02,459 [podnet.py] => Task 12, Epoch 131/160 (LR 0.00789) => LSC_loss 0.68, Spatial_loss 1.26, Flat_loss 0.14, Train_acc 82.21, Test_acc 50.23
2025-01-05 16:14:10,758 [podnet.py] => Task 12, Epoch 132/160 (LR 0.00737) => LSC_loss 0.67, Spatial_loss 1.25, Flat_loss 0.13, Train_acc 82.48, Test_acc 50.43
2025-01-05 16:14:18,823 [podnet.py] => Task 12, Epoch 133/160 (LR 0.00686) => LSC_loss 0.67, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 82.23, Test_acc 50.49
2025-01-05 16:14:27,140 [podnet.py] => Task 12, Epoch 134/160 (LR 0.00638) => LSC_loss 0.66, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 82.89, Test_acc 50.03
2025-01-05 16:14:35,491 [podnet.py] => Task 12, Epoch 135/160 (LR 0.00590) => LSC_loss 0.65, Spatial_loss 1.19, Flat_loss 0.13, Train_acc 83.27, Test_acc 50.88
2025-01-05 16:14:44,026 [podnet.py] => Task 12, Epoch 136/160 (LR 0.00545) => LSC_loss 0.64, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 83.22, Test_acc 50.78
2025-01-05 16:14:52,304 [podnet.py] => Task 12, Epoch 137/160 (LR 0.00501) => LSC_loss 0.64, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 83.07, Test_acc 51.54
2025-01-05 16:15:00,542 [podnet.py] => Task 12, Epoch 138/160 (LR 0.00459) => LSC_loss 0.62, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 83.77, Test_acc 51.68
2025-01-05 16:15:08,779 [podnet.py] => Task 12, Epoch 139/160 (LR 0.00419) => LSC_loss 0.62, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 84.44, Test_acc 50.52
2025-01-05 16:15:17,266 [podnet.py] => Task 12, Epoch 140/160 (LR 0.00381) => LSC_loss 0.62, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 83.95, Test_acc 51.08
2025-01-05 16:15:25,366 [podnet.py] => Task 12, Epoch 141/160 (LR 0.00344) => LSC_loss 0.61, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 84.60, Test_acc 50.28
2025-01-05 16:15:33,535 [podnet.py] => Task 12, Epoch 142/160 (LR 0.00309) => LSC_loss 0.60, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 85.04, Test_acc 50.78
2025-01-05 16:15:41,523 [podnet.py] => Task 12, Epoch 143/160 (LR 0.00276) => LSC_loss 0.61, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 84.49, Test_acc 51.08
2025-01-05 16:15:49,718 [podnet.py] => Task 12, Epoch 144/160 (LR 0.00245) => LSC_loss 0.60, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 85.23, Test_acc 51.08
2025-01-05 16:15:57,841 [podnet.py] => Task 12, Epoch 145/160 (LR 0.00215) => LSC_loss 0.60, Spatial_loss 1.06, Flat_loss 0.11, Train_acc 84.82, Test_acc 51.18
2025-01-05 16:16:06,616 [podnet.py] => Task 12, Epoch 146/160 (LR 0.00188) => LSC_loss 0.59, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 85.58, Test_acc 51.40
2025-01-05 16:16:15,388 [podnet.py] => Task 12, Epoch 147/160 (LR 0.00162) => LSC_loss 0.58, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 85.50, Test_acc 51.54
2025-01-05 16:16:23,596 [podnet.py] => Task 12, Epoch 148/160 (LR 0.00138) => LSC_loss 0.58, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 85.41, Test_acc 51.37
2025-01-05 16:16:31,945 [podnet.py] => Task 12, Epoch 149/160 (LR 0.00116) => LSC_loss 0.58, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 85.44, Test_acc 51.58
2025-01-05 16:16:40,039 [podnet.py] => Task 12, Epoch 150/160 (LR 0.00096) => LSC_loss 0.57, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 85.68, Test_acc 51.29
2025-01-05 16:16:48,114 [podnet.py] => Task 12, Epoch 151/160 (LR 0.00078) => LSC_loss 0.57, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 85.76, Test_acc 51.32
2025-01-05 16:16:56,303 [podnet.py] => Task 12, Epoch 152/160 (LR 0.00062) => LSC_loss 0.57, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 85.68, Test_acc 51.85
2025-01-05 16:17:04,427 [podnet.py] => Task 12, Epoch 153/160 (LR 0.00047) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 85.75, Test_acc 51.74
2025-01-05 16:17:12,811 [podnet.py] => Task 12, Epoch 154/160 (LR 0.00035) => LSC_loss 0.56, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 85.95, Test_acc 51.38
2025-01-05 16:17:21,027 [podnet.py] => Task 12, Epoch 155/160 (LR 0.00024) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 85.82, Test_acc 51.40
2025-01-05 16:17:29,314 [podnet.py] => Task 12, Epoch 156/160 (LR 0.00015) => LSC_loss 0.56, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 86.10, Test_acc 51.48
2025-01-05 16:17:37,595 [podnet.py] => Task 12, Epoch 157/160 (LR 0.00009) => LSC_loss 0.57, Spatial_loss 0.97, Flat_loss 0.10, Train_acc 85.73, Test_acc 51.62
2025-01-05 16:17:45,879 [podnet.py] => Task 12, Epoch 158/160 (LR 0.00004) => LSC_loss 0.57, Spatial_loss 0.99, Flat_loss 0.10, Train_acc 86.13, Test_acc 51.43
2025-01-05 16:17:53,863 [podnet.py] => Task 12, Epoch 159/160 (LR 0.00001) => LSC_loss 0.57, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 85.71, Test_acc 51.45
2025-01-05 16:18:02,055 [podnet.py] => Task 12, Epoch 160/160 (LR 0.00000) => LSC_loss 0.57, Spatial_loss 0.98, Flat_loss 0.10, Train_acc 85.91, Test_acc 51.52
2025-01-05 16:18:02,055 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 16:18:02,055 [base.py] => Reducing exemplars...(224 per classes)
2025-01-05 16:18:54,249 [base.py] => Constructing exemplars...(224 per classes)
2025-01-05 16:19:03,072 [podnet.py] => The size of finetune dataset: 14560
2025-01-05 16:19:10,720 [podnet.py] => Task 12, Epoch 1/20 (LR 0.00497) => LSC_loss 0.55, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 86.37, Test_acc 50.52
2025-01-05 16:19:18,288 [podnet.py] => Task 12, Epoch 2/20 (LR 0.00488) => LSC_loss 0.55, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 86.09, Test_acc 51.25
2025-01-05 16:19:25,960 [podnet.py] => Task 12, Epoch 3/20 (LR 0.00473) => LSC_loss 0.55, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 86.26, Test_acc 50.68
2025-01-05 16:19:33,536 [podnet.py] => Task 12, Epoch 4/20 (LR 0.00452) => LSC_loss 0.56, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 86.22, Test_acc 50.40
2025-01-05 16:19:41,234 [podnet.py] => Task 12, Epoch 5/20 (LR 0.00427) => LSC_loss 0.54, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 86.28, Test_acc 50.57
2025-01-05 16:19:48,934 [podnet.py] => Task 12, Epoch 6/20 (LR 0.00397) => LSC_loss 0.55, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 86.22, Test_acc 51.05
2025-01-05 16:19:56,633 [podnet.py] => Task 12, Epoch 7/20 (LR 0.00363) => LSC_loss 0.55, Spatial_loss 1.15, Flat_loss 0.10, Train_acc 85.91, Test_acc 50.92
2025-01-05 16:20:04,222 [podnet.py] => Task 12, Epoch 8/20 (LR 0.00327) => LSC_loss 0.53, Spatial_loss 1.10, Flat_loss 0.09, Train_acc 86.88, Test_acc 50.82
2025-01-05 16:20:12,028 [podnet.py] => Task 12, Epoch 9/20 (LR 0.00289) => LSC_loss 0.54, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 86.41, Test_acc 50.88
2025-01-05 16:20:19,612 [podnet.py] => Task 12, Epoch 10/20 (LR 0.00250) => LSC_loss 0.52, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 86.98, Test_acc 51.11
2025-01-05 16:20:27,405 [podnet.py] => Task 12, Epoch 11/20 (LR 0.00211) => LSC_loss 0.52, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 87.25, Test_acc 50.89
2025-01-05 16:20:34,929 [podnet.py] => Task 12, Epoch 12/20 (LR 0.00173) => LSC_loss 0.52, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 87.04, Test_acc 51.43
2025-01-05 16:20:42,510 [podnet.py] => Task 12, Epoch 13/20 (LR 0.00137) => LSC_loss 0.52, Spatial_loss 1.02, Flat_loss 0.08, Train_acc 87.20, Test_acc 51.38
2025-01-05 16:20:50,065 [podnet.py] => Task 12, Epoch 14/20 (LR 0.00103) => LSC_loss 0.51, Spatial_loss 1.01, Flat_loss 0.08, Train_acc 87.25, Test_acc 51.42
2025-01-05 16:20:57,830 [podnet.py] => Task 12, Epoch 15/20 (LR 0.00073) => LSC_loss 0.51, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 87.68, Test_acc 51.00
2025-01-05 16:21:05,454 [podnet.py] => Task 12, Epoch 16/20 (LR 0.00048) => LSC_loss 0.50, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 87.97, Test_acc 51.25
2025-01-05 16:21:12,954 [podnet.py] => Task 12, Epoch 17/20 (LR 0.00027) => LSC_loss 0.51, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 87.73, Test_acc 51.46
2025-01-05 16:21:20,729 [podnet.py] => Task 12, Epoch 18/20 (LR 0.00012) => LSC_loss 0.50, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.36, Test_acc 51.65
2025-01-05 16:21:28,349 [podnet.py] => Task 12, Epoch 19/20 (LR 0.00003) => LSC_loss 0.49, Spatial_loss 0.96, Flat_loss 0.08, Train_acc 88.34, Test_acc 51.37
2025-01-05 16:21:35,835 [podnet.py] => Task 12, Epoch 20/20 (LR 0.00000) => LSC_loss 0.49, Spatial_loss 0.97, Flat_loss 0.08, Train_acc 88.37, Test_acc 51.51
2025-01-05 16:21:35,841 [base.py] => Reducing exemplars...(207 per classes)
2025-01-05 16:22:26,632 [base.py] => Constructing exemplars...(207 per classes)
2025-01-05 16:22:38,157 [podnet.py] => Exemplar size: 13455
2025-01-05 16:22:38,157 [trainer.py] => CNN: {'total': np.float64(51.51), '00-09': np.float64(63.8), '10-19': np.float64(42.4), '20-29': np.float64(57.4), '30-39': np.float64(48.4), '40-49': np.float64(58.5), '50-59': np.float64(42.6), '60-69': np.float64(43.4), 'old': np.float64(52.18), 'new': np.float64(43.4)}
2025-01-05 16:22:38,157 [trainer.py] => NME: {'total': np.float64(49.83), '00-09': np.float64(64.5), '10-19': np.float64(40.3), '20-29': np.float64(55.5), '30-39': np.float64(48.0), '40-49': np.float64(54.7), '50-59': np.float64(40.0), '60-69': np.float64(41.8), 'old': np.float64(50.5), 'new': np.float64(41.8)}
2025-01-05 16:22:38,157 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51)]
2025-01-05 16:22:38,157 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14)]
2025-01-05 16:22:38,157 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83)]
2025-01-05 16:22:38,157 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69)]

2025-01-05 16:22:38,157 [trainer.py] => All params: 507857
2025-01-05 16:22:38,158 [trainer.py] => Trainable params: 507857
2025-01-05 16:22:38,158 [podnet.py] => Learning on 65-70
2025-01-05 16:22:38,208 [podnet.py] => Adaptive factor: 3.7416573867739413
2025-01-05 16:22:46,355 [podnet.py] => Task 13, Epoch 1/160 (LR 0.09999) => LSC_loss 2.52, Spatial_loss 3.59, Flat_loss 0.82, Train_acc 38.78, Test_acc 32.14
2025-01-05 16:22:54,572 [podnet.py] => Task 13, Epoch 2/160 (LR 0.09996) => LSC_loss 2.00, Spatial_loss 3.04, Flat_loss 0.59, Train_acc 47.03, Test_acc 34.47
2025-01-05 16:23:02,708 [podnet.py] => Task 13, Epoch 3/160 (LR 0.09991) => LSC_loss 1.88, Spatial_loss 2.90, Flat_loss 0.53, Train_acc 50.05, Test_acc 39.41
2025-01-05 16:23:10,947 [podnet.py] => Task 13, Epoch 4/160 (LR 0.09985) => LSC_loss 1.81, Spatial_loss 2.76, Flat_loss 0.50, Train_acc 51.31, Test_acc 36.86
2025-01-05 16:23:19,109 [podnet.py] => Task 13, Epoch 5/160 (LR 0.09976) => LSC_loss 1.78, Spatial_loss 2.72, Flat_loss 0.48, Train_acc 52.37, Test_acc 36.61
2025-01-05 16:23:27,302 [podnet.py] => Task 13, Epoch 6/160 (LR 0.09965) => LSC_loss 1.75, Spatial_loss 2.66, Flat_loss 0.47, Train_acc 53.78, Test_acc 39.89
2025-01-05 16:23:35,598 [podnet.py] => Task 13, Epoch 7/160 (LR 0.09953) => LSC_loss 1.71, Spatial_loss 2.65, Flat_loss 0.46, Train_acc 54.00, Test_acc 40.70
2025-01-05 16:23:43,796 [podnet.py] => Task 13, Epoch 8/160 (LR 0.09938) => LSC_loss 1.69, Spatial_loss 2.59, Flat_loss 0.46, Train_acc 54.83, Test_acc 38.16
2025-01-05 16:23:52,163 [podnet.py] => Task 13, Epoch 9/160 (LR 0.09922) => LSC_loss 1.68, Spatial_loss 2.58, Flat_loss 0.45, Train_acc 54.80, Test_acc 37.63
2025-01-05 16:24:00,367 [podnet.py] => Task 13, Epoch 10/160 (LR 0.09904) => LSC_loss 1.67, Spatial_loss 2.64, Flat_loss 0.45, Train_acc 54.92, Test_acc 39.76
2025-01-05 16:24:08,692 [podnet.py] => Task 13, Epoch 11/160 (LR 0.09884) => LSC_loss 1.66, Spatial_loss 2.57, Flat_loss 0.44, Train_acc 55.32, Test_acc 34.01
2025-01-05 16:24:16,876 [podnet.py] => Task 13, Epoch 12/160 (LR 0.09862) => LSC_loss 1.67, Spatial_loss 2.59, Flat_loss 0.45, Train_acc 55.10, Test_acc 34.74
2025-01-05 16:24:25,023 [podnet.py] => Task 13, Epoch 13/160 (LR 0.09838) => LSC_loss 1.64, Spatial_loss 2.61, Flat_loss 0.44, Train_acc 55.71, Test_acc 33.56
2025-01-05 16:24:33,359 [podnet.py] => Task 13, Epoch 14/160 (LR 0.09812) => LSC_loss 1.62, Spatial_loss 2.53, Flat_loss 0.43, Train_acc 56.11, Test_acc 39.33
2025-01-05 16:24:42,205 [podnet.py] => Task 13, Epoch 15/160 (LR 0.09785) => LSC_loss 1.62, Spatial_loss 2.52, Flat_loss 0.44, Train_acc 56.34, Test_acc 37.19
2025-01-05 16:24:50,841 [podnet.py] => Task 13, Epoch 16/160 (LR 0.09755) => LSC_loss 1.62, Spatial_loss 2.57, Flat_loss 0.44, Train_acc 56.50, Test_acc 40.77
2025-01-05 16:24:59,177 [podnet.py] => Task 13, Epoch 17/160 (LR 0.09724) => LSC_loss 1.61, Spatial_loss 2.55, Flat_loss 0.43, Train_acc 56.26, Test_acc 39.19
2025-01-05 16:25:07,261 [podnet.py] => Task 13, Epoch 18/160 (LR 0.09691) => LSC_loss 1.61, Spatial_loss 2.57, Flat_loss 0.44, Train_acc 56.75, Test_acc 41.54
2025-01-05 16:25:15,422 [podnet.py] => Task 13, Epoch 19/160 (LR 0.09656) => LSC_loss 1.57, Spatial_loss 2.51, Flat_loss 0.43, Train_acc 57.87, Test_acc 40.37
2025-01-05 16:25:23,868 [podnet.py] => Task 13, Epoch 20/160 (LR 0.09619) => LSC_loss 1.61, Spatial_loss 2.58, Flat_loss 0.44, Train_acc 56.67, Test_acc 38.47
2025-01-05 16:25:32,220 [podnet.py] => Task 13, Epoch 21/160 (LR 0.09581) => LSC_loss 1.59, Spatial_loss 2.53, Flat_loss 0.43, Train_acc 57.24, Test_acc 37.31
2025-01-05 16:25:40,285 [podnet.py] => Task 13, Epoch 22/160 (LR 0.09541) => LSC_loss 1.59, Spatial_loss 2.51, Flat_loss 0.43, Train_acc 57.15, Test_acc 39.10
2025-01-05 16:25:48,572 [podnet.py] => Task 13, Epoch 23/160 (LR 0.09499) => LSC_loss 1.58, Spatial_loss 2.52, Flat_loss 0.43, Train_acc 57.29, Test_acc 37.80
2025-01-05 16:25:56,948 [podnet.py] => Task 13, Epoch 24/160 (LR 0.09455) => LSC_loss 1.58, Spatial_loss 2.54, Flat_loss 0.43, Train_acc 57.51, Test_acc 41.50
2025-01-05 16:26:05,057 [podnet.py] => Task 13, Epoch 25/160 (LR 0.09410) => LSC_loss 1.59, Spatial_loss 2.56, Flat_loss 0.43, Train_acc 57.36, Test_acc 39.67
2025-01-05 16:26:13,257 [podnet.py] => Task 13, Epoch 26/160 (LR 0.09362) => LSC_loss 1.57, Spatial_loss 2.52, Flat_loss 0.43, Train_acc 57.76, Test_acc 36.77
2025-01-05 16:26:21,516 [podnet.py] => Task 13, Epoch 27/160 (LR 0.09314) => LSC_loss 1.57, Spatial_loss 2.48, Flat_loss 0.42, Train_acc 57.56, Test_acc 36.47
2025-01-05 16:26:29,777 [podnet.py] => Task 13, Epoch 28/160 (LR 0.09263) => LSC_loss 1.55, Spatial_loss 2.45, Flat_loss 0.42, Train_acc 58.13, Test_acc 40.16
2025-01-05 16:26:38,242 [podnet.py] => Task 13, Epoch 29/160 (LR 0.09211) => LSC_loss 1.56, Spatial_loss 2.46, Flat_loss 0.42, Train_acc 57.83, Test_acc 40.40
2025-01-05 16:26:46,347 [podnet.py] => Task 13, Epoch 30/160 (LR 0.09157) => LSC_loss 1.56, Spatial_loss 2.48, Flat_loss 0.42, Train_acc 57.99, Test_acc 38.40
2025-01-05 16:26:54,544 [podnet.py] => Task 13, Epoch 31/160 (LR 0.09102) => LSC_loss 1.54, Spatial_loss 2.51, Flat_loss 0.42, Train_acc 58.53, Test_acc 41.46
2025-01-05 16:27:02,547 [podnet.py] => Task 13, Epoch 32/160 (LR 0.09045) => LSC_loss 1.56, Spatial_loss 2.53, Flat_loss 0.42, Train_acc 57.80, Test_acc 41.39
2025-01-05 16:27:10,635 [podnet.py] => Task 13, Epoch 33/160 (LR 0.08987) => LSC_loss 1.55, Spatial_loss 2.49, Flat_loss 0.42, Train_acc 57.99, Test_acc 41.70
2025-01-05 16:27:18,786 [podnet.py] => Task 13, Epoch 34/160 (LR 0.08927) => LSC_loss 1.54, Spatial_loss 2.51, Flat_loss 0.42, Train_acc 58.31, Test_acc 35.87
2025-01-05 16:27:26,948 [podnet.py] => Task 13, Epoch 35/160 (LR 0.08865) => LSC_loss 1.55, Spatial_loss 2.50, Flat_loss 0.42, Train_acc 58.43, Test_acc 39.21
2025-01-05 16:27:35,248 [podnet.py] => Task 13, Epoch 36/160 (LR 0.08802) => LSC_loss 1.53, Spatial_loss 2.56, Flat_loss 0.42, Train_acc 58.53, Test_acc 40.46
2025-01-05 16:27:43,510 [podnet.py] => Task 13, Epoch 37/160 (LR 0.08738) => LSC_loss 1.53, Spatial_loss 2.46, Flat_loss 0.41, Train_acc 58.58, Test_acc 38.64
2025-01-05 16:27:51,707 [podnet.py] => Task 13, Epoch 38/160 (LR 0.08672) => LSC_loss 1.53, Spatial_loss 2.46, Flat_loss 0.41, Train_acc 58.69, Test_acc 40.97
2025-01-05 16:28:00,128 [podnet.py] => Task 13, Epoch 39/160 (LR 0.08604) => LSC_loss 1.51, Spatial_loss 2.44, Flat_loss 0.41, Train_acc 59.30, Test_acc 38.71
2025-01-05 16:28:08,433 [podnet.py] => Task 13, Epoch 40/160 (LR 0.08536) => LSC_loss 1.51, Spatial_loss 2.43, Flat_loss 0.41, Train_acc 59.37, Test_acc 41.69
2025-01-05 16:28:16,777 [podnet.py] => Task 13, Epoch 41/160 (LR 0.08465) => LSC_loss 1.51, Spatial_loss 2.45, Flat_loss 0.41, Train_acc 58.98, Test_acc 37.47
2025-01-05 16:28:25,009 [podnet.py] => Task 13, Epoch 42/160 (LR 0.08394) => LSC_loss 1.51, Spatial_loss 2.44, Flat_loss 0.41, Train_acc 58.97, Test_acc 40.47
2025-01-05 16:28:33,107 [podnet.py] => Task 13, Epoch 43/160 (LR 0.08321) => LSC_loss 1.50, Spatial_loss 2.43, Flat_loss 0.41, Train_acc 59.47, Test_acc 40.86
2025-01-05 16:28:41,235 [podnet.py] => Task 13, Epoch 44/160 (LR 0.08247) => LSC_loss 1.51, Spatial_loss 2.45, Flat_loss 0.41, Train_acc 59.12, Test_acc 39.27
2025-01-05 16:28:49,339 [podnet.py] => Task 13, Epoch 45/160 (LR 0.08172) => LSC_loss 1.47, Spatial_loss 2.43, Flat_loss 0.40, Train_acc 59.88, Test_acc 42.07
2025-01-05 16:28:57,551 [podnet.py] => Task 13, Epoch 46/160 (LR 0.08095) => LSC_loss 1.49, Spatial_loss 2.44, Flat_loss 0.40, Train_acc 59.52, Test_acc 38.87
2025-01-05 16:29:05,715 [podnet.py] => Task 13, Epoch 47/160 (LR 0.08018) => LSC_loss 1.50, Spatial_loss 2.43, Flat_loss 0.40, Train_acc 59.59, Test_acc 34.64
2025-01-05 16:29:14,181 [podnet.py] => Task 13, Epoch 48/160 (LR 0.07939) => LSC_loss 1.47, Spatial_loss 2.42, Flat_loss 0.40, Train_acc 60.63, Test_acc 45.23
2025-01-05 16:29:22,460 [podnet.py] => Task 13, Epoch 49/160 (LR 0.07859) => LSC_loss 1.47, Spatial_loss 2.41, Flat_loss 0.40, Train_acc 59.94, Test_acc 36.73
2025-01-05 16:29:30,593 [podnet.py] => Task 13, Epoch 50/160 (LR 0.07778) => LSC_loss 1.47, Spatial_loss 2.41, Flat_loss 0.39, Train_acc 60.28, Test_acc 41.16
2025-01-05 16:29:38,773 [podnet.py] => Task 13, Epoch 51/160 (LR 0.07696) => LSC_loss 1.44, Spatial_loss 2.35, Flat_loss 0.39, Train_acc 61.27, Test_acc 41.87
2025-01-05 16:29:47,005 [podnet.py] => Task 13, Epoch 52/160 (LR 0.07612) => LSC_loss 1.46, Spatial_loss 2.36, Flat_loss 0.39, Train_acc 60.07, Test_acc 42.79
2025-01-05 16:29:55,157 [podnet.py] => Task 13, Epoch 53/160 (LR 0.07528) => LSC_loss 1.43, Spatial_loss 2.34, Flat_loss 0.38, Train_acc 61.25, Test_acc 41.17
2025-01-05 16:30:03,308 [podnet.py] => Task 13, Epoch 54/160 (LR 0.07443) => LSC_loss 1.43, Spatial_loss 2.38, Flat_loss 0.38, Train_acc 61.03, Test_acc 40.01
2025-01-05 16:30:11,577 [podnet.py] => Task 13, Epoch 55/160 (LR 0.07357) => LSC_loss 1.44, Spatial_loss 2.37, Flat_loss 0.39, Train_acc 60.72, Test_acc 39.84
2025-01-05 16:30:19,747 [podnet.py] => Task 13, Epoch 56/160 (LR 0.07270) => LSC_loss 1.47, Spatial_loss 2.46, Flat_loss 0.39, Train_acc 60.39, Test_acc 41.29
2025-01-05 16:30:28,049 [podnet.py] => Task 13, Epoch 57/160 (LR 0.07182) => LSC_loss 1.43, Spatial_loss 2.37, Flat_loss 0.38, Train_acc 60.92, Test_acc 42.20
2025-01-05 16:30:36,105 [podnet.py] => Task 13, Epoch 58/160 (LR 0.07093) => LSC_loss 1.41, Spatial_loss 2.31, Flat_loss 0.37, Train_acc 61.91, Test_acc 44.99
2025-01-05 16:30:44,453 [podnet.py] => Task 13, Epoch 59/160 (LR 0.07004) => LSC_loss 1.42, Spatial_loss 2.34, Flat_loss 0.38, Train_acc 61.22, Test_acc 40.66
2025-01-05 16:30:52,681 [podnet.py] => Task 13, Epoch 60/160 (LR 0.06913) => LSC_loss 1.42, Spatial_loss 2.31, Flat_loss 0.37, Train_acc 61.18, Test_acc 40.73
2025-01-05 16:31:00,921 [podnet.py] => Task 13, Epoch 61/160 (LR 0.06822) => LSC_loss 1.41, Spatial_loss 2.31, Flat_loss 0.37, Train_acc 62.07, Test_acc 38.97
2025-01-05 16:31:09,143 [podnet.py] => Task 13, Epoch 62/160 (LR 0.06731) => LSC_loss 1.40, Spatial_loss 2.36, Flat_loss 0.37, Train_acc 62.08, Test_acc 40.44
2025-01-05 16:31:17,517 [podnet.py] => Task 13, Epoch 63/160 (LR 0.06638) => LSC_loss 1.40, Spatial_loss 2.33, Flat_loss 0.37, Train_acc 61.77, Test_acc 42.34
2025-01-05 16:31:26,069 [podnet.py] => Task 13, Epoch 64/160 (LR 0.06545) => LSC_loss 1.37, Spatial_loss 2.24, Flat_loss 0.36, Train_acc 63.42, Test_acc 42.56
2025-01-05 16:31:34,152 [podnet.py] => Task 13, Epoch 65/160 (LR 0.06451) => LSC_loss 1.38, Spatial_loss 2.27, Flat_loss 0.36, Train_acc 62.32, Test_acc 40.09
2025-01-05 16:31:42,638 [podnet.py] => Task 13, Epoch 66/160 (LR 0.06357) => LSC_loss 1.36, Spatial_loss 2.21, Flat_loss 0.36, Train_acc 62.55, Test_acc 41.61
2025-01-05 16:31:50,697 [podnet.py] => Task 13, Epoch 67/160 (LR 0.06262) => LSC_loss 1.38, Spatial_loss 2.28, Flat_loss 0.36, Train_acc 62.41, Test_acc 40.16
2025-01-05 16:31:58,772 [podnet.py] => Task 13, Epoch 68/160 (LR 0.06167) => LSC_loss 1.37, Spatial_loss 2.26, Flat_loss 0.36, Train_acc 62.79, Test_acc 44.37
2025-01-05 16:32:07,597 [podnet.py] => Task 13, Epoch 69/160 (LR 0.06072) => LSC_loss 1.36, Spatial_loss 2.26, Flat_loss 0.36, Train_acc 62.98, Test_acc 40.30
2025-01-05 16:32:16,197 [podnet.py] => Task 13, Epoch 70/160 (LR 0.05975) => LSC_loss 1.35, Spatial_loss 2.25, Flat_loss 0.35, Train_acc 63.16, Test_acc 40.90
2025-01-05 16:32:24,118 [podnet.py] => Task 13, Epoch 71/160 (LR 0.05879) => LSC_loss 1.32, Spatial_loss 2.20, Flat_loss 0.35, Train_acc 63.92, Test_acc 36.94
2025-01-05 16:32:32,200 [podnet.py] => Task 13, Epoch 72/160 (LR 0.05782) => LSC_loss 1.32, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 63.69, Test_acc 39.86
2025-01-05 16:32:40,454 [podnet.py] => Task 13, Epoch 73/160 (LR 0.05685) => LSC_loss 1.32, Spatial_loss 2.22, Flat_loss 0.35, Train_acc 64.02, Test_acc 43.63
2025-01-05 16:32:48,819 [podnet.py] => Task 13, Epoch 74/160 (LR 0.05588) => LSC_loss 1.30, Spatial_loss 2.20, Flat_loss 0.34, Train_acc 64.48, Test_acc 43.23
2025-01-05 16:32:57,220 [podnet.py] => Task 13, Epoch 75/160 (LR 0.05490) => LSC_loss 1.29, Spatial_loss 2.16, Flat_loss 0.34, Train_acc 64.48, Test_acc 42.44
2025-01-05 16:33:05,564 [podnet.py] => Task 13, Epoch 76/160 (LR 0.05392) => LSC_loss 1.30, Spatial_loss 2.18, Flat_loss 0.34, Train_acc 64.53, Test_acc 42.11
2025-01-05 16:33:13,923 [podnet.py] => Task 13, Epoch 77/160 (LR 0.05294) => LSC_loss 1.29, Spatial_loss 2.21, Flat_loss 0.34, Train_acc 64.93, Test_acc 42.90
2025-01-05 16:33:22,354 [podnet.py] => Task 13, Epoch 78/160 (LR 0.05196) => LSC_loss 1.28, Spatial_loss 2.17, Flat_loss 0.34, Train_acc 65.11, Test_acc 43.16
2025-01-05 16:33:30,696 [podnet.py] => Task 13, Epoch 79/160 (LR 0.05098) => LSC_loss 1.26, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 65.66, Test_acc 43.73
2025-01-05 16:33:39,040 [podnet.py] => Task 13, Epoch 80/160 (LR 0.05000) => LSC_loss 1.26, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 65.38, Test_acc 42.56
2025-01-05 16:33:47,337 [podnet.py] => Task 13, Epoch 81/160 (LR 0.04902) => LSC_loss 1.25, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 66.02, Test_acc 46.19
2025-01-05 16:33:55,677 [podnet.py] => Task 13, Epoch 82/160 (LR 0.04804) => LSC_loss 1.25, Spatial_loss 2.15, Flat_loss 0.32, Train_acc 65.36, Test_acc 43.84
2025-01-05 16:34:03,964 [podnet.py] => Task 13, Epoch 83/160 (LR 0.04706) => LSC_loss 1.23, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 66.45, Test_acc 42.64
2025-01-05 16:34:12,176 [podnet.py] => Task 13, Epoch 84/160 (LR 0.04608) => LSC_loss 1.21, Spatial_loss 2.03, Flat_loss 0.31, Train_acc 66.96, Test_acc 39.27
2025-01-05 16:34:20,337 [podnet.py] => Task 13, Epoch 85/160 (LR 0.04510) => LSC_loss 1.21, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 67.02, Test_acc 43.00
2025-01-05 16:34:28,739 [podnet.py] => Task 13, Epoch 86/160 (LR 0.04412) => LSC_loss 1.18, Spatial_loss 2.05, Flat_loss 0.30, Train_acc 67.48, Test_acc 43.61
2025-01-05 16:34:36,979 [podnet.py] => Task 13, Epoch 87/160 (LR 0.04315) => LSC_loss 1.18, Spatial_loss 2.03, Flat_loss 0.30, Train_acc 67.82, Test_acc 43.77
2025-01-05 16:34:45,540 [podnet.py] => Task 13, Epoch 88/160 (LR 0.04218) => LSC_loss 1.17, Spatial_loss 2.03, Flat_loss 0.30, Train_acc 67.64, Test_acc 44.46
2025-01-05 16:34:54,015 [podnet.py] => Task 13, Epoch 89/160 (LR 0.04121) => LSC_loss 1.17, Spatial_loss 2.00, Flat_loss 0.29, Train_acc 68.40, Test_acc 42.06
2025-01-05 16:35:02,136 [podnet.py] => Task 13, Epoch 90/160 (LR 0.04025) => LSC_loss 1.17, Spatial_loss 2.05, Flat_loss 0.29, Train_acc 67.91, Test_acc 43.96
2025-01-05 16:35:10,316 [podnet.py] => Task 13, Epoch 91/160 (LR 0.03928) => LSC_loss 1.15, Spatial_loss 2.01, Flat_loss 0.29, Train_acc 68.46, Test_acc 43.34
2025-01-05 16:35:18,657 [podnet.py] => Task 13, Epoch 92/160 (LR 0.03833) => LSC_loss 1.14, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 69.00, Test_acc 44.91
2025-01-05 16:35:26,878 [podnet.py] => Task 13, Epoch 93/160 (LR 0.03738) => LSC_loss 1.12, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 69.42, Test_acc 44.46
2025-01-05 16:35:34,970 [podnet.py] => Task 13, Epoch 94/160 (LR 0.03643) => LSC_loss 1.12, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 68.94, Test_acc 44.14
2025-01-05 16:35:43,146 [podnet.py] => Task 13, Epoch 95/160 (LR 0.03549) => LSC_loss 1.13, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 68.84, Test_acc 43.63
2025-01-05 16:35:51,207 [podnet.py] => Task 13, Epoch 96/160 (LR 0.03455) => LSC_loss 1.09, Spatial_loss 1.92, Flat_loss 0.27, Train_acc 70.44, Test_acc 45.23
2025-01-05 16:35:59,507 [podnet.py] => Task 13, Epoch 97/160 (LR 0.03362) => LSC_loss 1.10, Spatial_loss 1.89, Flat_loss 0.27, Train_acc 69.43, Test_acc 44.39
2025-01-05 16:36:07,833 [podnet.py] => Task 13, Epoch 98/160 (LR 0.03269) => LSC_loss 1.09, Spatial_loss 1.86, Flat_loss 0.27, Train_acc 70.39, Test_acc 45.36
2025-01-05 16:36:16,004 [podnet.py] => Task 13, Epoch 99/160 (LR 0.03178) => LSC_loss 1.05, Spatial_loss 1.88, Flat_loss 0.25, Train_acc 71.41, Test_acc 44.00
2025-01-05 16:36:24,192 [podnet.py] => Task 13, Epoch 100/160 (LR 0.03087) => LSC_loss 1.06, Spatial_loss 1.87, Flat_loss 0.26, Train_acc 71.16, Test_acc 41.86
2025-01-05 16:36:32,654 [podnet.py] => Task 13, Epoch 101/160 (LR 0.02996) => LSC_loss 1.01, Spatial_loss 1.80, Flat_loss 0.25, Train_acc 72.49, Test_acc 44.07
2025-01-05 16:36:41,013 [podnet.py] => Task 13, Epoch 102/160 (LR 0.02907) => LSC_loss 1.02, Spatial_loss 1.83, Flat_loss 0.25, Train_acc 71.99, Test_acc 44.87
2025-01-05 16:36:49,273 [podnet.py] => Task 13, Epoch 103/160 (LR 0.02818) => LSC_loss 0.99, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 72.94, Test_acc 46.87
2025-01-05 16:36:57,825 [podnet.py] => Task 13, Epoch 104/160 (LR 0.02730) => LSC_loss 0.99, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 73.29, Test_acc 45.64
2025-01-05 16:37:06,348 [podnet.py] => Task 13, Epoch 105/160 (LR 0.02643) => LSC_loss 0.99, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 72.92, Test_acc 45.43
2025-01-05 16:37:14,729 [podnet.py] => Task 13, Epoch 106/160 (LR 0.02557) => LSC_loss 0.97, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 73.69, Test_acc 46.93
2025-01-05 16:37:22,875 [podnet.py] => Task 13, Epoch 107/160 (LR 0.02472) => LSC_loss 0.95, Spatial_loss 1.72, Flat_loss 0.22, Train_acc 74.47, Test_acc 47.23
2025-01-05 16:37:31,161 [podnet.py] => Task 13, Epoch 108/160 (LR 0.02388) => LSC_loss 0.96, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 73.86, Test_acc 45.56
2025-01-05 16:37:39,318 [podnet.py] => Task 13, Epoch 109/160 (LR 0.02304) => LSC_loss 0.95, Spatial_loss 1.73, Flat_loss 0.22, Train_acc 74.01, Test_acc 45.74
2025-01-05 16:37:47,494 [podnet.py] => Task 13, Epoch 110/160 (LR 0.02222) => LSC_loss 0.93, Spatial_loss 1.69, Flat_loss 0.22, Train_acc 74.80, Test_acc 42.76
2025-01-05 16:37:55,743 [podnet.py] => Task 13, Epoch 111/160 (LR 0.02141) => LSC_loss 0.91, Spatial_loss 1.68, Flat_loss 0.21, Train_acc 75.22, Test_acc 45.97
2025-01-05 16:38:04,226 [podnet.py] => Task 13, Epoch 112/160 (LR 0.02061) => LSC_loss 0.90, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 75.43, Test_acc 45.20
2025-01-05 16:38:12,672 [podnet.py] => Task 13, Epoch 113/160 (LR 0.01982) => LSC_loss 0.89, Spatial_loss 1.63, Flat_loss 0.21, Train_acc 75.73, Test_acc 47.27
2025-01-05 16:38:21,013 [podnet.py] => Task 13, Epoch 114/160 (LR 0.01905) => LSC_loss 0.89, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 75.76, Test_acc 48.51
2025-01-05 16:38:29,364 [podnet.py] => Task 13, Epoch 115/160 (LR 0.01828) => LSC_loss 0.86, Spatial_loss 1.59, Flat_loss 0.20, Train_acc 76.57, Test_acc 46.81
2025-01-05 16:38:37,734 [podnet.py] => Task 13, Epoch 116/160 (LR 0.01753) => LSC_loss 0.85, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 77.24, Test_acc 46.01
2025-01-05 16:38:45,833 [podnet.py] => Task 13, Epoch 117/160 (LR 0.01679) => LSC_loss 0.83, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 77.94, Test_acc 47.87
2025-01-05 16:38:54,191 [podnet.py] => Task 13, Epoch 118/160 (LR 0.01606) => LSC_loss 0.81, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 77.79, Test_acc 47.86
2025-01-05 16:39:02,441 [podnet.py] => Task 13, Epoch 119/160 (LR 0.01535) => LSC_loss 0.82, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 78.28, Test_acc 46.81
2025-01-05 16:39:10,818 [podnet.py] => Task 13, Epoch 120/160 (LR 0.01464) => LSC_loss 0.80, Spatial_loss 1.46, Flat_loss 0.17, Train_acc 78.50, Test_acc 47.76
2025-01-05 16:39:18,862 [podnet.py] => Task 13, Epoch 121/160 (LR 0.01396) => LSC_loss 0.79, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 78.72, Test_acc 47.91
2025-01-05 16:39:27,174 [podnet.py] => Task 13, Epoch 122/160 (LR 0.01328) => LSC_loss 0.78, Spatial_loss 1.46, Flat_loss 0.17, Train_acc 78.98, Test_acc 48.19
2025-01-05 16:39:35,354 [podnet.py] => Task 13, Epoch 123/160 (LR 0.01262) => LSC_loss 0.78, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 79.47, Test_acc 49.46
2025-01-05 16:39:43,586 [podnet.py] => Task 13, Epoch 124/160 (LR 0.01198) => LSC_loss 0.75, Spatial_loss 1.40, Flat_loss 0.16, Train_acc 80.42, Test_acc 46.47
2025-01-05 16:39:51,889 [podnet.py] => Task 13, Epoch 125/160 (LR 0.01135) => LSC_loss 0.75, Spatial_loss 1.39, Flat_loss 0.15, Train_acc 80.14, Test_acc 49.96
2025-01-05 16:40:00,141 [podnet.py] => Task 13, Epoch 126/160 (LR 0.01073) => LSC_loss 0.73, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 80.66, Test_acc 48.51
2025-01-05 16:40:08,363 [podnet.py] => Task 13, Epoch 127/160 (LR 0.01013) => LSC_loss 0.73, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 81.15, Test_acc 48.10
2025-01-05 16:40:16,506 [podnet.py] => Task 13, Epoch 128/160 (LR 0.00955) => LSC_loss 0.72, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 80.92, Test_acc 48.60
2025-01-05 16:40:24,589 [podnet.py] => Task 13, Epoch 129/160 (LR 0.00898) => LSC_loss 0.71, Spatial_loss 1.33, Flat_loss 0.14, Train_acc 81.37, Test_acc 47.26
2025-01-05 16:40:32,827 [podnet.py] => Task 13, Epoch 130/160 (LR 0.00843) => LSC_loss 0.71, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 81.07, Test_acc 49.40
2025-01-05 16:40:41,232 [podnet.py] => Task 13, Epoch 131/160 (LR 0.00789) => LSC_loss 0.69, Spatial_loss 1.29, Flat_loss 0.14, Train_acc 82.10, Test_acc 49.73
2025-01-05 16:40:49,630 [podnet.py] => Task 13, Epoch 132/160 (LR 0.00737) => LSC_loss 0.68, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 82.53, Test_acc 48.93
2025-01-05 16:40:58,055 [podnet.py] => Task 13, Epoch 133/160 (LR 0.00686) => LSC_loss 0.68, Spatial_loss 1.30, Flat_loss 0.13, Train_acc 82.56, Test_acc 48.54
2025-01-05 16:41:06,944 [podnet.py] => Task 13, Epoch 134/160 (LR 0.00638) => LSC_loss 0.66, Spatial_loss 1.24, Flat_loss 0.13, Train_acc 83.24, Test_acc 49.49
2025-01-05 16:41:15,852 [podnet.py] => Task 13, Epoch 135/160 (LR 0.00590) => LSC_loss 0.66, Spatial_loss 1.25, Flat_loss 0.13, Train_acc 83.42, Test_acc 49.90
2025-01-05 16:41:24,087 [podnet.py] => Task 13, Epoch 136/160 (LR 0.00545) => LSC_loss 0.64, Spatial_loss 1.22, Flat_loss 0.12, Train_acc 83.59, Test_acc 49.74
2025-01-05 16:41:32,567 [podnet.py] => Task 13, Epoch 137/160 (LR 0.00501) => LSC_loss 0.64, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 83.72, Test_acc 50.14
2025-01-05 16:41:40,611 [podnet.py] => Task 13, Epoch 138/160 (LR 0.00459) => LSC_loss 0.63, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 83.76, Test_acc 49.87
2025-01-05 16:41:48,874 [podnet.py] => Task 13, Epoch 139/160 (LR 0.00419) => LSC_loss 0.63, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 83.81, Test_acc 50.46
2025-01-05 16:41:56,961 [podnet.py] => Task 13, Epoch 140/160 (LR 0.00381) => LSC_loss 0.62, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 84.59, Test_acc 50.39
2025-01-05 16:42:05,211 [podnet.py] => Task 13, Epoch 141/160 (LR 0.00344) => LSC_loss 0.62, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 84.32, Test_acc 50.67
2025-01-05 16:42:13,267 [podnet.py] => Task 13, Epoch 142/160 (LR 0.00309) => LSC_loss 0.62, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 84.46, Test_acc 50.11
2025-01-05 16:42:21,561 [podnet.py] => Task 13, Epoch 143/160 (LR 0.00276) => LSC_loss 0.62, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 84.53, Test_acc 50.77
2025-01-05 16:42:29,699 [podnet.py] => Task 13, Epoch 144/160 (LR 0.00245) => LSC_loss 0.61, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 84.72, Test_acc 50.70
2025-01-05 16:42:37,907 [podnet.py] => Task 13, Epoch 145/160 (LR 0.00215) => LSC_loss 0.60, Spatial_loss 1.10, Flat_loss 0.11, Train_acc 85.08, Test_acc 50.94
2025-01-05 16:42:46,435 [podnet.py] => Task 13, Epoch 146/160 (LR 0.00188) => LSC_loss 0.59, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 85.26, Test_acc 50.57
2025-01-05 16:42:54,813 [podnet.py] => Task 13, Epoch 147/160 (LR 0.00162) => LSC_loss 0.60, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 85.10, Test_acc 50.81
2025-01-05 16:43:02,827 [podnet.py] => Task 13, Epoch 148/160 (LR 0.00138) => LSC_loss 0.60, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 85.29, Test_acc 51.07
2025-01-05 16:43:11,168 [podnet.py] => Task 13, Epoch 149/160 (LR 0.00116) => LSC_loss 0.58, Spatial_loss 1.06, Flat_loss 0.10, Train_acc 85.70, Test_acc 51.07
2025-01-05 16:43:19,352 [podnet.py] => Task 13, Epoch 150/160 (LR 0.00096) => LSC_loss 0.59, Spatial_loss 1.05, Flat_loss 0.10, Train_acc 85.38, Test_acc 50.73
2025-01-05 16:43:27,860 [podnet.py] => Task 13, Epoch 151/160 (LR 0.00078) => LSC_loss 0.58, Spatial_loss 1.04, Flat_loss 0.10, Train_acc 86.03, Test_acc 50.87
2025-01-05 16:43:36,224 [podnet.py] => Task 13, Epoch 152/160 (LR 0.00062) => LSC_loss 0.59, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 85.54, Test_acc 51.11
2025-01-05 16:43:44,200 [podnet.py] => Task 13, Epoch 153/160 (LR 0.00047) => LSC_loss 0.58, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 86.00, Test_acc 50.96
2025-01-05 16:43:52,525 [podnet.py] => Task 13, Epoch 154/160 (LR 0.00035) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 86.20, Test_acc 51.17
2025-01-05 16:44:00,955 [podnet.py] => Task 13, Epoch 155/160 (LR 0.00024) => LSC_loss 0.58, Spatial_loss 1.03, Flat_loss 0.10, Train_acc 85.90, Test_acc 51.19
2025-01-05 16:44:09,217 [podnet.py] => Task 13, Epoch 156/160 (LR 0.00015) => LSC_loss 0.58, Spatial_loss 1.02, Flat_loss 0.10, Train_acc 85.99, Test_acc 51.11
2025-01-05 16:44:17,693 [podnet.py] => Task 13, Epoch 157/160 (LR 0.00009) => LSC_loss 0.58, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 85.65, Test_acc 50.97
2025-01-05 16:44:26,101 [podnet.py] => Task 13, Epoch 158/160 (LR 0.00004) => LSC_loss 0.57, Spatial_loss 1.01, Flat_loss 0.10, Train_acc 86.15, Test_acc 51.06
2025-01-05 16:44:34,280 [podnet.py] => Task 13, Epoch 159/160 (LR 0.00001) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 86.39, Test_acc 51.06
2025-01-05 16:44:42,516 [podnet.py] => Task 13, Epoch 160/160 (LR 0.00000) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.10, Train_acc 86.10, Test_acc 51.00
2025-01-05 16:44:42,517 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 16:44:42,517 [base.py] => Reducing exemplars...(207 per classes)
2025-01-05 16:45:38,777 [base.py] => Constructing exemplars...(207 per classes)
2025-01-05 16:45:47,345 [podnet.py] => The size of finetune dataset: 14490
2025-01-05 16:45:54,997 [podnet.py] => Task 13, Epoch 1/20 (LR 0.00497) => LSC_loss 0.57, Spatial_loss 1.17, Flat_loss 0.09, Train_acc 85.82, Test_acc 49.44
2025-01-05 16:46:02,601 [podnet.py] => Task 13, Epoch 2/20 (LR 0.00488) => LSC_loss 0.57, Spatial_loss 1.16, Flat_loss 0.10, Train_acc 85.76, Test_acc 50.59
2025-01-05 16:46:10,389 [podnet.py] => Task 13, Epoch 3/20 (LR 0.00473) => LSC_loss 0.57, Spatial_loss 1.18, Flat_loss 0.10, Train_acc 86.02, Test_acc 49.17
2025-01-05 16:46:18,065 [podnet.py] => Task 13, Epoch 4/20 (LR 0.00452) => LSC_loss 0.57, Spatial_loss 1.16, Flat_loss 0.10, Train_acc 85.92, Test_acc 49.31
2025-01-05 16:46:25,557 [podnet.py] => Task 13, Epoch 5/20 (LR 0.00427) => LSC_loss 0.57, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 86.10, Test_acc 50.21
2025-01-05 16:46:32,894 [podnet.py] => Task 13, Epoch 6/20 (LR 0.00397) => LSC_loss 0.56, Spatial_loss 1.09, Flat_loss 0.09, Train_acc 86.22, Test_acc 50.40
2025-01-05 16:46:40,506 [podnet.py] => Task 13, Epoch 7/20 (LR 0.00363) => LSC_loss 0.56, Spatial_loss 1.13, Flat_loss 0.09, Train_acc 86.33, Test_acc 50.86
2025-01-05 16:46:48,181 [podnet.py] => Task 13, Epoch 8/20 (LR 0.00327) => LSC_loss 0.55, Spatial_loss 1.11, Flat_loss 0.09, Train_acc 86.26, Test_acc 50.79
2025-01-05 16:46:55,668 [podnet.py] => Task 13, Epoch 9/20 (LR 0.00289) => LSC_loss 0.55, Spatial_loss 1.08, Flat_loss 0.09, Train_acc 86.93, Test_acc 50.17
2025-01-05 16:47:03,252 [podnet.py] => Task 13, Epoch 10/20 (LR 0.00250) => LSC_loss 0.54, Spatial_loss 1.09, Flat_loss 0.09, Train_acc 87.16, Test_acc 50.77
2025-01-05 16:47:11,138 [podnet.py] => Task 13, Epoch 11/20 (LR 0.00211) => LSC_loss 0.53, Spatial_loss 1.06, Flat_loss 0.08, Train_acc 87.34, Test_acc 50.80
2025-01-05 16:47:18,692 [podnet.py] => Task 13, Epoch 12/20 (LR 0.00173) => LSC_loss 0.54, Spatial_loss 1.03, Flat_loss 0.08, Train_acc 86.69, Test_acc 51.17
2025-01-05 16:47:26,456 [podnet.py] => Task 13, Epoch 13/20 (LR 0.00137) => LSC_loss 0.54, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 87.25, Test_acc 50.47
2025-01-05 16:47:34,188 [podnet.py] => Task 13, Epoch 14/20 (LR 0.00103) => LSC_loss 0.53, Spatial_loss 1.02, Flat_loss 0.08, Train_acc 87.17, Test_acc 50.89
2025-01-05 16:47:41,972 [podnet.py] => Task 13, Epoch 15/20 (LR 0.00073) => LSC_loss 0.52, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 87.39, Test_acc 50.89
2025-01-05 16:47:49,557 [podnet.py] => Task 13, Epoch 16/20 (LR 0.00048) => LSC_loss 0.53, Spatial_loss 1.00, Flat_loss 0.08, Train_acc 87.49, Test_acc 50.56
2025-01-05 16:47:57,164 [podnet.py] => Task 13, Epoch 17/20 (LR 0.00027) => LSC_loss 0.52, Spatial_loss 0.98, Flat_loss 0.08, Train_acc 87.58, Test_acc 50.96
2025-01-05 16:48:04,825 [podnet.py] => Task 13, Epoch 18/20 (LR 0.00012) => LSC_loss 0.51, Spatial_loss 0.96, Flat_loss 0.07, Train_acc 87.83, Test_acc 51.29
2025-01-05 16:48:12,471 [podnet.py] => Task 13, Epoch 19/20 (LR 0.00003) => LSC_loss 0.51, Spatial_loss 0.95, Flat_loss 0.07, Train_acc 87.78, Test_acc 50.80
2025-01-05 16:48:20,167 [podnet.py] => Task 13, Epoch 20/20 (LR 0.00000) => LSC_loss 0.52, Spatial_loss 0.94, Flat_loss 0.07, Train_acc 88.00, Test_acc 50.96
2025-01-05 16:48:20,170 [base.py] => Reducing exemplars...(192 per classes)
2025-01-05 16:49:14,823 [base.py] => Constructing exemplars...(192 per classes)
2025-01-05 16:49:25,743 [podnet.py] => Exemplar size: 13440
2025-01-05 16:49:25,743 [trainer.py] => CNN: {'total': np.float64(50.96), '00-09': np.float64(62.0), '10-19': np.float64(41.3), '20-29': np.float64(57.1), '30-39': np.float64(46.5), '40-49': np.float64(58.3), '50-59': np.float64(43.4), '60-69': np.float64(48.1), 'old': np.float64(51.26), 'new': np.float64(47.0)}
2025-01-05 16:49:25,743 [trainer.py] => NME: {'total': np.float64(48.96), '00-09': np.float64(63.3), '10-19': np.float64(39.6), '20-29': np.float64(54.8), '30-39': np.float64(47.0), '40-49': np.float64(53.5), '50-59': np.float64(41.2), '60-69': np.float64(43.3), 'old': np.float64(49.23), 'new': np.float64(45.4)}
2025-01-05 16:49:25,743 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96)]
2025-01-05 16:49:25,743 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43)]
2025-01-05 16:49:25,743 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96)]
2025-01-05 16:49:25,743 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2)]

2025-01-05 16:49:25,743 [trainer.py] => All params: 511057
2025-01-05 16:49:25,743 [trainer.py] => Trainable params: 511057
2025-01-05 16:49:25,744 [podnet.py] => Learning on 70-75
2025-01-05 16:49:25,793 [podnet.py] => Adaptive factor: 3.872983346207417
2025-01-05 16:49:34,056 [podnet.py] => Task 14, Epoch 1/160 (LR 0.09999) => LSC_loss 2.57, Spatial_loss 3.65, Flat_loss 0.87, Train_acc 38.31, Test_acc 29.80
2025-01-05 16:49:42,255 [podnet.py] => Task 14, Epoch 2/160 (LR 0.09996) => LSC_loss 2.04, Spatial_loss 3.09, Flat_loss 0.62, Train_acc 46.23, Test_acc 32.97
2025-01-05 16:49:50,731 [podnet.py] => Task 14, Epoch 3/160 (LR 0.09991) => LSC_loss 1.95, Spatial_loss 3.00, Flat_loss 0.57, Train_acc 48.26, Test_acc 35.15
2025-01-05 16:49:58,847 [podnet.py] => Task 14, Epoch 4/160 (LR 0.09985) => LSC_loss 1.88, Spatial_loss 2.87, Flat_loss 0.53, Train_acc 50.68, Test_acc 37.89
2025-01-05 16:50:06,923 [podnet.py] => Task 14, Epoch 5/160 (LR 0.09976) => LSC_loss 1.83, Spatial_loss 2.80, Flat_loss 0.51, Train_acc 51.51, Test_acc 37.41
2025-01-05 16:50:15,109 [podnet.py] => Task 14, Epoch 6/160 (LR 0.09965) => LSC_loss 1.79, Spatial_loss 2.82, Flat_loss 0.51, Train_acc 52.48, Test_acc 37.56
2025-01-05 16:50:23,235 [podnet.py] => Task 14, Epoch 7/160 (LR 0.09953) => LSC_loss 1.78, Spatial_loss 2.71, Flat_loss 0.49, Train_acc 52.70, Test_acc 37.75
2025-01-05 16:50:31,470 [podnet.py] => Task 14, Epoch 8/160 (LR 0.09938) => LSC_loss 1.75, Spatial_loss 2.75, Flat_loss 0.49, Train_acc 53.26, Test_acc 37.01
2025-01-05 16:50:39,685 [podnet.py] => Task 14, Epoch 9/160 (LR 0.09922) => LSC_loss 1.74, Spatial_loss 2.70, Flat_loss 0.48, Train_acc 53.52, Test_acc 38.88
2025-01-05 16:50:48,175 [podnet.py] => Task 14, Epoch 10/160 (LR 0.09904) => LSC_loss 1.72, Spatial_loss 2.63, Flat_loss 0.47, Train_acc 54.36, Test_acc 36.21
2025-01-05 16:50:56,368 [podnet.py] => Task 14, Epoch 11/160 (LR 0.09884) => LSC_loss 1.73, Spatial_loss 2.69, Flat_loss 0.48, Train_acc 53.66, Test_acc 37.65
2025-01-05 16:51:04,690 [podnet.py] => Task 14, Epoch 12/160 (LR 0.09862) => LSC_loss 1.72, Spatial_loss 2.70, Flat_loss 0.48, Train_acc 54.03, Test_acc 38.20
2025-01-05 16:51:13,007 [podnet.py] => Task 14, Epoch 13/160 (LR 0.09838) => LSC_loss 1.68, Spatial_loss 2.61, Flat_loss 0.46, Train_acc 54.86, Test_acc 40.13
2025-01-05 16:51:21,193 [podnet.py] => Task 14, Epoch 14/160 (LR 0.09812) => LSC_loss 1.68, Spatial_loss 2.65, Flat_loss 0.47, Train_acc 55.01, Test_acc 35.95
2025-01-05 16:51:29,351 [podnet.py] => Task 14, Epoch 15/160 (LR 0.09785) => LSC_loss 1.66, Spatial_loss 2.64, Flat_loss 0.46, Train_acc 55.29, Test_acc 36.52
2025-01-05 16:51:37,582 [podnet.py] => Task 14, Epoch 16/160 (LR 0.09755) => LSC_loss 1.69, Spatial_loss 2.63, Flat_loss 0.47, Train_acc 54.71, Test_acc 37.65
2025-01-05 16:51:45,936 [podnet.py] => Task 14, Epoch 17/160 (LR 0.09724) => LSC_loss 1.67, Spatial_loss 2.65, Flat_loss 0.46, Train_acc 55.21, Test_acc 35.48
2025-01-05 16:51:54,293 [podnet.py] => Task 14, Epoch 18/160 (LR 0.09691) => LSC_loss 1.66, Spatial_loss 2.61, Flat_loss 0.46, Train_acc 55.13, Test_acc 39.04
2025-01-05 16:52:02,549 [podnet.py] => Task 14, Epoch 19/160 (LR 0.09656) => LSC_loss 1.65, Spatial_loss 2.58, Flat_loss 0.46, Train_acc 55.60, Test_acc 38.49
2025-01-05 16:52:10,949 [podnet.py] => Task 14, Epoch 20/160 (LR 0.09619) => LSC_loss 1.64, Spatial_loss 2.62, Flat_loss 0.46, Train_acc 55.92, Test_acc 37.12
2025-01-05 16:52:19,191 [podnet.py] => Task 14, Epoch 21/160 (LR 0.09581) => LSC_loss 1.66, Spatial_loss 2.62, Flat_loss 0.46, Train_acc 55.88, Test_acc 39.52
2025-01-05 16:52:27,520 [podnet.py] => Task 14, Epoch 22/160 (LR 0.09541) => LSC_loss 1.63, Spatial_loss 2.57, Flat_loss 0.44, Train_acc 56.44, Test_acc 38.04
2025-01-05 16:52:35,775 [podnet.py] => Task 14, Epoch 23/160 (LR 0.09499) => LSC_loss 1.65, Spatial_loss 2.61, Flat_loss 0.45, Train_acc 56.24, Test_acc 36.93
2025-01-05 16:52:44,016 [podnet.py] => Task 14, Epoch 24/160 (LR 0.09455) => LSC_loss 1.65, Spatial_loss 2.59, Flat_loss 0.45, Train_acc 56.02, Test_acc 36.09
2025-01-05 16:52:52,152 [podnet.py] => Task 14, Epoch 25/160 (LR 0.09410) => LSC_loss 1.62, Spatial_loss 2.59, Flat_loss 0.45, Train_acc 56.48, Test_acc 38.97
2025-01-05 16:53:00,402 [podnet.py] => Task 14, Epoch 26/160 (LR 0.09362) => LSC_loss 1.65, Spatial_loss 2.65, Flat_loss 0.46, Train_acc 55.86, Test_acc 38.12
2025-01-05 16:53:09,029 [podnet.py] => Task 14, Epoch 27/160 (LR 0.09314) => LSC_loss 1.61, Spatial_loss 2.58, Flat_loss 0.45, Train_acc 56.56, Test_acc 37.21
2025-01-05 16:53:17,444 [podnet.py] => Task 14, Epoch 28/160 (LR 0.09263) => LSC_loss 1.61, Spatial_loss 2.57, Flat_loss 0.45, Train_acc 56.54, Test_acc 30.28
2025-01-05 16:53:25,661 [podnet.py] => Task 14, Epoch 29/160 (LR 0.09211) => LSC_loss 1.62, Spatial_loss 2.61, Flat_loss 0.45, Train_acc 56.29, Test_acc 39.79
2025-01-05 16:53:33,614 [podnet.py] => Task 14, Epoch 30/160 (LR 0.09157) => LSC_loss 1.61, Spatial_loss 2.60, Flat_loss 0.45, Train_acc 56.39, Test_acc 39.75
2025-01-05 16:53:41,744 [podnet.py] => Task 14, Epoch 31/160 (LR 0.09102) => LSC_loss 1.61, Spatial_loss 2.57, Flat_loss 0.45, Train_acc 56.32, Test_acc 38.21
2025-01-05 16:53:50,126 [podnet.py] => Task 14, Epoch 32/160 (LR 0.09045) => LSC_loss 1.60, Spatial_loss 2.53, Flat_loss 0.44, Train_acc 57.23, Test_acc 38.64
2025-01-05 16:53:58,368 [podnet.py] => Task 14, Epoch 33/160 (LR 0.08987) => LSC_loss 1.59, Spatial_loss 2.56, Flat_loss 0.44, Train_acc 57.32, Test_acc 40.47
2025-01-05 16:54:06,811 [podnet.py] => Task 14, Epoch 34/160 (LR 0.08927) => LSC_loss 1.59, Spatial_loss 2.55, Flat_loss 0.44, Train_acc 57.10, Test_acc 39.81
2025-01-05 16:54:14,923 [podnet.py] => Task 14, Epoch 35/160 (LR 0.08865) => LSC_loss 1.59, Spatial_loss 2.54, Flat_loss 0.44, Train_acc 57.02, Test_acc 37.01
2025-01-05 16:54:23,235 [podnet.py] => Task 14, Epoch 36/160 (LR 0.08802) => LSC_loss 1.59, Spatial_loss 2.53, Flat_loss 0.44, Train_acc 57.46, Test_acc 39.11
2025-01-05 16:54:31,556 [podnet.py] => Task 14, Epoch 37/160 (LR 0.08738) => LSC_loss 1.60, Spatial_loss 2.60, Flat_loss 0.44, Train_acc 56.71, Test_acc 38.08
2025-01-05 16:54:39,861 [podnet.py] => Task 14, Epoch 38/160 (LR 0.08672) => LSC_loss 1.58, Spatial_loss 2.54, Flat_loss 0.44, Train_acc 57.07, Test_acc 40.07
2025-01-05 16:54:48,096 [podnet.py] => Task 14, Epoch 39/160 (LR 0.08604) => LSC_loss 1.58, Spatial_loss 2.56, Flat_loss 0.44, Train_acc 57.62, Test_acc 38.25
2025-01-05 16:54:56,329 [podnet.py] => Task 14, Epoch 40/160 (LR 0.08536) => LSC_loss 1.57, Spatial_loss 2.59, Flat_loss 0.44, Train_acc 57.41, Test_acc 41.12
2025-01-05 16:55:04,707 [podnet.py] => Task 14, Epoch 41/160 (LR 0.08465) => LSC_loss 1.58, Spatial_loss 2.57, Flat_loss 0.44, Train_acc 57.30, Test_acc 36.19
2025-01-05 16:55:13,112 [podnet.py] => Task 14, Epoch 42/160 (LR 0.08394) => LSC_loss 1.56, Spatial_loss 2.46, Flat_loss 0.43, Train_acc 57.63, Test_acc 41.05
2025-01-05 16:55:21,430 [podnet.py] => Task 14, Epoch 43/160 (LR 0.08321) => LSC_loss 1.57, Spatial_loss 2.52, Flat_loss 0.43, Train_acc 57.51, Test_acc 40.53
2025-01-05 16:55:29,614 [podnet.py] => Task 14, Epoch 44/160 (LR 0.08247) => LSC_loss 1.54, Spatial_loss 2.51, Flat_loss 0.42, Train_acc 58.32, Test_acc 40.32
2025-01-05 16:55:37,910 [podnet.py] => Task 14, Epoch 45/160 (LR 0.08172) => LSC_loss 1.54, Spatial_loss 2.45, Flat_loss 0.42, Train_acc 58.34, Test_acc 38.27
2025-01-05 16:55:46,181 [podnet.py] => Task 14, Epoch 46/160 (LR 0.08095) => LSC_loss 1.53, Spatial_loss 2.50, Flat_loss 0.42, Train_acc 58.21, Test_acc 35.39
2025-01-05 16:55:54,485 [podnet.py] => Task 14, Epoch 47/160 (LR 0.08018) => LSC_loss 1.57, Spatial_loss 2.54, Flat_loss 0.43, Train_acc 57.93, Test_acc 36.57
2025-01-05 16:56:02,726 [podnet.py] => Task 14, Epoch 48/160 (LR 0.07939) => LSC_loss 1.53, Spatial_loss 2.46, Flat_loss 0.42, Train_acc 58.68, Test_acc 37.56
2025-01-05 16:56:10,989 [podnet.py] => Task 14, Epoch 49/160 (LR 0.07859) => LSC_loss 1.54, Spatial_loss 2.50, Flat_loss 0.42, Train_acc 58.34, Test_acc 38.08
2025-01-05 16:56:19,210 [podnet.py] => Task 14, Epoch 50/160 (LR 0.07778) => LSC_loss 1.52, Spatial_loss 2.46, Flat_loss 0.41, Train_acc 58.76, Test_acc 37.89
2025-01-05 16:56:27,490 [podnet.py] => Task 14, Epoch 51/160 (LR 0.07696) => LSC_loss 1.53, Spatial_loss 2.47, Flat_loss 0.42, Train_acc 58.37, Test_acc 41.71
2025-01-05 16:56:35,770 [podnet.py] => Task 14, Epoch 52/160 (LR 0.07612) => LSC_loss 1.52, Spatial_loss 2.46, Flat_loss 0.42, Train_acc 58.74, Test_acc 37.43
2025-01-05 16:56:44,006 [podnet.py] => Task 14, Epoch 53/160 (LR 0.07528) => LSC_loss 1.51, Spatial_loss 2.46, Flat_loss 0.41, Train_acc 59.21, Test_acc 40.80
2025-01-05 16:56:52,323 [podnet.py] => Task 14, Epoch 54/160 (LR 0.07443) => LSC_loss 1.51, Spatial_loss 2.47, Flat_loss 0.41, Train_acc 59.29, Test_acc 39.20
2025-01-05 16:57:00,597 [podnet.py] => Task 14, Epoch 55/160 (LR 0.07357) => LSC_loss 1.50, Spatial_loss 2.44, Flat_loss 0.41, Train_acc 58.90, Test_acc 39.45
2025-01-05 16:57:08,721 [podnet.py] => Task 14, Epoch 56/160 (LR 0.07270) => LSC_loss 1.49, Spatial_loss 2.41, Flat_loss 0.40, Train_acc 59.80, Test_acc 41.19
2025-01-05 16:57:17,039 [podnet.py] => Task 14, Epoch 57/160 (LR 0.07182) => LSC_loss 1.47, Spatial_loss 2.40, Flat_loss 0.39, Train_acc 59.41, Test_acc 37.76
2025-01-05 16:57:25,123 [podnet.py] => Task 14, Epoch 58/160 (LR 0.07093) => LSC_loss 1.49, Spatial_loss 2.43, Flat_loss 0.40, Train_acc 59.32, Test_acc 38.81
2025-01-05 16:57:33,570 [podnet.py] => Task 14, Epoch 59/160 (LR 0.07004) => LSC_loss 1.47, Spatial_loss 2.37, Flat_loss 0.39, Train_acc 60.14, Test_acc 40.09
2025-01-05 16:57:41,881 [podnet.py] => Task 14, Epoch 60/160 (LR 0.06913) => LSC_loss 1.45, Spatial_loss 2.39, Flat_loss 0.39, Train_acc 60.35, Test_acc 38.80
2025-01-05 16:57:49,978 [podnet.py] => Task 14, Epoch 61/160 (LR 0.06822) => LSC_loss 1.47, Spatial_loss 2.40, Flat_loss 0.39, Train_acc 59.91, Test_acc 38.16
2025-01-05 16:57:58,286 [podnet.py] => Task 14, Epoch 62/160 (LR 0.06731) => LSC_loss 1.45, Spatial_loss 2.36, Flat_loss 0.38, Train_acc 60.54, Test_acc 35.53
2025-01-05 16:58:06,637 [podnet.py] => Task 14, Epoch 63/160 (LR 0.06638) => LSC_loss 1.44, Spatial_loss 2.38, Flat_loss 0.39, Train_acc 60.58, Test_acc 38.88
2025-01-05 16:58:14,841 [podnet.py] => Task 14, Epoch 64/160 (LR 0.06545) => LSC_loss 1.42, Spatial_loss 2.34, Flat_loss 0.38, Train_acc 61.30, Test_acc 39.71
2025-01-05 16:58:23,215 [podnet.py] => Task 14, Epoch 65/160 (LR 0.06451) => LSC_loss 1.44, Spatial_loss 2.35, Flat_loss 0.38, Train_acc 60.45, Test_acc 41.69
2025-01-05 16:58:31,419 [podnet.py] => Task 14, Epoch 66/160 (LR 0.06357) => LSC_loss 1.42, Spatial_loss 2.35, Flat_loss 0.38, Train_acc 61.16, Test_acc 43.31
2025-01-05 16:58:39,563 [podnet.py] => Task 14, Epoch 67/160 (LR 0.06262) => LSC_loss 1.40, Spatial_loss 2.29, Flat_loss 0.37, Train_acc 62.23, Test_acc 39.51
2025-01-05 16:58:47,864 [podnet.py] => Task 14, Epoch 68/160 (LR 0.06167) => LSC_loss 1.43, Spatial_loss 2.37, Flat_loss 0.38, Train_acc 60.82, Test_acc 39.95
2025-01-05 16:58:56,007 [podnet.py] => Task 14, Epoch 69/160 (LR 0.06072) => LSC_loss 1.40, Spatial_loss 2.33, Flat_loss 0.37, Train_acc 62.17, Test_acc 41.91
2025-01-05 16:59:04,444 [podnet.py] => Task 14, Epoch 70/160 (LR 0.05975) => LSC_loss 1.39, Spatial_loss 2.36, Flat_loss 0.37, Train_acc 61.86, Test_acc 40.20
2025-01-05 16:59:12,792 [podnet.py] => Task 14, Epoch 71/160 (LR 0.05879) => LSC_loss 1.39, Spatial_loss 2.31, Flat_loss 0.37, Train_acc 61.81, Test_acc 38.55
2025-01-05 16:59:21,071 [podnet.py] => Task 14, Epoch 72/160 (LR 0.05782) => LSC_loss 1.39, Spatial_loss 2.28, Flat_loss 0.37, Train_acc 61.90, Test_acc 40.39
2025-01-05 16:59:29,305 [podnet.py] => Task 14, Epoch 73/160 (LR 0.05685) => LSC_loss 1.38, Spatial_loss 2.29, Flat_loss 0.36, Train_acc 62.12, Test_acc 42.80
2025-01-05 16:59:37,458 [podnet.py] => Task 14, Epoch 74/160 (LR 0.05588) => LSC_loss 1.36, Spatial_loss 2.26, Flat_loss 0.35, Train_acc 62.96, Test_acc 40.93
2025-01-05 16:59:45,584 [podnet.py] => Task 14, Epoch 75/160 (LR 0.05490) => LSC_loss 1.36, Spatial_loss 2.25, Flat_loss 0.35, Train_acc 63.05, Test_acc 41.57
2025-01-05 16:59:53,861 [podnet.py] => Task 14, Epoch 76/160 (LR 0.05392) => LSC_loss 1.35, Spatial_loss 2.22, Flat_loss 0.35, Train_acc 63.28, Test_acc 41.92
2025-01-05 17:00:01,941 [podnet.py] => Task 14, Epoch 77/160 (LR 0.05294) => LSC_loss 1.33, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 63.76, Test_acc 40.95
2025-01-05 17:00:10,083 [podnet.py] => Task 14, Epoch 78/160 (LR 0.05196) => LSC_loss 1.31, Spatial_loss 2.20, Flat_loss 0.34, Train_acc 64.28, Test_acc 41.32
2025-01-05 17:00:18,389 [podnet.py] => Task 14, Epoch 79/160 (LR 0.05098) => LSC_loss 1.31, Spatial_loss 2.19, Flat_loss 0.34, Train_acc 64.14, Test_acc 40.83
2025-01-05 17:00:26,825 [podnet.py] => Task 14, Epoch 80/160 (LR 0.05000) => LSC_loss 1.33, Spatial_loss 2.22, Flat_loss 0.34, Train_acc 63.91, Test_acc 40.48
2025-01-05 17:00:35,073 [podnet.py] => Task 14, Epoch 81/160 (LR 0.04902) => LSC_loss 1.29, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 64.96, Test_acc 40.45
2025-01-05 17:00:43,296 [podnet.py] => Task 14, Epoch 82/160 (LR 0.04804) => LSC_loss 1.31, Spatial_loss 2.19, Flat_loss 0.34, Train_acc 64.48, Test_acc 44.52
2025-01-05 17:00:51,553 [podnet.py] => Task 14, Epoch 83/160 (LR 0.04706) => LSC_loss 1.29, Spatial_loss 2.10, Flat_loss 0.32, Train_acc 64.67, Test_acc 42.21
2025-01-05 17:00:59,824 [podnet.py] => Task 14, Epoch 84/160 (LR 0.04608) => LSC_loss 1.27, Spatial_loss 2.12, Flat_loss 0.32, Train_acc 64.77, Test_acc 43.05
2025-01-05 17:01:08,195 [podnet.py] => Task 14, Epoch 85/160 (LR 0.04510) => LSC_loss 1.28, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 64.49, Test_acc 40.32
2025-01-05 17:01:16,593 [podnet.py] => Task 14, Epoch 86/160 (LR 0.04412) => LSC_loss 1.26, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 65.60, Test_acc 41.16
2025-01-05 17:01:24,744 [podnet.py] => Task 14, Epoch 87/160 (LR 0.04315) => LSC_loss 1.25, Spatial_loss 2.11, Flat_loss 0.31, Train_acc 65.83, Test_acc 42.87
2025-01-05 17:01:32,920 [podnet.py] => Task 14, Epoch 88/160 (LR 0.04218) => LSC_loss 1.24, Spatial_loss 2.08, Flat_loss 0.31, Train_acc 65.80, Test_acc 42.03
2025-01-05 17:01:41,095 [podnet.py] => Task 14, Epoch 89/160 (LR 0.04121) => LSC_loss 1.22, Spatial_loss 2.08, Flat_loss 0.30, Train_acc 66.67, Test_acc 36.67
2025-01-05 17:01:49,191 [podnet.py] => Task 14, Epoch 90/160 (LR 0.04025) => LSC_loss 1.22, Spatial_loss 2.03, Flat_loss 0.30, Train_acc 66.25, Test_acc 39.69
2025-01-05 17:01:57,649 [podnet.py] => Task 14, Epoch 91/160 (LR 0.03928) => LSC_loss 1.20, Spatial_loss 2.02, Flat_loss 0.29, Train_acc 67.33, Test_acc 42.44
2025-01-05 17:02:05,841 [podnet.py] => Task 14, Epoch 92/160 (LR 0.03833) => LSC_loss 1.18, Spatial_loss 2.00, Flat_loss 0.29, Train_acc 67.52, Test_acc 43.64
2025-01-05 17:02:14,005 [podnet.py] => Task 14, Epoch 93/160 (LR 0.03738) => LSC_loss 1.18, Spatial_loss 1.98, Flat_loss 0.29, Train_acc 67.06, Test_acc 43.09
2025-01-05 17:02:22,325 [podnet.py] => Task 14, Epoch 94/160 (LR 0.03643) => LSC_loss 1.17, Spatial_loss 1.99, Flat_loss 0.28, Train_acc 67.90, Test_acc 39.77
2025-01-05 17:02:30,517 [podnet.py] => Task 14, Epoch 95/160 (LR 0.03549) => LSC_loss 1.14, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 69.09, Test_acc 41.28
2025-01-05 17:02:39,015 [podnet.py] => Task 14, Epoch 96/160 (LR 0.03455) => LSC_loss 1.16, Spatial_loss 1.98, Flat_loss 0.29, Train_acc 68.01, Test_acc 44.49
2025-01-05 17:02:47,137 [podnet.py] => Task 14, Epoch 97/160 (LR 0.03362) => LSC_loss 1.14, Spatial_loss 1.96, Flat_loss 0.27, Train_acc 68.46, Test_acc 42.29
2025-01-05 17:02:55,230 [podnet.py] => Task 14, Epoch 98/160 (LR 0.03269) => LSC_loss 1.12, Spatial_loss 1.92, Flat_loss 0.27, Train_acc 68.78, Test_acc 44.16
2025-01-05 17:03:03,384 [podnet.py] => Task 14, Epoch 99/160 (LR 0.03178) => LSC_loss 1.11, Spatial_loss 1.94, Flat_loss 0.27, Train_acc 69.41, Test_acc 42.80
2025-01-05 17:03:11,792 [podnet.py] => Task 14, Epoch 100/160 (LR 0.03087) => LSC_loss 1.10, Spatial_loss 1.89, Flat_loss 0.26, Train_acc 69.50, Test_acc 44.85
2025-01-05 17:03:20,237 [podnet.py] => Task 14, Epoch 101/160 (LR 0.02996) => LSC_loss 1.08, Spatial_loss 1.90, Flat_loss 0.25, Train_acc 70.46, Test_acc 40.17
2025-01-05 17:03:28,481 [podnet.py] => Task 14, Epoch 102/160 (LR 0.02907) => LSC_loss 1.08, Spatial_loss 1.85, Flat_loss 0.25, Train_acc 70.60, Test_acc 43.29
2025-01-05 17:03:36,901 [podnet.py] => Task 14, Epoch 103/160 (LR 0.02818) => LSC_loss 1.07, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 70.77, Test_acc 43.12
2025-01-05 17:03:45,216 [podnet.py] => Task 14, Epoch 104/160 (LR 0.02730) => LSC_loss 1.03, Spatial_loss 1.80, Flat_loss 0.24, Train_acc 71.94, Test_acc 42.99
2025-01-05 17:03:53,252 [podnet.py] => Task 14, Epoch 105/160 (LR 0.02643) => LSC_loss 1.03, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 72.18, Test_acc 44.59
2025-01-05 17:04:01,450 [podnet.py] => Task 14, Epoch 106/160 (LR 0.02557) => LSC_loss 1.01, Spatial_loss 1.74, Flat_loss 0.23, Train_acc 72.31, Test_acc 43.72
2025-01-05 17:04:09,393 [podnet.py] => Task 14, Epoch 107/160 (LR 0.02472) => LSC_loss 1.01, Spatial_loss 1.78, Flat_loss 0.23, Train_acc 72.09, Test_acc 44.35
2025-01-05 17:04:17,690 [podnet.py] => Task 14, Epoch 108/160 (LR 0.02388) => LSC_loss 1.00, Spatial_loss 1.75, Flat_loss 0.22, Train_acc 72.23, Test_acc 43.20
2025-01-05 17:04:26,183 [podnet.py] => Task 14, Epoch 109/160 (LR 0.02304) => LSC_loss 1.00, Spatial_loss 1.76, Flat_loss 0.23, Train_acc 72.38, Test_acc 43.60
2025-01-05 17:04:34,529 [podnet.py] => Task 14, Epoch 110/160 (LR 0.02222) => LSC_loss 0.99, Spatial_loss 1.73, Flat_loss 0.22, Train_acc 72.94, Test_acc 45.01
2025-01-05 17:04:42,957 [podnet.py] => Task 14, Epoch 111/160 (LR 0.02141) => LSC_loss 0.98, Spatial_loss 1.69, Flat_loss 0.21, Train_acc 73.11, Test_acc 43.99
2025-01-05 17:04:51,276 [podnet.py] => Task 14, Epoch 112/160 (LR 0.02061) => LSC_loss 0.97, Spatial_loss 1.70, Flat_loss 0.21, Train_acc 73.47, Test_acc 43.96
2025-01-05 17:04:59,663 [podnet.py] => Task 14, Epoch 113/160 (LR 0.01982) => LSC_loss 0.94, Spatial_loss 1.66, Flat_loss 0.20, Train_acc 74.39, Test_acc 45.92
2025-01-05 17:05:07,998 [podnet.py] => Task 14, Epoch 114/160 (LR 0.01905) => LSC_loss 0.91, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 75.36, Test_acc 45.37
2025-01-05 17:05:16,425 [podnet.py] => Task 14, Epoch 115/160 (LR 0.01828) => LSC_loss 0.91, Spatial_loss 1.59, Flat_loss 0.19, Train_acc 75.01, Test_acc 44.45
2025-01-05 17:05:24,644 [podnet.py] => Task 14, Epoch 116/160 (LR 0.01753) => LSC_loss 0.92, Spatial_loss 1.62, Flat_loss 0.19, Train_acc 75.12, Test_acc 46.04
2025-01-05 17:05:33,165 [podnet.py] => Task 14, Epoch 117/160 (LR 0.01679) => LSC_loss 0.90, Spatial_loss 1.61, Flat_loss 0.19, Train_acc 75.82, Test_acc 44.83
2025-01-05 17:05:41,536 [podnet.py] => Task 14, Epoch 118/160 (LR 0.01606) => LSC_loss 0.87, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 76.37, Test_acc 48.63
2025-01-05 17:05:49,859 [podnet.py] => Task 14, Epoch 119/160 (LR 0.01535) => LSC_loss 0.87, Spatial_loss 1.54, Flat_loss 0.18, Train_acc 76.59, Test_acc 45.73
2025-01-05 17:05:58,112 [podnet.py] => Task 14, Epoch 120/160 (LR 0.01464) => LSC_loss 0.84, Spatial_loss 1.52, Flat_loss 0.17, Train_acc 76.86, Test_acc 47.00
2025-01-05 17:06:06,268 [podnet.py] => Task 14, Epoch 121/160 (LR 0.01396) => LSC_loss 0.83, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 77.63, Test_acc 46.59
2025-01-05 17:06:14,636 [podnet.py] => Task 14, Epoch 122/160 (LR 0.01328) => LSC_loss 0.83, Spatial_loss 1.48, Flat_loss 0.16, Train_acc 77.79, Test_acc 46.60
2025-01-05 17:06:22,814 [podnet.py] => Task 14, Epoch 123/160 (LR 0.01262) => LSC_loss 0.82, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 77.90, Test_acc 45.68
2025-01-05 17:06:30,999 [podnet.py] => Task 14, Epoch 124/160 (LR 0.01198) => LSC_loss 0.80, Spatial_loss 1.44, Flat_loss 0.16, Train_acc 78.75, Test_acc 47.15
2025-01-05 17:06:39,452 [podnet.py] => Task 14, Epoch 125/160 (LR 0.01135) => LSC_loss 0.79, Spatial_loss 1.42, Flat_loss 0.15, Train_acc 78.95, Test_acc 46.40
2025-01-05 17:06:47,745 [podnet.py] => Task 14, Epoch 126/160 (LR 0.01073) => LSC_loss 0.79, Spatial_loss 1.40, Flat_loss 0.15, Train_acc 78.85, Test_acc 47.47
2025-01-05 17:06:56,087 [podnet.py] => Task 14, Epoch 127/160 (LR 0.01013) => LSC_loss 0.77, Spatial_loss 1.40, Flat_loss 0.14, Train_acc 79.71, Test_acc 48.09
2025-01-05 17:07:04,439 [podnet.py] => Task 14, Epoch 128/160 (LR 0.00955) => LSC_loss 0.76, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 79.37, Test_acc 47.71
2025-01-05 17:07:12,788 [podnet.py] => Task 14, Epoch 129/160 (LR 0.00898) => LSC_loss 0.75, Spatial_loss 1.34, Flat_loss 0.14, Train_acc 79.97, Test_acc 47.12
2025-01-05 17:07:21,364 [podnet.py] => Task 14, Epoch 130/160 (LR 0.00843) => LSC_loss 0.74, Spatial_loss 1.33, Flat_loss 0.13, Train_acc 80.49, Test_acc 48.01
2025-01-05 17:07:29,678 [podnet.py] => Task 14, Epoch 131/160 (LR 0.00789) => LSC_loss 0.74, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 80.84, Test_acc 47.79
2025-01-05 17:07:38,088 [podnet.py] => Task 14, Epoch 132/160 (LR 0.00737) => LSC_loss 0.73, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 80.66, Test_acc 47.28
2025-01-05 17:07:46,561 [podnet.py] => Task 14, Epoch 133/160 (LR 0.00686) => LSC_loss 0.72, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 81.12, Test_acc 46.87
2025-01-05 17:07:54,980 [podnet.py] => Task 14, Epoch 134/160 (LR 0.00638) => LSC_loss 0.71, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 81.43, Test_acc 48.47
2025-01-05 17:08:03,362 [podnet.py] => Task 14, Epoch 135/160 (LR 0.00590) => LSC_loss 0.70, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 81.54, Test_acc 47.92
2025-01-05 17:08:11,537 [podnet.py] => Task 14, Epoch 136/160 (LR 0.00545) => LSC_loss 0.70, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 81.72, Test_acc 47.97
2025-01-05 17:08:19,742 [podnet.py] => Task 14, Epoch 137/160 (LR 0.00501) => LSC_loss 0.70, Spatial_loss 1.24, Flat_loss 0.11, Train_acc 81.99, Test_acc 48.45
2025-01-05 17:08:28,114 [podnet.py] => Task 14, Epoch 138/160 (LR 0.00459) => LSC_loss 0.68, Spatial_loss 1.21, Flat_loss 0.11, Train_acc 82.47, Test_acc 48.47
2025-01-05 17:08:36,460 [podnet.py] => Task 14, Epoch 139/160 (LR 0.00419) => LSC_loss 0.68, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 82.20, Test_acc 48.27
2025-01-05 17:08:44,768 [podnet.py] => Task 14, Epoch 140/160 (LR 0.00381) => LSC_loss 0.68, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 82.31, Test_acc 48.52
2025-01-05 17:08:53,161 [podnet.py] => Task 14, Epoch 141/160 (LR 0.00344) => LSC_loss 0.67, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 82.53, Test_acc 48.91
2025-01-05 17:09:01,418 [podnet.py] => Task 14, Epoch 142/160 (LR 0.00309) => LSC_loss 0.67, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 83.01, Test_acc 48.83
2025-01-05 17:09:09,682 [podnet.py] => Task 14, Epoch 143/160 (LR 0.00276) => LSC_loss 0.66, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 83.16, Test_acc 49.09
2025-01-05 17:09:17,959 [podnet.py] => Task 14, Epoch 144/160 (LR 0.00245) => LSC_loss 0.66, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 83.26, Test_acc 49.13
2025-01-05 17:09:26,045 [podnet.py] => Task 14, Epoch 145/160 (LR 0.00215) => LSC_loss 0.64, Spatial_loss 1.06, Flat_loss 0.09, Train_acc 83.51, Test_acc 48.99
2025-01-05 17:09:34,185 [podnet.py] => Task 14, Epoch 146/160 (LR 0.00188) => LSC_loss 0.64, Spatial_loss 1.08, Flat_loss 0.09, Train_acc 83.65, Test_acc 49.27
2025-01-05 17:09:42,523 [podnet.py] => Task 14, Epoch 147/160 (LR 0.00162) => LSC_loss 0.64, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 83.81, Test_acc 49.64
2025-01-05 17:09:50,813 [podnet.py] => Task 14, Epoch 148/160 (LR 0.00138) => LSC_loss 0.63, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 83.77, Test_acc 49.28
2025-01-05 17:09:58,978 [podnet.py] => Task 14, Epoch 149/160 (LR 0.00116) => LSC_loss 0.64, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 83.68, Test_acc 49.45
2025-01-05 17:10:07,399 [podnet.py] => Task 14, Epoch 150/160 (LR 0.00096) => LSC_loss 0.63, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 84.00, Test_acc 49.13
2025-01-05 17:10:15,630 [podnet.py] => Task 14, Epoch 151/160 (LR 0.00078) => LSC_loss 0.63, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 83.99, Test_acc 49.36
2025-01-05 17:10:24,115 [podnet.py] => Task 14, Epoch 152/160 (LR 0.00062) => LSC_loss 0.63, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 84.28, Test_acc 49.52
2025-01-05 17:10:32,422 [podnet.py] => Task 14, Epoch 153/160 (LR 0.00047) => LSC_loss 0.63, Spatial_loss 1.02, Flat_loss 0.09, Train_acc 84.10, Test_acc 49.36
2025-01-05 17:10:40,900 [podnet.py] => Task 14, Epoch 154/160 (LR 0.00035) => LSC_loss 0.63, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 84.35, Test_acc 49.19
2025-01-05 17:10:49,165 [podnet.py] => Task 14, Epoch 155/160 (LR 0.00024) => LSC_loss 0.63, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 84.32, Test_acc 49.55
2025-01-05 17:10:57,392 [podnet.py] => Task 14, Epoch 156/160 (LR 0.00015) => LSC_loss 0.62, Spatial_loss 0.99, Flat_loss 0.09, Train_acc 84.53, Test_acc 49.68
2025-01-05 17:11:05,845 [podnet.py] => Task 14, Epoch 157/160 (LR 0.00009) => LSC_loss 0.62, Spatial_loss 1.00, Flat_loss 0.09, Train_acc 84.51, Test_acc 49.45
2025-01-05 17:11:14,143 [podnet.py] => Task 14, Epoch 158/160 (LR 0.00004) => LSC_loss 0.63, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 84.10, Test_acc 49.67
2025-01-05 17:11:22,450 [podnet.py] => Task 14, Epoch 159/160 (LR 0.00001) => LSC_loss 0.62, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 84.20, Test_acc 49.75
2025-01-05 17:11:30,592 [podnet.py] => Task 14, Epoch 160/160 (LR 0.00000) => LSC_loss 0.62, Spatial_loss 0.99, Flat_loss 0.08, Train_acc 84.53, Test_acc 49.63
2025-01-05 17:11:30,593 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 17:11:30,593 [base.py] => Reducing exemplars...(192 per classes)
2025-01-05 17:12:29,488 [base.py] => Constructing exemplars...(192 per classes)
2025-01-05 17:12:38,237 [podnet.py] => The size of finetune dataset: 14400
2025-01-05 17:12:45,838 [podnet.py] => Task 14, Epoch 1/20 (LR 0.00497) => LSC_loss 0.60, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 84.86, Test_acc 48.72
2025-01-05 17:12:53,545 [podnet.py] => Task 14, Epoch 2/20 (LR 0.00488) => LSC_loss 0.58, Spatial_loss 1.15, Flat_loss 0.09, Train_acc 85.36, Test_acc 48.69
2025-01-05 17:13:01,171 [podnet.py] => Task 14, Epoch 3/20 (LR 0.00473) => LSC_loss 0.59, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 85.35, Test_acc 48.36
2025-01-05 17:13:08,754 [podnet.py] => Task 14, Epoch 4/20 (LR 0.00452) => LSC_loss 0.58, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 85.58, Test_acc 48.39
2025-01-05 17:13:16,555 [podnet.py] => Task 14, Epoch 5/20 (LR 0.00427) => LSC_loss 0.59, Spatial_loss 1.15, Flat_loss 0.09, Train_acc 85.58, Test_acc 48.64
2025-01-05 17:13:24,264 [podnet.py] => Task 14, Epoch 6/20 (LR 0.00397) => LSC_loss 0.58, Spatial_loss 1.16, Flat_loss 0.09, Train_acc 85.60, Test_acc 49.31
2025-01-05 17:13:31,932 [podnet.py] => Task 14, Epoch 7/20 (LR 0.00363) => LSC_loss 0.58, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 85.49, Test_acc 48.55
2025-01-05 17:13:39,310 [podnet.py] => Task 14, Epoch 8/20 (LR 0.00327) => LSC_loss 0.58, Spatial_loss 1.14, Flat_loss 0.08, Train_acc 85.42, Test_acc 48.72
2025-01-05 17:13:46,932 [podnet.py] => Task 14, Epoch 9/20 (LR 0.00289) => LSC_loss 0.57, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.78, Test_acc 49.08
2025-01-05 17:13:54,705 [podnet.py] => Task 14, Epoch 10/20 (LR 0.00250) => LSC_loss 0.56, Spatial_loss 1.08, Flat_loss 0.08, Train_acc 86.08, Test_acc 49.17
2025-01-05 17:14:02,419 [podnet.py] => Task 14, Epoch 11/20 (LR 0.00211) => LSC_loss 0.56, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 86.64, Test_acc 49.47
2025-01-05 17:14:10,103 [podnet.py] => Task 14, Epoch 12/20 (LR 0.00173) => LSC_loss 0.56, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 86.21, Test_acc 49.59
2025-01-05 17:14:17,698 [podnet.py] => Task 14, Epoch 13/20 (LR 0.00137) => LSC_loss 0.55, Spatial_loss 1.03, Flat_loss 0.07, Train_acc 86.54, Test_acc 49.67
2025-01-05 17:14:25,524 [podnet.py] => Task 14, Epoch 14/20 (LR 0.00103) => LSC_loss 0.55, Spatial_loss 1.02, Flat_loss 0.07, Train_acc 86.94, Test_acc 49.75
2025-01-05 17:14:33,434 [podnet.py] => Task 14, Epoch 15/20 (LR 0.00073) => LSC_loss 0.54, Spatial_loss 1.01, Flat_loss 0.07, Train_acc 86.58, Test_acc 49.56
2025-01-05 17:14:41,039 [podnet.py] => Task 14, Epoch 16/20 (LR 0.00048) => LSC_loss 0.55, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 86.69, Test_acc 49.64
2025-01-05 17:14:48,963 [podnet.py] => Task 14, Epoch 17/20 (LR 0.00027) => LSC_loss 0.54, Spatial_loss 0.97, Flat_loss 0.07, Train_acc 87.55, Test_acc 49.68
2025-01-05 17:14:56,695 [podnet.py] => Task 14, Epoch 18/20 (LR 0.00012) => LSC_loss 0.53, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 87.47, Test_acc 49.92
2025-01-05 17:15:04,307 [podnet.py] => Task 14, Epoch 19/20 (LR 0.00003) => LSC_loss 0.54, Spatial_loss 0.98, Flat_loss 0.07, Train_acc 86.97, Test_acc 49.72
2025-01-05 17:15:11,981 [podnet.py] => Task 14, Epoch 20/20 (LR 0.00000) => LSC_loss 0.53, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 87.30, Test_acc 49.84
2025-01-05 17:15:11,985 [base.py] => Reducing exemplars...(179 per classes)
2025-01-05 17:16:10,002 [base.py] => Constructing exemplars...(179 per classes)
2025-01-05 17:16:21,335 [podnet.py] => Exemplar size: 13425
2025-01-05 17:16:21,336 [trainer.py] => CNN: {'total': np.float64(49.84), '00-09': np.float64(61.1), '10-19': np.float64(40.5), '20-29': np.float64(56.6), '30-39': np.float64(46.9), '40-49': np.float64(55.9), '50-59': np.float64(43.6), '60-69': np.float64(50.4), '70-79': np.float64(37.6), 'old': np.float64(50.71), 'new': np.float64(37.6)}
2025-01-05 17:16:21,336 [trainer.py] => NME: {'total': np.float64(48.09), '00-09': np.float64(62.3), '10-19': np.float64(38.5), '20-29': np.float64(54.6), '30-39': np.float64(47.3), '40-49': np.float64(53.7), '50-59': np.float64(40.5), '60-69': np.float64(44.4), '70-79': np.float64(38.8), 'old': np.float64(48.76), 'new': np.float64(38.8)}
2025-01-05 17:16:21,336 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96), np.float64(49.84)]
2025-01-05 17:16:21,336 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43), np.float64(77.19)]
2025-01-05 17:16:21,336 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96), np.float64(48.09)]
2025-01-05 17:16:21,336 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2), np.float64(75.59)]

2025-01-05 17:16:21,336 [trainer.py] => All params: 514257
2025-01-05 17:16:21,336 [trainer.py] => Trainable params: 514257
2025-01-05 17:16:21,337 [podnet.py] => Learning on 75-80
2025-01-05 17:16:21,389 [podnet.py] => Adaptive factor: 4.0
2025-01-05 17:16:29,527 [podnet.py] => Task 15, Epoch 1/160 (LR 0.09999) => LSC_loss 2.63, Spatial_loss 3.79, Flat_loss 0.92, Train_acc 36.78, Test_acc 26.84
2025-01-05 17:16:37,742 [podnet.py] => Task 15, Epoch 2/160 (LR 0.09996) => LSC_loss 2.09, Spatial_loss 3.17, Flat_loss 0.66, Train_acc 44.74, Test_acc 32.42
2025-01-05 17:16:45,771 [podnet.py] => Task 15, Epoch 3/160 (LR 0.09991) => LSC_loss 1.98, Spatial_loss 3.04, Flat_loss 0.60, Train_acc 47.15, Test_acc 36.67
2025-01-05 17:16:54,149 [podnet.py] => Task 15, Epoch 4/160 (LR 0.09985) => LSC_loss 1.91, Spatial_loss 2.98, Flat_loss 0.57, Train_acc 48.90, Test_acc 35.12
2025-01-05 17:17:02,319 [podnet.py] => Task 15, Epoch 5/160 (LR 0.09976) => LSC_loss 1.87, Spatial_loss 2.90, Flat_loss 0.55, Train_acc 50.18, Test_acc 33.96
2025-01-05 17:17:11,211 [podnet.py] => Task 15, Epoch 6/160 (LR 0.09965) => LSC_loss 1.85, Spatial_loss 2.91, Flat_loss 0.54, Train_acc 50.71, Test_acc 35.66
2025-01-05 17:17:19,952 [podnet.py] => Task 15, Epoch 7/160 (LR 0.09953) => LSC_loss 1.80, Spatial_loss 2.75, Flat_loss 0.51, Train_acc 52.48, Test_acc 36.55
2025-01-05 17:17:28,634 [podnet.py] => Task 15, Epoch 8/160 (LR 0.09938) => LSC_loss 1.80, Spatial_loss 2.80, Flat_loss 0.52, Train_acc 51.48, Test_acc 37.01
2025-01-05 17:17:36,897 [podnet.py] => Task 15, Epoch 9/160 (LR 0.09922) => LSC_loss 1.77, Spatial_loss 2.73, Flat_loss 0.51, Train_acc 52.73, Test_acc 33.81
2025-01-05 17:17:44,961 [podnet.py] => Task 15, Epoch 10/160 (LR 0.09904) => LSC_loss 1.77, Spatial_loss 2.76, Flat_loss 0.51, Train_acc 52.57, Test_acc 32.75
2025-01-05 17:17:53,245 [podnet.py] => Task 15, Epoch 11/160 (LR 0.09884) => LSC_loss 1.78, Spatial_loss 2.78, Flat_loss 0.51, Train_acc 52.73, Test_acc 33.67
2025-01-05 17:18:01,323 [podnet.py] => Task 15, Epoch 12/160 (LR 0.09862) => LSC_loss 1.78, Spatial_loss 2.80, Flat_loss 0.51, Train_acc 52.76, Test_acc 33.42
2025-01-05 17:18:09,694 [podnet.py] => Task 15, Epoch 13/160 (LR 0.09838) => LSC_loss 1.73, Spatial_loss 2.71, Flat_loss 0.50, Train_acc 53.22, Test_acc 34.65
2025-01-05 17:18:17,953 [podnet.py] => Task 15, Epoch 14/160 (LR 0.09812) => LSC_loss 1.74, Spatial_loss 2.71, Flat_loss 0.50, Train_acc 53.44, Test_acc 37.38
2025-01-05 17:18:26,257 [podnet.py] => Task 15, Epoch 15/160 (LR 0.09785) => LSC_loss 1.71, Spatial_loss 2.71, Flat_loss 0.49, Train_acc 54.49, Test_acc 36.71
2025-01-05 17:18:34,449 [podnet.py] => Task 15, Epoch 16/160 (LR 0.09755) => LSC_loss 1.72, Spatial_loss 2.72, Flat_loss 0.49, Train_acc 53.84, Test_acc 32.85
2025-01-05 17:18:42,672 [podnet.py] => Task 15, Epoch 17/160 (LR 0.09724) => LSC_loss 1.72, Spatial_loss 2.70, Flat_loss 0.49, Train_acc 53.90, Test_acc 34.61
2025-01-05 17:18:50,734 [podnet.py] => Task 15, Epoch 18/160 (LR 0.09691) => LSC_loss 1.71, Spatial_loss 2.70, Flat_loss 0.49, Train_acc 53.97, Test_acc 29.49
2025-01-05 17:18:59,001 [podnet.py] => Task 15, Epoch 19/160 (LR 0.09656) => LSC_loss 1.70, Spatial_loss 2.73, Flat_loss 0.49, Train_acc 54.36, Test_acc 32.66
2025-01-05 17:19:07,372 [podnet.py] => Task 15, Epoch 20/160 (LR 0.09619) => LSC_loss 1.70, Spatial_loss 2.68, Flat_loss 0.49, Train_acc 54.22, Test_acc 32.85
2025-01-05 17:19:15,847 [podnet.py] => Task 15, Epoch 21/160 (LR 0.09581) => LSC_loss 1.70, Spatial_loss 2.64, Flat_loss 0.49, Train_acc 54.22, Test_acc 38.79
2025-01-05 17:19:24,072 [podnet.py] => Task 15, Epoch 22/160 (LR 0.09541) => LSC_loss 1.67, Spatial_loss 2.70, Flat_loss 0.48, Train_acc 55.29, Test_acc 34.95
2025-01-05 17:19:32,065 [podnet.py] => Task 15, Epoch 23/160 (LR 0.09499) => LSC_loss 1.69, Spatial_loss 2.73, Flat_loss 0.49, Train_acc 54.73, Test_acc 37.38
2025-01-05 17:19:40,368 [podnet.py] => Task 15, Epoch 24/160 (LR 0.09455) => LSC_loss 1.69, Spatial_loss 2.74, Flat_loss 0.49, Train_acc 54.41, Test_acc 36.83
2025-01-05 17:19:48,913 [podnet.py] => Task 15, Epoch 25/160 (LR 0.09410) => LSC_loss 1.68, Spatial_loss 2.70, Flat_loss 0.49, Train_acc 54.39, Test_acc 37.54
2025-01-05 17:19:57,172 [podnet.py] => Task 15, Epoch 26/160 (LR 0.09362) => LSC_loss 1.67, Spatial_loss 2.68, Flat_loss 0.48, Train_acc 54.83, Test_acc 37.33
2025-01-05 17:20:05,460 [podnet.py] => Task 15, Epoch 27/160 (LR 0.09314) => LSC_loss 1.66, Spatial_loss 2.65, Flat_loss 0.47, Train_acc 55.38, Test_acc 36.94
2025-01-05 17:20:13,857 [podnet.py] => Task 15, Epoch 28/160 (LR 0.09263) => LSC_loss 1.65, Spatial_loss 2.66, Flat_loss 0.47, Train_acc 55.64, Test_acc 36.94
2025-01-05 17:20:22,438 [podnet.py] => Task 15, Epoch 29/160 (LR 0.09211) => LSC_loss 1.69, Spatial_loss 2.68, Flat_loss 0.48, Train_acc 54.90, Test_acc 35.48
2025-01-05 17:20:30,904 [podnet.py] => Task 15, Epoch 30/160 (LR 0.09157) => LSC_loss 1.66, Spatial_loss 2.69, Flat_loss 0.48, Train_acc 55.22, Test_acc 37.17
2025-01-05 17:20:39,233 [podnet.py] => Task 15, Epoch 31/160 (LR 0.09102) => LSC_loss 1.66, Spatial_loss 2.67, Flat_loss 0.47, Train_acc 55.44, Test_acc 28.88
2025-01-05 17:20:47,550 [podnet.py] => Task 15, Epoch 32/160 (LR 0.09045) => LSC_loss 1.64, Spatial_loss 2.68, Flat_loss 0.47, Train_acc 55.99, Test_acc 37.70
2025-01-05 17:20:55,756 [podnet.py] => Task 15, Epoch 33/160 (LR 0.08987) => LSC_loss 1.63, Spatial_loss 2.64, Flat_loss 0.47, Train_acc 56.38, Test_acc 31.61
2025-01-05 17:21:04,140 [podnet.py] => Task 15, Epoch 34/160 (LR 0.08927) => LSC_loss 1.64, Spatial_loss 2.62, Flat_loss 0.47, Train_acc 55.30, Test_acc 36.45
2025-01-05 17:21:12,579 [podnet.py] => Task 15, Epoch 35/160 (LR 0.08865) => LSC_loss 1.64, Spatial_loss 2.63, Flat_loss 0.47, Train_acc 55.26, Test_acc 36.61
2025-01-05 17:21:20,802 [podnet.py] => Task 15, Epoch 36/160 (LR 0.08802) => LSC_loss 1.63, Spatial_loss 2.60, Flat_loss 0.46, Train_acc 56.06, Test_acc 37.52
2025-01-05 17:21:29,108 [podnet.py] => Task 15, Epoch 37/160 (LR 0.08738) => LSC_loss 1.64, Spatial_loss 2.63, Flat_loss 0.47, Train_acc 55.77, Test_acc 37.86
2025-01-05 17:21:37,227 [podnet.py] => Task 15, Epoch 38/160 (LR 0.08672) => LSC_loss 1.63, Spatial_loss 2.64, Flat_loss 0.47, Train_acc 55.77, Test_acc 38.10
2025-01-05 17:21:45,306 [podnet.py] => Task 15, Epoch 39/160 (LR 0.08604) => LSC_loss 1.62, Spatial_loss 2.67, Flat_loss 0.46, Train_acc 56.62, Test_acc 39.24
2025-01-05 17:21:53,502 [podnet.py] => Task 15, Epoch 40/160 (LR 0.08536) => LSC_loss 1.63, Spatial_loss 2.66, Flat_loss 0.46, Train_acc 56.14, Test_acc 40.02
2025-01-05 17:22:01,770 [podnet.py] => Task 15, Epoch 41/160 (LR 0.08465) => LSC_loss 1.61, Spatial_loss 2.64, Flat_loss 0.46, Train_acc 56.70, Test_acc 34.94
2025-01-05 17:22:10,008 [podnet.py] => Task 15, Epoch 42/160 (LR 0.08394) => LSC_loss 1.62, Spatial_loss 2.60, Flat_loss 0.46, Train_acc 56.05, Test_acc 35.48
2025-01-05 17:22:18,345 [podnet.py] => Task 15, Epoch 43/160 (LR 0.08321) => LSC_loss 1.60, Spatial_loss 2.58, Flat_loss 0.45, Train_acc 57.01, Test_acc 36.85
2025-01-05 17:22:26,526 [podnet.py] => Task 15, Epoch 44/160 (LR 0.08247) => LSC_loss 1.61, Spatial_loss 2.54, Flat_loss 0.46, Train_acc 56.46, Test_acc 37.58
2025-01-05 17:22:34,810 [podnet.py] => Task 15, Epoch 45/160 (LR 0.08172) => LSC_loss 1.58, Spatial_loss 2.56, Flat_loss 0.45, Train_acc 57.80, Test_acc 35.94
2025-01-05 17:22:43,012 [podnet.py] => Task 15, Epoch 46/160 (LR 0.08095) => LSC_loss 1.58, Spatial_loss 2.59, Flat_loss 0.45, Train_acc 57.09, Test_acc 36.42
2025-01-05 17:22:51,034 [podnet.py] => Task 15, Epoch 47/160 (LR 0.08018) => LSC_loss 1.57, Spatial_loss 2.56, Flat_loss 0.44, Train_acc 57.17, Test_acc 34.88
2025-01-05 17:22:59,550 [podnet.py] => Task 15, Epoch 48/160 (LR 0.07939) => LSC_loss 1.58, Spatial_loss 2.55, Flat_loss 0.44, Train_acc 57.40, Test_acc 35.29
2025-01-05 17:23:07,676 [podnet.py] => Task 15, Epoch 49/160 (LR 0.07859) => LSC_loss 1.57, Spatial_loss 2.53, Flat_loss 0.44, Train_acc 57.41, Test_acc 35.54
2025-01-05 17:23:15,978 [podnet.py] => Task 15, Epoch 50/160 (LR 0.07778) => LSC_loss 1.59, Spatial_loss 2.58, Flat_loss 0.45, Train_acc 57.03, Test_acc 38.79
2025-01-05 17:23:24,271 [podnet.py] => Task 15, Epoch 51/160 (LR 0.07696) => LSC_loss 1.58, Spatial_loss 2.61, Flat_loss 0.44, Train_acc 57.37, Test_acc 38.21
2025-01-05 17:23:32,671 [podnet.py] => Task 15, Epoch 52/160 (LR 0.07612) => LSC_loss 1.55, Spatial_loss 2.48, Flat_loss 0.43, Train_acc 58.29, Test_acc 36.44
2025-01-05 17:23:41,012 [podnet.py] => Task 15, Epoch 53/160 (LR 0.07528) => LSC_loss 1.55, Spatial_loss 2.52, Flat_loss 0.44, Train_acc 58.05, Test_acc 36.38
2025-01-05 17:23:49,331 [podnet.py] => Task 15, Epoch 54/160 (LR 0.07443) => LSC_loss 1.53, Spatial_loss 2.54, Flat_loss 0.43, Train_acc 58.38, Test_acc 38.58
2025-01-05 17:23:57,797 [podnet.py] => Task 15, Epoch 55/160 (LR 0.07357) => LSC_loss 1.54, Spatial_loss 2.53, Flat_loss 0.43, Train_acc 58.04, Test_acc 38.15
2025-01-05 17:24:05,971 [podnet.py] => Task 15, Epoch 56/160 (LR 0.07270) => LSC_loss 1.54, Spatial_loss 2.53, Flat_loss 0.43, Train_acc 58.22, Test_acc 37.01
2025-01-05 17:24:14,211 [podnet.py] => Task 15, Epoch 57/160 (LR 0.07182) => LSC_loss 1.55, Spatial_loss 2.55, Flat_loss 0.43, Train_acc 57.76, Test_acc 36.15
2025-01-05 17:24:22,597 [podnet.py] => Task 15, Epoch 58/160 (LR 0.07093) => LSC_loss 1.52, Spatial_loss 2.51, Flat_loss 0.43, Train_acc 58.58, Test_acc 38.67
2025-01-05 17:24:30,791 [podnet.py] => Task 15, Epoch 59/160 (LR 0.07004) => LSC_loss 1.51, Spatial_loss 2.46, Flat_loss 0.42, Train_acc 59.05, Test_acc 34.38
2025-01-05 17:24:39,180 [podnet.py] => Task 15, Epoch 60/160 (LR 0.06913) => LSC_loss 1.51, Spatial_loss 2.50, Flat_loss 0.42, Train_acc 58.97, Test_acc 37.21
2025-01-05 17:24:47,412 [podnet.py] => Task 15, Epoch 61/160 (LR 0.06822) => LSC_loss 1.52, Spatial_loss 2.41, Flat_loss 0.42, Train_acc 58.64, Test_acc 40.46
2025-01-05 17:24:55,808 [podnet.py] => Task 15, Epoch 62/160 (LR 0.06731) => LSC_loss 1.52, Spatial_loss 2.51, Flat_loss 0.42, Train_acc 58.93, Test_acc 39.58
2025-01-05 17:25:04,134 [podnet.py] => Task 15, Epoch 63/160 (LR 0.06638) => LSC_loss 1.50, Spatial_loss 2.46, Flat_loss 0.41, Train_acc 59.39, Test_acc 38.49
2025-01-05 17:25:12,380 [podnet.py] => Task 15, Epoch 64/160 (LR 0.06545) => LSC_loss 1.48, Spatial_loss 2.39, Flat_loss 0.40, Train_acc 59.81, Test_acc 37.36
2025-01-05 17:25:20,606 [podnet.py] => Task 15, Epoch 65/160 (LR 0.06451) => LSC_loss 1.49, Spatial_loss 2.44, Flat_loss 0.41, Train_acc 59.38, Test_acc 39.83
2025-01-05 17:25:29,036 [podnet.py] => Task 15, Epoch 66/160 (LR 0.06357) => LSC_loss 1.48, Spatial_loss 2.44, Flat_loss 0.40, Train_acc 59.99, Test_acc 38.42
2025-01-05 17:25:37,292 [podnet.py] => Task 15, Epoch 67/160 (LR 0.06262) => LSC_loss 1.48, Spatial_loss 2.40, Flat_loss 0.40, Train_acc 59.59, Test_acc 37.64
2025-01-05 17:25:45,592 [podnet.py] => Task 15, Epoch 68/160 (LR 0.06167) => LSC_loss 1.45, Spatial_loss 2.40, Flat_loss 0.40, Train_acc 60.41, Test_acc 37.61
2025-01-05 17:25:53,933 [podnet.py] => Task 15, Epoch 69/160 (LR 0.06072) => LSC_loss 1.43, Spatial_loss 2.38, Flat_loss 0.39, Train_acc 61.15, Test_acc 40.34
2025-01-05 17:26:02,311 [podnet.py] => Task 15, Epoch 70/160 (LR 0.05975) => LSC_loss 1.43, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 60.59, Test_acc 36.79
2025-01-05 17:26:11,131 [podnet.py] => Task 15, Epoch 71/160 (LR 0.05879) => LSC_loss 1.42, Spatial_loss 2.37, Flat_loss 0.39, Train_acc 60.99, Test_acc 33.33
2025-01-05 17:26:19,351 [podnet.py] => Task 15, Epoch 72/160 (LR 0.05782) => LSC_loss 1.42, Spatial_loss 2.36, Flat_loss 0.39, Train_acc 61.31, Test_acc 40.10
2025-01-05 17:26:27,655 [podnet.py] => Task 15, Epoch 73/160 (LR 0.05685) => LSC_loss 1.41, Spatial_loss 2.31, Flat_loss 0.38, Train_acc 61.44, Test_acc 39.61
2025-01-05 17:26:36,083 [podnet.py] => Task 15, Epoch 74/160 (LR 0.05588) => LSC_loss 1.39, Spatial_loss 2.29, Flat_loss 0.37, Train_acc 62.01, Test_acc 37.90
2025-01-05 17:26:44,556 [podnet.py] => Task 15, Epoch 75/160 (LR 0.05490) => LSC_loss 1.39, Spatial_loss 2.27, Flat_loss 0.37, Train_acc 61.89, Test_acc 38.81
2025-01-05 17:26:52,778 [podnet.py] => Task 15, Epoch 76/160 (LR 0.05392) => LSC_loss 1.38, Spatial_loss 2.26, Flat_loss 0.37, Train_acc 61.58, Test_acc 37.60
2025-01-05 17:27:01,221 [podnet.py] => Task 15, Epoch 77/160 (LR 0.05294) => LSC_loss 1.36, Spatial_loss 2.26, Flat_loss 0.36, Train_acc 62.51, Test_acc 36.59
2025-01-05 17:27:09,489 [podnet.py] => Task 15, Epoch 78/160 (LR 0.05196) => LSC_loss 1.36, Spatial_loss 2.27, Flat_loss 0.36, Train_acc 62.70, Test_acc 40.84
2025-01-05 17:27:17,865 [podnet.py] => Task 15, Epoch 79/160 (LR 0.05098) => LSC_loss 1.37, Spatial_loss 2.30, Flat_loss 0.36, Train_acc 62.50, Test_acc 38.76
2025-01-05 17:27:26,276 [podnet.py] => Task 15, Epoch 80/160 (LR 0.05000) => LSC_loss 1.35, Spatial_loss 2.24, Flat_loss 0.36, Train_acc 62.35, Test_acc 39.34
2025-01-05 17:27:34,602 [podnet.py] => Task 15, Epoch 81/160 (LR 0.04902) => LSC_loss 1.34, Spatial_loss 2.21, Flat_loss 0.35, Train_acc 63.16, Test_acc 40.50
2025-01-05 17:27:42,916 [podnet.py] => Task 15, Epoch 82/160 (LR 0.04804) => LSC_loss 1.34, Spatial_loss 2.24, Flat_loss 0.35, Train_acc 63.15, Test_acc 40.22
2025-01-05 17:27:51,309 [podnet.py] => Task 15, Epoch 83/160 (LR 0.04706) => LSC_loss 1.31, Spatial_loss 2.20, Flat_loss 0.35, Train_acc 64.30, Test_acc 40.31
2025-01-05 17:27:59,437 [podnet.py] => Task 15, Epoch 84/160 (LR 0.04608) => LSC_loss 1.31, Spatial_loss 2.18, Flat_loss 0.34, Train_acc 63.98, Test_acc 41.02
2025-01-05 17:28:07,590 [podnet.py] => Task 15, Epoch 85/160 (LR 0.04510) => LSC_loss 1.29, Spatial_loss 2.16, Flat_loss 0.33, Train_acc 64.28, Test_acc 40.50
2025-01-05 17:28:15,742 [podnet.py] => Task 15, Epoch 86/160 (LR 0.04412) => LSC_loss 1.31, Spatial_loss 2.18, Flat_loss 0.34, Train_acc 63.80, Test_acc 40.65
2025-01-05 17:28:23,900 [podnet.py] => Task 15, Epoch 87/160 (LR 0.04315) => LSC_loss 1.29, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 64.78, Test_acc 41.55
2025-01-05 17:28:32,242 [podnet.py] => Task 15, Epoch 88/160 (LR 0.04218) => LSC_loss 1.27, Spatial_loss 2.19, Flat_loss 0.33, Train_acc 64.81, Test_acc 43.69
2025-01-05 17:28:40,402 [podnet.py] => Task 15, Epoch 89/160 (LR 0.04121) => LSC_loss 1.25, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 65.92, Test_acc 40.28
2025-01-05 17:28:48,754 [podnet.py] => Task 15, Epoch 90/160 (LR 0.04025) => LSC_loss 1.25, Spatial_loss 2.13, Flat_loss 0.32, Train_acc 65.81, Test_acc 40.81
2025-01-05 17:28:57,019 [podnet.py] => Task 15, Epoch 91/160 (LR 0.03928) => LSC_loss 1.22, Spatial_loss 2.09, Flat_loss 0.31, Train_acc 66.54, Test_acc 39.65
2025-01-05 17:29:05,281 [podnet.py] => Task 15, Epoch 92/160 (LR 0.03833) => LSC_loss 1.23, Spatial_loss 2.14, Flat_loss 0.32, Train_acc 66.11, Test_acc 39.21
2025-01-05 17:29:13,575 [podnet.py] => Task 15, Epoch 93/160 (LR 0.03738) => LSC_loss 1.21, Spatial_loss 2.09, Flat_loss 0.31, Train_acc 66.19, Test_acc 39.06
2025-01-05 17:29:21,841 [podnet.py] => Task 15, Epoch 94/160 (LR 0.03643) => LSC_loss 1.21, Spatial_loss 2.05, Flat_loss 0.30, Train_acc 66.57, Test_acc 42.45
2025-01-05 17:29:30,060 [podnet.py] => Task 15, Epoch 95/160 (LR 0.03549) => LSC_loss 1.21, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 66.45, Test_acc 40.34
2025-01-05 17:29:38,371 [podnet.py] => Task 15, Epoch 96/160 (LR 0.03455) => LSC_loss 1.18, Spatial_loss 2.07, Flat_loss 0.30, Train_acc 67.42, Test_acc 39.85
2025-01-05 17:29:46,695 [podnet.py] => Task 15, Epoch 97/160 (LR 0.03362) => LSC_loss 1.18, Spatial_loss 2.05, Flat_loss 0.30, Train_acc 67.25, Test_acc 42.04
2025-01-05 17:29:54,792 [podnet.py] => Task 15, Epoch 98/160 (LR 0.03269) => LSC_loss 1.17, Spatial_loss 2.04, Flat_loss 0.29, Train_acc 67.74, Test_acc 44.22
2025-01-05 17:30:03,103 [podnet.py] => Task 15, Epoch 99/160 (LR 0.03178) => LSC_loss 1.16, Spatial_loss 2.01, Flat_loss 0.28, Train_acc 67.91, Test_acc 44.05
2025-01-05 17:30:11,361 [podnet.py] => Task 15, Epoch 100/160 (LR 0.03087) => LSC_loss 1.12, Spatial_loss 1.95, Flat_loss 0.28, Train_acc 68.67, Test_acc 38.51
2025-01-05 17:30:19,748 [podnet.py] => Task 15, Epoch 101/160 (LR 0.02996) => LSC_loss 1.11, Spatial_loss 1.95, Flat_loss 0.27, Train_acc 68.95, Test_acc 40.91
2025-01-05 17:30:28,129 [podnet.py] => Task 15, Epoch 102/160 (LR 0.02907) => LSC_loss 1.10, Spatial_loss 1.91, Flat_loss 0.26, Train_acc 69.82, Test_acc 44.15
2025-01-05 17:30:36,235 [podnet.py] => Task 15, Epoch 103/160 (LR 0.02818) => LSC_loss 1.11, Spatial_loss 1.90, Flat_loss 0.26, Train_acc 69.70, Test_acc 42.20
2025-01-05 17:30:44,434 [podnet.py] => Task 15, Epoch 104/160 (LR 0.02730) => LSC_loss 1.09, Spatial_loss 1.88, Flat_loss 0.26, Train_acc 70.11, Test_acc 40.80
2025-01-05 17:30:52,624 [podnet.py] => Task 15, Epoch 105/160 (LR 0.02643) => LSC_loss 1.06, Spatial_loss 1.92, Flat_loss 0.26, Train_acc 70.60, Test_acc 41.98
2025-01-05 17:31:01,006 [podnet.py] => Task 15, Epoch 106/160 (LR 0.02557) => LSC_loss 1.05, Spatial_loss 1.82, Flat_loss 0.25, Train_acc 71.23, Test_acc 41.95
2025-01-05 17:31:09,221 [podnet.py] => Task 15, Epoch 107/160 (LR 0.02472) => LSC_loss 1.05, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 71.03, Test_acc 42.18
2025-01-05 17:31:17,608 [podnet.py] => Task 15, Epoch 108/160 (LR 0.02388) => LSC_loss 1.03, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 72.06, Test_acc 42.89
2025-01-05 17:31:25,803 [podnet.py] => Task 15, Epoch 109/160 (LR 0.02304) => LSC_loss 1.01, Spatial_loss 1.78, Flat_loss 0.23, Train_acc 72.27, Test_acc 44.49
2025-01-05 17:31:34,235 [podnet.py] => Task 15, Epoch 110/160 (LR 0.02222) => LSC_loss 1.01, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 72.07, Test_acc 42.90
2025-01-05 17:31:42,550 [podnet.py] => Task 15, Epoch 111/160 (LR 0.02141) => LSC_loss 1.01, Spatial_loss 1.78, Flat_loss 0.23, Train_acc 72.23, Test_acc 41.94
2025-01-05 17:31:50,846 [podnet.py] => Task 15, Epoch 112/160 (LR 0.02061) => LSC_loss 0.98, Spatial_loss 1.73, Flat_loss 0.22, Train_acc 73.14, Test_acc 44.71
2025-01-05 17:31:59,137 [podnet.py] => Task 15, Epoch 113/160 (LR 0.01982) => LSC_loss 0.96, Spatial_loss 1.71, Flat_loss 0.22, Train_acc 73.58, Test_acc 40.92
2025-01-05 17:32:07,453 [podnet.py] => Task 15, Epoch 114/160 (LR 0.01905) => LSC_loss 0.97, Spatial_loss 1.76, Flat_loss 0.22, Train_acc 73.07, Test_acc 43.75
2025-01-05 17:32:15,463 [podnet.py] => Task 15, Epoch 115/160 (LR 0.01828) => LSC_loss 0.94, Spatial_loss 1.69, Flat_loss 0.21, Train_acc 74.32, Test_acc 43.86
2025-01-05 17:32:23,647 [podnet.py] => Task 15, Epoch 116/160 (LR 0.01753) => LSC_loss 0.93, Spatial_loss 1.66, Flat_loss 0.20, Train_acc 74.37, Test_acc 43.64
2025-01-05 17:32:31,791 [podnet.py] => Task 15, Epoch 117/160 (LR 0.01679) => LSC_loss 0.92, Spatial_loss 1.67, Flat_loss 0.20, Train_acc 75.11, Test_acc 44.48
2025-01-05 17:32:40,167 [podnet.py] => Task 15, Epoch 118/160 (LR 0.01606) => LSC_loss 0.90, Spatial_loss 1.61, Flat_loss 0.19, Train_acc 75.73, Test_acc 45.22
2025-01-05 17:32:48,452 [podnet.py] => Task 15, Epoch 119/160 (LR 0.01535) => LSC_loss 0.89, Spatial_loss 1.60, Flat_loss 0.19, Train_acc 76.11, Test_acc 43.39
2025-01-05 17:32:56,916 [podnet.py] => Task 15, Epoch 120/160 (LR 0.01464) => LSC_loss 0.88, Spatial_loss 1.55, Flat_loss 0.18, Train_acc 76.32, Test_acc 42.48
2025-01-05 17:33:05,453 [podnet.py] => Task 15, Epoch 121/160 (LR 0.01396) => LSC_loss 0.87, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 76.41, Test_acc 44.98
2025-01-05 17:33:13,549 [podnet.py] => Task 15, Epoch 122/160 (LR 0.01328) => LSC_loss 0.85, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 77.36, Test_acc 43.66
2025-01-05 17:33:21,664 [podnet.py] => Task 15, Epoch 123/160 (LR 0.01262) => LSC_loss 0.85, Spatial_loss 1.50, Flat_loss 0.17, Train_acc 77.31, Test_acc 43.71
2025-01-05 17:33:29,974 [podnet.py] => Task 15, Epoch 124/160 (LR 0.01198) => LSC_loss 0.85, Spatial_loss 1.48, Flat_loss 0.17, Train_acc 76.72, Test_acc 45.41
2025-01-05 17:33:38,253 [podnet.py] => Task 15, Epoch 125/160 (LR 0.01135) => LSC_loss 0.83, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 77.75, Test_acc 45.26
2025-01-05 17:33:46,782 [podnet.py] => Task 15, Epoch 126/160 (LR 0.01073) => LSC_loss 0.82, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 78.21, Test_acc 45.68
2025-01-05 17:33:55,046 [podnet.py] => Task 15, Epoch 127/160 (LR 0.01013) => LSC_loss 0.80, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 78.57, Test_acc 44.94
2025-01-05 17:34:03,321 [podnet.py] => Task 15, Epoch 128/160 (LR 0.00955) => LSC_loss 0.79, Spatial_loss 1.43, Flat_loss 0.15, Train_acc 78.84, Test_acc 46.08
2025-01-05 17:34:11,532 [podnet.py] => Task 15, Epoch 129/160 (LR 0.00898) => LSC_loss 0.78, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 79.15, Test_acc 45.65
2025-01-05 17:34:19,766 [podnet.py] => Task 15, Epoch 130/160 (LR 0.00843) => LSC_loss 0.77, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 80.03, Test_acc 46.58
2025-01-05 17:34:27,904 [podnet.py] => Task 15, Epoch 131/160 (LR 0.00789) => LSC_loss 0.76, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 79.62, Test_acc 46.56
2025-01-05 17:34:36,347 [podnet.py] => Task 15, Epoch 132/160 (LR 0.00737) => LSC_loss 0.75, Spatial_loss 1.35, Flat_loss 0.13, Train_acc 79.93, Test_acc 45.22
2025-01-05 17:34:44,812 [podnet.py] => Task 15, Epoch 133/160 (LR 0.00686) => LSC_loss 0.75, Spatial_loss 1.32, Flat_loss 0.13, Train_acc 80.72, Test_acc 45.81
2025-01-05 17:34:53,066 [podnet.py] => Task 15, Epoch 134/160 (LR 0.00638) => LSC_loss 0.73, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 80.91, Test_acc 46.39
2025-01-05 17:35:01,219 [podnet.py] => Task 15, Epoch 135/160 (LR 0.00590) => LSC_loss 0.73, Spatial_loss 1.32, Flat_loss 0.13, Train_acc 80.55, Test_acc 46.48
2025-01-05 17:35:09,801 [podnet.py] => Task 15, Epoch 136/160 (LR 0.00545) => LSC_loss 0.73, Spatial_loss 1.27, Flat_loss 0.12, Train_acc 81.02, Test_acc 46.18
2025-01-05 17:35:18,179 [podnet.py] => Task 15, Epoch 137/160 (LR 0.00501) => LSC_loss 0.71, Spatial_loss 1.25, Flat_loss 0.12, Train_acc 81.86, Test_acc 46.65
2025-01-05 17:35:26,398 [podnet.py] => Task 15, Epoch 138/160 (LR 0.00459) => LSC_loss 0.71, Spatial_loss 1.24, Flat_loss 0.12, Train_acc 81.31, Test_acc 46.58
2025-01-05 17:35:34,599 [podnet.py] => Task 15, Epoch 139/160 (LR 0.00419) => LSC_loss 0.69, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 82.21, Test_acc 47.14
2025-01-05 17:35:42,843 [podnet.py] => Task 15, Epoch 140/160 (LR 0.00381) => LSC_loss 0.70, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 82.10, Test_acc 46.79
2025-01-05 17:35:51,230 [podnet.py] => Task 15, Epoch 141/160 (LR 0.00344) => LSC_loss 0.69, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 82.23, Test_acc 47.31
2025-01-05 17:35:59,431 [podnet.py] => Task 15, Epoch 142/160 (LR 0.00309) => LSC_loss 0.69, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 82.37, Test_acc 47.42
2025-01-05 17:36:07,996 [podnet.py] => Task 15, Epoch 143/160 (LR 0.00276) => LSC_loss 0.68, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 82.73, Test_acc 47.12
2025-01-05 17:36:16,427 [podnet.py] => Task 15, Epoch 144/160 (LR 0.00245) => LSC_loss 0.68, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 82.36, Test_acc 46.78
2025-01-05 17:36:24,653 [podnet.py] => Task 15, Epoch 145/160 (LR 0.00215) => LSC_loss 0.68, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 82.72, Test_acc 47.29
2025-01-05 17:36:33,121 [podnet.py] => Task 15, Epoch 146/160 (LR 0.00188) => LSC_loss 0.67, Spatial_loss 1.11, Flat_loss 0.10, Train_acc 82.73, Test_acc 47.36
2025-01-05 17:36:41,387 [podnet.py] => Task 15, Epoch 147/160 (LR 0.00162) => LSC_loss 0.66, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 83.13, Test_acc 47.54
2025-01-05 17:36:49,621 [podnet.py] => Task 15, Epoch 148/160 (LR 0.00138) => LSC_loss 0.66, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 83.11, Test_acc 47.34
2025-01-05 17:36:57,886 [podnet.py] => Task 15, Epoch 149/160 (LR 0.00116) => LSC_loss 0.65, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 83.40, Test_acc 47.31
2025-01-05 17:37:06,019 [podnet.py] => Task 15, Epoch 150/160 (LR 0.00096) => LSC_loss 0.65, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 83.42, Test_acc 47.35
2025-01-05 17:37:14,426 [podnet.py] => Task 15, Epoch 151/160 (LR 0.00078) => LSC_loss 0.65, Spatial_loss 1.06, Flat_loss 0.09, Train_acc 83.33, Test_acc 47.65
2025-01-05 17:37:22,810 [podnet.py] => Task 15, Epoch 152/160 (LR 0.00062) => LSC_loss 0.64, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 83.73, Test_acc 47.16
2025-01-05 17:37:30,935 [podnet.py] => Task 15, Epoch 153/160 (LR 0.00047) => LSC_loss 0.65, Spatial_loss 1.07, Flat_loss 0.09, Train_acc 83.42, Test_acc 47.66
2025-01-05 17:37:39,048 [podnet.py] => Task 15, Epoch 154/160 (LR 0.00035) => LSC_loss 0.65, Spatial_loss 1.04, Flat_loss 0.09, Train_acc 83.60, Test_acc 47.60
2025-01-05 17:37:47,358 [podnet.py] => Task 15, Epoch 155/160 (LR 0.00024) => LSC_loss 0.65, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 83.59, Test_acc 47.28
2025-01-05 17:37:55,648 [podnet.py] => Task 15, Epoch 156/160 (LR 0.00015) => LSC_loss 0.63, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 84.10, Test_acc 47.60
2025-01-05 17:38:03,782 [podnet.py] => Task 15, Epoch 157/160 (LR 0.00009) => LSC_loss 0.64, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 83.96, Test_acc 47.82
2025-01-05 17:38:12,130 [podnet.py] => Task 15, Epoch 158/160 (LR 0.00004) => LSC_loss 0.65, Spatial_loss 1.05, Flat_loss 0.09, Train_acc 83.59, Test_acc 47.69
2025-01-05 17:38:20,435 [podnet.py] => Task 15, Epoch 159/160 (LR 0.00001) => LSC_loss 0.64, Spatial_loss 1.01, Flat_loss 0.09, Train_acc 83.64, Test_acc 47.68
2025-01-05 17:38:28,818 [podnet.py] => Task 15, Epoch 160/160 (LR 0.00000) => LSC_loss 0.64, Spatial_loss 1.03, Flat_loss 0.09, Train_acc 83.92, Test_acc 47.58
2025-01-05 17:38:28,819 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 17:38:28,819 [base.py] => Reducing exemplars...(179 per classes)
2025-01-05 17:39:31,745 [base.py] => Constructing exemplars...(179 per classes)
2025-01-05 17:39:40,257 [podnet.py] => The size of finetune dataset: 14320
2025-01-05 17:39:47,978 [podnet.py] => Task 15, Epoch 1/20 (LR 0.00497) => LSC_loss 0.62, Spatial_loss 1.16, Flat_loss 0.09, Train_acc 84.38, Test_acc 45.86
2025-01-05 17:39:55,606 [podnet.py] => Task 15, Epoch 2/20 (LR 0.00488) => LSC_loss 0.63, Spatial_loss 1.20, Flat_loss 0.10, Train_acc 84.16, Test_acc 47.40
2025-01-05 17:40:03,461 [podnet.py] => Task 15, Epoch 3/20 (LR 0.00473) => LSC_loss 0.63, Spatial_loss 1.19, Flat_loss 0.09, Train_acc 84.01, Test_acc 47.60
2025-01-05 17:40:11,236 [podnet.py] => Task 15, Epoch 4/20 (LR 0.00452) => LSC_loss 0.62, Spatial_loss 1.18, Flat_loss 0.09, Train_acc 84.30, Test_acc 47.06
2025-01-05 17:40:19,122 [podnet.py] => Task 15, Epoch 5/20 (LR 0.00427) => LSC_loss 0.61, Spatial_loss 1.17, Flat_loss 0.09, Train_acc 84.57, Test_acc 46.99
2025-01-05 17:40:26,659 [podnet.py] => Task 15, Epoch 6/20 (LR 0.00397) => LSC_loss 0.62, Spatial_loss 1.19, Flat_loss 0.09, Train_acc 84.26, Test_acc 46.74
2025-01-05 17:40:34,313 [podnet.py] => Task 15, Epoch 7/20 (LR 0.00363) => LSC_loss 0.61, Spatial_loss 1.16, Flat_loss 0.09, Train_acc 84.85, Test_acc 47.44
2025-01-05 17:40:42,141 [podnet.py] => Task 15, Epoch 8/20 (LR 0.00327) => LSC_loss 0.60, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 85.59, Test_acc 47.55
2025-01-05 17:40:49,858 [podnet.py] => Task 15, Epoch 9/20 (LR 0.00289) => LSC_loss 0.60, Spatial_loss 1.10, Flat_loss 0.08, Train_acc 84.83, Test_acc 47.42
2025-01-05 17:40:57,374 [podnet.py] => Task 15, Epoch 10/20 (LR 0.00250) => LSC_loss 0.59, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 85.40, Test_acc 47.49
2025-01-05 17:41:05,184 [podnet.py] => Task 15, Epoch 11/20 (LR 0.00211) => LSC_loss 0.59, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.39, Test_acc 47.60
2025-01-05 17:41:13,211 [podnet.py] => Task 15, Epoch 12/20 (LR 0.00173) => LSC_loss 0.59, Spatial_loss 1.06, Flat_loss 0.08, Train_acc 85.36, Test_acc 47.35
2025-01-05 17:41:21,070 [podnet.py] => Task 15, Epoch 13/20 (LR 0.00137) => LSC_loss 0.58, Spatial_loss 1.06, Flat_loss 0.08, Train_acc 85.71, Test_acc 47.56
2025-01-05 17:41:28,719 [podnet.py] => Task 15, Epoch 14/20 (LR 0.00103) => LSC_loss 0.58, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 85.83, Test_acc 47.65
2025-01-05 17:41:36,162 [podnet.py] => Task 15, Epoch 15/20 (LR 0.00073) => LSC_loss 0.58, Spatial_loss 1.02, Flat_loss 0.07, Train_acc 86.10, Test_acc 47.59
2025-01-05 17:41:43,715 [podnet.py] => Task 15, Epoch 16/20 (LR 0.00048) => LSC_loss 0.57, Spatial_loss 1.01, Flat_loss 0.07, Train_acc 86.23, Test_acc 47.65
2025-01-05 17:41:51,445 [podnet.py] => Task 15, Epoch 17/20 (LR 0.00027) => LSC_loss 0.57, Spatial_loss 1.02, Flat_loss 0.07, Train_acc 86.48, Test_acc 47.61
2025-01-05 17:41:59,186 [podnet.py] => Task 15, Epoch 18/20 (LR 0.00012) => LSC_loss 0.57, Spatial_loss 1.00, Flat_loss 0.07, Train_acc 86.26, Test_acc 47.65
2025-01-05 17:42:06,864 [podnet.py] => Task 15, Epoch 19/20 (LR 0.00003) => LSC_loss 0.56, Spatial_loss 1.00, Flat_loss 0.07, Train_acc 86.33, Test_acc 47.66
2025-01-05 17:42:14,722 [podnet.py] => Task 15, Epoch 20/20 (LR 0.00000) => LSC_loss 0.56, Spatial_loss 0.99, Flat_loss 0.07, Train_acc 86.36, Test_acc 47.70
2025-01-05 17:42:14,725 [base.py] => Reducing exemplars...(168 per classes)
2025-01-05 17:43:17,883 [base.py] => Constructing exemplars...(168 per classes)
2025-01-05 17:43:28,776 [podnet.py] => Exemplar size: 13440
2025-01-05 17:43:28,776 [trainer.py] => CNN: {'total': np.float64(47.7), '00-09': np.float64(59.5), '10-19': np.float64(38.4), '20-29': np.float64(54.1), '30-39': np.float64(45.3), '40-49': np.float64(55.6), '50-59': np.float64(41.7), '60-69': np.float64(50.4), '70-79': np.float64(36.6), 'old': np.float64(48.75), 'new': np.float64(32.0)}
2025-01-05 17:43:28,776 [trainer.py] => NME: {'total': np.float64(46.44), '00-09': np.float64(61.3), '10-19': np.float64(37.5), '20-29': np.float64(54.1), '30-39': np.float64(45.6), '40-49': np.float64(52.1), '50-59': np.float64(38.3), '60-69': np.float64(46.0), '70-79': np.float64(36.6), 'old': np.float64(47.29), 'new': np.float64(33.6)}
2025-01-05 17:43:28,776 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96), np.float64(49.84), np.float64(47.7)]
2025-01-05 17:43:28,776 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43), np.float64(77.19), np.float64(76.28)]
2025-01-05 17:43:28,776 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96), np.float64(48.09), np.float64(46.44)]
2025-01-05 17:43:28,776 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2), np.float64(75.59), np.float64(75.25)]

2025-01-05 17:43:28,776 [trainer.py] => All params: 517457
2025-01-05 17:43:28,777 [trainer.py] => Trainable params: 517457
2025-01-05 17:43:28,777 [podnet.py] => Learning on 80-85
2025-01-05 17:43:28,828 [podnet.py] => Adaptive factor: 4.123105625617661
2025-01-05 17:43:37,034 [podnet.py] => Task 16, Epoch 1/160 (LR 0.09999) => LSC_loss 2.61, Spatial_loss 3.81, Flat_loss 0.96, Train_acc 37.56, Test_acc 29.99
2025-01-05 17:43:45,377 [podnet.py] => Task 16, Epoch 2/160 (LR 0.09996) => LSC_loss 2.07, Spatial_loss 3.26, Flat_loss 0.69, Train_acc 45.98, Test_acc 35.14
2025-01-05 17:43:53,597 [podnet.py] => Task 16, Epoch 3/160 (LR 0.09991) => LSC_loss 1.96, Spatial_loss 3.10, Flat_loss 0.63, Train_acc 48.09, Test_acc 30.51
2025-01-05 17:44:01,833 [podnet.py] => Task 16, Epoch 4/160 (LR 0.09985) => LSC_loss 1.91, Spatial_loss 3.02, Flat_loss 0.60, Train_acc 49.17, Test_acc 32.20
2025-01-05 17:44:10,276 [podnet.py] => Task 16, Epoch 5/160 (LR 0.09976) => LSC_loss 1.86, Spatial_loss 3.00, Flat_loss 0.59, Train_acc 50.75, Test_acc 33.24
2025-01-05 17:44:18,499 [podnet.py] => Task 16, Epoch 6/160 (LR 0.09965) => LSC_loss 1.86, Spatial_loss 2.89, Flat_loss 0.57, Train_acc 50.66, Test_acc 27.79
2025-01-05 17:44:27,126 [podnet.py] => Task 16, Epoch 7/160 (LR 0.09953) => LSC_loss 1.81, Spatial_loss 2.90, Flat_loss 0.56, Train_acc 52.20, Test_acc 33.12
2025-01-05 17:44:35,408 [podnet.py] => Task 16, Epoch 8/160 (LR 0.09938) => LSC_loss 1.81, Spatial_loss 2.83, Flat_loss 0.55, Train_acc 52.23, Test_acc 36.05
2025-01-05 17:44:43,780 [podnet.py] => Task 16, Epoch 9/160 (LR 0.09922) => LSC_loss 1.78, Spatial_loss 2.82, Flat_loss 0.54, Train_acc 52.92, Test_acc 33.84
2025-01-05 17:44:52,237 [podnet.py] => Task 16, Epoch 10/160 (LR 0.09904) => LSC_loss 1.77, Spatial_loss 2.89, Flat_loss 0.54, Train_acc 52.65, Test_acc 31.49
2025-01-05 17:45:00,402 [podnet.py] => Task 16, Epoch 11/160 (LR 0.09884) => LSC_loss 1.76, Spatial_loss 2.80, Flat_loss 0.53, Train_acc 52.72, Test_acc 35.44
2025-01-05 17:45:08,749 [podnet.py] => Task 16, Epoch 12/160 (LR 0.09862) => LSC_loss 1.75, Spatial_loss 2.88, Flat_loss 0.54, Train_acc 53.16, Test_acc 31.14
2025-01-05 17:45:16,875 [podnet.py] => Task 16, Epoch 13/160 (LR 0.09838) => LSC_loss 1.74, Spatial_loss 2.83, Flat_loss 0.54, Train_acc 53.42, Test_acc 34.06
2025-01-05 17:45:25,353 [podnet.py] => Task 16, Epoch 14/160 (LR 0.09812) => LSC_loss 1.74, Spatial_loss 2.83, Flat_loss 0.53, Train_acc 53.72, Test_acc 35.44
2025-01-05 17:45:33,761 [podnet.py] => Task 16, Epoch 15/160 (LR 0.09785) => LSC_loss 1.72, Spatial_loss 2.81, Flat_loss 0.53, Train_acc 54.29, Test_acc 32.68
2025-01-05 17:45:41,987 [podnet.py] => Task 16, Epoch 16/160 (LR 0.09755) => LSC_loss 1.72, Spatial_loss 2.81, Flat_loss 0.53, Train_acc 54.18, Test_acc 34.15
2025-01-05 17:45:50,411 [podnet.py] => Task 16, Epoch 17/160 (LR 0.09724) => LSC_loss 1.71, Spatial_loss 2.76, Flat_loss 0.52, Train_acc 54.08, Test_acc 36.26
2025-01-05 17:45:59,037 [podnet.py] => Task 16, Epoch 18/160 (LR 0.09691) => LSC_loss 1.71, Spatial_loss 2.76, Flat_loss 0.53, Train_acc 53.83, Test_acc 36.42
2025-01-05 17:46:07,466 [podnet.py] => Task 16, Epoch 19/160 (LR 0.09656) => LSC_loss 1.70, Spatial_loss 2.79, Flat_loss 0.52, Train_acc 54.74, Test_acc 37.92
2025-01-05 17:46:15,783 [podnet.py] => Task 16, Epoch 20/160 (LR 0.09619) => LSC_loss 1.68, Spatial_loss 2.74, Flat_loss 0.51, Train_acc 55.26, Test_acc 34.71
2025-01-05 17:46:23,988 [podnet.py] => Task 16, Epoch 21/160 (LR 0.09581) => LSC_loss 1.68, Spatial_loss 2.76, Flat_loss 0.52, Train_acc 54.79, Test_acc 36.72
2025-01-05 17:46:32,254 [podnet.py] => Task 16, Epoch 22/160 (LR 0.09541) => LSC_loss 1.70, Spatial_loss 2.76, Flat_loss 0.51, Train_acc 54.43, Test_acc 33.55
2025-01-05 17:46:40,738 [podnet.py] => Task 16, Epoch 23/160 (LR 0.09499) => LSC_loss 1.69, Spatial_loss 2.78, Flat_loss 0.52, Train_acc 54.86, Test_acc 37.60
2025-01-05 17:46:48,966 [podnet.py] => Task 16, Epoch 24/160 (LR 0.09455) => LSC_loss 1.69, Spatial_loss 2.76, Flat_loss 0.52, Train_acc 55.16, Test_acc 33.20
2025-01-05 17:46:57,359 [podnet.py] => Task 16, Epoch 25/160 (LR 0.09410) => LSC_loss 1.67, Spatial_loss 2.77, Flat_loss 0.51, Train_acc 55.13, Test_acc 36.92
2025-01-05 17:47:05,836 [podnet.py] => Task 16, Epoch 26/160 (LR 0.09362) => LSC_loss 1.67, Spatial_loss 2.71, Flat_loss 0.51, Train_acc 55.38, Test_acc 34.56
2025-01-05 17:47:14,007 [podnet.py] => Task 16, Epoch 27/160 (LR 0.09314) => LSC_loss 1.69, Spatial_loss 2.77, Flat_loss 0.52, Train_acc 54.52, Test_acc 31.85
2025-01-05 17:47:22,385 [podnet.py] => Task 16, Epoch 28/160 (LR 0.09263) => LSC_loss 1.65, Spatial_loss 2.73, Flat_loss 0.51, Train_acc 55.43, Test_acc 38.64
2025-01-05 17:47:30,816 [podnet.py] => Task 16, Epoch 29/160 (LR 0.09211) => LSC_loss 1.65, Spatial_loss 2.79, Flat_loss 0.51, Train_acc 55.61, Test_acc 34.47
2025-01-05 17:47:39,156 [podnet.py] => Task 16, Epoch 30/160 (LR 0.09157) => LSC_loss 1.65, Spatial_loss 2.73, Flat_loss 0.50, Train_acc 55.60, Test_acc 36.08
2025-01-05 17:47:47,453 [podnet.py] => Task 16, Epoch 31/160 (LR 0.09102) => LSC_loss 1.65, Spatial_loss 2.75, Flat_loss 0.51, Train_acc 55.89, Test_acc 38.11
2025-01-05 17:47:55,878 [podnet.py] => Task 16, Epoch 32/160 (LR 0.09045) => LSC_loss 1.63, Spatial_loss 2.72, Flat_loss 0.50, Train_acc 56.47, Test_acc 36.64
2025-01-05 17:48:04,283 [podnet.py] => Task 16, Epoch 33/160 (LR 0.08987) => LSC_loss 1.65, Spatial_loss 2.73, Flat_loss 0.51, Train_acc 55.55, Test_acc 36.98
2025-01-05 17:48:12,922 [podnet.py] => Task 16, Epoch 34/160 (LR 0.08927) => LSC_loss 1.63, Spatial_loss 2.70, Flat_loss 0.50, Train_acc 56.46, Test_acc 33.59
2025-01-05 17:48:21,297 [podnet.py] => Task 16, Epoch 35/160 (LR 0.08865) => LSC_loss 1.64, Spatial_loss 2.73, Flat_loss 0.50, Train_acc 55.95, Test_acc 32.29
2025-01-05 17:48:29,673 [podnet.py] => Task 16, Epoch 36/160 (LR 0.08802) => LSC_loss 1.63, Spatial_loss 2.66, Flat_loss 0.49, Train_acc 56.14, Test_acc 34.49
2025-01-05 17:48:37,973 [podnet.py] => Task 16, Epoch 37/160 (LR 0.08738) => LSC_loss 1.64, Spatial_loss 2.68, Flat_loss 0.50, Train_acc 55.64, Test_acc 31.42
2025-01-05 17:48:46,191 [podnet.py] => Task 16, Epoch 38/160 (LR 0.08672) => LSC_loss 1.64, Spatial_loss 2.72, Flat_loss 0.50, Train_acc 55.89, Test_acc 37.09
2025-01-05 17:48:54,589 [podnet.py] => Task 16, Epoch 39/160 (LR 0.08604) => LSC_loss 1.62, Spatial_loss 2.76, Flat_loss 0.50, Train_acc 56.56, Test_acc 38.80
2025-01-05 17:49:02,746 [podnet.py] => Task 16, Epoch 40/160 (LR 0.08536) => LSC_loss 1.62, Spatial_loss 2.66, Flat_loss 0.49, Train_acc 56.21, Test_acc 35.42
2025-01-05 17:49:11,152 [podnet.py] => Task 16, Epoch 41/160 (LR 0.08465) => LSC_loss 1.60, Spatial_loss 2.71, Flat_loss 0.49, Train_acc 57.55, Test_acc 33.55
2025-01-05 17:49:19,562 [podnet.py] => Task 16, Epoch 42/160 (LR 0.08394) => LSC_loss 1.60, Spatial_loss 2.70, Flat_loss 0.49, Train_acc 56.80, Test_acc 36.35
2025-01-05 17:49:28,017 [podnet.py] => Task 16, Epoch 43/160 (LR 0.08321) => LSC_loss 1.60, Spatial_loss 2.68, Flat_loss 0.48, Train_acc 57.37, Test_acc 32.58
2025-01-05 17:49:36,323 [podnet.py] => Task 16, Epoch 44/160 (LR 0.08247) => LSC_loss 1.59, Spatial_loss 2.69, Flat_loss 0.49, Train_acc 57.43, Test_acc 37.96
2025-01-05 17:49:44,661 [podnet.py] => Task 16, Epoch 45/160 (LR 0.08172) => LSC_loss 1.60, Spatial_loss 2.70, Flat_loss 0.49, Train_acc 56.83, Test_acc 36.19
2025-01-05 17:49:53,157 [podnet.py] => Task 16, Epoch 46/160 (LR 0.08095) => LSC_loss 1.59, Spatial_loss 2.62, Flat_loss 0.48, Train_acc 57.11, Test_acc 36.51
2025-01-05 17:50:01,653 [podnet.py] => Task 16, Epoch 47/160 (LR 0.08018) => LSC_loss 1.59, Spatial_loss 2.64, Flat_loss 0.48, Train_acc 56.88, Test_acc 34.53
2025-01-05 17:50:09,951 [podnet.py] => Task 16, Epoch 48/160 (LR 0.07939) => LSC_loss 1.58, Spatial_loss 2.66, Flat_loss 0.48, Train_acc 57.58, Test_acc 39.73
2025-01-05 17:50:18,467 [podnet.py] => Task 16, Epoch 49/160 (LR 0.07859) => LSC_loss 1.57, Spatial_loss 2.59, Flat_loss 0.47, Train_acc 57.57, Test_acc 36.79
2025-01-05 17:50:26,787 [podnet.py] => Task 16, Epoch 50/160 (LR 0.07778) => LSC_loss 1.57, Spatial_loss 2.64, Flat_loss 0.47, Train_acc 57.41, Test_acc 37.42
2025-01-05 17:50:35,239 [podnet.py] => Task 16, Epoch 51/160 (LR 0.07696) => LSC_loss 1.55, Spatial_loss 2.65, Flat_loss 0.47, Train_acc 58.12, Test_acc 35.93
2025-01-05 17:50:43,515 [podnet.py] => Task 16, Epoch 52/160 (LR 0.07612) => LSC_loss 1.55, Spatial_loss 2.58, Flat_loss 0.47, Train_acc 57.98, Test_acc 36.81
2025-01-05 17:50:51,822 [podnet.py] => Task 16, Epoch 53/160 (LR 0.07528) => LSC_loss 1.54, Spatial_loss 2.57, Flat_loss 0.46, Train_acc 58.91, Test_acc 37.20
2025-01-05 17:51:00,284 [podnet.py] => Task 16, Epoch 54/160 (LR 0.07443) => LSC_loss 1.54, Spatial_loss 2.61, Flat_loss 0.47, Train_acc 58.40, Test_acc 39.27
2025-01-05 17:51:08,735 [podnet.py] => Task 16, Epoch 55/160 (LR 0.07357) => LSC_loss 1.54, Spatial_loss 2.62, Flat_loss 0.47, Train_acc 58.63, Test_acc 38.08
2025-01-05 17:51:16,987 [podnet.py] => Task 16, Epoch 56/160 (LR 0.07270) => LSC_loss 1.53, Spatial_loss 2.58, Flat_loss 0.45, Train_acc 58.64, Test_acc 35.53
2025-01-05 17:51:25,490 [podnet.py] => Task 16, Epoch 57/160 (LR 0.07182) => LSC_loss 1.53, Spatial_loss 2.58, Flat_loss 0.46, Train_acc 58.45, Test_acc 36.09
2025-01-05 17:51:33,833 [podnet.py] => Task 16, Epoch 58/160 (LR 0.07093) => LSC_loss 1.51, Spatial_loss 2.57, Flat_loss 0.45, Train_acc 58.86, Test_acc 38.49
2025-01-05 17:51:42,169 [podnet.py] => Task 16, Epoch 59/160 (LR 0.07004) => LSC_loss 1.53, Spatial_loss 2.57, Flat_loss 0.46, Train_acc 58.56, Test_acc 38.49
2025-01-05 17:51:50,468 [podnet.py] => Task 16, Epoch 60/160 (LR 0.06913) => LSC_loss 1.49, Spatial_loss 2.52, Flat_loss 0.44, Train_acc 59.65, Test_acc 37.66
2025-01-05 17:51:58,731 [podnet.py] => Task 16, Epoch 61/160 (LR 0.06822) => LSC_loss 1.51, Spatial_loss 2.56, Flat_loss 0.45, Train_acc 58.63, Test_acc 35.31
2025-01-05 17:52:07,067 [podnet.py] => Task 16, Epoch 62/160 (LR 0.06731) => LSC_loss 1.49, Spatial_loss 2.51, Flat_loss 0.44, Train_acc 59.41, Test_acc 30.80
2025-01-05 17:52:15,512 [podnet.py] => Task 16, Epoch 63/160 (LR 0.06638) => LSC_loss 1.48, Spatial_loss 2.53, Flat_loss 0.44, Train_acc 59.91, Test_acc 37.13
2025-01-05 17:52:23,605 [podnet.py] => Task 16, Epoch 64/160 (LR 0.06545) => LSC_loss 1.50, Spatial_loss 2.55, Flat_loss 0.44, Train_acc 59.00, Test_acc 38.38
2025-01-05 17:52:31,937 [podnet.py] => Task 16, Epoch 65/160 (LR 0.06451) => LSC_loss 1.47, Spatial_loss 2.49, Flat_loss 0.44, Train_acc 60.53, Test_acc 38.28
2025-01-05 17:52:40,274 [podnet.py] => Task 16, Epoch 66/160 (LR 0.06357) => LSC_loss 1.45, Spatial_loss 2.49, Flat_loss 0.43, Train_acc 60.70, Test_acc 35.00
2025-01-05 17:52:48,687 [podnet.py] => Task 16, Epoch 67/160 (LR 0.06262) => LSC_loss 1.46, Spatial_loss 2.48, Flat_loss 0.43, Train_acc 60.38, Test_acc 36.74
2025-01-05 17:52:57,018 [podnet.py] => Task 16, Epoch 68/160 (LR 0.06167) => LSC_loss 1.45, Spatial_loss 2.45, Flat_loss 0.42, Train_acc 60.64, Test_acc 38.07
2025-01-05 17:53:05,649 [podnet.py] => Task 16, Epoch 69/160 (LR 0.06072) => LSC_loss 1.43, Spatial_loss 2.45, Flat_loss 0.42, Train_acc 60.62, Test_acc 31.69
2025-01-05 17:53:14,098 [podnet.py] => Task 16, Epoch 70/160 (LR 0.05975) => LSC_loss 1.44, Spatial_loss 2.46, Flat_loss 0.43, Train_acc 60.93, Test_acc 38.91
2025-01-05 17:53:22,443 [podnet.py] => Task 16, Epoch 71/160 (LR 0.05879) => LSC_loss 1.44, Spatial_loss 2.48, Flat_loss 0.42, Train_acc 60.72, Test_acc 35.98
2025-01-05 17:53:30,751 [podnet.py] => Task 16, Epoch 72/160 (LR 0.05782) => LSC_loss 1.43, Spatial_loss 2.45, Flat_loss 0.42, Train_acc 61.25, Test_acc 37.40
2025-01-05 17:53:38,974 [podnet.py] => Task 16, Epoch 73/160 (LR 0.05685) => LSC_loss 1.41, Spatial_loss 2.40, Flat_loss 0.41, Train_acc 61.36, Test_acc 34.58
2025-01-05 17:53:47,254 [podnet.py] => Task 16, Epoch 74/160 (LR 0.05588) => LSC_loss 1.39, Spatial_loss 2.39, Flat_loss 0.40, Train_acc 62.26, Test_acc 36.42
2025-01-05 17:53:55,602 [podnet.py] => Task 16, Epoch 75/160 (LR 0.05490) => LSC_loss 1.40, Spatial_loss 2.39, Flat_loss 0.40, Train_acc 61.90, Test_acc 37.61
2025-01-05 17:54:04,005 [podnet.py] => Task 16, Epoch 76/160 (LR 0.05392) => LSC_loss 1.38, Spatial_loss 2.36, Flat_loss 0.40, Train_acc 62.35, Test_acc 39.44
2025-01-05 17:54:12,548 [podnet.py] => Task 16, Epoch 77/160 (LR 0.05294) => LSC_loss 1.37, Spatial_loss 2.31, Flat_loss 0.39, Train_acc 62.93, Test_acc 36.05
2025-01-05 17:54:20,866 [podnet.py] => Task 16, Epoch 78/160 (LR 0.05196) => LSC_loss 1.37, Spatial_loss 2.38, Flat_loss 0.39, Train_acc 62.58, Test_acc 37.21
2025-01-05 17:54:29,346 [podnet.py] => Task 16, Epoch 79/160 (LR 0.05098) => LSC_loss 1.36, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 62.53, Test_acc 39.78
2025-01-05 17:54:37,626 [podnet.py] => Task 16, Epoch 80/160 (LR 0.05000) => LSC_loss 1.33, Spatial_loss 2.33, Flat_loss 0.38, Train_acc 63.81, Test_acc 38.24
2025-01-05 17:54:45,955 [podnet.py] => Task 16, Epoch 81/160 (LR 0.04902) => LSC_loss 1.32, Spatial_loss 2.30, Flat_loss 0.37, Train_acc 64.06, Test_acc 40.64
2025-01-05 17:54:54,312 [podnet.py] => Task 16, Epoch 82/160 (LR 0.04804) => LSC_loss 1.32, Spatial_loss 2.27, Flat_loss 0.37, Train_acc 63.70, Test_acc 39.59
2025-01-05 17:55:02,791 [podnet.py] => Task 16, Epoch 83/160 (LR 0.04706) => LSC_loss 1.35, Spatial_loss 2.36, Flat_loss 0.39, Train_acc 62.96, Test_acc 36.52
2025-01-05 17:55:11,333 [podnet.py] => Task 16, Epoch 84/160 (LR 0.04608) => LSC_loss 1.33, Spatial_loss 2.34, Flat_loss 0.38, Train_acc 63.71, Test_acc 39.38
2025-01-05 17:55:19,860 [podnet.py] => Task 16, Epoch 85/160 (LR 0.04510) => LSC_loss 1.30, Spatial_loss 2.25, Flat_loss 0.37, Train_acc 64.08, Test_acc 38.25
2025-01-05 17:55:28,330 [podnet.py] => Task 16, Epoch 86/160 (LR 0.04412) => LSC_loss 1.27, Spatial_loss 2.24, Flat_loss 0.36, Train_acc 64.99, Test_acc 37.98
2025-01-05 17:55:36,688 [podnet.py] => Task 16, Epoch 87/160 (LR 0.04315) => LSC_loss 1.27, Spatial_loss 2.23, Flat_loss 0.35, Train_acc 64.91, Test_acc 40.59
2025-01-05 17:55:45,043 [podnet.py] => Task 16, Epoch 88/160 (LR 0.04218) => LSC_loss 1.28, Spatial_loss 2.23, Flat_loss 0.36, Train_acc 64.65, Test_acc 36.94
2025-01-05 17:55:53,508 [podnet.py] => Task 16, Epoch 89/160 (LR 0.04121) => LSC_loss 1.29, Spatial_loss 2.20, Flat_loss 0.36, Train_acc 64.36, Test_acc 38.86
2025-01-05 17:56:01,745 [podnet.py] => Task 16, Epoch 90/160 (LR 0.04025) => LSC_loss 1.25, Spatial_loss 2.17, Flat_loss 0.34, Train_acc 65.46, Test_acc 38.75
2025-01-05 17:56:10,029 [podnet.py] => Task 16, Epoch 91/160 (LR 0.03928) => LSC_loss 1.23, Spatial_loss 2.15, Flat_loss 0.33, Train_acc 66.14, Test_acc 40.00
2025-01-05 17:56:18,382 [podnet.py] => Task 16, Epoch 92/160 (LR 0.03833) => LSC_loss 1.22, Spatial_loss 2.15, Flat_loss 0.33, Train_acc 65.88, Test_acc 40.73
2025-01-05 17:56:26,610 [podnet.py] => Task 16, Epoch 93/160 (LR 0.03738) => LSC_loss 1.21, Spatial_loss 2.17, Flat_loss 0.33, Train_acc 66.69, Test_acc 39.75
2025-01-05 17:56:35,105 [podnet.py] => Task 16, Epoch 94/160 (LR 0.03643) => LSC_loss 1.22, Spatial_loss 2.15, Flat_loss 0.33, Train_acc 66.47, Test_acc 38.88
2025-01-05 17:56:43,420 [podnet.py] => Task 16, Epoch 95/160 (LR 0.03549) => LSC_loss 1.18, Spatial_loss 2.06, Flat_loss 0.32, Train_acc 67.38, Test_acc 39.60
2025-01-05 17:56:51,968 [podnet.py] => Task 16, Epoch 96/160 (LR 0.03455) => LSC_loss 1.17, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 68.00, Test_acc 40.71
2025-01-05 17:57:00,291 [podnet.py] => Task 16, Epoch 97/160 (LR 0.03362) => LSC_loss 1.17, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 67.69, Test_acc 40.74
2025-01-05 17:57:08,748 [podnet.py] => Task 16, Epoch 98/160 (LR 0.03269) => LSC_loss 1.16, Spatial_loss 2.13, Flat_loss 0.31, Train_acc 67.74, Test_acc 41.12
2025-01-05 17:57:17,236 [podnet.py] => Task 16, Epoch 99/160 (LR 0.03178) => LSC_loss 1.14, Spatial_loss 2.06, Flat_loss 0.31, Train_acc 68.85, Test_acc 39.98
2025-01-05 17:57:25,544 [podnet.py] => Task 16, Epoch 100/160 (LR 0.03087) => LSC_loss 1.14, Spatial_loss 2.02, Flat_loss 0.30, Train_acc 68.71, Test_acc 42.18
2025-01-05 17:57:33,897 [podnet.py] => Task 16, Epoch 101/160 (LR 0.02996) => LSC_loss 1.12, Spatial_loss 2.01, Flat_loss 0.29, Train_acc 69.27, Test_acc 42.24
2025-01-05 17:57:42,033 [podnet.py] => Task 16, Epoch 102/160 (LR 0.02907) => LSC_loss 1.10, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 69.88, Test_acc 39.34
2025-01-05 17:57:50,157 [podnet.py] => Task 16, Epoch 103/160 (LR 0.02818) => LSC_loss 1.09, Spatial_loss 1.94, Flat_loss 0.28, Train_acc 69.96, Test_acc 41.05
2025-01-05 17:57:58,480 [podnet.py] => Task 16, Epoch 104/160 (LR 0.02730) => LSC_loss 1.09, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 69.89, Test_acc 41.65
2025-01-05 17:58:06,856 [podnet.py] => Task 16, Epoch 105/160 (LR 0.02643) => LSC_loss 1.05, Spatial_loss 1.89, Flat_loss 0.27, Train_acc 71.01, Test_acc 41.28
2025-01-05 17:58:15,160 [podnet.py] => Task 16, Epoch 106/160 (LR 0.02557) => LSC_loss 1.06, Spatial_loss 1.91, Flat_loss 0.27, Train_acc 70.92, Test_acc 42.99
2025-01-05 17:58:23,622 [podnet.py] => Task 16, Epoch 107/160 (LR 0.02472) => LSC_loss 1.04, Spatial_loss 1.89, Flat_loss 0.26, Train_acc 71.01, Test_acc 41.49
2025-01-05 17:58:31,803 [podnet.py] => Task 16, Epoch 108/160 (LR 0.02388) => LSC_loss 1.03, Spatial_loss 1.85, Flat_loss 0.26, Train_acc 72.01, Test_acc 41.32
2025-01-05 17:58:40,180 [podnet.py] => Task 16, Epoch 109/160 (LR 0.02304) => LSC_loss 1.01, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 71.90, Test_acc 38.21
2025-01-05 17:58:48,713 [podnet.py] => Task 16, Epoch 110/160 (LR 0.02222) => LSC_loss 1.02, Spatial_loss 1.85, Flat_loss 0.25, Train_acc 72.09, Test_acc 42.46
2025-01-05 17:58:57,096 [podnet.py] => Task 16, Epoch 111/160 (LR 0.02141) => LSC_loss 0.99, Spatial_loss 1.78, Flat_loss 0.24, Train_acc 72.54, Test_acc 43.86
2025-01-05 17:59:05,448 [podnet.py] => Task 16, Epoch 112/160 (LR 0.02061) => LSC_loss 0.97, Spatial_loss 1.77, Flat_loss 0.23, Train_acc 73.83, Test_acc 42.18
2025-01-05 17:59:13,769 [podnet.py] => Task 16, Epoch 113/160 (LR 0.01982) => LSC_loss 0.95, Spatial_loss 1.72, Flat_loss 0.22, Train_acc 74.09, Test_acc 42.68
2025-01-05 17:59:22,129 [podnet.py] => Task 16, Epoch 114/160 (LR 0.01905) => LSC_loss 0.95, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 73.75, Test_acc 43.48
2025-01-05 17:59:30,321 [podnet.py] => Task 16, Epoch 115/160 (LR 0.01828) => LSC_loss 0.94, Spatial_loss 1.74, Flat_loss 0.22, Train_acc 74.00, Test_acc 41.19
2025-01-05 17:59:38,619 [podnet.py] => Task 16, Epoch 116/160 (LR 0.01753) => LSC_loss 0.92, Spatial_loss 1.73, Flat_loss 0.22, Train_acc 74.80, Test_acc 43.56
2025-01-05 17:59:47,125 [podnet.py] => Task 16, Epoch 117/160 (LR 0.01679) => LSC_loss 0.91, Spatial_loss 1.67, Flat_loss 0.21, Train_acc 75.41, Test_acc 44.38
2025-01-05 17:59:55,347 [podnet.py] => Task 16, Epoch 118/160 (LR 0.01606) => LSC_loss 0.89, Spatial_loss 1.66, Flat_loss 0.21, Train_acc 75.86, Test_acc 43.92
2025-01-05 18:00:03,580 [podnet.py] => Task 16, Epoch 119/160 (LR 0.01535) => LSC_loss 0.88, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 75.90, Test_acc 43.62
2025-01-05 18:00:11,803 [podnet.py] => Task 16, Epoch 120/160 (LR 0.01464) => LSC_loss 0.87, Spatial_loss 1.63, Flat_loss 0.20, Train_acc 76.33, Test_acc 43.35
2025-01-05 18:00:20,114 [podnet.py] => Task 16, Epoch 121/160 (LR 0.01396) => LSC_loss 0.86, Spatial_loss 1.60, Flat_loss 0.19, Train_acc 76.88, Test_acc 44.34
2025-01-05 18:00:28,505 [podnet.py] => Task 16, Epoch 122/160 (LR 0.01328) => LSC_loss 0.84, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 77.67, Test_acc 43.20
2025-01-05 18:00:36,795 [podnet.py] => Task 16, Epoch 123/160 (LR 0.01262) => LSC_loss 0.85, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 77.03, Test_acc 44.39
2025-01-05 18:00:45,281 [podnet.py] => Task 16, Epoch 124/160 (LR 0.01198) => LSC_loss 0.83, Spatial_loss 1.53, Flat_loss 0.18, Train_acc 77.65, Test_acc 43.79
2025-01-05 18:00:53,521 [podnet.py] => Task 16, Epoch 125/160 (LR 0.01135) => LSC_loss 0.81, Spatial_loss 1.53, Flat_loss 0.17, Train_acc 77.67, Test_acc 44.04
2025-01-05 18:01:01,922 [podnet.py] => Task 16, Epoch 126/160 (LR 0.01073) => LSC_loss 0.80, Spatial_loss 1.49, Flat_loss 0.17, Train_acc 78.81, Test_acc 43.89
2025-01-05 18:01:10,084 [podnet.py] => Task 16, Epoch 127/160 (LR 0.01013) => LSC_loss 0.79, Spatial_loss 1.47, Flat_loss 0.16, Train_acc 79.39, Test_acc 45.34
2025-01-05 18:01:18,304 [podnet.py] => Task 16, Epoch 128/160 (LR 0.00955) => LSC_loss 0.78, Spatial_loss 1.44, Flat_loss 0.16, Train_acc 78.92, Test_acc 44.48
2025-01-05 18:01:26,696 [podnet.py] => Task 16, Epoch 129/160 (LR 0.00898) => LSC_loss 0.78, Spatial_loss 1.44, Flat_loss 0.16, Train_acc 79.55, Test_acc 44.99
2025-01-05 18:01:35,132 [podnet.py] => Task 16, Epoch 130/160 (LR 0.00843) => LSC_loss 0.76, Spatial_loss 1.40, Flat_loss 0.15, Train_acc 79.72, Test_acc 45.44
2025-01-05 18:01:43,338 [podnet.py] => Task 16, Epoch 131/160 (LR 0.00789) => LSC_loss 0.76, Spatial_loss 1.39, Flat_loss 0.15, Train_acc 80.36, Test_acc 44.36
2025-01-05 18:01:51,769 [podnet.py] => Task 16, Epoch 132/160 (LR 0.00737) => LSC_loss 0.74, Spatial_loss 1.38, Flat_loss 0.15, Train_acc 80.30, Test_acc 45.72
2025-01-05 18:02:00,138 [podnet.py] => Task 16, Epoch 133/160 (LR 0.00686) => LSC_loss 0.73, Spatial_loss 1.35, Flat_loss 0.14, Train_acc 80.48, Test_acc 44.07
2025-01-05 18:02:08,434 [podnet.py] => Task 16, Epoch 134/160 (LR 0.00638) => LSC_loss 0.74, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 80.25, Test_acc 45.25
2025-01-05 18:02:16,890 [podnet.py] => Task 16, Epoch 135/160 (LR 0.00590) => LSC_loss 0.73, Spatial_loss 1.33, Flat_loss 0.14, Train_acc 80.90, Test_acc 44.51
2025-01-05 18:02:25,267 [podnet.py] => Task 16, Epoch 136/160 (LR 0.00545) => LSC_loss 0.71, Spatial_loss 1.32, Flat_loss 0.13, Train_acc 81.41, Test_acc 44.95
2025-01-05 18:02:33,430 [podnet.py] => Task 16, Epoch 137/160 (LR 0.00501) => LSC_loss 0.70, Spatial_loss 1.30, Flat_loss 0.13, Train_acc 81.87, Test_acc 45.56
2025-01-05 18:02:41,822 [podnet.py] => Task 16, Epoch 138/160 (LR 0.00459) => LSC_loss 0.70, Spatial_loss 1.29, Flat_loss 0.13, Train_acc 81.73, Test_acc 44.93
2025-01-05 18:02:50,129 [podnet.py] => Task 16, Epoch 139/160 (LR 0.00419) => LSC_loss 0.69, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 82.14, Test_acc 45.47
2025-01-05 18:02:58,384 [podnet.py] => Task 16, Epoch 140/160 (LR 0.00381) => LSC_loss 0.69, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 82.03, Test_acc 45.59
2025-01-05 18:03:06,648 [podnet.py] => Task 16, Epoch 141/160 (LR 0.00344) => LSC_loss 0.69, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 82.03, Test_acc 45.80
2025-01-05 18:03:15,231 [podnet.py] => Task 16, Epoch 142/160 (LR 0.00309) => LSC_loss 0.68, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 82.96, Test_acc 46.09
2025-01-05 18:03:23,663 [podnet.py] => Task 16, Epoch 143/160 (LR 0.00276) => LSC_loss 0.68, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 82.37, Test_acc 46.01
2025-01-05 18:03:32,053 [podnet.py] => Task 16, Epoch 144/160 (LR 0.00245) => LSC_loss 0.66, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 83.14, Test_acc 45.87
2025-01-05 18:03:40,411 [podnet.py] => Task 16, Epoch 145/160 (LR 0.00215) => LSC_loss 0.67, Spatial_loss 1.18, Flat_loss 0.11, Train_acc 82.90, Test_acc 46.44
2025-01-05 18:03:48,654 [podnet.py] => Task 16, Epoch 146/160 (LR 0.00188) => LSC_loss 0.66, Spatial_loss 1.16, Flat_loss 0.11, Train_acc 82.92, Test_acc 46.42
2025-01-05 18:03:56,846 [podnet.py] => Task 16, Epoch 147/160 (LR 0.00162) => LSC_loss 0.65, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 83.76, Test_acc 46.54
2025-01-05 18:04:04,978 [podnet.py] => Task 16, Epoch 148/160 (LR 0.00138) => LSC_loss 0.65, Spatial_loss 1.13, Flat_loss 0.10, Train_acc 83.51, Test_acc 46.24
2025-01-05 18:04:13,265 [podnet.py] => Task 16, Epoch 149/160 (LR 0.00116) => LSC_loss 0.65, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 83.54, Test_acc 46.28
2025-01-05 18:04:21,461 [podnet.py] => Task 16, Epoch 150/160 (LR 0.00096) => LSC_loss 0.64, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 83.49, Test_acc 46.12
2025-01-05 18:04:29,910 [podnet.py] => Task 16, Epoch 151/160 (LR 0.00078) => LSC_loss 0.64, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 83.62, Test_acc 46.18
2025-01-05 18:04:38,311 [podnet.py] => Task 16, Epoch 152/160 (LR 0.00062) => LSC_loss 0.65, Spatial_loss 1.12, Flat_loss 0.10, Train_acc 83.83, Test_acc 46.56
2025-01-05 18:04:46,928 [podnet.py] => Task 16, Epoch 153/160 (LR 0.00047) => LSC_loss 0.63, Spatial_loss 1.08, Flat_loss 0.10, Train_acc 83.86, Test_acc 46.41
2025-01-05 18:04:55,209 [podnet.py] => Task 16, Epoch 154/160 (LR 0.00035) => LSC_loss 0.63, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 84.14, Test_acc 46.48
2025-01-05 18:05:03,416 [podnet.py] => Task 16, Epoch 155/160 (LR 0.00024) => LSC_loss 0.64, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 84.25, Test_acc 46.32
2025-01-05 18:05:11,811 [podnet.py] => Task 16, Epoch 156/160 (LR 0.00015) => LSC_loss 0.64, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 83.87, Test_acc 46.22
2025-01-05 18:05:20,074 [podnet.py] => Task 16, Epoch 157/160 (LR 0.00009) => LSC_loss 0.64, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 84.11, Test_acc 46.42
2025-01-05 18:05:28,445 [podnet.py] => Task 16, Epoch 158/160 (LR 0.00004) => LSC_loss 0.63, Spatial_loss 1.10, Flat_loss 0.10, Train_acc 84.10, Test_acc 46.29
2025-01-05 18:05:36,882 [podnet.py] => Task 16, Epoch 159/160 (LR 0.00001) => LSC_loss 0.63, Spatial_loss 1.07, Flat_loss 0.10, Train_acc 84.27, Test_acc 46.41
2025-01-05 18:05:45,285 [podnet.py] => Task 16, Epoch 160/160 (LR 0.00000) => LSC_loss 0.64, Spatial_loss 1.09, Flat_loss 0.10, Train_acc 83.94, Test_acc 46.48
2025-01-05 18:05:45,286 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 18:05:45,286 [base.py] => Reducing exemplars...(168 per classes)
2025-01-05 18:06:51,808 [base.py] => Constructing exemplars...(168 per classes)
2025-01-05 18:07:00,179 [podnet.py] => The size of finetune dataset: 14280
2025-01-05 18:07:07,843 [podnet.py] => Task 16, Epoch 1/20 (LR 0.00497) => LSC_loss 0.64, Spatial_loss 1.24, Flat_loss 0.10, Train_acc 83.64, Test_acc 45.55
2025-01-05 18:07:15,588 [podnet.py] => Task 16, Epoch 2/20 (LR 0.00488) => LSC_loss 0.64, Spatial_loss 1.26, Flat_loss 0.10, Train_acc 83.91, Test_acc 45.38
2025-01-05 18:07:23,448 [podnet.py] => Task 16, Epoch 3/20 (LR 0.00473) => LSC_loss 0.64, Spatial_loss 1.21, Flat_loss 0.10, Train_acc 84.39, Test_acc 44.93
2025-01-05 18:07:31,167 [podnet.py] => Task 16, Epoch 4/20 (LR 0.00452) => LSC_loss 0.63, Spatial_loss 1.23, Flat_loss 0.10, Train_acc 84.37, Test_acc 45.44
2025-01-05 18:07:38,808 [podnet.py] => Task 16, Epoch 5/20 (LR 0.00427) => LSC_loss 0.63, Spatial_loss 1.23, Flat_loss 0.10, Train_acc 84.17, Test_acc 45.66
2025-01-05 18:07:46,468 [podnet.py] => Task 16, Epoch 6/20 (LR 0.00397) => LSC_loss 0.63, Spatial_loss 1.19, Flat_loss 0.10, Train_acc 84.22, Test_acc 45.41
2025-01-05 18:07:54,284 [podnet.py] => Task 16, Epoch 7/20 (LR 0.00363) => LSC_loss 0.62, Spatial_loss 1.17, Flat_loss 0.10, Train_acc 84.10, Test_acc 45.59
2025-01-05 18:08:01,991 [podnet.py] => Task 16, Epoch 8/20 (LR 0.00327) => LSC_loss 0.62, Spatial_loss 1.21, Flat_loss 0.10, Train_acc 84.64, Test_acc 46.05
2025-01-05 18:08:09,711 [podnet.py] => Task 16, Epoch 9/20 (LR 0.00289) => LSC_loss 0.62, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 84.45, Test_acc 46.12
2025-01-05 18:08:17,130 [podnet.py] => Task 16, Epoch 10/20 (LR 0.00250) => LSC_loss 0.61, Spatial_loss 1.17, Flat_loss 0.09, Train_acc 84.89, Test_acc 46.02
2025-01-05 18:08:24,871 [podnet.py] => Task 16, Epoch 11/20 (LR 0.00211) => LSC_loss 0.61, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 84.92, Test_acc 45.68
2025-01-05 18:08:32,712 [podnet.py] => Task 16, Epoch 12/20 (LR 0.00173) => LSC_loss 0.60, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 85.19, Test_acc 46.46
2025-01-05 18:08:40,328 [podnet.py] => Task 16, Epoch 13/20 (LR 0.00137) => LSC_loss 0.59, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 85.55, Test_acc 45.92
2025-01-05 18:08:47,919 [podnet.py] => Task 16, Epoch 14/20 (LR 0.00103) => LSC_loss 0.59, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.69, Test_acc 46.11
2025-01-05 18:08:55,496 [podnet.py] => Task 16, Epoch 15/20 (LR 0.00073) => LSC_loss 0.59, Spatial_loss 1.07, Flat_loss 0.08, Train_acc 85.50, Test_acc 46.54
2025-01-05 18:09:03,280 [podnet.py] => Task 16, Epoch 16/20 (LR 0.00048) => LSC_loss 0.58, Spatial_loss 1.07, Flat_loss 0.08, Train_acc 85.93, Test_acc 46.62
2025-01-05 18:09:10,908 [podnet.py] => Task 16, Epoch 17/20 (LR 0.00027) => LSC_loss 0.57, Spatial_loss 1.03, Flat_loss 0.08, Train_acc 86.44, Test_acc 46.62
2025-01-05 18:09:18,646 [podnet.py] => Task 16, Epoch 18/20 (LR 0.00012) => LSC_loss 0.58, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 86.08, Test_acc 46.76
2025-01-05 18:09:26,221 [podnet.py] => Task 16, Epoch 19/20 (LR 0.00003) => LSC_loss 0.58, Spatial_loss 1.04, Flat_loss 0.08, Train_acc 86.10, Test_acc 46.84
2025-01-05 18:09:33,916 [podnet.py] => Task 16, Epoch 20/20 (LR 0.00000) => LSC_loss 0.58, Spatial_loss 1.02, Flat_loss 0.08, Train_acc 85.71, Test_acc 46.51
2025-01-05 18:09:33,920 [base.py] => Reducing exemplars...(158 per classes)
2025-01-05 18:10:40,035 [base.py] => Constructing exemplars...(158 per classes)
2025-01-05 18:10:51,411 [podnet.py] => Exemplar size: 13430
2025-01-05 18:10:51,411 [trainer.py] => CNN: {'total': np.float64(46.51), '00-09': np.float64(58.1), '10-19': np.float64(36.9), '20-29': np.float64(52.7), '30-39': np.float64(43.6), '40-49': np.float64(54.9), '50-59': np.float64(38.5), '60-69': np.float64(49.8), '70-79': np.float64(38.3), '80-89': np.float64(45.0), 'old': np.float64(46.6), 'new': np.float64(45.0)}
2025-01-05 18:10:51,411 [trainer.py] => NME: {'total': np.float64(45.32), '00-09': np.float64(60.0), '10-19': np.float64(35.9), '20-29': np.float64(52.0), '30-39': np.float64(45.5), '40-49': np.float64(51.6), '50-59': np.float64(37.0), '60-69': np.float64(43.0), '70-79': np.float64(37.3), '80-89': np.float64(45.8), 'old': np.float64(45.29), 'new': np.float64(45.8)}
2025-01-05 18:10:51,411 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96), np.float64(49.84), np.float64(47.7), np.float64(46.51)]
2025-01-05 18:10:51,411 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43), np.float64(77.19), np.float64(76.28), np.float64(75.69)]
2025-01-05 18:10:51,411 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96), np.float64(48.09), np.float64(46.44), np.float64(45.32)]
2025-01-05 18:10:51,411 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2), np.float64(75.59), np.float64(75.25), np.float64(74.59)]

2025-01-05 18:10:51,411 [trainer.py] => All params: 520657
2025-01-05 18:10:51,412 [trainer.py] => Trainable params: 520657
2025-01-05 18:10:51,412 [podnet.py] => Learning on 85-90
2025-01-05 18:10:51,467 [podnet.py] => Adaptive factor: 4.242640687119285
2025-01-05 18:10:59,671 [podnet.py] => Task 17, Epoch 1/160 (LR 0.09999) => LSC_loss 2.75, Spatial_loss 3.92, Flat_loss 1.04, Train_acc 34.61, Test_acc 29.06
2025-01-05 18:11:07,841 [podnet.py] => Task 17, Epoch 2/160 (LR 0.09996) => LSC_loss 2.22, Spatial_loss 3.33, Flat_loss 0.75, Train_acc 41.62, Test_acc 27.06
2025-01-05 18:11:16,235 [podnet.py] => Task 17, Epoch 3/160 (LR 0.09991) => LSC_loss 2.11, Spatial_loss 3.16, Flat_loss 0.68, Train_acc 44.80, Test_acc 34.41
2025-01-05 18:11:24,710 [podnet.py] => Task 17, Epoch 4/160 (LR 0.09985) => LSC_loss 2.04, Spatial_loss 3.07, Flat_loss 0.65, Train_acc 46.05, Test_acc 32.74
2025-01-05 18:11:32,890 [podnet.py] => Task 17, Epoch 5/160 (LR 0.09976) => LSC_loss 2.00, Spatial_loss 3.05, Flat_loss 0.62, Train_acc 47.50, Test_acc 33.87
2025-01-05 18:11:41,122 [podnet.py] => Task 17, Epoch 6/160 (LR 0.09965) => LSC_loss 1.97, Spatial_loss 2.97, Flat_loss 0.61, Train_acc 48.38, Test_acc 35.69
2025-01-05 18:11:49,217 [podnet.py] => Task 17, Epoch 7/160 (LR 0.09953) => LSC_loss 1.97, Spatial_loss 3.00, Flat_loss 0.61, Train_acc 48.04, Test_acc 34.27
2025-01-05 18:11:57,446 [podnet.py] => Task 17, Epoch 8/160 (LR 0.09938) => LSC_loss 1.90, Spatial_loss 2.88, Flat_loss 0.58, Train_acc 49.57, Test_acc 33.43
2025-01-05 18:12:05,532 [podnet.py] => Task 17, Epoch 9/160 (LR 0.09922) => LSC_loss 1.90, Spatial_loss 2.89, Flat_loss 0.58, Train_acc 50.11, Test_acc 37.88
2025-01-05 18:12:13,737 [podnet.py] => Task 17, Epoch 10/160 (LR 0.09904) => LSC_loss 1.88, Spatial_loss 2.95, Flat_loss 0.59, Train_acc 50.18, Test_acc 31.77
2025-01-05 18:12:22,084 [podnet.py] => Task 17, Epoch 11/160 (LR 0.09884) => LSC_loss 1.89, Spatial_loss 2.88, Flat_loss 0.57, Train_acc 49.84, Test_acc 36.46
2025-01-05 18:12:30,404 [podnet.py] => Task 17, Epoch 12/160 (LR 0.09862) => LSC_loss 1.87, Spatial_loss 2.87, Flat_loss 0.57, Train_acc 50.48, Test_acc 33.76
2025-01-05 18:12:38,851 [podnet.py] => Task 17, Epoch 13/160 (LR 0.09838) => LSC_loss 1.86, Spatial_loss 2.87, Flat_loss 0.57, Train_acc 51.00, Test_acc 33.91
2025-01-05 18:12:47,180 [podnet.py] => Task 17, Epoch 14/160 (LR 0.09812) => LSC_loss 1.84, Spatial_loss 2.83, Flat_loss 0.56, Train_acc 50.97, Test_acc 33.51
2025-01-05 18:12:55,603 [podnet.py] => Task 17, Epoch 15/160 (LR 0.09785) => LSC_loss 1.84, Spatial_loss 2.82, Flat_loss 0.56, Train_acc 51.02, Test_acc 33.01
2025-01-05 18:13:03,857 [podnet.py] => Task 17, Epoch 16/160 (LR 0.09755) => LSC_loss 1.83, Spatial_loss 2.88, Flat_loss 0.56, Train_acc 51.43, Test_acc 36.98
2025-01-05 18:13:12,000 [podnet.py] => Task 17, Epoch 17/160 (LR 0.09724) => LSC_loss 1.84, Spatial_loss 2.87, Flat_loss 0.56, Train_acc 51.04, Test_acc 35.63
2025-01-05 18:13:20,569 [podnet.py] => Task 17, Epoch 18/160 (LR 0.09691) => LSC_loss 1.82, Spatial_loss 2.88, Flat_loss 0.56, Train_acc 51.57, Test_acc 29.39
2025-01-05 18:13:28,787 [podnet.py] => Task 17, Epoch 19/160 (LR 0.09656) => LSC_loss 1.82, Spatial_loss 2.76, Flat_loss 0.56, Train_acc 51.98, Test_acc 34.38
2025-01-05 18:13:37,135 [podnet.py] => Task 17, Epoch 20/160 (LR 0.09619) => LSC_loss 1.81, Spatial_loss 2.83, Flat_loss 0.56, Train_acc 52.27, Test_acc 28.93
2025-01-05 18:13:45,611 [podnet.py] => Task 17, Epoch 21/160 (LR 0.09581) => LSC_loss 1.82, Spatial_loss 2.81, Flat_loss 0.56, Train_acc 51.87, Test_acc 36.04
2025-01-05 18:13:53,959 [podnet.py] => Task 17, Epoch 22/160 (LR 0.09541) => LSC_loss 1.79, Spatial_loss 2.76, Flat_loss 0.54, Train_acc 52.68, Test_acc 30.39
2025-01-05 18:14:02,447 [podnet.py] => Task 17, Epoch 23/160 (LR 0.09499) => LSC_loss 1.82, Spatial_loss 2.83, Flat_loss 0.56, Train_acc 51.98, Test_acc 33.08
2025-01-05 18:14:10,660 [podnet.py] => Task 17, Epoch 24/160 (LR 0.09455) => LSC_loss 1.79, Spatial_loss 2.82, Flat_loss 0.55, Train_acc 52.14, Test_acc 31.98
2025-01-05 18:14:19,055 [podnet.py] => Task 17, Epoch 25/160 (LR 0.09410) => LSC_loss 1.80, Spatial_loss 2.81, Flat_loss 0.55, Train_acc 52.33, Test_acc 30.84
2025-01-05 18:14:27,533 [podnet.py] => Task 17, Epoch 26/160 (LR 0.09362) => LSC_loss 1.78, Spatial_loss 2.78, Flat_loss 0.54, Train_acc 52.49, Test_acc 36.37
2025-01-05 18:14:35,781 [podnet.py] => Task 17, Epoch 27/160 (LR 0.09314) => LSC_loss 1.77, Spatial_loss 2.80, Flat_loss 0.54, Train_acc 53.11, Test_acc 31.67
2025-01-05 18:14:44,035 [podnet.py] => Task 17, Epoch 28/160 (LR 0.09263) => LSC_loss 1.76, Spatial_loss 2.79, Flat_loss 0.54, Train_acc 52.51, Test_acc 32.54
2025-01-05 18:14:52,492 [podnet.py] => Task 17, Epoch 29/160 (LR 0.09211) => LSC_loss 1.78, Spatial_loss 2.80, Flat_loss 0.54, Train_acc 52.56, Test_acc 34.59
2025-01-05 18:15:00,820 [podnet.py] => Task 17, Epoch 30/160 (LR 0.09157) => LSC_loss 1.78, Spatial_loss 2.80, Flat_loss 0.54, Train_acc 52.86, Test_acc 34.18
2025-01-05 18:15:09,339 [podnet.py] => Task 17, Epoch 31/160 (LR 0.09102) => LSC_loss 1.78, Spatial_loss 2.78, Flat_loss 0.54, Train_acc 52.67, Test_acc 32.54
2025-01-05 18:15:17,710 [podnet.py] => Task 17, Epoch 32/160 (LR 0.09045) => LSC_loss 1.76, Spatial_loss 2.78, Flat_loss 0.54, Train_acc 53.17, Test_acc 35.23
2025-01-05 18:15:26,005 [podnet.py] => Task 17, Epoch 33/160 (LR 0.08987) => LSC_loss 1.78, Spatial_loss 2.79, Flat_loss 0.55, Train_acc 52.69, Test_acc 34.87
2025-01-05 18:15:34,304 [podnet.py] => Task 17, Epoch 34/160 (LR 0.08927) => LSC_loss 1.76, Spatial_loss 2.76, Flat_loss 0.54, Train_acc 53.17, Test_acc 35.29
2025-01-05 18:15:42,712 [podnet.py] => Task 17, Epoch 35/160 (LR 0.08865) => LSC_loss 1.73, Spatial_loss 2.75, Flat_loss 0.54, Train_acc 53.89, Test_acc 35.02
2025-01-05 18:15:51,004 [podnet.py] => Task 17, Epoch 36/160 (LR 0.08802) => LSC_loss 1.73, Spatial_loss 2.76, Flat_loss 0.53, Train_acc 54.16, Test_acc 34.92
2025-01-05 18:15:59,429 [podnet.py] => Task 17, Epoch 37/160 (LR 0.08738) => LSC_loss 1.74, Spatial_loss 2.73, Flat_loss 0.53, Train_acc 53.03, Test_acc 34.87
2025-01-05 18:16:07,559 [podnet.py] => Task 17, Epoch 38/160 (LR 0.08672) => LSC_loss 1.76, Spatial_loss 2.78, Flat_loss 0.54, Train_acc 52.89, Test_acc 35.98
2025-01-05 18:16:15,902 [podnet.py] => Task 17, Epoch 39/160 (LR 0.08604) => LSC_loss 1.73, Spatial_loss 2.72, Flat_loss 0.52, Train_acc 53.87, Test_acc 37.02
2025-01-05 18:16:24,355 [podnet.py] => Task 17, Epoch 40/160 (LR 0.08536) => LSC_loss 1.73, Spatial_loss 2.79, Flat_loss 0.53, Train_acc 53.68, Test_acc 36.47
2025-01-05 18:16:32,600 [podnet.py] => Task 17, Epoch 41/160 (LR 0.08465) => LSC_loss 1.72, Spatial_loss 2.71, Flat_loss 0.53, Train_acc 53.88, Test_acc 37.46
2025-01-05 18:16:40,909 [podnet.py] => Task 17, Epoch 42/160 (LR 0.08394) => LSC_loss 1.71, Spatial_loss 2.67, Flat_loss 0.52, Train_acc 54.24, Test_acc 34.97
2025-01-05 18:16:49,288 [podnet.py] => Task 17, Epoch 43/160 (LR 0.08321) => LSC_loss 1.72, Spatial_loss 2.68, Flat_loss 0.52, Train_acc 53.99, Test_acc 36.67
2025-01-05 18:16:57,709 [podnet.py] => Task 17, Epoch 44/160 (LR 0.08247) => LSC_loss 1.72, Spatial_loss 2.68, Flat_loss 0.52, Train_acc 54.29, Test_acc 33.66
2025-01-05 18:17:06,012 [podnet.py] => Task 17, Epoch 45/160 (LR 0.08172) => LSC_loss 1.70, Spatial_loss 2.67, Flat_loss 0.51, Train_acc 54.53, Test_acc 34.78
2025-01-05 18:17:14,279 [podnet.py] => Task 17, Epoch 46/160 (LR 0.08095) => LSC_loss 1.69, Spatial_loss 2.74, Flat_loss 0.51, Train_acc 54.64, Test_acc 36.13
2025-01-05 18:17:22,645 [podnet.py] => Task 17, Epoch 47/160 (LR 0.08018) => LSC_loss 1.70, Spatial_loss 2.66, Flat_loss 0.51, Train_acc 54.41, Test_acc 34.02
2025-01-05 18:17:30,804 [podnet.py] => Task 17, Epoch 48/160 (LR 0.07939) => LSC_loss 1.68, Spatial_loss 2.65, Flat_loss 0.51, Train_acc 55.02, Test_acc 35.76
2025-01-05 18:17:39,124 [podnet.py] => Task 17, Epoch 49/160 (LR 0.07859) => LSC_loss 1.68, Spatial_loss 2.69, Flat_loss 0.51, Train_acc 54.81, Test_acc 33.72
2025-01-05 18:17:47,378 [podnet.py] => Task 17, Epoch 50/160 (LR 0.07778) => LSC_loss 1.68, Spatial_loss 2.63, Flat_loss 0.51, Train_acc 54.63, Test_acc 32.62
2025-01-05 18:17:55,517 [podnet.py] => Task 17, Epoch 51/160 (LR 0.07696) => LSC_loss 1.67, Spatial_loss 2.66, Flat_loss 0.51, Train_acc 55.09, Test_acc 36.44
2025-01-05 18:18:03,725 [podnet.py] => Task 17, Epoch 52/160 (LR 0.07612) => LSC_loss 1.68, Spatial_loss 2.70, Flat_loss 0.51, Train_acc 55.07, Test_acc 35.73
2025-01-05 18:18:11,932 [podnet.py] => Task 17, Epoch 53/160 (LR 0.07528) => LSC_loss 1.66, Spatial_loss 2.67, Flat_loss 0.50, Train_acc 55.57, Test_acc 39.24
2025-01-05 18:18:20,141 [podnet.py] => Task 17, Epoch 54/160 (LR 0.07443) => LSC_loss 1.68, Spatial_loss 2.66, Flat_loss 0.51, Train_acc 54.85, Test_acc 34.52
2025-01-05 18:18:28,341 [podnet.py] => Task 17, Epoch 55/160 (LR 0.07357) => LSC_loss 1.65, Spatial_loss 2.60, Flat_loss 0.49, Train_acc 55.75, Test_acc 36.13
2025-01-05 18:18:36,628 [podnet.py] => Task 17, Epoch 56/160 (LR 0.07270) => LSC_loss 1.65, Spatial_loss 2.70, Flat_loss 0.49, Train_acc 55.68, Test_acc 31.18
2025-01-05 18:18:45,006 [podnet.py] => Task 17, Epoch 57/160 (LR 0.07182) => LSC_loss 1.65, Spatial_loss 2.60, Flat_loss 0.49, Train_acc 55.47, Test_acc 34.60
2025-01-05 18:18:53,172 [podnet.py] => Task 17, Epoch 58/160 (LR 0.07093) => LSC_loss 1.63, Spatial_loss 2.61, Flat_loss 0.48, Train_acc 55.96, Test_acc 36.84
2025-01-05 18:19:01,752 [podnet.py] => Task 17, Epoch 59/160 (LR 0.07004) => LSC_loss 1.63, Spatial_loss 2.58, Flat_loss 0.48, Train_acc 56.31, Test_acc 35.22
2025-01-05 18:19:10,172 [podnet.py] => Task 17, Epoch 60/160 (LR 0.06913) => LSC_loss 1.62, Spatial_loss 2.60, Flat_loss 0.48, Train_acc 56.01, Test_acc 35.71
2025-01-05 18:19:18,384 [podnet.py] => Task 17, Epoch 61/160 (LR 0.06822) => LSC_loss 1.63, Spatial_loss 2.66, Flat_loss 0.48, Train_acc 56.12, Test_acc 33.26
2025-01-05 18:19:26,830 [podnet.py] => Task 17, Epoch 62/160 (LR 0.06731) => LSC_loss 1.62, Spatial_loss 2.61, Flat_loss 0.48, Train_acc 56.49, Test_acc 35.91
2025-01-05 18:19:35,174 [podnet.py] => Task 17, Epoch 63/160 (LR 0.06638) => LSC_loss 1.60, Spatial_loss 2.56, Flat_loss 0.47, Train_acc 56.55, Test_acc 34.07
2025-01-05 18:19:43,350 [podnet.py] => Task 17, Epoch 64/160 (LR 0.06545) => LSC_loss 1.60, Spatial_loss 2.55, Flat_loss 0.47, Train_acc 56.96, Test_acc 36.59
2025-01-05 18:19:51,490 [podnet.py] => Task 17, Epoch 65/160 (LR 0.06451) => LSC_loss 1.55, Spatial_loss 2.47, Flat_loss 0.45, Train_acc 58.34, Test_acc 35.09
2025-01-05 18:19:59,839 [podnet.py] => Task 17, Epoch 66/160 (LR 0.06357) => LSC_loss 1.58, Spatial_loss 2.51, Flat_loss 0.47, Train_acc 57.08, Test_acc 34.09
2025-01-05 18:20:08,255 [podnet.py] => Task 17, Epoch 67/160 (LR 0.06262) => LSC_loss 1.56, Spatial_loss 2.51, Flat_loss 0.45, Train_acc 57.74, Test_acc 36.92
2025-01-05 18:20:16,463 [podnet.py] => Task 17, Epoch 68/160 (LR 0.06167) => LSC_loss 1.57, Spatial_loss 2.53, Flat_loss 0.46, Train_acc 57.50, Test_acc 36.28
2025-01-05 18:20:24,800 [podnet.py] => Task 17, Epoch 69/160 (LR 0.06072) => LSC_loss 1.55, Spatial_loss 2.49, Flat_loss 0.45, Train_acc 57.71, Test_acc 36.43
2025-01-05 18:20:33,124 [podnet.py] => Task 17, Epoch 70/160 (LR 0.05975) => LSC_loss 1.55, Spatial_loss 2.51, Flat_loss 0.45, Train_acc 57.81, Test_acc 36.90
2025-01-05 18:20:41,323 [podnet.py] => Task 17, Epoch 71/160 (LR 0.05879) => LSC_loss 1.54, Spatial_loss 2.47, Flat_loss 0.45, Train_acc 58.13, Test_acc 36.62
2025-01-05 18:20:49,912 [podnet.py] => Task 17, Epoch 72/160 (LR 0.05782) => LSC_loss 1.51, Spatial_loss 2.46, Flat_loss 0.44, Train_acc 59.45, Test_acc 37.61
2025-01-05 18:20:58,111 [podnet.py] => Task 17, Epoch 73/160 (LR 0.05685) => LSC_loss 1.52, Spatial_loss 2.46, Flat_loss 0.44, Train_acc 58.52, Test_acc 35.46
2025-01-05 18:21:06,557 [podnet.py] => Task 17, Epoch 74/160 (LR 0.05588) => LSC_loss 1.51, Spatial_loss 2.45, Flat_loss 0.43, Train_acc 59.30, Test_acc 39.43
2025-01-05 18:21:14,938 [podnet.py] => Task 17, Epoch 75/160 (LR 0.05490) => LSC_loss 1.49, Spatial_loss 2.39, Flat_loss 0.43, Train_acc 59.69, Test_acc 34.84
2025-01-05 18:21:23,335 [podnet.py] => Task 17, Epoch 76/160 (LR 0.05392) => LSC_loss 1.49, Spatial_loss 2.44, Flat_loss 0.43, Train_acc 59.50, Test_acc 37.38
2025-01-05 18:21:31,674 [podnet.py] => Task 17, Epoch 77/160 (LR 0.05294) => LSC_loss 1.49, Spatial_loss 2.42, Flat_loss 0.43, Train_acc 60.08, Test_acc 37.62
2025-01-05 18:21:39,987 [podnet.py] => Task 17, Epoch 78/160 (LR 0.05196) => LSC_loss 1.47, Spatial_loss 2.36, Flat_loss 0.41, Train_acc 60.56, Test_acc 35.26
2025-01-05 18:21:48,466 [podnet.py] => Task 17, Epoch 79/160 (LR 0.05098) => LSC_loss 1.48, Spatial_loss 2.39, Flat_loss 0.42, Train_acc 59.72, Test_acc 37.47
2025-01-05 18:21:56,699 [podnet.py] => Task 17, Epoch 80/160 (LR 0.05000) => LSC_loss 1.45, Spatial_loss 2.34, Flat_loss 0.41, Train_acc 60.57, Test_acc 38.79
2025-01-05 18:22:05,167 [podnet.py] => Task 17, Epoch 81/160 (LR 0.04902) => LSC_loss 1.42, Spatial_loss 2.33, Flat_loss 0.40, Train_acc 61.48, Test_acc 37.80
2025-01-05 18:22:13,779 [podnet.py] => Task 17, Epoch 82/160 (LR 0.04804) => LSC_loss 1.41, Spatial_loss 2.30, Flat_loss 0.40, Train_acc 62.01, Test_acc 37.57
2025-01-05 18:22:21,945 [podnet.py] => Task 17, Epoch 83/160 (LR 0.04706) => LSC_loss 1.45, Spatial_loss 2.34, Flat_loss 0.40, Train_acc 60.27, Test_acc 37.71
2025-01-05 18:22:29,994 [podnet.py] => Task 17, Epoch 84/160 (LR 0.04608) => LSC_loss 1.39, Spatial_loss 2.28, Flat_loss 0.39, Train_acc 61.98, Test_acc 38.18
2025-01-05 18:22:38,346 [podnet.py] => Task 17, Epoch 85/160 (LR 0.04510) => LSC_loss 1.41, Spatial_loss 2.28, Flat_loss 0.39, Train_acc 62.06, Test_acc 36.13
2025-01-05 18:22:46,720 [podnet.py] => Task 17, Epoch 86/160 (LR 0.04412) => LSC_loss 1.40, Spatial_loss 2.27, Flat_loss 0.39, Train_acc 61.95, Test_acc 38.16
2025-01-05 18:22:54,974 [podnet.py] => Task 17, Epoch 87/160 (LR 0.04315) => LSC_loss 1.39, Spatial_loss 2.29, Flat_loss 0.38, Train_acc 62.01, Test_acc 38.38
2025-01-05 18:23:03,269 [podnet.py] => Task 17, Epoch 88/160 (LR 0.04218) => LSC_loss 1.37, Spatial_loss 2.21, Flat_loss 0.38, Train_acc 62.77, Test_acc 37.17
2025-01-05 18:23:11,661 [podnet.py] => Task 17, Epoch 89/160 (LR 0.04121) => LSC_loss 1.37, Spatial_loss 2.28, Flat_loss 0.38, Train_acc 62.99, Test_acc 37.47
2025-01-05 18:23:19,904 [podnet.py] => Task 17, Epoch 90/160 (LR 0.04025) => LSC_loss 1.34, Spatial_loss 2.23, Flat_loss 0.36, Train_acc 63.90, Test_acc 39.93
2025-01-05 18:23:28,478 [podnet.py] => Task 17, Epoch 91/160 (LR 0.03928) => LSC_loss 1.35, Spatial_loss 2.27, Flat_loss 0.37, Train_acc 63.18, Test_acc 39.08
2025-01-05 18:23:36,732 [podnet.py] => Task 17, Epoch 92/160 (LR 0.03833) => LSC_loss 1.34, Spatial_loss 2.20, Flat_loss 0.36, Train_acc 62.97, Test_acc 41.91
2025-01-05 18:23:45,112 [podnet.py] => Task 17, Epoch 93/160 (LR 0.03738) => LSC_loss 1.31, Spatial_loss 2.15, Flat_loss 0.36, Train_acc 64.07, Test_acc 35.11
2025-01-05 18:23:53,450 [podnet.py] => Task 17, Epoch 94/160 (LR 0.03643) => LSC_loss 1.32, Spatial_loss 2.17, Flat_loss 0.35, Train_acc 64.18, Test_acc 38.81
2025-01-05 18:24:01,674 [podnet.py] => Task 17, Epoch 95/160 (LR 0.03549) => LSC_loss 1.29, Spatial_loss 2.17, Flat_loss 0.35, Train_acc 64.45, Test_acc 39.69
2025-01-05 18:24:09,850 [podnet.py] => Task 17, Epoch 96/160 (LR 0.03455) => LSC_loss 1.27, Spatial_loss 2.10, Flat_loss 0.34, Train_acc 65.08, Test_acc 37.83
2025-01-05 18:24:18,234 [podnet.py] => Task 17, Epoch 97/160 (LR 0.03362) => LSC_loss 1.27, Spatial_loss 2.12, Flat_loss 0.34, Train_acc 65.26, Test_acc 40.24
2025-01-05 18:24:26,474 [podnet.py] => Task 17, Epoch 98/160 (LR 0.03269) => LSC_loss 1.25, Spatial_loss 2.08, Flat_loss 0.33, Train_acc 65.71, Test_acc 39.36
2025-01-05 18:24:34,926 [podnet.py] => Task 17, Epoch 99/160 (LR 0.03178) => LSC_loss 1.22, Spatial_loss 2.05, Flat_loss 0.32, Train_acc 66.79, Test_acc 39.32
2025-01-05 18:24:43,250 [podnet.py] => Task 17, Epoch 100/160 (LR 0.03087) => LSC_loss 1.23, Spatial_loss 2.05, Flat_loss 0.32, Train_acc 66.26, Test_acc 39.14
2025-01-05 18:24:51,594 [podnet.py] => Task 17, Epoch 101/160 (LR 0.02996) => LSC_loss 1.21, Spatial_loss 2.02, Flat_loss 0.31, Train_acc 67.15, Test_acc 40.79
2025-01-05 18:24:59,974 [podnet.py] => Task 17, Epoch 102/160 (LR 0.02907) => LSC_loss 1.20, Spatial_loss 1.96, Flat_loss 0.31, Train_acc 67.04, Test_acc 37.19
2025-01-05 18:25:08,041 [podnet.py] => Task 17, Epoch 103/160 (LR 0.02818) => LSC_loss 1.19, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 67.04, Test_acc 39.32
2025-01-05 18:25:16,524 [podnet.py] => Task 17, Epoch 104/160 (LR 0.02730) => LSC_loss 1.17, Spatial_loss 1.97, Flat_loss 0.30, Train_acc 68.09, Test_acc 38.99
2025-01-05 18:25:24,784 [podnet.py] => Task 17, Epoch 105/160 (LR 0.02643) => LSC_loss 1.17, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 67.95, Test_acc 38.24
2025-01-05 18:25:33,023 [podnet.py] => Task 17, Epoch 106/160 (LR 0.02557) => LSC_loss 1.15, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 68.34, Test_acc 39.41
2025-01-05 18:25:41,236 [podnet.py] => Task 17, Epoch 107/160 (LR 0.02472) => LSC_loss 1.13, Spatial_loss 1.91, Flat_loss 0.28, Train_acc 69.18, Test_acc 40.13
2025-01-05 18:25:49,540 [podnet.py] => Task 17, Epoch 108/160 (LR 0.02388) => LSC_loss 1.12, Spatial_loss 1.87, Flat_loss 0.28, Train_acc 69.84, Test_acc 39.64
2025-01-05 18:25:57,960 [podnet.py] => Task 17, Epoch 109/160 (LR 0.02304) => LSC_loss 1.12, Spatial_loss 1.89, Flat_loss 0.28, Train_acc 69.67, Test_acc 41.00
2025-01-05 18:26:06,761 [podnet.py] => Task 17, Epoch 110/160 (LR 0.02222) => LSC_loss 1.09, Spatial_loss 1.84, Flat_loss 0.26, Train_acc 70.33, Test_acc 40.72
2025-01-05 18:26:15,117 [podnet.py] => Task 17, Epoch 111/160 (LR 0.02141) => LSC_loss 1.09, Spatial_loss 1.90, Flat_loss 0.27, Train_acc 70.36, Test_acc 40.57
2025-01-05 18:26:23,572 [podnet.py] => Task 17, Epoch 112/160 (LR 0.02061) => LSC_loss 1.07, Spatial_loss 1.83, Flat_loss 0.26, Train_acc 70.77, Test_acc 39.13
2025-01-05 18:26:31,926 [podnet.py] => Task 17, Epoch 113/160 (LR 0.01982) => LSC_loss 1.06, Spatial_loss 1.80, Flat_loss 0.25, Train_acc 71.06, Test_acc 39.43
2025-01-05 18:26:40,151 [podnet.py] => Task 17, Epoch 114/160 (LR 0.01905) => LSC_loss 1.05, Spatial_loss 1.78, Flat_loss 0.25, Train_acc 71.56, Test_acc 42.70
2025-01-05 18:26:48,492 [podnet.py] => Task 17, Epoch 115/160 (LR 0.01828) => LSC_loss 1.03, Spatial_loss 1.71, Flat_loss 0.24, Train_acc 72.10, Test_acc 41.82
2025-01-05 18:26:56,761 [podnet.py] => Task 17, Epoch 116/160 (LR 0.01753) => LSC_loss 1.00, Spatial_loss 1.72, Flat_loss 0.24, Train_acc 73.11, Test_acc 42.13
2025-01-05 18:27:05,096 [podnet.py] => Task 17, Epoch 117/160 (LR 0.01679) => LSC_loss 1.00, Spatial_loss 1.70, Flat_loss 0.23, Train_acc 73.11, Test_acc 41.06
2025-01-05 18:27:13,591 [podnet.py] => Task 17, Epoch 118/160 (LR 0.01606) => LSC_loss 0.99, Spatial_loss 1.69, Flat_loss 0.22, Train_acc 73.52, Test_acc 41.18
2025-01-05 18:27:21,953 [podnet.py] => Task 17, Epoch 119/160 (LR 0.01535) => LSC_loss 0.97, Spatial_loss 1.65, Flat_loss 0.22, Train_acc 74.42, Test_acc 41.94
2025-01-05 18:27:30,468 [podnet.py] => Task 17, Epoch 120/160 (LR 0.01464) => LSC_loss 0.97, Spatial_loss 1.69, Flat_loss 0.21, Train_acc 73.73, Test_acc 41.60
2025-01-05 18:27:39,007 [podnet.py] => Task 17, Epoch 121/160 (LR 0.01396) => LSC_loss 0.95, Spatial_loss 1.62, Flat_loss 0.21, Train_acc 74.63, Test_acc 43.58
2025-01-05 18:27:47,316 [podnet.py] => Task 17, Epoch 122/160 (LR 0.01328) => LSC_loss 0.92, Spatial_loss 1.62, Flat_loss 0.20, Train_acc 74.92, Test_acc 41.42
2025-01-05 18:27:55,817 [podnet.py] => Task 17, Epoch 123/160 (LR 0.01262) => LSC_loss 0.93, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 75.06, Test_acc 41.14
2025-01-05 18:28:04,000 [podnet.py] => Task 17, Epoch 124/160 (LR 0.01198) => LSC_loss 0.92, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 75.15, Test_acc 42.10
2025-01-05 18:28:12,414 [podnet.py] => Task 17, Epoch 125/160 (LR 0.01135) => LSC_loss 0.91, Spatial_loss 1.56, Flat_loss 0.19, Train_acc 75.94, Test_acc 43.79
2025-01-05 18:28:20,729 [podnet.py] => Task 17, Epoch 126/160 (LR 0.01073) => LSC_loss 0.89, Spatial_loss 1.52, Flat_loss 0.19, Train_acc 76.21, Test_acc 41.06
2025-01-05 18:28:29,012 [podnet.py] => Task 17, Epoch 127/160 (LR 0.01013) => LSC_loss 0.88, Spatial_loss 1.51, Flat_loss 0.18, Train_acc 77.19, Test_acc 42.48
2025-01-05 18:28:37,483 [podnet.py] => Task 17, Epoch 128/160 (LR 0.00955) => LSC_loss 0.87, Spatial_loss 1.47, Flat_loss 0.18, Train_acc 76.98, Test_acc 42.27
2025-01-05 18:28:45,933 [podnet.py] => Task 17, Epoch 129/160 (LR 0.00898) => LSC_loss 0.88, Spatial_loss 1.49, Flat_loss 0.17, Train_acc 76.84, Test_acc 43.21
2025-01-05 18:28:54,013 [podnet.py] => Task 17, Epoch 130/160 (LR 0.00843) => LSC_loss 0.86, Spatial_loss 1.41, Flat_loss 0.17, Train_acc 76.95, Test_acc 43.19
2025-01-05 18:29:02,269 [podnet.py] => Task 17, Epoch 131/160 (LR 0.00789) => LSC_loss 0.83, Spatial_loss 1.39, Flat_loss 0.16, Train_acc 77.82, Test_acc 42.17
2025-01-05 18:29:10,410 [podnet.py] => Task 17, Epoch 132/160 (LR 0.00737) => LSC_loss 0.84, Spatial_loss 1.40, Flat_loss 0.16, Train_acc 77.79, Test_acc 42.79
2025-01-05 18:29:18,840 [podnet.py] => Task 17, Epoch 133/160 (LR 0.00686) => LSC_loss 0.81, Spatial_loss 1.38, Flat_loss 0.15, Train_acc 79.10, Test_acc 43.22
2025-01-05 18:29:27,218 [podnet.py] => Task 17, Epoch 134/160 (LR 0.00638) => LSC_loss 0.81, Spatial_loss 1.36, Flat_loss 0.15, Train_acc 79.04, Test_acc 43.40
2025-01-05 18:29:35,586 [podnet.py] => Task 17, Epoch 135/160 (LR 0.00590) => LSC_loss 0.80, Spatial_loss 1.35, Flat_loss 0.15, Train_acc 78.88, Test_acc 44.23
2025-01-05 18:29:43,996 [podnet.py] => Task 17, Epoch 136/160 (LR 0.00545) => LSC_loss 0.80, Spatial_loss 1.32, Flat_loss 0.15, Train_acc 79.01, Test_acc 43.48
2025-01-05 18:29:52,352 [podnet.py] => Task 17, Epoch 137/160 (LR 0.00501) => LSC_loss 0.79, Spatial_loss 1.32, Flat_loss 0.14, Train_acc 79.63, Test_acc 43.13
2025-01-05 18:30:00,699 [podnet.py] => Task 17, Epoch 138/160 (LR 0.00459) => LSC_loss 0.78, Spatial_loss 1.27, Flat_loss 0.14, Train_acc 80.04, Test_acc 44.08
2025-01-05 18:30:08,889 [podnet.py] => Task 17, Epoch 139/160 (LR 0.00419) => LSC_loss 0.80, Spatial_loss 1.28, Flat_loss 0.14, Train_acc 79.33, Test_acc 43.42
2025-01-05 18:30:17,222 [podnet.py] => Task 17, Epoch 140/160 (LR 0.00381) => LSC_loss 0.77, Spatial_loss 1.25, Flat_loss 0.13, Train_acc 80.47, Test_acc 44.20
2025-01-05 18:30:25,601 [podnet.py] => Task 17, Epoch 141/160 (LR 0.00344) => LSC_loss 0.77, Spatial_loss 1.24, Flat_loss 0.13, Train_acc 79.96, Test_acc 44.31
2025-01-05 18:30:33,943 [podnet.py] => Task 17, Epoch 142/160 (LR 0.00309) => LSC_loss 0.76, Spatial_loss 1.24, Flat_loss 0.13, Train_acc 80.46, Test_acc 43.50
2025-01-05 18:30:42,297 [podnet.py] => Task 17, Epoch 143/160 (LR 0.00276) => LSC_loss 0.76, Spatial_loss 1.21, Flat_loss 0.13, Train_acc 80.84, Test_acc 44.40
2025-01-05 18:30:50,519 [podnet.py] => Task 17, Epoch 144/160 (LR 0.00245) => LSC_loss 0.75, Spatial_loss 1.21, Flat_loss 0.12, Train_acc 80.98, Test_acc 44.02
2025-01-05 18:30:58,895 [podnet.py] => Task 17, Epoch 145/160 (LR 0.00215) => LSC_loss 0.74, Spatial_loss 1.19, Flat_loss 0.12, Train_acc 81.02, Test_acc 44.59
2025-01-05 18:31:07,209 [podnet.py] => Task 17, Epoch 146/160 (LR 0.00188) => LSC_loss 0.74, Spatial_loss 1.18, Flat_loss 0.12, Train_acc 80.87, Test_acc 44.90
2025-01-05 18:31:15,531 [podnet.py] => Task 17, Epoch 147/160 (LR 0.00162) => LSC_loss 0.74, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 81.48, Test_acc 44.96
2025-01-05 18:31:24,056 [podnet.py] => Task 17, Epoch 148/160 (LR 0.00138) => LSC_loss 0.73, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 81.22, Test_acc 44.81
2025-01-05 18:31:32,472 [podnet.py] => Task 17, Epoch 149/160 (LR 0.00116) => LSC_loss 0.74, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 81.21, Test_acc 44.49
2025-01-05 18:31:40,809 [podnet.py] => Task 17, Epoch 150/160 (LR 0.00096) => LSC_loss 0.73, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 81.22, Test_acc 44.43
2025-01-05 18:31:49,312 [podnet.py] => Task 17, Epoch 151/160 (LR 0.00078) => LSC_loss 0.73, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 81.71, Test_acc 44.60
2025-01-05 18:31:57,626 [podnet.py] => Task 17, Epoch 152/160 (LR 0.00062) => LSC_loss 0.71, Spatial_loss 1.11, Flat_loss 0.11, Train_acc 81.77, Test_acc 44.51
2025-01-05 18:32:06,012 [podnet.py] => Task 17, Epoch 153/160 (LR 0.00047) => LSC_loss 0.72, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 81.69, Test_acc 44.86
2025-01-05 18:32:14,404 [podnet.py] => Task 17, Epoch 154/160 (LR 0.00035) => LSC_loss 0.73, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 81.68, Test_acc 44.53
2025-01-05 18:32:22,846 [podnet.py] => Task 17, Epoch 155/160 (LR 0.00024) => LSC_loss 0.72, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 81.79, Test_acc 44.59
2025-01-05 18:32:31,165 [podnet.py] => Task 17, Epoch 156/160 (LR 0.00015) => LSC_loss 0.71, Spatial_loss 1.07, Flat_loss 0.11, Train_acc 82.30, Test_acc 44.60
2025-01-05 18:32:39,709 [podnet.py] => Task 17, Epoch 157/160 (LR 0.00009) => LSC_loss 0.72, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 81.99, Test_acc 44.54
2025-01-05 18:32:47,915 [podnet.py] => Task 17, Epoch 158/160 (LR 0.00004) => LSC_loss 0.71, Spatial_loss 1.08, Flat_loss 0.11, Train_acc 82.15, Test_acc 44.63
2025-01-05 18:32:56,166 [podnet.py] => Task 17, Epoch 159/160 (LR 0.00001) => LSC_loss 0.71, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 81.86, Test_acc 44.79
2025-01-05 18:33:04,407 [podnet.py] => Task 17, Epoch 160/160 (LR 0.00000) => LSC_loss 0.71, Spatial_loss 1.09, Flat_loss 0.11, Train_acc 82.43, Test_acc 44.51
2025-01-05 18:33:04,408 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 18:33:04,408 [base.py] => Reducing exemplars...(158 per classes)
2025-01-05 18:34:15,570 [base.py] => Constructing exemplars...(158 per classes)
2025-01-05 18:34:24,403 [podnet.py] => The size of finetune dataset: 14220
2025-01-05 18:34:32,209 [podnet.py] => Task 17, Epoch 1/20 (LR 0.00497) => LSC_loss 0.67, Spatial_loss 1.25, Flat_loss 0.11, Train_acc 83.21, Test_acc 43.68
2025-01-05 18:34:39,998 [podnet.py] => Task 17, Epoch 2/20 (LR 0.00488) => LSC_loss 0.68, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 82.86, Test_acc 43.57
2025-01-05 18:34:47,476 [podnet.py] => Task 17, Epoch 3/20 (LR 0.00473) => LSC_loss 0.68, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 83.19, Test_acc 44.39
2025-01-05 18:34:55,081 [podnet.py] => Task 17, Epoch 4/20 (LR 0.00452) => LSC_loss 0.69, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 82.31, Test_acc 44.57
2025-01-05 18:35:02,834 [podnet.py] => Task 17, Epoch 5/20 (LR 0.00427) => LSC_loss 0.67, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 83.47, Test_acc 44.30
2025-01-05 18:35:10,511 [podnet.py] => Task 17, Epoch 6/20 (LR 0.00397) => LSC_loss 0.66, Spatial_loss 1.25, Flat_loss 0.11, Train_acc 83.52, Test_acc 44.09
2025-01-05 18:35:18,419 [podnet.py] => Task 17, Epoch 7/20 (LR 0.00363) => LSC_loss 0.67, Spatial_loss 1.24, Flat_loss 0.10, Train_acc 83.28, Test_acc 43.49
2025-01-05 18:35:26,185 [podnet.py] => Task 17, Epoch 8/20 (LR 0.00327) => LSC_loss 0.67, Spatial_loss 1.20, Flat_loss 0.10, Train_acc 83.35, Test_acc 44.01
2025-01-05 18:35:33,846 [podnet.py] => Task 17, Epoch 9/20 (LR 0.00289) => LSC_loss 0.65, Spatial_loss 1.19, Flat_loss 0.10, Train_acc 83.97, Test_acc 44.36
2025-01-05 18:35:41,526 [podnet.py] => Task 17, Epoch 10/20 (LR 0.00250) => LSC_loss 0.65, Spatial_loss 1.19, Flat_loss 0.09, Train_acc 84.01, Test_acc 44.29
2025-01-05 18:35:49,296 [podnet.py] => Task 17, Epoch 11/20 (LR 0.00211) => LSC_loss 0.64, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 84.31, Test_acc 44.56
2025-01-05 18:35:56,966 [podnet.py] => Task 17, Epoch 12/20 (LR 0.00173) => LSC_loss 0.64, Spatial_loss 1.15, Flat_loss 0.09, Train_acc 84.28, Test_acc 44.06
2025-01-05 18:36:04,722 [podnet.py] => Task 17, Epoch 13/20 (LR 0.00137) => LSC_loss 0.64, Spatial_loss 1.12, Flat_loss 0.09, Train_acc 84.23, Test_acc 44.87
2025-01-05 18:36:12,261 [podnet.py] => Task 17, Epoch 14/20 (LR 0.00103) => LSC_loss 0.62, Spatial_loss 1.10, Flat_loss 0.09, Train_acc 85.24, Test_acc 44.56
2025-01-05 18:36:20,072 [podnet.py] => Task 17, Epoch 15/20 (LR 0.00073) => LSC_loss 0.62, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 84.97, Test_acc 44.62
2025-01-05 18:36:27,837 [podnet.py] => Task 17, Epoch 16/20 (LR 0.00048) => LSC_loss 0.61, Spatial_loss 1.06, Flat_loss 0.08, Train_acc 85.63, Test_acc 44.43
2025-01-05 18:36:35,381 [podnet.py] => Task 17, Epoch 17/20 (LR 0.00027) => LSC_loss 0.62, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 84.92, Test_acc 44.46
2025-01-05 18:36:43,169 [podnet.py] => Task 17, Epoch 18/20 (LR 0.00012) => LSC_loss 0.61, Spatial_loss 1.03, Flat_loss 0.08, Train_acc 85.26, Test_acc 44.28
2025-01-05 18:36:50,696 [podnet.py] => Task 17, Epoch 19/20 (LR 0.00003) => LSC_loss 0.61, Spatial_loss 1.07, Flat_loss 0.08, Train_acc 85.25, Test_acc 44.58
2025-01-05 18:36:58,403 [podnet.py] => Task 17, Epoch 20/20 (LR 0.00000) => LSC_loss 0.61, Spatial_loss 1.05, Flat_loss 0.08, Train_acc 85.45, Test_acc 44.78
2025-01-05 18:36:58,407 [base.py] => Reducing exemplars...(149 per classes)
2025-01-05 18:38:06,998 [base.py] => Constructing exemplars...(149 per classes)
2025-01-05 18:38:18,440 [podnet.py] => Exemplar size: 13410
2025-01-05 18:38:18,440 [trainer.py] => CNN: {'total': np.float64(44.78), '00-09': np.float64(57.5), '10-19': np.float64(35.4), '20-29': np.float64(51.0), '30-39': np.float64(41.3), '40-49': np.float64(54.1), '50-59': np.float64(39.1), '60-69': np.float64(47.8), '70-79': np.float64(38.8), '80-89': np.float64(38.0), 'old': np.float64(45.73), 'new': np.float64(28.6)}
2025-01-05 18:38:18,440 [trainer.py] => NME: {'total': np.float64(43.64), '00-09': np.float64(59.1), '10-19': np.float64(35.3), '20-29': np.float64(51.1), '30-39': np.float64(45.1), '40-49': np.float64(50.9), '50-59': np.float64(38.0), '60-69': np.float64(42.1), '70-79': np.float64(36.3), '80-89': np.float64(34.9), 'old': np.float64(44.72), 'new': np.float64(25.4)}
2025-01-05 18:38:18,440 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96), np.float64(49.84), np.float64(47.7), np.float64(46.51), np.float64(44.78)]
2025-01-05 18:38:18,441 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43), np.float64(77.19), np.float64(76.28), np.float64(75.69), np.float64(73.81)]
2025-01-05 18:38:18,441 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96), np.float64(48.09), np.float64(46.44), np.float64(45.32), np.float64(43.64)]
2025-01-05 18:38:18,441 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2), np.float64(75.59), np.float64(75.25), np.float64(74.59), np.float64(72.57)]

2025-01-05 18:38:18,441 [trainer.py] => All params: 523857
2025-01-05 18:38:18,441 [trainer.py] => Trainable params: 523857
2025-01-05 18:38:18,442 [podnet.py] => Learning on 90-95
2025-01-05 18:38:18,494 [podnet.py] => Adaptive factor: 4.358898943540674
2025-01-05 18:38:26,614 [podnet.py] => Task 18, Epoch 1/160 (LR 0.09999) => LSC_loss 2.84, Spatial_loss 4.09, Flat_loss 1.11, Train_acc 33.59, Test_acc 29.86
2025-01-05 18:38:34,967 [podnet.py] => Task 18, Epoch 2/160 (LR 0.09996) => LSC_loss 2.28, Spatial_loss 3.50, Flat_loss 0.81, Train_acc 41.24, Test_acc 33.24
2025-01-05 18:38:43,305 [podnet.py] => Task 18, Epoch 3/160 (LR 0.09991) => LSC_loss 2.14, Spatial_loss 3.33, Flat_loss 0.73, Train_acc 45.11, Test_acc 29.96
2025-01-05 18:38:51,563 [podnet.py] => Task 18, Epoch 4/160 (LR 0.09985) => LSC_loss 2.08, Spatial_loss 3.15, Flat_loss 0.69, Train_acc 46.60, Test_acc 31.64
2025-01-05 18:38:59,882 [podnet.py] => Task 18, Epoch 5/160 (LR 0.09976) => LSC_loss 2.03, Spatial_loss 3.16, Flat_loss 0.67, Train_acc 47.25, Test_acc 29.14
2025-01-05 18:39:08,493 [podnet.py] => Task 18, Epoch 6/160 (LR 0.09965) => LSC_loss 2.02, Spatial_loss 3.13, Flat_loss 0.66, Train_acc 47.49, Test_acc 32.07
2025-01-05 18:39:17,007 [podnet.py] => Task 18, Epoch 7/160 (LR 0.09953) => LSC_loss 1.98, Spatial_loss 3.02, Flat_loss 0.64, Train_acc 48.63, Test_acc 33.78
2025-01-05 18:39:25,503 [podnet.py] => Task 18, Epoch 8/160 (LR 0.09938) => LSC_loss 1.96, Spatial_loss 3.08, Flat_loss 0.64, Train_acc 49.00, Test_acc 30.95
2025-01-05 18:39:33,942 [podnet.py] => Task 18, Epoch 9/160 (LR 0.09922) => LSC_loss 1.98, Spatial_loss 3.10, Flat_loss 0.65, Train_acc 48.27, Test_acc 32.72
2025-01-05 18:39:42,226 [podnet.py] => Task 18, Epoch 10/160 (LR 0.09904) => LSC_loss 1.92, Spatial_loss 3.01, Flat_loss 0.62, Train_acc 49.81, Test_acc 33.81
2025-01-05 18:39:50,571 [podnet.py] => Task 18, Epoch 11/160 (LR 0.09884) => LSC_loss 1.90, Spatial_loss 2.97, Flat_loss 0.61, Train_acc 49.74, Test_acc 33.82
2025-01-05 18:39:59,050 [podnet.py] => Task 18, Epoch 12/160 (LR 0.09862) => LSC_loss 1.93, Spatial_loss 3.04, Flat_loss 0.63, Train_acc 49.94, Test_acc 29.31
2025-01-05 18:40:07,302 [podnet.py] => Task 18, Epoch 13/160 (LR 0.09838) => LSC_loss 1.90, Spatial_loss 2.99, Flat_loss 0.61, Train_acc 50.15, Test_acc 31.21
2025-01-05 18:40:15,746 [podnet.py] => Task 18, Epoch 14/160 (LR 0.09812) => LSC_loss 1.89, Spatial_loss 3.01, Flat_loss 0.61, Train_acc 50.71, Test_acc 36.24
2025-01-05 18:40:24,048 [podnet.py] => Task 18, Epoch 15/160 (LR 0.09785) => LSC_loss 1.88, Spatial_loss 2.99, Flat_loss 0.60, Train_acc 51.12, Test_acc 33.82
2025-01-05 18:40:32,452 [podnet.py] => Task 18, Epoch 16/160 (LR 0.09755) => LSC_loss 1.89, Spatial_loss 2.99, Flat_loss 0.61, Train_acc 50.71, Test_acc 31.49
2025-01-05 18:40:40,959 [podnet.py] => Task 18, Epoch 17/160 (LR 0.09724) => LSC_loss 1.88, Spatial_loss 2.94, Flat_loss 0.60, Train_acc 50.91, Test_acc 32.23
2025-01-05 18:40:49,381 [podnet.py] => Task 18, Epoch 18/160 (LR 0.09691) => LSC_loss 1.86, Spatial_loss 3.02, Flat_loss 0.60, Train_acc 51.52, Test_acc 35.01
2025-01-05 18:40:57,759 [podnet.py] => Task 18, Epoch 19/160 (LR 0.09656) => LSC_loss 1.87, Spatial_loss 2.99, Flat_loss 0.61, Train_acc 51.10, Test_acc 33.02
2025-01-05 18:41:06,074 [podnet.py] => Task 18, Epoch 20/160 (LR 0.09619) => LSC_loss 1.85, Spatial_loss 2.95, Flat_loss 0.59, Train_acc 51.47, Test_acc 28.39
2025-01-05 18:41:14,403 [podnet.py] => Task 18, Epoch 21/160 (LR 0.09581) => LSC_loss 1.86, Spatial_loss 2.94, Flat_loss 0.60, Train_acc 51.21, Test_acc 32.82
2025-01-05 18:41:22,663 [podnet.py] => Task 18, Epoch 22/160 (LR 0.09541) => LSC_loss 1.82, Spatial_loss 2.88, Flat_loss 0.58, Train_acc 52.21, Test_acc 30.31
2025-01-05 18:41:31,154 [podnet.py] => Task 18, Epoch 23/160 (LR 0.09499) => LSC_loss 1.85, Spatial_loss 2.99, Flat_loss 0.60, Train_acc 51.04, Test_acc 30.03
2025-01-05 18:41:39,594 [podnet.py] => Task 18, Epoch 24/160 (LR 0.09455) => LSC_loss 1.86, Spatial_loss 2.92, Flat_loss 0.60, Train_acc 51.24, Test_acc 33.99
2025-01-05 18:41:47,986 [podnet.py] => Task 18, Epoch 25/160 (LR 0.09410) => LSC_loss 1.84, Spatial_loss 2.92, Flat_loss 0.59, Train_acc 51.55, Test_acc 29.67
2025-01-05 18:41:56,272 [podnet.py] => Task 18, Epoch 26/160 (LR 0.09362) => LSC_loss 1.80, Spatial_loss 2.88, Flat_loss 0.59, Train_acc 52.97, Test_acc 33.02
2025-01-05 18:42:04,838 [podnet.py] => Task 18, Epoch 27/160 (LR 0.09314) => LSC_loss 1.80, Spatial_loss 2.89, Flat_loss 0.58, Train_acc 52.98, Test_acc 21.49
2025-01-05 18:42:13,200 [podnet.py] => Task 18, Epoch 28/160 (LR 0.09263) => LSC_loss 1.83, Spatial_loss 2.97, Flat_loss 0.60, Train_acc 51.90, Test_acc 35.08
2025-01-05 18:42:21,698 [podnet.py] => Task 18, Epoch 29/160 (LR 0.09211) => LSC_loss 1.84, Spatial_loss 2.91, Flat_loss 0.59, Train_acc 51.77, Test_acc 34.87
2025-01-05 18:42:30,049 [podnet.py] => Task 18, Epoch 30/160 (LR 0.09157) => LSC_loss 1.81, Spatial_loss 2.87, Flat_loss 0.58, Train_acc 52.33, Test_acc 34.27
2025-01-05 18:42:38,483 [podnet.py] => Task 18, Epoch 31/160 (LR 0.09102) => LSC_loss 1.77, Spatial_loss 2.84, Flat_loss 0.57, Train_acc 53.61, Test_acc 25.38
2025-01-05 18:42:46,841 [podnet.py] => Task 18, Epoch 32/160 (LR 0.09045) => LSC_loss 1.81, Spatial_loss 2.95, Flat_loss 0.59, Train_acc 51.89, Test_acc 34.24
2025-01-05 18:42:55,342 [podnet.py] => Task 18, Epoch 33/160 (LR 0.08987) => LSC_loss 1.78, Spatial_loss 2.88, Flat_loss 0.58, Train_acc 53.36, Test_acc 31.51
2025-01-05 18:43:03,736 [podnet.py] => Task 18, Epoch 34/160 (LR 0.08927) => LSC_loss 1.78, Spatial_loss 2.85, Flat_loss 0.57, Train_acc 53.02, Test_acc 35.17
2025-01-05 18:43:11,977 [podnet.py] => Task 18, Epoch 35/160 (LR 0.08865) => LSC_loss 1.79, Spatial_loss 2.87, Flat_loss 0.57, Train_acc 52.68, Test_acc 31.02
2025-01-05 18:43:20,325 [podnet.py] => Task 18, Epoch 36/160 (LR 0.08802) => LSC_loss 1.79, Spatial_loss 2.87, Flat_loss 0.58, Train_acc 52.81, Test_acc 34.38
2025-01-05 18:43:28,642 [podnet.py] => Task 18, Epoch 37/160 (LR 0.08738) => LSC_loss 1.79, Spatial_loss 2.89, Flat_loss 0.58, Train_acc 52.48, Test_acc 33.37
2025-01-05 18:43:37,016 [podnet.py] => Task 18, Epoch 38/160 (LR 0.08672) => LSC_loss 1.77, Spatial_loss 2.83, Flat_loss 0.56, Train_acc 53.60, Test_acc 32.49
2025-01-05 18:43:45,333 [podnet.py] => Task 18, Epoch 39/160 (LR 0.08604) => LSC_loss 1.77, Spatial_loss 2.85, Flat_loss 0.56, Train_acc 53.78, Test_acc 32.66
2025-01-05 18:43:53,423 [podnet.py] => Task 18, Epoch 40/160 (LR 0.08536) => LSC_loss 1.76, Spatial_loss 2.84, Flat_loss 0.56, Train_acc 53.09, Test_acc 32.55
2025-01-05 18:44:01,923 [podnet.py] => Task 18, Epoch 41/160 (LR 0.08465) => LSC_loss 1.75, Spatial_loss 2.87, Flat_loss 0.56, Train_acc 53.93, Test_acc 29.88
2025-01-05 18:44:10,363 [podnet.py] => Task 18, Epoch 42/160 (LR 0.08394) => LSC_loss 1.79, Spatial_loss 2.90, Flat_loss 0.57, Train_acc 52.96, Test_acc 36.78
2025-01-05 18:44:18,793 [podnet.py] => Task 18, Epoch 43/160 (LR 0.08321) => LSC_loss 1.74, Spatial_loss 2.82, Flat_loss 0.56, Train_acc 54.27, Test_acc 33.20
2025-01-05 18:44:27,191 [podnet.py] => Task 18, Epoch 44/160 (LR 0.08247) => LSC_loss 1.74, Spatial_loss 2.79, Flat_loss 0.55, Train_acc 54.37, Test_acc 32.80
2025-01-05 18:44:35,494 [podnet.py] => Task 18, Epoch 45/160 (LR 0.08172) => LSC_loss 1.74, Spatial_loss 2.83, Flat_loss 0.55, Train_acc 54.22, Test_acc 31.87
2025-01-05 18:44:43,791 [podnet.py] => Task 18, Epoch 46/160 (LR 0.08095) => LSC_loss 1.73, Spatial_loss 2.77, Flat_loss 0.55, Train_acc 53.80, Test_acc 33.28
2025-01-05 18:44:52,348 [podnet.py] => Task 18, Epoch 47/160 (LR 0.08018) => LSC_loss 1.73, Spatial_loss 2.80, Flat_loss 0.54, Train_acc 54.67, Test_acc 33.53
2025-01-05 18:45:00,680 [podnet.py] => Task 18, Epoch 48/160 (LR 0.07939) => LSC_loss 1.73, Spatial_loss 2.74, Flat_loss 0.54, Train_acc 54.72, Test_acc 34.06
2025-01-05 18:45:09,237 [podnet.py] => Task 18, Epoch 49/160 (LR 0.07859) => LSC_loss 1.70, Spatial_loss 2.78, Flat_loss 0.54, Train_acc 54.50, Test_acc 29.33
2025-01-05 18:45:17,654 [podnet.py] => Task 18, Epoch 50/160 (LR 0.07778) => LSC_loss 1.73, Spatial_loss 2.77, Flat_loss 0.55, Train_acc 54.41, Test_acc 36.61
2025-01-05 18:45:26,059 [podnet.py] => Task 18, Epoch 51/160 (LR 0.07696) => LSC_loss 1.71, Spatial_loss 2.78, Flat_loss 0.54, Train_acc 54.82, Test_acc 35.26
2025-01-05 18:45:34,300 [podnet.py] => Task 18, Epoch 52/160 (LR 0.07612) => LSC_loss 1.70, Spatial_loss 2.76, Flat_loss 0.53, Train_acc 55.11, Test_acc 35.08
2025-01-05 18:45:42,735 [podnet.py] => Task 18, Epoch 53/160 (LR 0.07528) => LSC_loss 1.68, Spatial_loss 2.77, Flat_loss 0.53, Train_acc 55.36, Test_acc 34.67
2025-01-05 18:45:50,909 [podnet.py] => Task 18, Epoch 54/160 (LR 0.07443) => LSC_loss 1.70, Spatial_loss 2.79, Flat_loss 0.54, Train_acc 55.09, Test_acc 33.38
2025-01-05 18:45:58,951 [podnet.py] => Task 18, Epoch 55/160 (LR 0.07357) => LSC_loss 1.70, Spatial_loss 2.76, Flat_loss 0.54, Train_acc 55.17, Test_acc 34.14
2025-01-05 18:46:07,429 [podnet.py] => Task 18, Epoch 56/160 (LR 0.07270) => LSC_loss 1.69, Spatial_loss 2.74, Flat_loss 0.53, Train_acc 55.12, Test_acc 37.09
2025-01-05 18:46:15,816 [podnet.py] => Task 18, Epoch 57/160 (LR 0.07182) => LSC_loss 1.68, Spatial_loss 2.76, Flat_loss 0.53, Train_acc 55.82, Test_acc 32.66
2025-01-05 18:46:24,310 [podnet.py] => Task 18, Epoch 58/160 (LR 0.07093) => LSC_loss 1.66, Spatial_loss 2.72, Flat_loss 0.52, Train_acc 56.25, Test_acc 34.68
2025-01-05 18:46:32,741 [podnet.py] => Task 18, Epoch 59/160 (LR 0.07004) => LSC_loss 1.67, Spatial_loss 2.68, Flat_loss 0.52, Train_acc 55.69, Test_acc 36.44
2025-01-05 18:46:41,192 [podnet.py] => Task 18, Epoch 60/160 (LR 0.06913) => LSC_loss 1.64, Spatial_loss 2.63, Flat_loss 0.50, Train_acc 56.57, Test_acc 29.75
2025-01-05 18:46:49,665 [podnet.py] => Task 18, Epoch 61/160 (LR 0.06822) => LSC_loss 1.64, Spatial_loss 2.67, Flat_loss 0.51, Train_acc 56.88, Test_acc 36.32
2025-01-05 18:46:58,170 [podnet.py] => Task 18, Epoch 62/160 (LR 0.06731) => LSC_loss 1.62, Spatial_loss 2.70, Flat_loss 0.51, Train_acc 56.78, Test_acc 34.63
2025-01-05 18:47:06,406 [podnet.py] => Task 18, Epoch 63/160 (LR 0.06638) => LSC_loss 1.62, Spatial_loss 2.65, Flat_loss 0.50, Train_acc 56.66, Test_acc 34.63
2025-01-05 18:47:14,722 [podnet.py] => Task 18, Epoch 64/160 (LR 0.06545) => LSC_loss 1.62, Spatial_loss 2.60, Flat_loss 0.49, Train_acc 56.69, Test_acc 33.80
2025-01-05 18:47:23,228 [podnet.py] => Task 18, Epoch 65/160 (LR 0.06451) => LSC_loss 1.61, Spatial_loss 2.66, Flat_loss 0.50, Train_acc 57.02, Test_acc 36.29
2025-01-05 18:47:31,696 [podnet.py] => Task 18, Epoch 66/160 (LR 0.06357) => LSC_loss 1.59, Spatial_loss 2.63, Flat_loss 0.49, Train_acc 57.83, Test_acc 31.36
2025-01-05 18:47:40,182 [podnet.py] => Task 18, Epoch 67/160 (LR 0.06262) => LSC_loss 1.57, Spatial_loss 2.57, Flat_loss 0.49, Train_acc 58.33, Test_acc 35.95
2025-01-05 18:47:48,281 [podnet.py] => Task 18, Epoch 68/160 (LR 0.06167) => LSC_loss 1.58, Spatial_loss 2.66, Flat_loss 0.49, Train_acc 58.18, Test_acc 37.93
2025-01-05 18:47:56,732 [podnet.py] => Task 18, Epoch 69/160 (LR 0.06072) => LSC_loss 1.57, Spatial_loss 2.56, Flat_loss 0.48, Train_acc 58.29, Test_acc 32.45
2025-01-05 18:48:05,025 [podnet.py] => Task 18, Epoch 70/160 (LR 0.05975) => LSC_loss 1.56, Spatial_loss 2.59, Flat_loss 0.48, Train_acc 57.98, Test_acc 37.55
2025-01-05 18:48:13,256 [podnet.py] => Task 18, Epoch 71/160 (LR 0.05879) => LSC_loss 1.54, Spatial_loss 2.56, Flat_loss 0.47, Train_acc 59.08, Test_acc 33.46
2025-01-05 18:48:21,869 [podnet.py] => Task 18, Epoch 72/160 (LR 0.05782) => LSC_loss 1.55, Spatial_loss 2.55, Flat_loss 0.47, Train_acc 59.10, Test_acc 37.15
2025-01-05 18:48:30,051 [podnet.py] => Task 18, Epoch 73/160 (LR 0.05685) => LSC_loss 1.55, Spatial_loss 2.61, Flat_loss 0.48, Train_acc 58.50, Test_acc 33.56
2025-01-05 18:48:38,438 [podnet.py] => Task 18, Epoch 74/160 (LR 0.05588) => LSC_loss 1.52, Spatial_loss 2.54, Flat_loss 0.46, Train_acc 59.03, Test_acc 36.28
2025-01-05 18:48:46,862 [podnet.py] => Task 18, Epoch 75/160 (LR 0.05490) => LSC_loss 1.50, Spatial_loss 2.51, Flat_loss 0.46, Train_acc 60.47, Test_acc 37.95
2025-01-05 18:48:55,106 [podnet.py] => Task 18, Epoch 76/160 (LR 0.05392) => LSC_loss 1.53, Spatial_loss 2.52, Flat_loss 0.46, Train_acc 59.37, Test_acc 34.96
2025-01-05 18:49:03,404 [podnet.py] => Task 18, Epoch 77/160 (LR 0.05294) => LSC_loss 1.53, Spatial_loss 2.52, Flat_loss 0.46, Train_acc 59.31, Test_acc 37.68
2025-01-05 18:49:11,837 [podnet.py] => Task 18, Epoch 78/160 (LR 0.05196) => LSC_loss 1.48, Spatial_loss 2.50, Flat_loss 0.44, Train_acc 60.70, Test_acc 34.29
2025-01-05 18:49:20,081 [podnet.py] => Task 18, Epoch 79/160 (LR 0.05098) => LSC_loss 1.49, Spatial_loss 2.48, Flat_loss 0.44, Train_acc 60.11, Test_acc 37.75
2025-01-05 18:49:28,382 [podnet.py] => Task 18, Epoch 80/160 (LR 0.05000) => LSC_loss 1.46, Spatial_loss 2.46, Flat_loss 0.43, Train_acc 60.70, Test_acc 34.51
2025-01-05 18:49:36,836 [podnet.py] => Task 18, Epoch 81/160 (LR 0.04902) => LSC_loss 1.47, Spatial_loss 2.43, Flat_loss 0.44, Train_acc 60.34, Test_acc 34.93
2025-01-05 18:49:45,284 [podnet.py] => Task 18, Epoch 82/160 (LR 0.04804) => LSC_loss 1.46, Spatial_loss 2.47, Flat_loss 0.44, Train_acc 60.92, Test_acc 35.11
2025-01-05 18:49:53,696 [podnet.py] => Task 18, Epoch 83/160 (LR 0.04706) => LSC_loss 1.45, Spatial_loss 2.39, Flat_loss 0.42, Train_acc 61.16, Test_acc 38.18
2025-01-05 18:50:01,972 [podnet.py] => Task 18, Epoch 84/160 (LR 0.04608) => LSC_loss 1.45, Spatial_loss 2.39, Flat_loss 0.42, Train_acc 60.77, Test_acc 36.87
2025-01-05 18:50:10,548 [podnet.py] => Task 18, Epoch 85/160 (LR 0.04510) => LSC_loss 1.43, Spatial_loss 2.38, Flat_loss 0.42, Train_acc 61.40, Test_acc 34.58
2025-01-05 18:50:19,026 [podnet.py] => Task 18, Epoch 86/160 (LR 0.04412) => LSC_loss 1.40, Spatial_loss 2.38, Flat_loss 0.41, Train_acc 62.54, Test_acc 35.58
2025-01-05 18:50:27,238 [podnet.py] => Task 18, Epoch 87/160 (LR 0.04315) => LSC_loss 1.40, Spatial_loss 2.34, Flat_loss 0.41, Train_acc 62.26, Test_acc 38.37
2025-01-05 18:50:35,542 [podnet.py] => Task 18, Epoch 88/160 (LR 0.04218) => LSC_loss 1.39, Spatial_loss 2.34, Flat_loss 0.40, Train_acc 62.61, Test_acc 37.04
2025-01-05 18:50:43,669 [podnet.py] => Task 18, Epoch 89/160 (LR 0.04121) => LSC_loss 1.38, Spatial_loss 2.30, Flat_loss 0.40, Train_acc 62.91, Test_acc 37.71
2025-01-05 18:50:51,951 [podnet.py] => Task 18, Epoch 90/160 (LR 0.04025) => LSC_loss 1.38, Spatial_loss 2.30, Flat_loss 0.40, Train_acc 63.14, Test_acc 36.78
2025-01-05 18:51:00,312 [podnet.py] => Task 18, Epoch 91/160 (LR 0.03928) => LSC_loss 1.35, Spatial_loss 2.31, Flat_loss 0.39, Train_acc 63.95, Test_acc 38.78
2025-01-05 18:51:08,578 [podnet.py] => Task 18, Epoch 92/160 (LR 0.03833) => LSC_loss 1.38, Spatial_loss 2.32, Flat_loss 0.39, Train_acc 63.16, Test_acc 33.40
2025-01-05 18:51:16,776 [podnet.py] => Task 18, Epoch 93/160 (LR 0.03738) => LSC_loss 1.34, Spatial_loss 2.29, Flat_loss 0.38, Train_acc 64.05, Test_acc 38.52
2025-01-05 18:51:25,291 [podnet.py] => Task 18, Epoch 94/160 (LR 0.03643) => LSC_loss 1.33, Spatial_loss 2.23, Flat_loss 0.37, Train_acc 64.75, Test_acc 38.48
2025-01-05 18:51:33,803 [podnet.py] => Task 18, Epoch 95/160 (LR 0.03549) => LSC_loss 1.30, Spatial_loss 2.20, Flat_loss 0.37, Train_acc 65.02, Test_acc 36.77
2025-01-05 18:51:42,186 [podnet.py] => Task 18, Epoch 96/160 (LR 0.03455) => LSC_loss 1.27, Spatial_loss 2.20, Flat_loss 0.36, Train_acc 65.96, Test_acc 39.07
2025-01-05 18:51:50,392 [podnet.py] => Task 18, Epoch 97/160 (LR 0.03362) => LSC_loss 1.27, Spatial_loss 2.19, Flat_loss 0.35, Train_acc 65.91, Test_acc 38.00
2025-01-05 18:51:58,880 [podnet.py] => Task 18, Epoch 98/160 (LR 0.03269) => LSC_loss 1.29, Spatial_loss 2.23, Flat_loss 0.36, Train_acc 65.12, Test_acc 38.86
2025-01-05 18:52:07,335 [podnet.py] => Task 18, Epoch 99/160 (LR 0.03178) => LSC_loss 1.26, Spatial_loss 2.16, Flat_loss 0.35, Train_acc 66.08, Test_acc 36.27
2025-01-05 18:52:15,512 [podnet.py] => Task 18, Epoch 100/160 (LR 0.03087) => LSC_loss 1.24, Spatial_loss 2.09, Flat_loss 0.34, Train_acc 66.30, Test_acc 39.20
2025-01-05 18:52:23,881 [podnet.py] => Task 18, Epoch 101/160 (LR 0.02996) => LSC_loss 1.23, Spatial_loss 2.10, Flat_loss 0.34, Train_acc 66.54, Test_acc 38.84
2025-01-05 18:52:32,247 [podnet.py] => Task 18, Epoch 102/160 (LR 0.02907) => LSC_loss 1.22, Spatial_loss 2.11, Flat_loss 0.33, Train_acc 66.74, Test_acc 38.60
2025-01-05 18:52:40,563 [podnet.py] => Task 18, Epoch 103/160 (LR 0.02818) => LSC_loss 1.22, Spatial_loss 2.07, Flat_loss 0.32, Train_acc 67.37, Test_acc 36.89
2025-01-05 18:52:48,834 [podnet.py] => Task 18, Epoch 104/160 (LR 0.02730) => LSC_loss 1.20, Spatial_loss 2.07, Flat_loss 0.32, Train_acc 67.95, Test_acc 38.79
2025-01-05 18:52:57,035 [podnet.py] => Task 18, Epoch 105/160 (LR 0.02643) => LSC_loss 1.18, Spatial_loss 2.05, Flat_loss 0.32, Train_acc 68.06, Test_acc 38.13
2025-01-05 18:53:05,644 [podnet.py] => Task 18, Epoch 106/160 (LR 0.02557) => LSC_loss 1.19, Spatial_loss 2.04, Flat_loss 0.31, Train_acc 68.22, Test_acc 39.17
2025-01-05 18:53:14,112 [podnet.py] => Task 18, Epoch 107/160 (LR 0.02472) => LSC_loss 1.14, Spatial_loss 1.96, Flat_loss 0.30, Train_acc 68.86, Test_acc 39.48
2025-01-05 18:53:22,423 [podnet.py] => Task 18, Epoch 108/160 (LR 0.02388) => LSC_loss 1.14, Spatial_loss 1.93, Flat_loss 0.29, Train_acc 69.92, Test_acc 39.44
2025-01-05 18:53:30,982 [podnet.py] => Task 18, Epoch 109/160 (LR 0.02304) => LSC_loss 1.14, Spatial_loss 1.98, Flat_loss 0.30, Train_acc 69.42, Test_acc 39.41
2025-01-05 18:53:39,388 [podnet.py] => Task 18, Epoch 110/160 (LR 0.02222) => LSC_loss 1.12, Spatial_loss 1.93, Flat_loss 0.29, Train_acc 69.98, Test_acc 40.13
2025-01-05 18:53:47,639 [podnet.py] => Task 18, Epoch 111/160 (LR 0.02141) => LSC_loss 1.10, Spatial_loss 1.90, Flat_loss 0.28, Train_acc 70.36, Test_acc 38.03
2025-01-05 18:53:56,101 [podnet.py] => Task 18, Epoch 112/160 (LR 0.02061) => LSC_loss 1.10, Spatial_loss 1.91, Flat_loss 0.28, Train_acc 70.63, Test_acc 35.96
2025-01-05 18:54:04,547 [podnet.py] => Task 18, Epoch 113/160 (LR 0.01982) => LSC_loss 1.08, Spatial_loss 1.89, Flat_loss 0.27, Train_acc 71.34, Test_acc 40.99
2025-01-05 18:54:13,006 [podnet.py] => Task 18, Epoch 114/160 (LR 0.01905) => LSC_loss 1.07, Spatial_loss 1.88, Flat_loss 0.26, Train_acc 71.77, Test_acc 39.05
2025-01-05 18:54:21,242 [podnet.py] => Task 18, Epoch 115/160 (LR 0.01828) => LSC_loss 1.04, Spatial_loss 1.83, Flat_loss 0.26, Train_acc 72.37, Test_acc 40.84
2025-01-05 18:54:29,543 [podnet.py] => Task 18, Epoch 116/160 (LR 0.01753) => LSC_loss 1.03, Spatial_loss 1.80, Flat_loss 0.25, Train_acc 72.57, Test_acc 40.03
2025-01-05 18:54:38,008 [podnet.py] => Task 18, Epoch 117/160 (LR 0.01679) => LSC_loss 1.03, Spatial_loss 1.82, Flat_loss 0.25, Train_acc 72.82, Test_acc 38.42
2025-01-05 18:54:46,458 [podnet.py] => Task 18, Epoch 118/160 (LR 0.01606) => LSC_loss 1.00, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 73.52, Test_acc 39.75
2025-01-05 18:54:54,858 [podnet.py] => Task 18, Epoch 119/160 (LR 0.01535) => LSC_loss 1.00, Spatial_loss 1.75, Flat_loss 0.24, Train_acc 73.57, Test_acc 40.76
2025-01-05 18:55:03,261 [podnet.py] => Task 18, Epoch 120/160 (LR 0.01464) => LSC_loss 0.96, Spatial_loss 1.73, Flat_loss 0.23, Train_acc 74.74, Test_acc 41.09
2025-01-05 18:55:11,600 [podnet.py] => Task 18, Epoch 121/160 (LR 0.01396) => LSC_loss 0.96, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 74.46, Test_acc 40.31
2025-01-05 18:55:20,015 [podnet.py] => Task 18, Epoch 122/160 (LR 0.01328) => LSC_loss 0.95, Spatial_loss 1.68, Flat_loss 0.22, Train_acc 74.96, Test_acc 41.45
2025-01-05 18:55:28,447 [podnet.py] => Task 18, Epoch 123/160 (LR 0.01262) => LSC_loss 0.95, Spatial_loss 1.64, Flat_loss 0.21, Train_acc 75.18, Test_acc 40.29
2025-01-05 18:55:36,877 [podnet.py] => Task 18, Epoch 124/160 (LR 0.01198) => LSC_loss 0.92, Spatial_loss 1.60, Flat_loss 0.21, Train_acc 75.81, Test_acc 40.54
2025-01-05 18:55:45,002 [podnet.py] => Task 18, Epoch 125/160 (LR 0.01135) => LSC_loss 0.91, Spatial_loss 1.59, Flat_loss 0.20, Train_acc 76.09, Test_acc 41.16
2025-01-05 18:55:53,396 [podnet.py] => Task 18, Epoch 126/160 (LR 0.01073) => LSC_loss 0.90, Spatial_loss 1.59, Flat_loss 0.20, Train_acc 76.81, Test_acc 41.54
2025-01-05 18:56:01,820 [podnet.py] => Task 18, Epoch 127/160 (LR 0.01013) => LSC_loss 0.89, Spatial_loss 1.57, Flat_loss 0.19, Train_acc 77.03, Test_acc 41.26
2025-01-05 18:56:10,164 [podnet.py] => Task 18, Epoch 128/160 (LR 0.00955) => LSC_loss 0.89, Spatial_loss 1.55, Flat_loss 0.19, Train_acc 77.00, Test_acc 41.68
2025-01-05 18:56:18,659 [podnet.py] => Task 18, Epoch 129/160 (LR 0.00898) => LSC_loss 0.86, Spatial_loss 1.49, Flat_loss 0.18, Train_acc 77.46, Test_acc 42.43
2025-01-05 18:56:27,123 [podnet.py] => Task 18, Epoch 130/160 (LR 0.00843) => LSC_loss 0.86, Spatial_loss 1.50, Flat_loss 0.18, Train_acc 78.01, Test_acc 42.45
2025-01-05 18:56:35,364 [podnet.py] => Task 18, Epoch 131/160 (LR 0.00789) => LSC_loss 0.85, Spatial_loss 1.47, Flat_loss 0.17, Train_acc 78.18, Test_acc 42.56
2025-01-05 18:56:43,688 [podnet.py] => Task 18, Epoch 132/160 (LR 0.00737) => LSC_loss 0.85, Spatial_loss 1.48, Flat_loss 0.17, Train_acc 78.24, Test_acc 43.19
2025-01-05 18:56:52,019 [podnet.py] => Task 18, Epoch 133/160 (LR 0.00686) => LSC_loss 0.82, Spatial_loss 1.44, Flat_loss 0.17, Train_acc 79.37, Test_acc 42.17
2025-01-05 18:57:00,356 [podnet.py] => Task 18, Epoch 134/160 (LR 0.00638) => LSC_loss 0.83, Spatial_loss 1.45, Flat_loss 0.17, Train_acc 78.75, Test_acc 42.24
2025-01-05 18:57:08,641 [podnet.py] => Task 18, Epoch 135/160 (LR 0.00590) => LSC_loss 0.82, Spatial_loss 1.44, Flat_loss 0.16, Train_acc 79.18, Test_acc 42.73
2025-01-05 18:57:17,205 [podnet.py] => Task 18, Epoch 136/160 (LR 0.00545) => LSC_loss 0.82, Spatial_loss 1.40, Flat_loss 0.16, Train_acc 79.11, Test_acc 41.28
2025-01-05 18:57:25,528 [podnet.py] => Task 18, Epoch 137/160 (LR 0.00501) => LSC_loss 0.80, Spatial_loss 1.37, Flat_loss 0.15, Train_acc 79.58, Test_acc 42.87
2025-01-05 18:57:33,745 [podnet.py] => Task 18, Epoch 138/160 (LR 0.00459) => LSC_loss 0.79, Spatial_loss 1.33, Flat_loss 0.15, Train_acc 80.01, Test_acc 42.08
2025-01-05 18:57:42,025 [podnet.py] => Task 18, Epoch 139/160 (LR 0.00419) => LSC_loss 0.78, Spatial_loss 1.34, Flat_loss 0.15, Train_acc 80.36, Test_acc 42.07
2025-01-05 18:57:50,317 [podnet.py] => Task 18, Epoch 140/160 (LR 0.00381) => LSC_loss 0.77, Spatial_loss 1.34, Flat_loss 0.15, Train_acc 80.62, Test_acc 42.89
2025-01-05 18:57:58,610 [podnet.py] => Task 18, Epoch 141/160 (LR 0.00344) => LSC_loss 0.77, Spatial_loss 1.31, Flat_loss 0.14, Train_acc 80.63, Test_acc 42.82
2025-01-05 18:58:07,102 [podnet.py] => Task 18, Epoch 142/160 (LR 0.00309) => LSC_loss 0.77, Spatial_loss 1.28, Flat_loss 0.14, Train_acc 80.09, Test_acc 42.85
2025-01-05 18:58:15,438 [podnet.py] => Task 18, Epoch 143/160 (LR 0.00276) => LSC_loss 0.76, Spatial_loss 1.25, Flat_loss 0.13, Train_acc 81.02, Test_acc 43.48
2025-01-05 18:58:23,816 [podnet.py] => Task 18, Epoch 144/160 (LR 0.00245) => LSC_loss 0.74, Spatial_loss 1.26, Flat_loss 0.13, Train_acc 81.31, Test_acc 43.43
2025-01-05 18:58:32,357 [podnet.py] => Task 18, Epoch 145/160 (LR 0.00215) => LSC_loss 0.75, Spatial_loss 1.25, Flat_loss 0.13, Train_acc 81.28, Test_acc 42.91
2025-01-05 18:58:40,702 [podnet.py] => Task 18, Epoch 146/160 (LR 0.00188) => LSC_loss 0.74, Spatial_loss 1.22, Flat_loss 0.13, Train_acc 81.48, Test_acc 43.48
2025-01-05 18:58:49,154 [podnet.py] => Task 18, Epoch 147/160 (LR 0.00162) => LSC_loss 0.74, Spatial_loss 1.23, Flat_loss 0.13, Train_acc 81.65, Test_acc 43.35
2025-01-05 18:58:57,732 [podnet.py] => Task 18, Epoch 148/160 (LR 0.00138) => LSC_loss 0.74, Spatial_loss 1.20, Flat_loss 0.13, Train_acc 81.55, Test_acc 43.28
2025-01-05 18:59:06,347 [podnet.py] => Task 18, Epoch 149/160 (LR 0.00116) => LSC_loss 0.73, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 81.82, Test_acc 43.59
2025-01-05 18:59:14,748 [podnet.py] => Task 18, Epoch 150/160 (LR 0.00096) => LSC_loss 0.72, Spatial_loss 1.19, Flat_loss 0.12, Train_acc 82.20, Test_acc 43.59
2025-01-05 18:59:23,432 [podnet.py] => Task 18, Epoch 151/160 (LR 0.00078) => LSC_loss 0.72, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 82.43, Test_acc 43.39
2025-01-05 18:59:31,839 [podnet.py] => Task 18, Epoch 152/160 (LR 0.00062) => LSC_loss 0.73, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 82.41, Test_acc 43.66
2025-01-05 18:59:40,431 [podnet.py] => Task 18, Epoch 153/160 (LR 0.00047) => LSC_loss 0.73, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 82.16, Test_acc 43.68
2025-01-05 18:59:48,784 [podnet.py] => Task 18, Epoch 154/160 (LR 0.00035) => LSC_loss 0.72, Spatial_loss 1.17, Flat_loss 0.12, Train_acc 82.51, Test_acc 43.52
2025-01-05 18:59:57,240 [podnet.py] => Task 18, Epoch 155/160 (LR 0.00024) => LSC_loss 0.71, Spatial_loss 1.15, Flat_loss 0.12, Train_acc 82.86, Test_acc 43.69
2025-01-05 19:00:05,693 [podnet.py] => Task 18, Epoch 156/160 (LR 0.00015) => LSC_loss 0.72, Spatial_loss 1.14, Flat_loss 0.12, Train_acc 82.73, Test_acc 43.52
2025-01-05 19:00:13,913 [podnet.py] => Task 18, Epoch 157/160 (LR 0.00009) => LSC_loss 0.72, Spatial_loss 1.16, Flat_loss 0.12, Train_acc 82.26, Test_acc 43.66
2025-01-05 19:00:22,434 [podnet.py] => Task 18, Epoch 158/160 (LR 0.00004) => LSC_loss 0.71, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 82.21, Test_acc 43.71
2025-01-05 19:00:30,823 [podnet.py] => Task 18, Epoch 159/160 (LR 0.00001) => LSC_loss 0.71, Spatial_loss 1.12, Flat_loss 0.12, Train_acc 82.73, Test_acc 43.51
2025-01-05 19:00:39,235 [podnet.py] => Task 18, Epoch 160/160 (LR 0.00000) => LSC_loss 0.71, Spatial_loss 1.13, Flat_loss 0.11, Train_acc 82.58, Test_acc 43.74
2025-01-05 19:00:39,235 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 19:00:39,235 [base.py] => Reducing exemplars...(149 per classes)
2025-01-05 19:01:53,617 [base.py] => Constructing exemplars...(149 per classes)
2025-01-05 19:02:02,699 [podnet.py] => The size of finetune dataset: 14155
2025-01-05 19:02:10,670 [podnet.py] => Task 18, Epoch 1/20 (LR 0.00497) => LSC_loss 0.69, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 83.36, Test_acc 43.27
2025-01-05 19:02:18,413 [podnet.py] => Task 18, Epoch 2/20 (LR 0.00488) => LSC_loss 0.68, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 83.25, Test_acc 42.62
2025-01-05 19:02:26,080 [podnet.py] => Task 18, Epoch 3/20 (LR 0.00473) => LSC_loss 0.67, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 83.80, Test_acc 42.77
2025-01-05 19:02:33,755 [podnet.py] => Task 18, Epoch 4/20 (LR 0.00452) => LSC_loss 0.68, Spatial_loss 1.28, Flat_loss 0.11, Train_acc 83.31, Test_acc 42.97
2025-01-05 19:02:41,486 [podnet.py] => Task 18, Epoch 5/20 (LR 0.00427) => LSC_loss 0.67, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 83.57, Test_acc 43.09
2025-01-05 19:02:49,293 [podnet.py] => Task 18, Epoch 6/20 (LR 0.00397) => LSC_loss 0.67, Spatial_loss 1.24, Flat_loss 0.10, Train_acc 83.62, Test_acc 42.80
2025-01-05 19:02:57,128 [podnet.py] => Task 18, Epoch 7/20 (LR 0.00363) => LSC_loss 0.67, Spatial_loss 1.25, Flat_loss 0.10, Train_acc 83.72, Test_acc 42.94
2025-01-05 19:03:04,805 [podnet.py] => Task 18, Epoch 8/20 (LR 0.00327) => LSC_loss 0.67, Spatial_loss 1.26, Flat_loss 0.10, Train_acc 83.63, Test_acc 42.84
2025-01-05 19:03:12,710 [podnet.py] => Task 18, Epoch 9/20 (LR 0.00289) => LSC_loss 0.65, Spatial_loss 1.21, Flat_loss 0.10, Train_acc 84.27, Test_acc 42.97
2025-01-05 19:03:20,247 [podnet.py] => Task 18, Epoch 10/20 (LR 0.00250) => LSC_loss 0.66, Spatial_loss 1.22, Flat_loss 0.10, Train_acc 84.08, Test_acc 43.23
2025-01-05 19:03:28,177 [podnet.py] => Task 18, Epoch 11/20 (LR 0.00211) => LSC_loss 0.65, Spatial_loss 1.19, Flat_loss 0.10, Train_acc 84.31, Test_acc 43.24
2025-01-05 19:03:35,870 [podnet.py] => Task 18, Epoch 12/20 (LR 0.00173) => LSC_loss 0.63, Spatial_loss 1.18, Flat_loss 0.09, Train_acc 84.92, Test_acc 43.54
2025-01-05 19:03:43,612 [podnet.py] => Task 18, Epoch 13/20 (LR 0.00137) => LSC_loss 0.63, Spatial_loss 1.14, Flat_loss 0.09, Train_acc 84.65, Test_acc 43.38
2025-01-05 19:03:51,289 [podnet.py] => Task 18, Epoch 14/20 (LR 0.00103) => LSC_loss 0.63, Spatial_loss 1.18, Flat_loss 0.09, Train_acc 84.99, Test_acc 43.84
2025-01-05 19:03:58,913 [podnet.py] => Task 18, Epoch 15/20 (LR 0.00073) => LSC_loss 0.63, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 85.22, Test_acc 43.55
2025-01-05 19:04:06,349 [podnet.py] => Task 18, Epoch 16/20 (LR 0.00048) => LSC_loss 0.63, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.43, Test_acc 43.57
2025-01-05 19:04:14,151 [podnet.py] => Task 18, Epoch 17/20 (LR 0.00027) => LSC_loss 0.62, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.62, Test_acc 43.51
2025-01-05 19:04:21,860 [podnet.py] => Task 18, Epoch 18/20 (LR 0.00012) => LSC_loss 0.62, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.41, Test_acc 43.63
2025-01-05 19:04:29,435 [podnet.py] => Task 18, Epoch 19/20 (LR 0.00003) => LSC_loss 0.62, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.50, Test_acc 43.38
2025-01-05 19:04:37,316 [podnet.py] => Task 18, Epoch 20/20 (LR 0.00000) => LSC_loss 0.62, Spatial_loss 1.06, Flat_loss 0.08, Train_acc 85.48, Test_acc 43.47
2025-01-05 19:04:37,317 [base.py] => Reducing exemplars...(141 per classes)
2025-01-05 19:05:52,455 [base.py] => Constructing exemplars...(141 per classes)
2025-01-05 19:06:03,943 [podnet.py] => Exemplar size: 13395
2025-01-05 19:06:03,943 [trainer.py] => CNN: {'total': np.float64(43.47), '00-09': np.float64(56.4), '10-19': np.float64(34.5), '20-29': np.float64(49.9), '30-39': np.float64(41.3), '40-49': np.float64(52.6), '50-59': np.float64(37.9), '60-69': np.float64(46.6), '70-79': np.float64(37.0), '80-89': np.float64(40.3), '90-99': np.float64(33.0), 'old': np.float64(44.06), 'new': np.float64(33.0)}
2025-01-05 19:06:03,943 [trainer.py] => NME: {'total': np.float64(42.18), '00-09': np.float64(58.0), '10-19': np.float64(34.5), '20-29': np.float64(50.6), '30-39': np.float64(43.5), '40-49': np.float64(49.8), '50-59': np.float64(37.5), '60-69': np.float64(42.7), '70-79': np.float64(34.0), '80-89': np.float64(35.3), '90-99': np.float64(29.6), 'old': np.float64(42.88), 'new': np.float64(29.6)}
2025-01-05 19:06:03,943 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96), np.float64(49.84), np.float64(47.7), np.float64(46.51), np.float64(44.78), np.float64(43.47)]
2025-01-05 19:06:03,943 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43), np.float64(77.19), np.float64(76.28), np.float64(75.69), np.float64(73.81), np.float64(72.28)]
2025-01-05 19:06:03,943 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96), np.float64(48.09), np.float64(46.44), np.float64(45.32), np.float64(43.64), np.float64(42.18)]
2025-01-05 19:06:03,943 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2), np.float64(75.59), np.float64(75.25), np.float64(74.59), np.float64(72.57), np.float64(71.18)]

2025-01-05 19:06:03,944 [trainer.py] => All params: 527057
2025-01-05 19:06:03,944 [trainer.py] => Trainable params: 527057
2025-01-05 19:06:03,945 [podnet.py] => Learning on 95-100
2025-01-05 19:06:03,997 [podnet.py] => Adaptive factor: 4.47213595499958
2025-01-05 19:06:12,434 [podnet.py] => Task 19, Epoch 1/160 (LR 0.09999) => LSC_loss 2.80, Spatial_loss 4.16, Flat_loss 1.13, Train_acc 34.89, Test_acc 20.07
2025-01-05 19:06:20,776 [podnet.py] => Task 19, Epoch 2/160 (LR 0.09996) => LSC_loss 2.31, Spatial_loss 3.64, Flat_loss 0.85, Train_acc 41.44, Test_acc 25.80
2025-01-05 19:06:29,132 [podnet.py] => Task 19, Epoch 3/160 (LR 0.09991) => LSC_loss 2.16, Spatial_loss 3.41, Flat_loss 0.76, Train_acc 45.03, Test_acc 29.32
2025-01-05 19:06:37,550 [podnet.py] => Task 19, Epoch 4/160 (LR 0.09985) => LSC_loss 2.12, Spatial_loss 3.31, Flat_loss 0.73, Train_acc 45.47, Test_acc 26.45
2025-01-05 19:06:46,119 [podnet.py] => Task 19, Epoch 5/160 (LR 0.09976) => LSC_loss 2.06, Spatial_loss 3.22, Flat_loss 0.70, Train_acc 47.11, Test_acc 27.42
2025-01-05 19:06:54,686 [podnet.py] => Task 19, Epoch 6/160 (LR 0.09965) => LSC_loss 2.02, Spatial_loss 3.21, Flat_loss 0.68, Train_acc 47.51, Test_acc 31.02
2025-01-05 19:07:03,098 [podnet.py] => Task 19, Epoch 7/160 (LR 0.09953) => LSC_loss 1.99, Spatial_loss 3.17, Flat_loss 0.67, Train_acc 48.47, Test_acc 27.51
2025-01-05 19:07:11,434 [podnet.py] => Task 19, Epoch 8/160 (LR 0.09938) => LSC_loss 1.98, Spatial_loss 3.19, Flat_loss 0.67, Train_acc 49.42, Test_acc 33.34
2025-01-05 19:07:19,961 [podnet.py] => Task 19, Epoch 9/160 (LR 0.09922) => LSC_loss 1.96, Spatial_loss 3.08, Flat_loss 0.65, Train_acc 49.56, Test_acc 32.77
2025-01-05 19:07:28,373 [podnet.py] => Task 19, Epoch 10/160 (LR 0.09904) => LSC_loss 1.94, Spatial_loss 3.08, Flat_loss 0.64, Train_acc 49.73, Test_acc 23.91
2025-01-05 19:07:36,932 [podnet.py] => Task 19, Epoch 11/160 (LR 0.09884) => LSC_loss 1.94, Spatial_loss 3.08, Flat_loss 0.64, Train_acc 49.93, Test_acc 31.76
2025-01-05 19:07:45,275 [podnet.py] => Task 19, Epoch 12/160 (LR 0.09862) => LSC_loss 1.93, Spatial_loss 3.09, Flat_loss 0.64, Train_acc 50.30, Test_acc 33.59
2025-01-05 19:07:53,935 [podnet.py] => Task 19, Epoch 13/160 (LR 0.09838) => LSC_loss 1.92, Spatial_loss 3.05, Flat_loss 0.64, Train_acc 49.69, Test_acc 32.08
2025-01-05 19:08:02,595 [podnet.py] => Task 19, Epoch 14/160 (LR 0.09812) => LSC_loss 1.89, Spatial_loss 3.07, Flat_loss 0.63, Train_acc 51.09, Test_acc 28.45
2025-01-05 19:08:11,008 [podnet.py] => Task 19, Epoch 15/160 (LR 0.09785) => LSC_loss 1.89, Spatial_loss 3.01, Flat_loss 0.62, Train_acc 50.89, Test_acc 28.61
2025-01-05 19:08:19,315 [podnet.py] => Task 19, Epoch 16/160 (LR 0.09755) => LSC_loss 1.91, Spatial_loss 3.08, Flat_loss 0.63, Train_acc 50.19, Test_acc 26.53
2025-01-05 19:08:27,710 [podnet.py] => Task 19, Epoch 17/160 (LR 0.09724) => LSC_loss 1.88, Spatial_loss 2.99, Flat_loss 0.62, Train_acc 51.13, Test_acc 31.60
2025-01-05 19:08:35,968 [podnet.py] => Task 19, Epoch 18/160 (LR 0.09691) => LSC_loss 1.87, Spatial_loss 3.07, Flat_loss 0.62, Train_acc 51.58, Test_acc 29.07
2025-01-05 19:08:44,623 [podnet.py] => Task 19, Epoch 19/160 (LR 0.09656) => LSC_loss 1.89, Spatial_loss 3.05, Flat_loss 0.63, Train_acc 50.64, Test_acc 30.88
2025-01-05 19:08:53,124 [podnet.py] => Task 19, Epoch 20/160 (LR 0.09619) => LSC_loss 1.88, Spatial_loss 3.07, Flat_loss 0.63, Train_acc 51.21, Test_acc 28.94
2025-01-05 19:09:01,561 [podnet.py] => Task 19, Epoch 21/160 (LR 0.09581) => LSC_loss 1.87, Spatial_loss 3.05, Flat_loss 0.62, Train_acc 51.66, Test_acc 29.77
2025-01-05 19:09:09,821 [podnet.py] => Task 19, Epoch 22/160 (LR 0.09541) => LSC_loss 1.86, Spatial_loss 3.04, Flat_loss 0.62, Train_acc 51.71, Test_acc 33.58
2025-01-05 19:09:18,256 [podnet.py] => Task 19, Epoch 23/160 (LR 0.09499) => LSC_loss 1.86, Spatial_loss 3.01, Flat_loss 0.62, Train_acc 51.58, Test_acc 31.86
2025-01-05 19:09:26,480 [podnet.py] => Task 19, Epoch 24/160 (LR 0.09455) => LSC_loss 1.84, Spatial_loss 3.03, Flat_loss 0.61, Train_acc 51.75, Test_acc 32.70
2025-01-05 19:09:34,865 [podnet.py] => Task 19, Epoch 25/160 (LR 0.09410) => LSC_loss 1.86, Spatial_loss 3.00, Flat_loss 0.62, Train_acc 52.15, Test_acc 32.81
2025-01-05 19:09:43,235 [podnet.py] => Task 19, Epoch 26/160 (LR 0.09362) => LSC_loss 1.85, Spatial_loss 2.99, Flat_loss 0.61, Train_acc 51.99, Test_acc 32.50
2025-01-05 19:09:51,509 [podnet.py] => Task 19, Epoch 27/160 (LR 0.09314) => LSC_loss 1.86, Spatial_loss 2.94, Flat_loss 0.61, Train_acc 51.21, Test_acc 28.15
2025-01-05 19:09:59,880 [podnet.py] => Task 19, Epoch 28/160 (LR 0.09263) => LSC_loss 1.82, Spatial_loss 2.97, Flat_loss 0.60, Train_acc 52.66, Test_acc 30.73
2025-01-05 19:10:08,196 [podnet.py] => Task 19, Epoch 29/160 (LR 0.09211) => LSC_loss 1.82, Spatial_loss 2.96, Flat_loss 0.60, Train_acc 51.99, Test_acc 31.03
2025-01-05 19:10:16,648 [podnet.py] => Task 19, Epoch 30/160 (LR 0.09157) => LSC_loss 1.83, Spatial_loss 2.96, Flat_loss 0.60, Train_acc 52.22, Test_acc 33.44
2025-01-05 19:10:24,954 [podnet.py] => Task 19, Epoch 31/160 (LR 0.09102) => LSC_loss 1.82, Spatial_loss 3.01, Flat_loss 0.61, Train_acc 52.52, Test_acc 32.13
2025-01-05 19:10:33,303 [podnet.py] => Task 19, Epoch 32/160 (LR 0.09045) => LSC_loss 1.82, Spatial_loss 2.97, Flat_loss 0.60, Train_acc 52.35, Test_acc 27.91
2025-01-05 19:10:41,861 [podnet.py] => Task 19, Epoch 33/160 (LR 0.08987) => LSC_loss 1.82, Spatial_loss 2.96, Flat_loss 0.60, Train_acc 52.95, Test_acc 33.54
2025-01-05 19:10:50,209 [podnet.py] => Task 19, Epoch 34/160 (LR 0.08927) => LSC_loss 1.83, Spatial_loss 3.00, Flat_loss 0.61, Train_acc 52.74, Test_acc 33.70
2025-01-05 19:10:58,477 [podnet.py] => Task 19, Epoch 35/160 (LR 0.08865) => LSC_loss 1.81, Spatial_loss 2.98, Flat_loss 0.59, Train_acc 52.82, Test_acc 31.79
2025-01-05 19:11:06,966 [podnet.py] => Task 19, Epoch 36/160 (LR 0.08802) => LSC_loss 1.79, Spatial_loss 2.98, Flat_loss 0.59, Train_acc 52.86, Test_acc 30.57
2025-01-05 19:11:15,443 [podnet.py] => Task 19, Epoch 37/160 (LR 0.08738) => LSC_loss 1.79, Spatial_loss 2.94, Flat_loss 0.59, Train_acc 53.43, Test_acc 33.76
2025-01-05 19:11:23,999 [podnet.py] => Task 19, Epoch 38/160 (LR 0.08672) => LSC_loss 1.82, Spatial_loss 2.98, Flat_loss 0.61, Train_acc 52.70, Test_acc 30.81
2025-01-05 19:11:32,540 [podnet.py] => Task 19, Epoch 39/160 (LR 0.08604) => LSC_loss 1.80, Spatial_loss 2.91, Flat_loss 0.59, Train_acc 53.19, Test_acc 33.44
2025-01-05 19:11:41,065 [podnet.py] => Task 19, Epoch 40/160 (LR 0.08536) => LSC_loss 1.78, Spatial_loss 2.93, Flat_loss 0.58, Train_acc 53.64, Test_acc 32.44
2025-01-05 19:11:49,401 [podnet.py] => Task 19, Epoch 41/160 (LR 0.08465) => LSC_loss 1.78, Spatial_loss 2.91, Flat_loss 0.59, Train_acc 53.53, Test_acc 29.67
2025-01-05 19:11:58,029 [podnet.py] => Task 19, Epoch 42/160 (LR 0.08394) => LSC_loss 1.79, Spatial_loss 2.98, Flat_loss 0.59, Train_acc 52.70, Test_acc 32.83
2025-01-05 19:12:06,376 [podnet.py] => Task 19, Epoch 43/160 (LR 0.08321) => LSC_loss 1.77, Spatial_loss 2.92, Flat_loss 0.58, Train_acc 53.83, Test_acc 34.12
2025-01-05 19:12:14,659 [podnet.py] => Task 19, Epoch 44/160 (LR 0.08247) => LSC_loss 1.77, Spatial_loss 2.92, Flat_loss 0.59, Train_acc 53.90, Test_acc 34.71
2025-01-05 19:12:23,222 [podnet.py] => Task 19, Epoch 45/160 (LR 0.08172) => LSC_loss 1.74, Spatial_loss 2.83, Flat_loss 0.56, Train_acc 54.77, Test_acc 28.93
2025-01-05 19:12:31,712 [podnet.py] => Task 19, Epoch 46/160 (LR 0.08095) => LSC_loss 1.75, Spatial_loss 2.89, Flat_loss 0.57, Train_acc 54.30, Test_acc 33.49
2025-01-05 19:12:39,931 [podnet.py] => Task 19, Epoch 47/160 (LR 0.08018) => LSC_loss 1.74, Spatial_loss 2.93, Flat_loss 0.57, Train_acc 54.67, Test_acc 32.18
2025-01-05 19:12:48,416 [podnet.py] => Task 19, Epoch 48/160 (LR 0.07939) => LSC_loss 1.77, Spatial_loss 2.86, Flat_loss 0.58, Train_acc 53.62, Test_acc 28.23
2025-01-05 19:12:56,934 [podnet.py] => Task 19, Epoch 49/160 (LR 0.07859) => LSC_loss 1.75, Spatial_loss 2.87, Flat_loss 0.57, Train_acc 54.23, Test_acc 31.21
2025-01-05 19:13:05,479 [podnet.py] => Task 19, Epoch 50/160 (LR 0.07778) => LSC_loss 1.72, Spatial_loss 2.89, Flat_loss 0.56, Train_acc 54.85, Test_acc 31.27
2025-01-05 19:13:13,843 [podnet.py] => Task 19, Epoch 51/160 (LR 0.07696) => LSC_loss 1.74, Spatial_loss 2.86, Flat_loss 0.56, Train_acc 54.71, Test_acc 36.23
2025-01-05 19:13:22,138 [podnet.py] => Task 19, Epoch 52/160 (LR 0.07612) => LSC_loss 1.71, Spatial_loss 2.83, Flat_loss 0.55, Train_acc 55.27, Test_acc 33.27
2025-01-05 19:13:30,454 [podnet.py] => Task 19, Epoch 53/160 (LR 0.07528) => LSC_loss 1.71, Spatial_loss 2.85, Flat_loss 0.55, Train_acc 54.87, Test_acc 35.15
2025-01-05 19:13:38,714 [podnet.py] => Task 19, Epoch 54/160 (LR 0.07443) => LSC_loss 1.70, Spatial_loss 2.83, Flat_loss 0.55, Train_acc 55.37, Test_acc 33.16
2025-01-05 19:13:46,905 [podnet.py] => Task 19, Epoch 55/160 (LR 0.07357) => LSC_loss 1.70, Spatial_loss 2.79, Flat_loss 0.55, Train_acc 55.26, Test_acc 24.62
2025-01-05 19:13:55,351 [podnet.py] => Task 19, Epoch 56/160 (LR 0.07270) => LSC_loss 1.69, Spatial_loss 2.80, Flat_loss 0.54, Train_acc 55.73, Test_acc 33.24
2025-01-05 19:14:03,940 [podnet.py] => Task 19, Epoch 57/160 (LR 0.07182) => LSC_loss 1.67, Spatial_loss 2.80, Flat_loss 0.54, Train_acc 56.38, Test_acc 32.45
2025-01-05 19:14:12,418 [podnet.py] => Task 19, Epoch 58/160 (LR 0.07093) => LSC_loss 1.68, Spatial_loss 2.78, Flat_loss 0.53, Train_acc 56.10, Test_acc 31.44
2025-01-05 19:14:20,954 [podnet.py] => Task 19, Epoch 59/160 (LR 0.07004) => LSC_loss 1.68, Spatial_loss 2.80, Flat_loss 0.53, Train_acc 55.92, Test_acc 32.64
2025-01-05 19:14:29,491 [podnet.py] => Task 19, Epoch 60/160 (LR 0.06913) => LSC_loss 1.65, Spatial_loss 2.78, Flat_loss 0.53, Train_acc 56.31, Test_acc 33.06
2025-01-05 19:14:38,003 [podnet.py] => Task 19, Epoch 61/160 (LR 0.06822) => LSC_loss 1.68, Spatial_loss 2.75, Flat_loss 0.53, Train_acc 55.69, Test_acc 32.37
2025-01-05 19:14:46,346 [podnet.py] => Task 19, Epoch 62/160 (LR 0.06731) => LSC_loss 1.66, Spatial_loss 2.74, Flat_loss 0.53, Train_acc 56.23, Test_acc 31.49
2025-01-05 19:14:54,641 [podnet.py] => Task 19, Epoch 63/160 (LR 0.06638) => LSC_loss 1.67, Spatial_loss 2.79, Flat_loss 0.53, Train_acc 55.54, Test_acc 30.23
2025-01-05 19:15:03,218 [podnet.py] => Task 19, Epoch 64/160 (LR 0.06545) => LSC_loss 1.64, Spatial_loss 2.71, Flat_loss 0.52, Train_acc 56.97, Test_acc 33.88
2025-01-05 19:15:11,553 [podnet.py] => Task 19, Epoch 65/160 (LR 0.06451) => LSC_loss 1.61, Spatial_loss 2.70, Flat_loss 0.51, Train_acc 57.23, Test_acc 34.90
2025-01-05 19:15:19,800 [podnet.py] => Task 19, Epoch 66/160 (LR 0.06357) => LSC_loss 1.65, Spatial_loss 2.78, Flat_loss 0.52, Train_acc 56.26, Test_acc 34.37
2025-01-05 19:15:28,024 [podnet.py] => Task 19, Epoch 67/160 (LR 0.06262) => LSC_loss 1.64, Spatial_loss 2.73, Flat_loss 0.51, Train_acc 56.68, Test_acc 34.82
2025-01-05 19:15:36,503 [podnet.py] => Task 19, Epoch 68/160 (LR 0.06167) => LSC_loss 1.63, Spatial_loss 2.69, Flat_loss 0.51, Train_acc 56.94, Test_acc 35.66
2025-01-05 19:15:45,096 [podnet.py] => Task 19, Epoch 69/160 (LR 0.06072) => LSC_loss 1.59, Spatial_loss 2.64, Flat_loss 0.49, Train_acc 57.86, Test_acc 33.49
2025-01-05 19:15:53,506 [podnet.py] => Task 19, Epoch 70/160 (LR 0.05975) => LSC_loss 1.60, Spatial_loss 2.63, Flat_loss 0.50, Train_acc 57.67, Test_acc 34.56
2025-01-05 19:16:01,867 [podnet.py] => Task 19, Epoch 71/160 (LR 0.05879) => LSC_loss 1.60, Spatial_loss 2.67, Flat_loss 0.50, Train_acc 57.72, Test_acc 35.54
2025-01-05 19:16:10,455 [podnet.py] => Task 19, Epoch 72/160 (LR 0.05782) => LSC_loss 1.57, Spatial_loss 2.64, Flat_loss 0.49, Train_acc 58.66, Test_acc 33.15
2025-01-05 19:16:19,106 [podnet.py] => Task 19, Epoch 73/160 (LR 0.05685) => LSC_loss 1.56, Spatial_loss 2.65, Flat_loss 0.49, Train_acc 59.29, Test_acc 33.24
2025-01-05 19:16:27,442 [podnet.py] => Task 19, Epoch 74/160 (LR 0.05588) => LSC_loss 1.55, Spatial_loss 2.54, Flat_loss 0.48, Train_acc 58.76, Test_acc 36.41
2025-01-05 19:16:35,959 [podnet.py] => Task 19, Epoch 75/160 (LR 0.05490) => LSC_loss 1.52, Spatial_loss 2.55, Flat_loss 0.47, Train_acc 59.66, Test_acc 32.71
2025-01-05 19:16:44,333 [podnet.py] => Task 19, Epoch 76/160 (LR 0.05392) => LSC_loss 1.55, Spatial_loss 2.61, Flat_loss 0.48, Train_acc 58.25, Test_acc 30.95
2025-01-05 19:16:52,644 [podnet.py] => Task 19, Epoch 77/160 (LR 0.05294) => LSC_loss 1.52, Spatial_loss 2.58, Flat_loss 0.47, Train_acc 59.64, Test_acc 34.82
2025-01-05 19:17:01,019 [podnet.py] => Task 19, Epoch 78/160 (LR 0.05196) => LSC_loss 1.50, Spatial_loss 2.54, Flat_loss 0.46, Train_acc 60.33, Test_acc 33.85
2025-01-05 19:17:09,432 [podnet.py] => Task 19, Epoch 79/160 (LR 0.05098) => LSC_loss 1.51, Spatial_loss 2.55, Flat_loss 0.46, Train_acc 60.23, Test_acc 33.50
2025-01-05 19:17:17,824 [podnet.py] => Task 19, Epoch 80/160 (LR 0.05000) => LSC_loss 1.50, Spatial_loss 2.57, Flat_loss 0.46, Train_acc 60.13, Test_acc 31.71
2025-01-05 19:17:26,210 [podnet.py] => Task 19, Epoch 81/160 (LR 0.04902) => LSC_loss 1.49, Spatial_loss 2.54, Flat_loss 0.45, Train_acc 60.52, Test_acc 35.11
2025-01-05 19:17:34,634 [podnet.py] => Task 19, Epoch 82/160 (LR 0.04804) => LSC_loss 1.48, Spatial_loss 2.51, Flat_loss 0.45, Train_acc 60.68, Test_acc 36.55
2025-01-05 19:17:43,214 [podnet.py] => Task 19, Epoch 83/160 (LR 0.04706) => LSC_loss 1.46, Spatial_loss 2.51, Flat_loss 0.44, Train_acc 61.11, Test_acc 36.16
2025-01-05 19:17:51,710 [podnet.py] => Task 19, Epoch 84/160 (LR 0.04608) => LSC_loss 1.46, Spatial_loss 2.49, Flat_loss 0.44, Train_acc 60.80, Test_acc 36.01
2025-01-05 19:17:59,959 [podnet.py] => Task 19, Epoch 85/160 (LR 0.04510) => LSC_loss 1.45, Spatial_loss 2.48, Flat_loss 0.43, Train_acc 61.49, Test_acc 35.46
2025-01-05 19:18:08,416 [podnet.py] => Task 19, Epoch 86/160 (LR 0.04412) => LSC_loss 1.44, Spatial_loss 2.40, Flat_loss 0.43, Train_acc 61.83, Test_acc 29.56
2025-01-05 19:18:16,635 [podnet.py] => Task 19, Epoch 87/160 (LR 0.04315) => LSC_loss 1.45, Spatial_loss 2.47, Flat_loss 0.43, Train_acc 61.51, Test_acc 34.35
2025-01-05 19:18:24,823 [podnet.py] => Task 19, Epoch 88/160 (LR 0.04218) => LSC_loss 1.41, Spatial_loss 2.44, Flat_loss 0.42, Train_acc 62.45, Test_acc 34.18
2025-01-05 19:18:33,351 [podnet.py] => Task 19, Epoch 89/160 (LR 0.04121) => LSC_loss 1.38, Spatial_loss 2.35, Flat_loss 0.40, Train_acc 63.21, Test_acc 37.32
2025-01-05 19:18:41,627 [podnet.py] => Task 19, Epoch 90/160 (LR 0.04025) => LSC_loss 1.38, Spatial_loss 2.37, Flat_loss 0.41, Train_acc 62.82, Test_acc 33.44
2025-01-05 19:18:49,826 [podnet.py] => Task 19, Epoch 91/160 (LR 0.03928) => LSC_loss 1.39, Spatial_loss 2.38, Flat_loss 0.41, Train_acc 62.91, Test_acc 35.94
2025-01-05 19:18:58,378 [podnet.py] => Task 19, Epoch 92/160 (LR 0.03833) => LSC_loss 1.36, Spatial_loss 2.39, Flat_loss 0.40, Train_acc 63.48, Test_acc 33.38
2025-01-05 19:19:06,664 [podnet.py] => Task 19, Epoch 93/160 (LR 0.03738) => LSC_loss 1.33, Spatial_loss 2.30, Flat_loss 0.38, Train_acc 64.45, Test_acc 35.97
2025-01-05 19:19:15,207 [podnet.py] => Task 19, Epoch 94/160 (LR 0.03643) => LSC_loss 1.32, Spatial_loss 2.30, Flat_loss 0.38, Train_acc 64.75, Test_acc 33.33
2025-01-05 19:19:23,721 [podnet.py] => Task 19, Epoch 95/160 (LR 0.03549) => LSC_loss 1.33, Spatial_loss 2.33, Flat_loss 0.38, Train_acc 64.54, Test_acc 37.67
2025-01-05 19:19:31,945 [podnet.py] => Task 19, Epoch 96/160 (LR 0.03455) => LSC_loss 1.30, Spatial_loss 2.24, Flat_loss 0.36, Train_acc 65.59, Test_acc 38.12
2025-01-05 19:19:40,206 [podnet.py] => Task 19, Epoch 97/160 (LR 0.03362) => LSC_loss 1.28, Spatial_loss 2.20, Flat_loss 0.36, Train_acc 66.15, Test_acc 33.47
2025-01-05 19:19:48,565 [podnet.py] => Task 19, Epoch 98/160 (LR 0.03269) => LSC_loss 1.29, Spatial_loss 2.23, Flat_loss 0.36, Train_acc 66.08, Test_acc 35.11
2025-01-05 19:19:56,961 [podnet.py] => Task 19, Epoch 99/160 (LR 0.03178) => LSC_loss 1.26, Spatial_loss 2.22, Flat_loss 0.35, Train_acc 66.69, Test_acc 36.46
2025-01-05 19:20:05,187 [podnet.py] => Task 19, Epoch 100/160 (LR 0.03087) => LSC_loss 1.27, Spatial_loss 2.19, Flat_loss 0.35, Train_acc 66.23, Test_acc 37.08
2025-01-05 19:20:13,661 [podnet.py] => Task 19, Epoch 101/160 (LR 0.02996) => LSC_loss 1.24, Spatial_loss 2.20, Flat_loss 0.35, Train_acc 67.15, Test_acc 33.22
2025-01-05 19:20:21,883 [podnet.py] => Task 19, Epoch 102/160 (LR 0.02907) => LSC_loss 1.24, Spatial_loss 2.16, Flat_loss 0.34, Train_acc 66.96, Test_acc 36.83
2025-01-05 19:20:30,264 [podnet.py] => Task 19, Epoch 103/160 (LR 0.02818) => LSC_loss 1.22, Spatial_loss 2.14, Flat_loss 0.33, Train_acc 67.31, Test_acc 39.94
2025-01-05 19:20:38,800 [podnet.py] => Task 19, Epoch 104/160 (LR 0.02730) => LSC_loss 1.23, Spatial_loss 2.14, Flat_loss 0.34, Train_acc 67.03, Test_acc 38.16
2025-01-05 19:20:47,226 [podnet.py] => Task 19, Epoch 105/160 (LR 0.02643) => LSC_loss 1.22, Spatial_loss 2.20, Flat_loss 0.34, Train_acc 67.34, Test_acc 37.93
2025-01-05 19:20:55,616 [podnet.py] => Task 19, Epoch 106/160 (LR 0.02557) => LSC_loss 1.19, Spatial_loss 2.11, Flat_loss 0.32, Train_acc 68.60, Test_acc 37.43
2025-01-05 19:21:03,873 [podnet.py] => Task 19, Epoch 107/160 (LR 0.02472) => LSC_loss 1.17, Spatial_loss 2.07, Flat_loss 0.31, Train_acc 68.88, Test_acc 37.47
2025-01-05 19:21:12,357 [podnet.py] => Task 19, Epoch 108/160 (LR 0.02388) => LSC_loss 1.16, Spatial_loss 2.03, Flat_loss 0.31, Train_acc 69.41, Test_acc 38.67
2025-01-05 19:21:20,725 [podnet.py] => Task 19, Epoch 109/160 (LR 0.02304) => LSC_loss 1.16, Spatial_loss 2.06, Flat_loss 0.30, Train_acc 68.93, Test_acc 39.59
2025-01-05 19:21:29,197 [podnet.py] => Task 19, Epoch 110/160 (LR 0.02222) => LSC_loss 1.11, Spatial_loss 2.01, Flat_loss 0.29, Train_acc 70.46, Test_acc 36.86
2025-01-05 19:21:37,393 [podnet.py] => Task 19, Epoch 111/160 (LR 0.02141) => LSC_loss 1.12, Spatial_loss 2.00, Flat_loss 0.29, Train_acc 70.34, Test_acc 39.65
2025-01-05 19:21:45,840 [podnet.py] => Task 19, Epoch 112/160 (LR 0.02061) => LSC_loss 1.08, Spatial_loss 1.96, Flat_loss 0.28, Train_acc 71.62, Test_acc 39.27
2025-01-05 19:21:54,238 [podnet.py] => Task 19, Epoch 113/160 (LR 0.01982) => LSC_loss 1.09, Spatial_loss 1.98, Flat_loss 0.28, Train_acc 70.96, Test_acc 39.64
2025-01-05 19:22:02,630 [podnet.py] => Task 19, Epoch 114/160 (LR 0.01905) => LSC_loss 1.06, Spatial_loss 1.93, Flat_loss 0.27, Train_acc 72.26, Test_acc 38.53
2025-01-05 19:22:11,002 [podnet.py] => Task 19, Epoch 115/160 (LR 0.01828) => LSC_loss 1.05, Spatial_loss 1.87, Flat_loss 0.26, Train_acc 72.20, Test_acc 39.39
2025-01-05 19:22:19,146 [podnet.py] => Task 19, Epoch 116/160 (LR 0.01753) => LSC_loss 1.04, Spatial_loss 1.86, Flat_loss 0.25, Train_acc 72.78, Test_acc 38.06
2025-01-05 19:22:27,508 [podnet.py] => Task 19, Epoch 117/160 (LR 0.01679) => LSC_loss 1.04, Spatial_loss 1.87, Flat_loss 0.26, Train_acc 72.48, Test_acc 40.25
2025-01-05 19:22:36,250 [podnet.py] => Task 19, Epoch 118/160 (LR 0.01606) => LSC_loss 1.01, Spatial_loss 1.81, Flat_loss 0.24, Train_acc 73.40, Test_acc 39.08
2025-01-05 19:22:44,588 [podnet.py] => Task 19, Epoch 119/160 (LR 0.01535) => LSC_loss 1.00, Spatial_loss 1.79, Flat_loss 0.24, Train_acc 73.85, Test_acc 39.19
2025-01-05 19:22:52,950 [podnet.py] => Task 19, Epoch 120/160 (LR 0.01464) => LSC_loss 0.99, Spatial_loss 1.78, Flat_loss 0.23, Train_acc 73.95, Test_acc 40.12
2025-01-05 19:23:01,301 [podnet.py] => Task 19, Epoch 121/160 (LR 0.01396) => LSC_loss 0.98, Spatial_loss 1.75, Flat_loss 0.23, Train_acc 74.34, Test_acc 38.51
2025-01-05 19:23:09,628 [podnet.py] => Task 19, Epoch 122/160 (LR 0.01328) => LSC_loss 0.95, Spatial_loss 1.74, Flat_loss 0.22, Train_acc 75.29, Test_acc 41.22
2025-01-05 19:23:17,827 [podnet.py] => Task 19, Epoch 123/160 (LR 0.01262) => LSC_loss 0.93, Spatial_loss 1.69, Flat_loss 0.21, Train_acc 76.09, Test_acc 41.23
2025-01-05 19:23:26,233 [podnet.py] => Task 19, Epoch 124/160 (LR 0.01198) => LSC_loss 0.93, Spatial_loss 1.66, Flat_loss 0.21, Train_acc 75.68, Test_acc 41.12
2025-01-05 19:23:34,544 [podnet.py] => Task 19, Epoch 125/160 (LR 0.01135) => LSC_loss 0.90, Spatial_loss 1.60, Flat_loss 0.20, Train_acc 77.03, Test_acc 40.78
2025-01-05 19:23:42,926 [podnet.py] => Task 19, Epoch 126/160 (LR 0.01073) => LSC_loss 0.90, Spatial_loss 1.60, Flat_loss 0.19, Train_acc 76.85, Test_acc 40.21
2025-01-05 19:23:51,313 [podnet.py] => Task 19, Epoch 127/160 (LR 0.01013) => LSC_loss 0.90, Spatial_loss 1.62, Flat_loss 0.19, Train_acc 77.01, Test_acc 41.59
2025-01-05 19:23:59,719 [podnet.py] => Task 19, Epoch 128/160 (LR 0.00955) => LSC_loss 0.88, Spatial_loss 1.57, Flat_loss 0.18, Train_acc 77.26, Test_acc 40.59
2025-01-05 19:24:08,318 [podnet.py] => Task 19, Epoch 129/160 (LR 0.00898) => LSC_loss 0.87, Spatial_loss 1.57, Flat_loss 0.18, Train_acc 77.75, Test_acc 40.77
2025-01-05 19:24:16,689 [podnet.py] => Task 19, Epoch 130/160 (LR 0.00843) => LSC_loss 0.87, Spatial_loss 1.54, Flat_loss 0.18, Train_acc 77.67, Test_acc 41.34
2025-01-05 19:24:25,001 [podnet.py] => Task 19, Epoch 131/160 (LR 0.00789) => LSC_loss 0.85, Spatial_loss 1.51, Flat_loss 0.17, Train_acc 78.60, Test_acc 41.73
2025-01-05 19:24:33,254 [podnet.py] => Task 19, Epoch 132/160 (LR 0.00737) => LSC_loss 0.85, Spatial_loss 1.49, Flat_loss 0.17, Train_acc 78.63, Test_acc 41.85
2025-01-05 19:24:41,514 [podnet.py] => Task 19, Epoch 133/160 (LR 0.00686) => LSC_loss 0.83, Spatial_loss 1.45, Flat_loss 0.16, Train_acc 78.88, Test_acc 41.36
2025-01-05 19:24:49,993 [podnet.py] => Task 19, Epoch 134/160 (LR 0.00638) => LSC_loss 0.82, Spatial_loss 1.46, Flat_loss 0.16, Train_acc 79.52, Test_acc 42.76
2025-01-05 19:24:58,474 [podnet.py] => Task 19, Epoch 135/160 (LR 0.00590) => LSC_loss 0.82, Spatial_loss 1.43, Flat_loss 0.16, Train_acc 79.55, Test_acc 41.42
2025-01-05 19:25:07,036 [podnet.py] => Task 19, Epoch 136/160 (LR 0.00545) => LSC_loss 0.81, Spatial_loss 1.40, Flat_loss 0.15, Train_acc 79.13, Test_acc 42.52
2025-01-05 19:25:15,390 [podnet.py] => Task 19, Epoch 137/160 (LR 0.00501) => LSC_loss 0.80, Spatial_loss 1.40, Flat_loss 0.15, Train_acc 79.89, Test_acc 41.99
2025-01-05 19:25:23,791 [podnet.py] => Task 19, Epoch 138/160 (LR 0.00459) => LSC_loss 0.79, Spatial_loss 1.36, Flat_loss 0.14, Train_acc 80.48, Test_acc 41.71
2025-01-05 19:25:32,266 [podnet.py] => Task 19, Epoch 139/160 (LR 0.00419) => LSC_loss 0.78, Spatial_loss 1.37, Flat_loss 0.14, Train_acc 80.56, Test_acc 42.46
2025-01-05 19:25:40,519 [podnet.py] => Task 19, Epoch 140/160 (LR 0.00381) => LSC_loss 0.78, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 80.39, Test_acc 42.04
2025-01-05 19:25:49,075 [podnet.py] => Task 19, Epoch 141/160 (LR 0.00344) => LSC_loss 0.77, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 80.86, Test_acc 42.23
2025-01-05 19:25:57,346 [podnet.py] => Task 19, Epoch 142/160 (LR 0.00309) => LSC_loss 0.77, Spatial_loss 1.31, Flat_loss 0.13, Train_acc 81.04, Test_acc 42.51
2025-01-05 19:26:05,996 [podnet.py] => Task 19, Epoch 143/160 (LR 0.00276) => LSC_loss 0.75, Spatial_loss 1.28, Flat_loss 0.13, Train_acc 81.21, Test_acc 42.86
2025-01-05 19:26:14,514 [podnet.py] => Task 19, Epoch 144/160 (LR 0.00245) => LSC_loss 0.76, Spatial_loss 1.27, Flat_loss 0.13, Train_acc 81.72, Test_acc 42.17
2025-01-05 19:26:22,929 [podnet.py] => Task 19, Epoch 145/160 (LR 0.00215) => LSC_loss 0.75, Spatial_loss 1.29, Flat_loss 0.12, Train_acc 81.90, Test_acc 42.15
2025-01-05 19:26:31,411 [podnet.py] => Task 19, Epoch 146/160 (LR 0.00188) => LSC_loss 0.75, Spatial_loss 1.26, Flat_loss 0.12, Train_acc 81.23, Test_acc 42.52
2025-01-05 19:26:39,723 [podnet.py] => Task 19, Epoch 147/160 (LR 0.00162) => LSC_loss 0.73, Spatial_loss 1.23, Flat_loss 0.12, Train_acc 81.96, Test_acc 42.60
2025-01-05 19:26:48,002 [podnet.py] => Task 19, Epoch 148/160 (LR 0.00138) => LSC_loss 0.73, Spatial_loss 1.20, Flat_loss 0.12, Train_acc 81.65, Test_acc 42.79
2025-01-05 19:26:56,557 [podnet.py] => Task 19, Epoch 149/160 (LR 0.00116) => LSC_loss 0.73, Spatial_loss 1.22, Flat_loss 0.11, Train_acc 81.82, Test_acc 42.58
2025-01-05 19:27:04,688 [podnet.py] => Task 19, Epoch 150/160 (LR 0.00096) => LSC_loss 0.72, Spatial_loss 1.20, Flat_loss 0.11, Train_acc 82.94, Test_acc 43.24
2025-01-05 19:27:13,150 [podnet.py] => Task 19, Epoch 151/160 (LR 0.00078) => LSC_loss 0.72, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 82.40, Test_acc 42.93
2025-01-05 19:27:21,631 [podnet.py] => Task 19, Epoch 152/160 (LR 0.00062) => LSC_loss 0.72, Spatial_loss 1.19, Flat_loss 0.11, Train_acc 82.39, Test_acc 42.88
2025-01-05 19:27:30,073 [podnet.py] => Task 19, Epoch 153/160 (LR 0.00047) => LSC_loss 0.72, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 82.47, Test_acc 42.99
2025-01-05 19:27:38,631 [podnet.py] => Task 19, Epoch 154/160 (LR 0.00035) => LSC_loss 0.72, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 82.77, Test_acc 42.76
2025-01-05 19:27:47,316 [podnet.py] => Task 19, Epoch 155/160 (LR 0.00024) => LSC_loss 0.71, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 83.00, Test_acc 42.82
2025-01-05 19:27:55,702 [podnet.py] => Task 19, Epoch 156/160 (LR 0.00015) => LSC_loss 0.71, Spatial_loss 1.15, Flat_loss 0.11, Train_acc 83.06, Test_acc 42.95
2025-01-05 19:28:03,925 [podnet.py] => Task 19, Epoch 157/160 (LR 0.00009) => LSC_loss 0.72, Spatial_loss 1.17, Flat_loss 0.11, Train_acc 82.63, Test_acc 43.00
2025-01-05 19:28:12,299 [podnet.py] => Task 19, Epoch 158/160 (LR 0.00004) => LSC_loss 0.71, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 82.77, Test_acc 43.04
2025-01-05 19:28:20,657 [podnet.py] => Task 19, Epoch 159/160 (LR 0.00001) => LSC_loss 0.71, Spatial_loss 1.14, Flat_loss 0.11, Train_acc 82.88, Test_acc 42.83
2025-01-05 19:28:29,001 [podnet.py] => Task 19, Epoch 160/160 (LR 0.00000) => LSC_loss 0.71, Spatial_loss 1.12, Flat_loss 0.11, Train_acc 83.08, Test_acc 42.84
2025-01-05 19:28:29,002 [podnet.py] => Finetune the network (classifier part) with the undersampled dataset!
2025-01-05 19:28:29,002 [base.py] => Reducing exemplars...(141 per classes)
2025-01-05 19:29:47,345 [base.py] => Constructing exemplars...(141 per classes)
2025-01-05 19:29:55,936 [podnet.py] => The size of finetune dataset: 14100
2025-01-05 19:30:03,595 [podnet.py] => Task 19, Epoch 1/20 (LR 0.00497) => LSC_loss 0.69, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 83.32, Test_acc 42.09
2025-01-05 19:30:11,298 [podnet.py] => Task 19, Epoch 2/20 (LR 0.00488) => LSC_loss 0.70, Spatial_loss 1.33, Flat_loss 0.11, Train_acc 83.53, Test_acc 41.85
2025-01-05 19:30:19,223 [podnet.py] => Task 19, Epoch 3/20 (LR 0.00473) => LSC_loss 0.68, Spatial_loss 1.31, Flat_loss 0.11, Train_acc 83.60, Test_acc 42.29
2025-01-05 19:30:26,892 [podnet.py] => Task 19, Epoch 4/20 (LR 0.00452) => LSC_loss 0.69, Spatial_loss 1.29, Flat_loss 0.11, Train_acc 83.03, Test_acc 42.01
2025-01-05 19:30:34,549 [podnet.py] => Task 19, Epoch 5/20 (LR 0.00427) => LSC_loss 0.69, Spatial_loss 1.31, Flat_loss 0.11, Train_acc 83.33, Test_acc 42.10
2025-01-05 19:30:42,406 [podnet.py] => Task 19, Epoch 6/20 (LR 0.00397) => LSC_loss 0.68, Spatial_loss 1.27, Flat_loss 0.10, Train_acc 83.84, Test_acc 42.40
2025-01-05 19:30:50,112 [podnet.py] => Task 19, Epoch 7/20 (LR 0.00363) => LSC_loss 0.67, Spatial_loss 1.27, Flat_loss 0.11, Train_acc 83.80, Test_acc 41.98
2025-01-05 19:30:57,865 [podnet.py] => Task 19, Epoch 8/20 (LR 0.00327) => LSC_loss 0.68, Spatial_loss 1.25, Flat_loss 0.10, Train_acc 83.48, Test_acc 41.81
2025-01-05 19:31:05,664 [podnet.py] => Task 19, Epoch 9/20 (LR 0.00289) => LSC_loss 0.67, Spatial_loss 1.24, Flat_loss 0.10, Train_acc 84.12, Test_acc 42.76
2025-01-05 19:31:13,572 [podnet.py] => Task 19, Epoch 10/20 (LR 0.00250) => LSC_loss 0.66, Spatial_loss 1.25, Flat_loss 0.10, Train_acc 84.28, Test_acc 42.81
2025-01-05 19:31:21,303 [podnet.py] => Task 19, Epoch 11/20 (LR 0.00211) => LSC_loss 0.65, Spatial_loss 1.19, Flat_loss 0.09, Train_acc 84.35, Test_acc 42.88
2025-01-05 19:31:28,992 [podnet.py] => Task 19, Epoch 12/20 (LR 0.00173) => LSC_loss 0.65, Spatial_loss 1.18, Flat_loss 0.09, Train_acc 84.78, Test_acc 42.50
2025-01-05 19:31:36,461 [podnet.py] => Task 19, Epoch 13/20 (LR 0.00137) => LSC_loss 0.64, Spatial_loss 1.16, Flat_loss 0.09, Train_acc 84.60, Test_acc 42.83
2025-01-05 19:31:44,046 [podnet.py] => Task 19, Epoch 14/20 (LR 0.00103) => LSC_loss 0.64, Spatial_loss 1.13, Flat_loss 0.08, Train_acc 85.27, Test_acc 42.83
2025-01-05 19:31:51,922 [podnet.py] => Task 19, Epoch 15/20 (LR 0.00073) => LSC_loss 0.64, Spatial_loss 1.17, Flat_loss 0.09, Train_acc 85.05, Test_acc 42.89
2025-01-05 19:31:59,518 [podnet.py] => Task 19, Epoch 16/20 (LR 0.00048) => LSC_loss 0.63, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 85.35, Test_acc 42.61
2025-01-05 19:32:07,001 [podnet.py] => Task 19, Epoch 17/20 (LR 0.00027) => LSC_loss 0.63, Spatial_loss 1.13, Flat_loss 0.08, Train_acc 85.01, Test_acc 42.93
2025-01-05 19:32:14,922 [podnet.py] => Task 19, Epoch 18/20 (LR 0.00012) => LSC_loss 0.62, Spatial_loss 1.12, Flat_loss 0.08, Train_acc 85.45, Test_acc 43.04
2025-01-05 19:32:22,687 [podnet.py] => Task 19, Epoch 19/20 (LR 0.00003) => LSC_loss 0.62, Spatial_loss 1.09, Flat_loss 0.08, Train_acc 85.32, Test_acc 42.93
2025-01-05 19:32:30,296 [podnet.py] => Task 19, Epoch 20/20 (LR 0.00000) => LSC_loss 0.62, Spatial_loss 1.08, Flat_loss 0.08, Train_acc 85.79, Test_acc 42.73
2025-01-05 19:32:30,300 [base.py] => Reducing exemplars...(134 per classes)
2025-01-05 19:33:50,035 [base.py] => Constructing exemplars...(134 per classes)
2025-01-05 19:34:01,948 [podnet.py] => Exemplar size: 13400
2025-01-05 19:34:01,948 [trainer.py] => CNN: {'total': np.float64(42.73), '00-09': np.float64(52.9), '10-19': np.float64(34.4), '20-29': np.float64(49.5), '30-39': np.float64(40.2), '40-49': np.float64(52.1), '50-59': np.float64(37.9), '60-69': np.float64(46.3), '70-79': np.float64(36.1), '80-89': np.float64(40.0), '90-99': np.float64(37.9), 'old': np.float64(43.0), 'new': np.float64(37.6)}
2025-01-05 19:34:01,948 [trainer.py] => NME: {'total': np.float64(41.39), '00-09': np.float64(56.3), '10-19': np.float64(35.4), '20-29': np.float64(49.1), '30-39': np.float64(41.4), '40-49': np.float64(49.6), '50-59': np.float64(36.9), '60-69': np.float64(42.2), '70-79': np.float64(35.6), '80-89': np.float64(35.5), '90-99': np.float64(31.9), 'old': np.float64(41.88), 'new': np.float64(32.0)}
2025-01-05 19:34:01,948 [trainer.py] => CNN top1 curve: [np.float64(97.8), np.float64(86.3), np.float64(74.67), np.float64(67.65), np.float64(66.32), np.float64(64.47), np.float64(61.26), np.float64(59.22), np.float64(58.4), np.float64(57.24), np.float64(55.56), np.float64(53.1), np.float64(51.51), np.float64(50.96), np.float64(49.84), np.float64(47.7), np.float64(46.51), np.float64(44.78), np.float64(43.47), np.float64(42.73)]
2025-01-05 19:34:01,948 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(98.9), np.float64(95.8), np.float64(91.5), np.float64(90.44), np.float64(88.4), np.float64(87.09), np.float64(85.42), np.float64(84.36), np.float64(83.78), np.float64(82.82), np.float64(80.48), np.float64(79.14), np.float64(78.43), np.float64(77.19), np.float64(76.28), np.float64(75.69), np.float64(73.81), np.float64(72.28), np.float64(71.39)]
2025-01-05 19:34:01,948 [trainer.py] => NME top1 curve: [np.float64(97.8), np.float64(85.7), np.float64(74.67), np.float64(67.15), np.float64(65.4), np.float64(63.57), np.float64(60.23), np.float64(58.08), np.float64(56.67), np.float64(55.34), np.float64(53.89), np.float64(51.65), np.float64(49.83), np.float64(48.96), np.float64(48.09), np.float64(46.44), np.float64(45.32), np.float64(43.64), np.float64(42.18), np.float64(41.39)]
2025-01-05 19:34:01,948 [trainer.py] => NME top5 curve: [np.float64(100.0), np.float64(98.7), np.float64(95.6), np.float64(90.35), np.float64(89.24), np.float64(88.2), np.float64(85.66), np.float64(84.15), np.float64(83.2), np.float64(82.86), np.float64(81.91), np.float64(79.22), np.float64(77.69), np.float64(77.2), np.float64(75.59), np.float64(75.25), np.float64(74.59), np.float64(72.57), np.float64(71.18), np.float64(70.37)]

2025-01-05 19:34:01,949 [trainer.py] => End Time:1736102041.9490154
