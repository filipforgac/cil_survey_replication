2025-01-13 12:44:50,164 [trainer.py] => Time Str >>> 0113-12-44-50-097
2025-01-13 12:44:50,354 [trainer.py] => memory_per_class: 20
2025-01-13 12:44:50,354 [trainer.py] => fixed_memory: False
2025-01-13 12:44:50,354 [trainer.py] => shuffle: True
2025-01-13 12:44:50,354 [trainer.py] => model_name: memo
2025-01-13 12:44:50,355 [trainer.py] => seed: 1993
2025-01-13 12:44:50,355 [trainer.py] => dataset: cifar100
2025-01-13 12:44:50,355 [trainer.py] => memory_size: 2583
2025-01-13 12:44:50,355 [trainer.py] => init_cls: 20
2025-01-13 12:44:50,355 [trainer.py] => increment: 20
2025-01-13 12:44:50,355 [trainer.py] => convnet_type: memo_resnet32
2025-01-13 12:44:50,355 [trainer.py] => prefix: fair
2025-01-13 12:44:50,355 [trainer.py] => device: [device(type='cuda', index=3)]
2025-01-13 12:44:50,355 [trainer.py] => debug: False
2025-01-13 12:44:50,355 [trainer.py] => skip: False
2025-01-13 12:44:50,355 [trainer.py] => train_base: True
2025-01-13 12:44:50,355 [trainer.py] => train_adaptive: False
2025-01-13 12:44:50,355 [trainer.py] => scheduler: cosine
2025-01-13 12:44:50,356 [trainer.py] => init_epoch: 200
2025-01-13 12:44:50,356 [trainer.py] => t_max: 170
2025-01-13 12:44:50,356 [trainer.py] => init_lr: 0.1
2025-01-13 12:44:50,356 [trainer.py] => init_milestones: [60, 120, 170]
2025-01-13 12:44:50,356 [trainer.py] => init_lr_decay: 0.1
2025-01-13 12:44:50,356 [trainer.py] => init_weight_decay: 0.0005
2025-01-13 12:44:50,356 [trainer.py] => epochs: 170
2025-01-13 12:44:50,356 [trainer.py] => lrate: 0.1
2025-01-13 12:44:50,356 [trainer.py] => milestones: [80, 120, 150]
2025-01-13 12:44:50,356 [trainer.py] => lrate_decay: 0.1
2025-01-13 12:44:50,356 [trainer.py] => batch_size: 128
2025-01-13 12:44:50,356 [trainer.py] => weight_decay: 0.0002
2025-01-13 12:44:50,357 [trainer.py] => alpha_aux: 1.0
2025-01-13 12:44:50,357 [trainer.py] => config: ./exps/memo.json
2025-01-13 12:44:50,357 [trainer.py] => time_str: 0113-12-44-50-097
2025-01-13 12:44:50,357 [trainer.py] => exp_name: 0113-12-44-50-097_cifar100_memo_resnet32_1993_B0_Inc20
2025-01-13 12:44:50,357 [trainer.py] => logfilename: logs/fair/cifar100/memo/0113-12-44-50-097_cifar100_memo_resnet32_1993_B0_Inc20
2025-01-13 12:44:50,357 [trainer.py] => csv_name: cifar100_1993_memo_resnet32_B0_Inc20
2025-01-13 12:44:53,005 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-01-13 12:44:53,186 [memo.py] => >>> train generalized blocks:True train_adaptive:False
2025-01-13 12:44:53,194 [trainer.py] => Start time:1736768693.194078
2025-01-13 12:44:53,194 [trainer.py] => All params: 112016
2025-01-13 12:44:53,194 [trainer.py] => Trainable params: 112016
2025-01-13 12:44:53,212 [inc_net.py] => SpecializedResNet_cifar(
  (final_stage): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
)
2025-01-13 12:44:53,213 [memo.py] => Learning on 0-20
2025-01-13 12:44:53,214 [memo.py] => All params: 466169
2025-01-13 12:44:53,214 [memo.py] => Trainable params: 466169
2025-01-13 12:44:59,371 [memo.py] => Task 0, Epoch 1/200 => Loss 2.951, Train_accy 13.48, Test_accy 17.80
2025-01-13 12:45:03,313 [memo.py] => Task 0, Epoch 2/200 => Loss 2.506, Train_accy 22.25
2025-01-13 12:45:07,574 [memo.py] => Task 0, Epoch 3/200 => Loss 2.338, Train_accy 27.69
2025-01-13 12:45:11,583 [memo.py] => Task 0, Epoch 4/200 => Loss 2.155, Train_accy 33.35
2025-01-13 12:45:15,471 [memo.py] => Task 0, Epoch 5/200 => Loss 2.013, Train_accy 38.02
2025-01-13 12:45:20,570 [memo.py] => Task 0, Epoch 6/200 => Loss 1.882, Train_accy 41.10, Test_accy 40.25
2025-01-13 12:45:24,453 [memo.py] => Task 0, Epoch 7/200 => Loss 1.758, Train_accy 44.87
2025-01-13 12:45:28,371 [memo.py] => Task 0, Epoch 8/200 => Loss 1.694, Train_accy 46.74
2025-01-13 12:45:32,320 [memo.py] => Task 0, Epoch 9/200 => Loss 1.586, Train_accy 50.21
2025-01-13 12:45:36,287 [memo.py] => Task 0, Epoch 10/200 => Loss 1.499, Train_accy 53.32
2025-01-13 12:45:41,302 [memo.py] => Task 0, Epoch 11/200 => Loss 1.421, Train_accy 55.28, Test_accy 43.60
2025-01-13 12:45:45,276 [memo.py] => Task 0, Epoch 12/200 => Loss 1.385, Train_accy 56.33
2025-01-13 12:45:49,372 [memo.py] => Task 0, Epoch 13/200 => Loss 1.314, Train_accy 58.68
2025-01-13 12:45:53,320 [memo.py] => Task 0, Epoch 14/200 => Loss 1.251, Train_accy 60.67
2025-01-13 12:45:57,313 [memo.py] => Task 0, Epoch 15/200 => Loss 1.199, Train_accy 61.71
2025-01-13 12:46:02,249 [memo.py] => Task 0, Epoch 16/200 => Loss 1.138, Train_accy 63.89, Test_accy 54.25
2025-01-13 12:46:06,202 [memo.py] => Task 0, Epoch 17/200 => Loss 1.136, Train_accy 64.17
2025-01-13 12:46:10,354 [memo.py] => Task 0, Epoch 18/200 => Loss 1.063, Train_accy 66.43
2025-01-13 12:46:14,557 [memo.py] => Task 0, Epoch 19/200 => Loss 1.015, Train_accy 68.22
2025-01-13 12:46:18,572 [memo.py] => Task 0, Epoch 20/200 => Loss 1.012, Train_accy 67.55
2025-01-13 12:46:23,715 [memo.py] => Task 0, Epoch 21/200 => Loss 0.947, Train_accy 70.04, Test_accy 62.55
2025-01-13 12:46:27,620 [memo.py] => Task 0, Epoch 22/200 => Loss 0.937, Train_accy 70.43
2025-01-13 12:46:31,757 [memo.py] => Task 0, Epoch 23/200 => Loss 0.930, Train_accy 70.13
2025-01-13 12:46:35,831 [memo.py] => Task 0, Epoch 24/200 => Loss 0.887, Train_accy 71.91
2025-01-13 12:46:39,899 [memo.py] => Task 0, Epoch 25/200 => Loss 0.867, Train_accy 72.42
2025-01-13 12:46:45,107 [memo.py] => Task 0, Epoch 26/200 => Loss 0.820, Train_accy 74.18, Test_accy 56.35
2025-01-13 12:46:49,076 [memo.py] => Task 0, Epoch 27/200 => Loss 0.824, Train_accy 73.61
2025-01-13 12:46:53,181 [memo.py] => Task 0, Epoch 28/200 => Loss 0.834, Train_accy 73.44
2025-01-13 12:46:57,291 [memo.py] => Task 0, Epoch 29/200 => Loss 0.776, Train_accy 75.28
2025-01-13 12:47:01,361 [memo.py] => Task 0, Epoch 30/200 => Loss 0.783, Train_accy 74.97
2025-01-13 12:47:06,399 [memo.py] => Task 0, Epoch 31/200 => Loss 0.757, Train_accy 75.50, Test_accy 60.20
2025-01-13 12:47:10,511 [memo.py] => Task 0, Epoch 32/200 => Loss 0.738, Train_accy 76.01
2025-01-13 12:47:14,609 [memo.py] => Task 0, Epoch 33/200 => Loss 0.761, Train_accy 75.82
2025-01-13 12:47:18,609 [memo.py] => Task 0, Epoch 34/200 => Loss 0.723, Train_accy 76.73
2025-01-13 12:47:22,606 [memo.py] => Task 0, Epoch 35/200 => Loss 0.667, Train_accy 78.26
2025-01-13 12:47:27,620 [memo.py] => Task 0, Epoch 36/200 => Loss 0.654, Train_accy 79.13, Test_accy 58.95
2025-01-13 12:47:31,652 [memo.py] => Task 0, Epoch 37/200 => Loss 0.649, Train_accy 78.94
2025-01-13 12:47:35,669 [memo.py] => Task 0, Epoch 38/200 => Loss 0.635, Train_accy 79.17
2025-01-13 12:47:39,557 [memo.py] => Task 0, Epoch 39/200 => Loss 0.654, Train_accy 78.66
2025-01-13 12:47:43,499 [memo.py] => Task 0, Epoch 40/200 => Loss 0.621, Train_accy 80.11
2025-01-13 12:47:48,663 [memo.py] => Task 0, Epoch 41/200 => Loss 0.597, Train_accy 80.67, Test_accy 68.25
2025-01-13 12:47:52,693 [memo.py] => Task 0, Epoch 42/200 => Loss 0.617, Train_accy 80.67
2025-01-13 12:47:56,808 [memo.py] => Task 0, Epoch 43/200 => Loss 0.647, Train_accy 79.34
2025-01-13 12:48:00,870 [memo.py] => Task 0, Epoch 44/200 => Loss 0.558, Train_accy 82.40
2025-01-13 12:48:04,907 [memo.py] => Task 0, Epoch 45/200 => Loss 0.544, Train_accy 82.65
2025-01-13 12:48:09,945 [memo.py] => Task 0, Epoch 46/200 => Loss 0.565, Train_accy 81.72, Test_accy 64.60
2025-01-13 12:48:13,804 [memo.py] => Task 0, Epoch 47/200 => Loss 0.549, Train_accy 82.42
2025-01-13 12:48:17,837 [memo.py] => Task 0, Epoch 48/200 => Loss 0.568, Train_accy 82.11
2025-01-13 12:48:21,818 [memo.py] => Task 0, Epoch 49/200 => Loss 0.532, Train_accy 82.42
2025-01-13 12:48:25,739 [memo.py] => Task 0, Epoch 50/200 => Loss 0.515, Train_accy 83.43
2025-01-13 12:48:30,784 [memo.py] => Task 0, Epoch 51/200 => Loss 0.549, Train_accy 82.19, Test_accy 65.65
2025-01-13 12:48:34,797 [memo.py] => Task 0, Epoch 52/200 => Loss 0.499, Train_accy 83.54
2025-01-13 12:48:38,893 [memo.py] => Task 0, Epoch 53/200 => Loss 0.531, Train_accy 82.94
2025-01-13 12:48:43,128 [memo.py] => Task 0, Epoch 54/200 => Loss 0.480, Train_accy 84.33
2025-01-13 12:48:47,046 [memo.py] => Task 0, Epoch 55/200 => Loss 0.478, Train_accy 84.46
2025-01-13 12:48:52,016 [memo.py] => Task 0, Epoch 56/200 => Loss 0.521, Train_accy 83.05, Test_accy 64.45
2025-01-13 12:48:56,097 [memo.py] => Task 0, Epoch 57/200 => Loss 0.453, Train_accy 85.44
2025-01-13 12:49:00,130 [memo.py] => Task 0, Epoch 58/200 => Loss 0.481, Train_accy 84.18
2025-01-13 12:49:04,156 [memo.py] => Task 0, Epoch 59/200 => Loss 0.468, Train_accy 84.62
2025-01-13 12:49:08,211 [memo.py] => Task 0, Epoch 60/200 => Loss 0.470, Train_accy 85.08
2025-01-13 12:49:13,234 [memo.py] => Task 0, Epoch 61/200 => Loss 0.479, Train_accy 84.62, Test_accy 69.80
2025-01-13 12:49:17,167 [memo.py] => Task 0, Epoch 62/200 => Loss 0.436, Train_accy 85.51
2025-01-13 12:49:21,099 [memo.py] => Task 0, Epoch 63/200 => Loss 0.399, Train_accy 86.98
2025-01-13 12:49:25,119 [memo.py] => Task 0, Epoch 64/200 => Loss 0.408, Train_accy 86.64
2025-01-13 12:49:29,121 [memo.py] => Task 0, Epoch 65/200 => Loss 0.414, Train_accy 86.48
2025-01-13 12:49:34,205 [memo.py] => Task 0, Epoch 66/200 => Loss 0.401, Train_accy 86.61, Test_accy 73.50
2025-01-13 12:49:38,274 [memo.py] => Task 0, Epoch 67/200 => Loss 0.388, Train_accy 87.50
2025-01-13 12:49:42,604 [memo.py] => Task 0, Epoch 68/200 => Loss 0.460, Train_accy 85.07
2025-01-13 12:49:46,638 [memo.py] => Task 0, Epoch 69/200 => Loss 0.385, Train_accy 87.70
2025-01-13 12:49:50,695 [memo.py] => Task 0, Epoch 70/200 => Loss 0.386, Train_accy 87.25
2025-01-13 12:49:55,993 [memo.py] => Task 0, Epoch 71/200 => Loss 0.402, Train_accy 87.09, Test_accy 72.80
2025-01-13 12:50:00,154 [memo.py] => Task 0, Epoch 72/200 => Loss 0.465, Train_accy 85.58
2025-01-13 12:50:04,211 [memo.py] => Task 0, Epoch 73/200 => Loss 0.376, Train_accy 87.99
2025-01-13 12:50:08,324 [memo.py] => Task 0, Epoch 74/200 => Loss 0.383, Train_accy 87.93
2025-01-13 12:50:12,562 [memo.py] => Task 0, Epoch 75/200 => Loss 0.385, Train_accy 87.65
2025-01-13 12:50:17,857 [memo.py] => Task 0, Epoch 76/200 => Loss 0.344, Train_accy 89.02, Test_accy 74.55
2025-01-13 12:50:21,963 [memo.py] => Task 0, Epoch 77/200 => Loss 0.321, Train_accy 89.40
2025-01-13 12:50:26,059 [memo.py] => Task 0, Epoch 78/200 => Loss 0.357, Train_accy 88.49
2025-01-13 12:50:30,188 [memo.py] => Task 0, Epoch 79/200 => Loss 0.282, Train_accy 90.84
2025-01-13 12:50:34,177 [memo.py] => Task 0, Epoch 80/200 => Loss 0.333, Train_accy 89.53
2025-01-13 12:50:39,494 [memo.py] => Task 0, Epoch 81/200 => Loss 0.377, Train_accy 87.57, Test_accy 71.25
2025-01-13 12:50:43,560 [memo.py] => Task 0, Epoch 82/200 => Loss 0.285, Train_accy 90.82
2025-01-13 12:50:47,677 [memo.py] => Task 0, Epoch 83/200 => Loss 0.331, Train_accy 89.26
2025-01-13 12:50:51,806 [memo.py] => Task 0, Epoch 84/200 => Loss 0.297, Train_accy 90.39
2025-01-13 12:50:55,748 [memo.py] => Task 0, Epoch 85/200 => Loss 0.299, Train_accy 90.14
2025-01-13 12:51:00,824 [memo.py] => Task 0, Epoch 86/200 => Loss 0.311, Train_accy 89.80, Test_accy 71.75
2025-01-13 12:51:04,955 [memo.py] => Task 0, Epoch 87/200 => Loss 0.329, Train_accy 89.37
2025-01-13 12:51:09,231 [memo.py] => Task 0, Epoch 88/200 => Loss 0.326, Train_accy 89.64
2025-01-13 12:51:13,140 [memo.py] => Task 0, Epoch 89/200 => Loss 0.244, Train_accy 92.11
2025-01-13 12:51:17,175 [memo.py] => Task 0, Epoch 90/200 => Loss 0.231, Train_accy 92.50
2025-01-13 12:51:22,391 [memo.py] => Task 0, Epoch 91/200 => Loss 0.243, Train_accy 92.26, Test_accy 73.25
2025-01-13 12:51:26,391 [memo.py] => Task 0, Epoch 92/200 => Loss 0.238, Train_accy 92.34
2025-01-13 12:51:30,355 [memo.py] => Task 0, Epoch 93/200 => Loss 0.270, Train_accy 91.22
2025-01-13 12:51:34,366 [memo.py] => Task 0, Epoch 94/200 => Loss 0.315, Train_accy 89.92
2025-01-13 12:51:38,436 [memo.py] => Task 0, Epoch 95/200 => Loss 0.273, Train_accy 91.27
2025-01-13 12:51:43,618 [memo.py] => Task 0, Epoch 96/200 => Loss 0.363, Train_accy 88.29, Test_accy 76.45
2025-01-13 12:51:47,682 [memo.py] => Task 0, Epoch 97/200 => Loss 0.238, Train_accy 92.52
2025-01-13 12:51:51,744 [memo.py] => Task 0, Epoch 98/200 => Loss 0.234, Train_accy 92.20
2025-01-13 12:51:55,702 [memo.py] => Task 0, Epoch 99/200 => Loss 0.204, Train_accy 93.55
2025-01-13 12:51:59,864 [memo.py] => Task 0, Epoch 100/200 => Loss 0.235, Train_accy 91.80
2025-01-13 12:52:05,039 [memo.py] => Task 0, Epoch 101/200 => Loss 0.199, Train_accy 93.63, Test_accy 76.00
2025-01-13 12:52:09,035 [memo.py] => Task 0, Epoch 102/200 => Loss 0.232, Train_accy 92.88
2025-01-13 12:52:12,987 [memo.py] => Task 0, Epoch 103/200 => Loss 0.199, Train_accy 93.62
2025-01-13 12:52:16,922 [memo.py] => Task 0, Epoch 104/200 => Loss 0.170, Train_accy 94.54
2025-01-13 12:52:20,940 [memo.py] => Task 0, Epoch 105/200 => Loss 0.168, Train_accy 94.57
2025-01-13 12:52:25,983 [memo.py] => Task 0, Epoch 106/200 => Loss 0.158, Train_accy 94.96, Test_accy 72.55
2025-01-13 12:52:30,002 [memo.py] => Task 0, Epoch 107/200 => Loss 0.206, Train_accy 93.50
2025-01-13 12:52:34,057 [memo.py] => Task 0, Epoch 108/200 => Loss 0.232, Train_accy 92.74
2025-01-13 12:52:38,148 [memo.py] => Task 0, Epoch 109/200 => Loss 0.192, Train_accy 94.05
2025-01-13 12:52:42,183 [memo.py] => Task 0, Epoch 110/200 => Loss 0.199, Train_accy 93.81
2025-01-13 12:52:47,284 [memo.py] => Task 0, Epoch 111/200 => Loss 0.173, Train_accy 94.32, Test_accy 76.15
2025-01-13 12:52:51,443 [memo.py] => Task 0, Epoch 112/200 => Loss 0.147, Train_accy 95.63
2025-01-13 12:52:55,415 [memo.py] => Task 0, Epoch 113/200 => Loss 0.224, Train_accy 92.67
2025-01-13 12:52:59,385 [memo.py] => Task 0, Epoch 114/200 => Loss 0.179, Train_accy 94.17
2025-01-13 12:53:03,468 [memo.py] => Task 0, Epoch 115/200 => Loss 0.116, Train_accy 96.45
2025-01-13 12:53:08,559 [memo.py] => Task 0, Epoch 116/200 => Loss 0.122, Train_accy 96.22, Test_accy 75.20
2025-01-13 12:53:12,463 [memo.py] => Task 0, Epoch 117/200 => Loss 0.140, Train_accy 95.62
2025-01-13 12:53:16,436 [memo.py] => Task 0, Epoch 118/200 => Loss 0.102, Train_accy 97.18
2025-01-13 12:53:20,374 [memo.py] => Task 0, Epoch 119/200 => Loss 0.125, Train_accy 96.04
2025-01-13 12:53:24,349 [memo.py] => Task 0, Epoch 120/200 => Loss 0.111, Train_accy 96.63
2025-01-13 12:53:29,412 [memo.py] => Task 0, Epoch 121/200 => Loss 0.102, Train_accy 96.91, Test_accy 77.80
2025-01-13 12:53:33,401 [memo.py] => Task 0, Epoch 122/200 => Loss 0.113, Train_accy 96.67
2025-01-13 12:53:37,389 [memo.py] => Task 0, Epoch 123/200 => Loss 0.115, Train_accy 96.23
2025-01-13 12:53:41,290 [memo.py] => Task 0, Epoch 124/200 => Loss 0.120, Train_accy 96.21
2025-01-13 12:53:45,215 [memo.py] => Task 0, Epoch 125/200 => Loss 0.134, Train_accy 95.86
2025-01-13 12:53:50,419 [memo.py] => Task 0, Epoch 126/200 => Loss 0.078, Train_accy 97.77, Test_accy 78.75
2025-01-13 12:53:54,586 [memo.py] => Task 0, Epoch 127/200 => Loss 0.080, Train_accy 97.46
2025-01-13 12:53:58,596 [memo.py] => Task 0, Epoch 128/200 => Loss 0.059, Train_accy 98.49
2025-01-13 12:54:02,564 [memo.py] => Task 0, Epoch 129/200 => Loss 0.055, Train_accy 98.48
2025-01-13 12:54:06,911 [memo.py] => Task 0, Epoch 130/200 => Loss 0.076, Train_accy 97.83
2025-01-13 12:54:12,064 [memo.py] => Task 0, Epoch 131/200 => Loss 0.104, Train_accy 96.97, Test_accy 78.95
2025-01-13 12:54:16,125 [memo.py] => Task 0, Epoch 132/200 => Loss 0.100, Train_accy 96.92
2025-01-13 12:54:20,293 [memo.py] => Task 0, Epoch 133/200 => Loss 0.065, Train_accy 98.21
2025-01-13 12:54:24,369 [memo.py] => Task 0, Epoch 134/200 => Loss 0.121, Train_accy 96.22
2025-01-13 12:54:28,493 [memo.py] => Task 0, Epoch 135/200 => Loss 0.064, Train_accy 98.25
2025-01-13 12:54:33,549 [memo.py] => Task 0, Epoch 136/200 => Loss 0.072, Train_accy 97.92, Test_accy 79.85
2025-01-13 12:54:37,545 [memo.py] => Task 0, Epoch 137/200 => Loss 0.058, Train_accy 98.34
2025-01-13 12:54:41,630 [memo.py] => Task 0, Epoch 138/200 => Loss 0.050, Train_accy 98.82
2025-01-13 12:54:45,646 [memo.py] => Task 0, Epoch 139/200 => Loss 0.064, Train_accy 98.29
2025-01-13 12:54:49,765 [memo.py] => Task 0, Epoch 140/200 => Loss 0.036, Train_accy 99.19
2025-01-13 12:54:54,901 [memo.py] => Task 0, Epoch 141/200 => Loss 0.033, Train_accy 99.40, Test_accy 81.35
2025-01-13 12:54:59,078 [memo.py] => Task 0, Epoch 142/200 => Loss 0.037, Train_accy 99.16
2025-01-13 12:55:03,108 [memo.py] => Task 0, Epoch 143/200 => Loss 0.046, Train_accy 98.80
2025-01-13 12:55:07,269 [memo.py] => Task 0, Epoch 144/200 => Loss 0.027, Train_accy 99.56
2025-01-13 12:55:11,321 [memo.py] => Task 0, Epoch 145/200 => Loss 0.029, Train_accy 99.42
2025-01-13 12:55:16,335 [memo.py] => Task 0, Epoch 146/200 => Loss 0.035, Train_accy 99.19, Test_accy 81.35
2025-01-13 12:55:20,218 [memo.py] => Task 0, Epoch 147/200 => Loss 0.028, Train_accy 99.40
2025-01-13 12:55:24,342 [memo.py] => Task 0, Epoch 148/200 => Loss 0.022, Train_accy 99.65
2025-01-13 12:55:28,349 [memo.py] => Task 0, Epoch 149/200 => Loss 0.020, Train_accy 99.65
2025-01-13 12:55:32,345 [memo.py] => Task 0, Epoch 150/200 => Loss 0.017, Train_accy 99.79
2025-01-13 12:55:37,461 [memo.py] => Task 0, Epoch 151/200 => Loss 0.016, Train_accy 99.81, Test_accy 82.75
2025-01-13 12:55:41,426 [memo.py] => Task 0, Epoch 152/200 => Loss 0.015, Train_accy 99.71
2025-01-13 12:55:45,433 [memo.py] => Task 0, Epoch 153/200 => Loss 0.016, Train_accy 99.93
2025-01-13 12:55:49,709 [memo.py] => Task 0, Epoch 154/200 => Loss 0.033, Train_accy 99.31
2025-01-13 12:55:53,860 [memo.py] => Task 0, Epoch 155/200 => Loss 0.027, Train_accy 99.40
2025-01-13 12:55:58,904 [memo.py] => Task 0, Epoch 156/200 => Loss 0.019, Train_accy 99.70, Test_accy 82.95
2025-01-13 12:56:02,886 [memo.py] => Task 0, Epoch 157/200 => Loss 0.021, Train_accy 99.62
2025-01-13 12:56:07,142 [memo.py] => Task 0, Epoch 158/200 => Loss 0.017, Train_accy 99.68
2025-01-13 12:56:11,020 [memo.py] => Task 0, Epoch 159/200 => Loss 0.014, Train_accy 99.90
2025-01-13 12:56:15,284 [memo.py] => Task 0, Epoch 160/200 => Loss 0.016, Train_accy 99.78
2025-01-13 12:56:20,468 [memo.py] => Task 0, Epoch 161/200 => Loss 0.015, Train_accy 99.83, Test_accy 83.40
2025-01-13 12:56:24,446 [memo.py] => Task 0, Epoch 162/200 => Loss 0.017, Train_accy 99.77
2025-01-13 12:56:28,382 [memo.py] => Task 0, Epoch 163/200 => Loss 0.016, Train_accy 99.81
2025-01-13 12:56:32,336 [memo.py] => Task 0, Epoch 164/200 => Loss 0.012, Train_accy 99.87
2025-01-13 12:56:36,331 [memo.py] => Task 0, Epoch 165/200 => Loss 0.011, Train_accy 99.90
2025-01-13 12:56:41,392 [memo.py] => Task 0, Epoch 166/200 => Loss 0.014, Train_accy 99.87, Test_accy 82.75
2025-01-13 12:56:45,418 [memo.py] => Task 0, Epoch 167/200 => Loss 0.017, Train_accy 99.78
2025-01-13 12:56:49,340 [memo.py] => Task 0, Epoch 168/200 => Loss 0.014, Train_accy 99.84
2025-01-13 12:56:53,246 [memo.py] => Task 0, Epoch 169/200 => Loss 0.011, Train_accy 99.87
2025-01-13 12:56:57,213 [memo.py] => Task 0, Epoch 170/200 => Loss 0.009, Train_accy 99.95
2025-01-13 12:57:02,365 [memo.py] => Task 0, Epoch 171/200 => Loss 0.008, Train_accy 99.99, Test_accy 83.55
2025-01-13 12:57:07,078 [memo.py] => Task 0, Epoch 172/200 => Loss 0.008, Train_accy 99.97
2025-01-13 12:57:11,665 [memo.py] => Task 0, Epoch 173/200 => Loss 0.008, Train_accy 99.96
2025-01-13 12:57:15,883 [memo.py] => Task 0, Epoch 174/200 => Loss 0.009, Train_accy 99.93
2025-01-13 12:57:19,890 [memo.py] => Task 0, Epoch 175/200 => Loss 0.009, Train_accy 99.92
2025-01-13 12:57:24,961 [memo.py] => Task 0, Epoch 176/200 => Loss 0.010, Train_accy 99.92, Test_accy 83.70
2025-01-13 12:57:28,939 [memo.py] => Task 0, Epoch 177/200 => Loss 0.011, Train_accy 99.89
2025-01-13 12:57:33,073 [memo.py] => Task 0, Epoch 178/200 => Loss 0.008, Train_accy 99.95
2025-01-13 12:57:37,118 [memo.py] => Task 0, Epoch 179/200 => Loss 0.008, Train_accy 99.97
2025-01-13 12:57:41,096 [memo.py] => Task 0, Epoch 180/200 => Loss 0.007, Train_accy 99.98
2025-01-13 12:57:46,094 [memo.py] => Task 0, Epoch 181/200 => Loss 0.007, Train_accy 99.95, Test_accy 84.00
2025-01-13 12:57:50,134 [memo.py] => Task 0, Epoch 182/200 => Loss 0.008, Train_accy 99.96
2025-01-13 12:57:54,185 [memo.py] => Task 0, Epoch 183/200 => Loss 0.007, Train_accy 99.96
2025-01-13 12:57:58,173 [memo.py] => Task 0, Epoch 184/200 => Loss 0.007, Train_accy 99.98
2025-01-13 12:58:02,282 [memo.py] => Task 0, Epoch 185/200 => Loss 0.008, Train_accy 99.94
2025-01-13 12:58:07,780 [memo.py] => Task 0, Epoch 186/200 => Loss 0.006, Train_accy 99.99, Test_accy 83.60
2025-01-13 12:58:11,708 [memo.py] => Task 0, Epoch 187/200 => Loss 0.007, Train_accy 99.98
2025-01-13 12:58:15,717 [memo.py] => Task 0, Epoch 188/200 => Loss 0.006, Train_accy 99.97
2025-01-13 12:58:19,883 [memo.py] => Task 0, Epoch 189/200 => Loss 0.008, Train_accy 99.96
2025-01-13 12:58:24,007 [memo.py] => Task 0, Epoch 190/200 => Loss 0.007, Train_accy 99.99
2025-01-13 12:58:29,242 [memo.py] => Task 0, Epoch 191/200 => Loss 0.007, Train_accy 99.97, Test_accy 83.75
2025-01-13 12:58:33,276 [memo.py] => Task 0, Epoch 192/200 => Loss 0.006, Train_accy 99.98
2025-01-13 12:58:37,363 [memo.py] => Task 0, Epoch 193/200 => Loss 0.011, Train_accy 99.97
2025-01-13 12:58:41,469 [memo.py] => Task 0, Epoch 194/200 => Loss 0.006, Train_accy 99.98
2025-01-13 12:58:45,852 [memo.py] => Task 0, Epoch 195/200 => Loss 0.006, Train_accy 99.96
2025-01-13 12:58:51,510 [memo.py] => Task 0, Epoch 196/200 => Loss 0.007, Train_accy 99.94, Test_accy 83.35
2025-01-13 12:58:55,670 [memo.py] => Task 0, Epoch 197/200 => Loss 0.007, Train_accy 99.98
2025-01-13 12:58:59,971 [memo.py] => Task 0, Epoch 198/200 => Loss 0.008, Train_accy 99.95
2025-01-13 12:59:04,033 [memo.py] => Task 0, Epoch 199/200 => Loss 0.005, Train_accy 99.99
2025-01-13 12:59:08,288 [memo.py] => Task 0, Epoch 200/200 => Loss 0.006, Train_accy 99.97
2025-01-13 12:59:08,290 [base.py] => Reducing exemplars...(129 per classes)
2025-01-13 12:59:08,290 [base.py] => Constructing exemplars...(129 per classes)
2025-01-13 12:59:37,159 [memo.py] => Train Generalized Blocks...
2025-01-13 12:59:37,167 [memo.py] => Exemplar size: 2580
2025-01-13 12:59:37,168 [trainer.py] => CNN: {'total': np.float64(83.6), '00-09': np.float64(87.2), '10-19': np.float64(80.0), 'old': 0, 'new': np.float64(83.6)}
2025-01-13 12:59:37,168 [trainer.py] => NME: {'total': np.float64(83.35), '00-09': np.float64(86.9), '10-19': np.float64(79.8), 'old': 0, 'new': np.float64(83.35)}
2025-01-13 12:59:37,168 [trainer.py] => CNN top1 curve: [np.float64(83.6)]
2025-01-13 12:59:37,168 [trainer.py] => CNN top5 curve: [np.float64(96.85)]
2025-01-13 12:59:37,168 [trainer.py] => NME top1 curve: [np.float64(83.35)]
2025-01-13 12:59:37,168 [trainer.py] => NME top5 curve: [np.float64(97.2)]

2025-01-13 12:59:37,168 [trainer.py] => All params: 466169
2025-01-13 12:59:37,168 [trainer.py] => Trainable params: 466169
2025-01-13 12:59:37,191 [memo.py] => Learning on 20-40
2025-01-13 12:59:37,192 [memo.py] => All params: 821517
2025-01-13 12:59:37,192 [memo.py] => Trainable params: 470029
2025-01-13 13:15:30,556 [memo.py] => Task 1, Epoch 170/170 => Loss 0.011, Loss_clf 0.006, Loss_aux 0.005, Train_accy 99.95
2025-01-13 13:15:32,690 [base.py] => Reducing exemplars...(64 per classes)
2025-01-13 13:15:43,504 [base.py] => Constructing exemplars...(64 per classes)
2025-01-13 13:16:11,006 [memo.py] => Exemplar size: 2560
2025-01-13 13:16:11,007 [trainer.py] => CNN: {'total': np.float64(75.95), '00-09': np.float64(77.7), '10-19': np.float64(66.5), '20-29': np.float64(81.8), '30-39': np.float64(77.8), 'old': np.float64(72.1), 'new': np.float64(79.8)}
2025-01-13 13:16:11,007 [trainer.py] => NME: {'total': np.float64(75.6), '00-09': np.float64(76.0), '10-19': np.float64(67.4), '20-29': np.float64(82.3), '30-39': np.float64(76.7), 'old': np.float64(71.7), 'new': np.float64(79.5)}
2025-01-13 13:16:11,007 [trainer.py] => CNN top1 curve: [np.float64(83.6), np.float64(75.95)]
2025-01-13 13:16:11,007 [trainer.py] => CNN top5 curve: [np.float64(96.85), np.float64(93.75)]
2025-01-13 13:16:11,007 [trainer.py] => NME top1 curve: [np.float64(83.35), np.float64(75.6)]
2025-01-13 13:16:11,007 [trainer.py] => NME top5 curve: [np.float64(97.2), np.float64(94.08)]

2025-01-13 13:16:11,008 [trainer.py] => All params: 821517
2025-01-13 13:16:11,008 [trainer.py] => Trainable params: 470029
2025-01-13 13:16:11,029 [memo.py] => Learning on 40-60
2025-01-13 13:16:11,030 [memo.py] => All params: 1179425
2025-01-13 13:16:11,030 [memo.py] => Trainable params: 476449
2025-01-13 13:33:16,848 [memo.py] => Task 2, Epoch 170/170 => Loss 0.016, Loss_clf 0.009, Loss_aux 0.007, Train_accy 99.86
2025-01-13 13:33:16,859 [base.py] => Reducing exemplars...(43 per classes)
2025-01-13 13:33:38,784 [base.py] => Constructing exemplars...(43 per classes)
2025-01-13 13:34:06,412 [memo.py] => Exemplar size: 2580
2025-01-13 13:34:06,412 [trainer.py] => CNN: {'total': np.float64(69.67), '00-09': np.float64(67.4), '10-19': np.float64(57.0), '20-29': np.float64(74.1), '30-39': np.float64(66.3), '40-49': np.float64(79.7), '50-59': np.float64(73.5), 'old': np.float64(66.2), 'new': np.float64(76.6)}
2025-01-13 13:34:06,413 [trainer.py] => NME: {'total': np.float64(69.97), '00-09': np.float64(69.4), '10-19': np.float64(58.1), '20-29': np.float64(73.7), '30-39': np.float64(65.5), '40-49': np.float64(79.9), '50-59': np.float64(73.2), 'old': np.float64(66.68), 'new': np.float64(76.55)}
2025-01-13 13:34:06,413 [trainer.py] => CNN top1 curve: [np.float64(83.6), np.float64(75.95), np.float64(69.67)]
2025-01-13 13:34:06,413 [trainer.py] => CNN top5 curve: [np.float64(96.85), np.float64(93.75), np.float64(91.2)]
2025-01-13 13:34:06,413 [trainer.py] => NME top1 curve: [np.float64(83.35), np.float64(75.6), np.float64(69.97)]
2025-01-13 13:34:06,413 [trainer.py] => NME top5 curve: [np.float64(97.2), np.float64(94.08), np.float64(91.95)]

2025-01-13 13:34:06,413 [trainer.py] => All params: 1179425
2025-01-13 13:34:06,414 [trainer.py] => Trainable params: 476449
2025-01-13 13:34:06,435 [memo.py] => Learning on 60-80
2025-01-13 13:34:06,436 [memo.py] => All params: 1539893
2025-01-13 13:34:06,436 [memo.py] => Trainable params: 485429
2025-01-13 13:52:31,371 [memo.py] => Task 3, Epoch 170/170 => Loss 0.012, Loss_clf 0.008, Loss_aux 0.004, Train_accy 99.97
2025-01-13 13:52:31,382 [base.py] => Reducing exemplars...(32 per classes)
2025-01-13 13:53:05,999 [base.py] => Constructing exemplars...(32 per classes)
2025-01-13 13:53:35,629 [memo.py] => Exemplar size: 2560
2025-01-13 13:53:35,629 [trainer.py] => CNN: {'total': np.float64(65.92), '00-09': np.float64(63.0), '10-19': np.float64(51.4), '20-29': np.float64(70.6), '30-39': np.float64(60.4), '40-49': np.float64(73.4), '50-59': np.float64(63.1), '60-69': np.float64(75.8), '70-79': np.float64(69.7), 'old': np.float64(63.65), 'new': np.float64(72.75)}
2025-01-13 13:53:35,629 [trainer.py] => NME: {'total': np.float64(65.46), '00-09': np.float64(63.8), '10-19': np.float64(50.8), '20-29': np.float64(71.9), '30-39': np.float64(59.9), '40-49': np.float64(68.8), '50-59': np.float64(58.8), '60-69': np.float64(76.4), '70-79': np.float64(73.3), 'old': np.float64(62.33), 'new': np.float64(74.85)}
2025-01-13 13:53:35,629 [trainer.py] => CNN top1 curve: [np.float64(83.6), np.float64(75.95), np.float64(69.67), np.float64(65.92)]
2025-01-13 13:53:35,630 [trainer.py] => CNN top5 curve: [np.float64(96.85), np.float64(93.75), np.float64(91.2), np.float64(89.12)]
2025-01-13 13:53:35,630 [trainer.py] => NME top1 curve: [np.float64(83.35), np.float64(75.6), np.float64(69.97), np.float64(65.46)]
2025-01-13 13:53:35,630 [trainer.py] => NME top5 curve: [np.float64(97.2), np.float64(94.08), np.float64(91.95), np.float64(89.76)]

2025-01-13 13:53:35,630 [trainer.py] => All params: 1539893
2025-01-13 13:53:35,631 [trainer.py] => Trainable params: 485429
2025-01-13 13:53:35,652 [memo.py] => Learning on 80-100
2025-01-13 13:53:35,653 [memo.py] => All params: 1902921
2025-01-13 13:53:35,653 [memo.py] => Trainable params: 496969
2025-01-13 14:13:30,828 [memo.py] => Task 4, Epoch 170/170 => Loss 0.016, Loss_clf 0.009, Loss_aux 0.007, Train_accy 99.97
2025-01-13 14:13:30,839 [base.py] => Reducing exemplars...(25 per classes)
2025-01-13 14:14:14,583 [base.py] => Constructing exemplars...(25 per classes)
2025-01-13 14:14:44,860 [memo.py] => Exemplar size: 2500
2025-01-13 14:14:44,860 [trainer.py] => CNN: {'total': np.float64(62.34), '00-09': np.float64(56.2), '10-19': np.float64(46.3), '20-29': np.float64(65.2), '30-39': np.float64(55.6), '40-49': np.float64(66.0), '50-59': np.float64(54.5), '60-69': np.float64(72.6), '70-79': np.float64(67.2), '80-89': np.float64(70.1), '90-99': np.float64(69.7), 'old': np.float64(60.45), 'new': np.float64(69.9)}
2025-01-13 14:14:44,860 [trainer.py] => NME: {'total': np.float64(61.41), '00-09': np.float64(56.3), '10-19': np.float64(46.2), '20-29': np.float64(65.1), '30-39': np.float64(57.6), '40-49': np.float64(66.8), '50-59': np.float64(53.6), '60-69': np.float64(67.7), '70-79': np.float64(62.7), '80-89': np.float64(70.1), '90-99': np.float64(68.0), 'old': np.float64(59.5), 'new': np.float64(69.05)}
2025-01-13 14:14:44,861 [trainer.py] => CNN top1 curve: [np.float64(83.6), np.float64(75.95), np.float64(69.67), np.float64(65.92), np.float64(62.34)]
2025-01-13 14:14:44,861 [trainer.py] => CNN top5 curve: [np.float64(96.85), np.float64(93.75), np.float64(91.2), np.float64(89.12), np.float64(86.52)]
2025-01-13 14:14:44,861 [trainer.py] => NME top1 curve: [np.float64(83.35), np.float64(75.6), np.float64(69.97), np.float64(65.46), np.float64(61.41)]
2025-01-13 14:14:44,861 [trainer.py] => NME top5 curve: [np.float64(97.2), np.float64(94.08), np.float64(91.95), np.float64(89.76), np.float64(87.03)]

2025-01-13 14:14:44,861 [trainer.py] => End Time:1736774084.861314
